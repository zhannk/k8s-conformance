I0323 18:34:08.468651      19 e2e.go:116] Starting e2e run "2e572e98-56a7-4dc0-ab28-a625aca4b3e9" on Ginkgo node 1
Mar 23 18:34:08.506: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1679596447 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Mar 23 18:34:08.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
E0323 18:34:08.714997      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0323 18:34:08.714997      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Mar 23 18:34:08.716: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Mar 23 18:34:08.753: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Mar 23 18:34:08.806: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Mar 23 18:34:08.806: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Mar 23 18:34:08.806: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
Mar 23 18:34:08.815: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'azure-npm' (0 seconds elapsed)
Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager' (0 seconds elapsed)
Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node' (0 seconds elapsed)
Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Mar 23 18:34:08.815: INFO: e2e test version: v1.25.7
Mar 23 18:34:08.816: INFO: kube-apiserver version: v1.25.7
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Mar 23 18:34:08.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:34:08.820: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.107 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 23 18:34:08.713: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    E0323 18:34:08.714997      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Mar 23 18:34:08.716: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Mar 23 18:34:08.753: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Mar 23 18:34:08.806: INFO: 48 / 48 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Mar 23 18:34:08.806: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Mar 23 18:34:08.806: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'azure-ip-masq-agent' (0 seconds elapsed)
    Mar 23 18:34:08.815: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'azure-npm' (0 seconds elapsed)
    Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'cloud-node-manager' (0 seconds elapsed)
    Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'csi-azuredisk-node' (0 seconds elapsed)
    Mar 23 18:34:08.815: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Mar 23 18:34:08.815: INFO: e2e test version: v1.25.7
    Mar 23 18:34:08.816: INFO: kube-apiserver version: v1.25.7
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Mar 23 18:34:08.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:34:08.820: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:34:08.866
Mar 23 18:34:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pod-network-test 03/23/23 18:34:08.867
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:08.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:08.889
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-245 03/23/23 18:34:08.891
STEP: creating a selector 03/23/23 18:34:08.891
STEP: Creating the service pods in kubernetes 03/23/23 18:34:08.892
Mar 23 18:34:08.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 23 18:34:08.963: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-245" to be "running and ready"
Mar 23 18:34:08.977: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.32417ms
Mar 23 18:34:08.977: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:10.984: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021183903s
Mar 23 18:34:10.984: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:12.983: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020422252s
Mar 23 18:34:12.983: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:14.982: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019366776s
Mar 23 18:34:14.982: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:16.986: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022928551s
Mar 23 18:34:16.986: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:18.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022089536s
Mar 23 18:34:18.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:20.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021873219s
Mar 23 18:34:20.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:34:22.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01954961s
Mar 23 18:34:22.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:24.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019658602s
Mar 23 18:34:24.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:26.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018797896s
Mar 23 18:34:26.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:29.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.853632732s
Mar 23 18:34:29.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:30.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.020688814s
Mar 23 18:34:30.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:32.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.018747407s
Mar 23 18:34:32.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:34.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.019479495s
Mar 23 18:34:34.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:36.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.021211181s
Mar 23 18:34:36.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:38.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.019054879s
Mar 23 18:34:38.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 18:34:40.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.018699779s
Mar 23 18:34:40.982: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 23 18:34:40.982: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 23 18:34:40.985: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-245" to be "running and ready"
Mar 23 18:34:40.988: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.984594ms
Mar 23 18:34:40.988: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 23 18:34:40.988: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 23 18:34:40.991: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-245" to be "running and ready"
Mar 23 18:34:40.994: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.139494ms
Mar 23 18:34:40.994: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 23 18:34:40.994: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/23/23 18:34:40.997
Mar 23 18:34:41.012: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-245" to be "running"
Mar 23 18:34:41.029: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.357964ms
Mar 23 18:34:43.033: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021138456s
Mar 23 18:34:43.033: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 23 18:34:43.036: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 23 18:34:43.036: INFO: Breadth first check of 10.240.0.31 on host 10.240.0.30...
Mar 23 18:34:43.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.31&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:34:43.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:34:43.040: INFO: ExecWithOptions: Clientset creation
Mar 23 18:34:43.040: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.31%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 18:34:43.164: INFO: Waiting for responses: map[]
Mar 23 18:34:43.164: INFO: reached 10.240.0.31 after 0/1 tries
Mar 23 18:34:43.164: INFO: Breadth first check of 10.240.0.58 on host 10.240.0.56...
Mar 23 18:34:43.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.58&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:34:43.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:34:43.169: INFO: ExecWithOptions: Clientset creation
Mar 23 18:34:43.169: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 18:34:43.307: INFO: Waiting for responses: map[]
Mar 23 18:34:43.307: INFO: reached 10.240.0.58 after 0/1 tries
Mar 23 18:34:43.307: INFO: Breadth first check of 10.240.0.21 on host 10.240.0.4...
Mar 23 18:34:43.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.21&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:34:43.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:34:43.312: INFO: ExecWithOptions: Clientset creation
Mar 23 18:34:43.312: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 18:34:43.418: INFO: Waiting for responses: map[]
Mar 23 18:34:43.419: INFO: reached 10.240.0.21 after 0/1 tries
Mar 23 18:34:43.419: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 23 18:34:43.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-245" for this suite. 03/23/23 18:34:43.424
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":1,"skipped":9,"failed":0}
------------------------------
• [SLOW TEST] [34.568 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:34:08.866
    Mar 23 18:34:08.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pod-network-test 03/23/23 18:34:08.867
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:08.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:08.889
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-245 03/23/23 18:34:08.891
    STEP: creating a selector 03/23/23 18:34:08.891
    STEP: Creating the service pods in kubernetes 03/23/23 18:34:08.892
    Mar 23 18:34:08.892: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 23 18:34:08.963: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-245" to be "running and ready"
    Mar 23 18:34:08.977: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.32417ms
    Mar 23 18:34:08.977: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:10.984: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021183903s
    Mar 23 18:34:10.984: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:12.983: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020422252s
    Mar 23 18:34:12.983: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:14.982: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019366776s
    Mar 23 18:34:14.982: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:16.986: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022928551s
    Mar 23 18:34:16.986: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:18.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022089536s
    Mar 23 18:34:18.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:20.985: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.021873219s
    Mar 23 18:34:20.985: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:34:22.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.01954961s
    Mar 23 18:34:22.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:24.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.019658602s
    Mar 23 18:34:24.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:26.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.018797896s
    Mar 23 18:34:26.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:29.816: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.853632732s
    Mar 23 18:34:29.816: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:30.983: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 22.020688814s
    Mar 23 18:34:30.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:32.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 24.018747407s
    Mar 23 18:34:32.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:34.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 26.019479495s
    Mar 23 18:34:34.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:36.984: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 28.021211181s
    Mar 23 18:34:36.984: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:38.982: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 30.019054879s
    Mar 23 18:34:38.982: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 18:34:40.981: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 32.018699779s
    Mar 23 18:34:40.982: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 23 18:34:40.982: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 23 18:34:40.985: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-245" to be "running and ready"
    Mar 23 18:34:40.988: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.984594ms
    Mar 23 18:34:40.988: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 23 18:34:40.988: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 23 18:34:40.991: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-245" to be "running and ready"
    Mar 23 18:34:40.994: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.139494ms
    Mar 23 18:34:40.994: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 23 18:34:40.994: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/23/23 18:34:40.997
    Mar 23 18:34:41.012: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-245" to be "running"
    Mar 23 18:34:41.029: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.357964ms
    Mar 23 18:34:43.033: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021138456s
    Mar 23 18:34:43.033: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 23 18:34:43.036: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 23 18:34:43.036: INFO: Breadth first check of 10.240.0.31 on host 10.240.0.30...
    Mar 23 18:34:43.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.31&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:34:43.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:34:43.040: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:34:43.040: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.31%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 18:34:43.164: INFO: Waiting for responses: map[]
    Mar 23 18:34:43.164: INFO: reached 10.240.0.31 after 0/1 tries
    Mar 23 18:34:43.164: INFO: Breadth first check of 10.240.0.58 on host 10.240.0.56...
    Mar 23 18:34:43.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.58&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:34:43.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:34:43.169: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:34:43.169: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 18:34:43.307: INFO: Waiting for responses: map[]
    Mar 23 18:34:43.307: INFO: reached 10.240.0.58 after 0/1 tries
    Mar 23 18:34:43.307: INFO: Breadth first check of 10.240.0.21 on host 10.240.0.4...
    Mar 23 18:34:43.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.20:9080/dial?request=hostname&protocol=udp&host=10.240.0.21&port=8081&tries=1'] Namespace:pod-network-test-245 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:34:43.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:34:43.312: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:34:43.312: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-245/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.20%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.240.0.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 18:34:43.418: INFO: Waiting for responses: map[]
    Mar 23 18:34:43.419: INFO: reached 10.240.0.21 after 0/1 tries
    Mar 23 18:34:43.419: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 23 18:34:43.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-245" for this suite. 03/23/23 18:34:43.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:34:43.436
Mar 23 18:34:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption 03/23/23 18:34:43.438
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:43.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:43.462
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 03/23/23 18:34:43.479
STEP: Updating PodDisruptionBudget status 03/23/23 18:34:45.492
STEP: Waiting for all pods to be running 03/23/23 18:34:45.502
Mar 23 18:34:45.513: INFO: running pods: 0 < 1
Mar 23 18:34:47.517: INFO: running pods: 0 < 1
Mar 23 18:34:49.531: INFO: running pods: 0 < 1
Mar 23 18:34:51.520: INFO: running pods: 0 < 1
STEP: locating a running pod 03/23/23 18:34:53.519
STEP: Waiting for the pdb to be processed 03/23/23 18:34:53.531
STEP: Patching PodDisruptionBudget status 03/23/23 18:34:53.538
STEP: Waiting for the pdb to be processed 03/23/23 18:34:53.555
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 23 18:34:53.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9149" for this suite. 03/23/23 18:34:53.564
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":2,"skipped":40,"failed":0}
------------------------------
• [SLOW TEST] [10.156 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:34:43.436
    Mar 23 18:34:43.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption 03/23/23 18:34:43.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:43.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:43.462
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 03/23/23 18:34:43.479
    STEP: Updating PodDisruptionBudget status 03/23/23 18:34:45.492
    STEP: Waiting for all pods to be running 03/23/23 18:34:45.502
    Mar 23 18:34:45.513: INFO: running pods: 0 < 1
    Mar 23 18:34:47.517: INFO: running pods: 0 < 1
    Mar 23 18:34:49.531: INFO: running pods: 0 < 1
    Mar 23 18:34:51.520: INFO: running pods: 0 < 1
    STEP: locating a running pod 03/23/23 18:34:53.519
    STEP: Waiting for the pdb to be processed 03/23/23 18:34:53.531
    STEP: Patching PodDisruptionBudget status 03/23/23 18:34:53.538
    STEP: Waiting for the pdb to be processed 03/23/23 18:34:53.555
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 23 18:34:53.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9149" for this suite. 03/23/23 18:34:53.564
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:34:53.597
Mar 23 18:34:53.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename watch 03/23/23 18:34:53.598
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:53.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:53.637
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 03/23/23 18:34:53.642
STEP: starting a background goroutine to produce watch events 03/23/23 18:34:53.646
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/23/23 18:34:53.646
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 23 18:34:56.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2943" for this suite. 03/23/23 18:34:56.455
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":3,"skipped":41,"failed":0}
------------------------------
• [2.912 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:34:53.597
    Mar 23 18:34:53.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename watch 03/23/23 18:34:53.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:53.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:53.637
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 03/23/23 18:34:53.642
    STEP: starting a background goroutine to produce watch events 03/23/23 18:34:53.646
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 03/23/23 18:34:53.646
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 23 18:34:56.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2943" for this suite. 03/23/23 18:34:56.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:34:56.516
Mar 23 18:34:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename namespaces 03/23/23 18:34:56.518
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:56.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:56.538
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 03/23/23 18:34:56.542
Mar 23 18:34:56.546: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 03/23/23 18:34:56.546
Mar 23 18:34:56.554: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 03/23/23 18:34:56.554
Mar 23 18:34:56.568: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:34:56.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8994" for this suite. 03/23/23 18:34:56.573
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":4,"skipped":49,"failed":0}
------------------------------
• [0.067 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:34:56.516
    Mar 23 18:34:56.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename namespaces 03/23/23 18:34:56.518
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:56.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:56.538
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 03/23/23 18:34:56.542
    Mar 23 18:34:56.546: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 03/23/23 18:34:56.546
    Mar 23 18:34:56.554: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 03/23/23 18:34:56.554
    Mar 23 18:34:56.568: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:34:56.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8994" for this suite. 03/23/23 18:34:56.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:34:56.588
Mar 23 18:34:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 18:34:56.589
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:56.608
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:56.613
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Mar 23 18:34:56.657: INFO: created pod
Mar 23 18:34:56.657: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2600" to be "Succeeded or Failed"
Mar 23 18:34:56.675: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.888762ms
Mar 23 18:34:58.680: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022692857s
Mar 23 18:35:00.681: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024304958s
STEP: Saw pod success 03/23/23 18:35:00.681
Mar 23 18:35:00.682: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Mar 23 18:35:30.683: INFO: polling logs
Mar 23 18:35:30.719: INFO: Pod logs: 
I0323 18:34:57.888491       1 log.go:195] OK: Got token
I0323 18:34:57.890424       1 log.go:195] validating with in-cluster discovery
I0323 18:34:57.890970       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0323 18:34:57.891022       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2600:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679597096, NotBefore:1679596496, IssuedAt:1679596496, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2600", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"32b612a2-4aca-497c-b8fb-40a9f94015c5"}}}
I0323 18:34:57.920655       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0323 18:34:57.943445       1 log.go:195] OK: Validated signature on JWT
I0323 18:34:57.943625       1 log.go:195] OK: Got valid claims from token!
I0323 18:34:57.943675       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2600:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679597096, NotBefore:1679596496, IssuedAt:1679596496, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2600", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"32b612a2-4aca-497c-b8fb-40a9f94015c5"}}}

Mar 23 18:35:30.719: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 18:35:30.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2600" for this suite. 03/23/23 18:35:30.731
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":5,"skipped":76,"failed":0}
------------------------------
• [SLOW TEST] [34.150 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:34:56.588
    Mar 23 18:34:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 18:34:56.589
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:34:56.608
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:34:56.613
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Mar 23 18:34:56.657: INFO: created pod
    Mar 23 18:34:56.657: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2600" to be "Succeeded or Failed"
    Mar 23 18:34:56.675: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 17.888762ms
    Mar 23 18:34:58.680: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022692857s
    Mar 23 18:35:00.681: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024304958s
    STEP: Saw pod success 03/23/23 18:35:00.681
    Mar 23 18:35:00.682: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Mar 23 18:35:30.683: INFO: polling logs
    Mar 23 18:35:30.719: INFO: Pod logs: 
    I0323 18:34:57.888491       1 log.go:195] OK: Got token
    I0323 18:34:57.890424       1 log.go:195] validating with in-cluster discovery
    I0323 18:34:57.890970       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0323 18:34:57.891022       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2600:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679597096, NotBefore:1679596496, IssuedAt:1679596496, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2600", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"32b612a2-4aca-497c-b8fb-40a9f94015c5"}}}
    I0323 18:34:57.920655       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0323 18:34:57.943445       1 log.go:195] OK: Validated signature on JWT
    I0323 18:34:57.943625       1 log.go:195] OK: Got valid claims from token!
    I0323 18:34:57.943675       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2600:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1679597096, NotBefore:1679596496, IssuedAt:1679596496, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2600", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"32b612a2-4aca-497c-b8fb-40a9f94015c5"}}}

    Mar 23 18:35:30.719: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 18:35:30.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2600" for this suite. 03/23/23 18:35:30.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:35:30.753
Mar 23 18:35:30.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 18:35:30.755
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:30.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:30.782
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Mar 23 18:35:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/23/23 18:35:33.905
Mar 23 18:35:33.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
Mar 23 18:35:34.743: INFO: stderr: ""
Mar 23 18:35:34.743: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 18:35:34.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 delete e2e-test-crd-publish-openapi-3963-crds test-foo'
Mar 23 18:35:34.906: INFO: stderr: ""
Mar 23 18:35:34.906: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Mar 23 18:35:34.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
Mar 23 18:35:35.545: INFO: stderr: ""
Mar 23 18:35:35.545: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Mar 23 18:35:35.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 delete e2e-test-crd-publish-openapi-3963-crds test-foo'
Mar 23 18:35:35.654: INFO: stderr: ""
Mar 23 18:35:35.654: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/23/23 18:35:35.654
Mar 23 18:35:35.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
Mar 23 18:35:35.860: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/23/23 18:35:35.86
Mar 23 18:35:35.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
Mar 23 18:35:36.530: INFO: rc: 1
Mar 23 18:35:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
Mar 23 18:35:36.818: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/23/23 18:35:36.818
Mar 23 18:35:36.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
Mar 23 18:35:37.034: INFO: rc: 1
Mar 23 18:35:37.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
Mar 23 18:35:37.263: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 03/23/23 18:35:37.264
Mar 23 18:35:37.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds'
Mar 23 18:35:37.475: INFO: stderr: ""
Mar 23 18:35:37.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 03/23/23 18:35:37.476
Mar 23 18:35:37.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.metadata'
Mar 23 18:35:37.706: INFO: stderr: ""
Mar 23 18:35:37.706: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Mar 23 18:35:37.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec'
Mar 23 18:35:37.989: INFO: stderr: ""
Mar 23 18:35:37.989: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Mar 23 18:35:37.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec.bars'
Mar 23 18:35:38.186: INFO: stderr: ""
Mar 23 18:35:38.186: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/23/23 18:35:38.186
Mar 23 18:35:38.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec.bars2'
Mar 23 18:35:38.411: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:35:41.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4795" for this suite. 03/23/23 18:35:41.381
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":6,"skipped":106,"failed":0}
------------------------------
• [SLOW TEST] [10.634 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:35:30.753
    Mar 23 18:35:30.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 18:35:30.755
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:30.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:30.782
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Mar 23 18:35:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 03/23/23 18:35:33.905
    Mar 23 18:35:33.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
    Mar 23 18:35:34.743: INFO: stderr: ""
    Mar 23 18:35:34.743: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 23 18:35:34.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 delete e2e-test-crd-publish-openapi-3963-crds test-foo'
    Mar 23 18:35:34.906: INFO: stderr: ""
    Mar 23 18:35:34.906: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Mar 23 18:35:34.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
    Mar 23 18:35:35.545: INFO: stderr: ""
    Mar 23 18:35:35.545: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Mar 23 18:35:35.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 delete e2e-test-crd-publish-openapi-3963-crds test-foo'
    Mar 23 18:35:35.654: INFO: stderr: ""
    Mar 23 18:35:35.654: INFO: stdout: "e2e-test-crd-publish-openapi-3963-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 03/23/23 18:35:35.654
    Mar 23 18:35:35.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
    Mar 23 18:35:35.860: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 03/23/23 18:35:35.86
    Mar 23 18:35:35.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
    Mar 23 18:35:36.530: INFO: rc: 1
    Mar 23 18:35:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
    Mar 23 18:35:36.818: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 03/23/23 18:35:36.818
    Mar 23 18:35:36.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 create -f -'
    Mar 23 18:35:37.034: INFO: rc: 1
    Mar 23 18:35:37.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 --namespace=crd-publish-openapi-4795 apply -f -'
    Mar 23 18:35:37.263: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 03/23/23 18:35:37.264
    Mar 23 18:35:37.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds'
    Mar 23 18:35:37.475: INFO: stderr: ""
    Mar 23 18:35:37.475: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 03/23/23 18:35:37.476
    Mar 23 18:35:37.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.metadata'
    Mar 23 18:35:37.706: INFO: stderr: ""
    Mar 23 18:35:37.706: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Mar 23 18:35:37.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec'
    Mar 23 18:35:37.989: INFO: stderr: ""
    Mar 23 18:35:37.989: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Mar 23 18:35:37.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec.bars'
    Mar 23 18:35:38.186: INFO: stderr: ""
    Mar 23 18:35:38.186: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3963-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 03/23/23 18:35:38.186
    Mar 23 18:35:38.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-4795 explain e2e-test-crd-publish-openapi-3963-crds.spec.bars2'
    Mar 23 18:35:38.411: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:35:41.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4795" for this suite. 03/23/23 18:35:41.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:35:41.398
Mar 23 18:35:41.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 18:35:41.4
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:41.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:41.422
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 03/23/23 18:35:41.424
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_tcp@PTR;sleep 1; done
 03/23/23 18:35:41.45
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_tcp@PTR;sleep 1; done
 03/23/23 18:35:41.451
STEP: creating a pod to probe DNS 03/23/23 18:35:41.452
STEP: submitting the pod to kubernetes 03/23/23 18:35:41.452
Mar 23 18:35:41.477: INFO: Waiting up to 15m0s for pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32" in namespace "dns-9711" to be "running"
Mar 23 18:35:41.487: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.108581ms
Mar 23 18:35:43.492: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014380091s
Mar 23 18:35:45.492: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013876353s
Mar 23 18:35:47.493: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015339379s
Mar 23 18:35:49.493: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015337718s
Mar 23 18:35:51.494: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015770322s
Mar 23 18:35:53.494: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Running", Reason="", readiness=true. Elapsed: 12.01647237s
Mar 23 18:35:53.495: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32" satisfied condition "running"
STEP: retrieving the pod 03/23/23 18:35:53.495
STEP: looking for the results for each expected name from probers 03/23/23 18:35:53.501
Mar 23 18:35:53.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.532: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.540: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.615: INFO: Unable to read jessie_udp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.622: INFO: Unable to read jessie_tcp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.644: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
Mar 23 18:35:53.659: INFO: Lookups using dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32 failed for: [wheezy_udp@dns-test-service.dns-9711.svc.cluster.local wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local jessie_udp@dns-test-service.dns-9711.svc.cluster.local jessie_tcp@dns-test-service.dns-9711.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local]

Mar 23 18:35:58.718: INFO: DNS probes using dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32 succeeded

STEP: deleting the pod 03/23/23 18:35:58.718
STEP: deleting the test service 03/23/23 18:35:58.736
STEP: deleting the test headless service 03/23/23 18:35:58.779
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 18:35:58.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9711" for this suite. 03/23/23 18:35:58.82
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":7,"skipped":119,"failed":0}
------------------------------
• [SLOW TEST] [17.433 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:35:41.398
    Mar 23 18:35:41.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 18:35:41.4
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:41.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:41.422
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 03/23/23 18:35:41.424
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_tcp@PTR;sleep 1; done
     03/23/23 18:35:41.45
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9711.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9711.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9711.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_udp@PTR;check="$$(dig +tcp +noall +answer +search 161.80.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.80.161_tcp@PTR;sleep 1; done
     03/23/23 18:35:41.451
    STEP: creating a pod to probe DNS 03/23/23 18:35:41.452
    STEP: submitting the pod to kubernetes 03/23/23 18:35:41.452
    Mar 23 18:35:41.477: INFO: Waiting up to 15m0s for pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32" in namespace "dns-9711" to be "running"
    Mar 23 18:35:41.487: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 9.108581ms
    Mar 23 18:35:43.492: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014380091s
    Mar 23 18:35:45.492: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013876353s
    Mar 23 18:35:47.493: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015339379s
    Mar 23 18:35:49.493: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 8.015337718s
    Mar 23 18:35:51.494: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Pending", Reason="", readiness=false. Elapsed: 10.015770322s
    Mar 23 18:35:53.494: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32": Phase="Running", Reason="", readiness=true. Elapsed: 12.01647237s
    Mar 23 18:35:53.495: INFO: Pod "dns-test-f07c89d1-a276-4add-ade2-471c907e3f32" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 18:35:53.495
    STEP: looking for the results for each expected name from probers 03/23/23 18:35:53.501
    Mar 23 18:35:53.515: INFO: Unable to read wheezy_udp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.525: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.532: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.540: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.615: INFO: Unable to read jessie_udp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.622: INFO: Unable to read jessie_tcp@dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.640: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.644: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local from pod dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32: the server could not find the requested resource (get pods dns-test-f07c89d1-a276-4add-ade2-471c907e3f32)
    Mar 23 18:35:53.659: INFO: Lookups using dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32 failed for: [wheezy_udp@dns-test-service.dns-9711.svc.cluster.local wheezy_tcp@dns-test-service.dns-9711.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local jessie_udp@dns-test-service.dns-9711.svc.cluster.local jessie_tcp@dns-test-service.dns-9711.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9711.svc.cluster.local]

    Mar 23 18:35:58.718: INFO: DNS probes using dns-9711/dns-test-f07c89d1-a276-4add-ade2-471c907e3f32 succeeded

    STEP: deleting the pod 03/23/23 18:35:58.718
    STEP: deleting the test service 03/23/23 18:35:58.736
    STEP: deleting the test headless service 03/23/23 18:35:58.779
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 18:35:58.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9711" for this suite. 03/23/23 18:35:58.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:35:58.883
Mar 23 18:35:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 18:35:58.887
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:58.946
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:58.953
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Mar 23 18:35:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: creating the pod 03/23/23 18:35:58.958
STEP: submitting the pod to kubernetes 03/23/23 18:35:58.958
Mar 23 18:35:58.966: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4" in namespace "pods-4064" to be "running and ready"
Mar 23 18:35:58.976: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.211988ms
Mar 23 18:35:58.976: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:00.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014484089s
Mar 23 18:36:00.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:02.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013961303s
Mar 23 18:36:02.980: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:04.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014620615s
Mar 23 18:36:04.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:06.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013424158s
Mar 23 18:36:06.980: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:08.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014210649s
Mar 23 18:36:08.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:10.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014123843s
Mar 23 18:36:10.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:12.982: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015355134s
Mar 23 18:36:12.982: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:36:14.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Running", Reason="", readiness=true. Elapsed: 16.014596804s
Mar 23 18:36:14.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Running (Ready = true)
Mar 23 18:36:14.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 18:36:15.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4064" for this suite. 03/23/23 18:36:15.127
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":8,"skipped":159,"failed":0}
------------------------------
• [SLOW TEST] [16.255 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:35:58.883
    Mar 23 18:35:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 18:35:58.887
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:35:58.946
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:35:58.953
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Mar 23 18:35:58.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: creating the pod 03/23/23 18:35:58.958
    STEP: submitting the pod to kubernetes 03/23/23 18:35:58.958
    Mar 23 18:35:58.966: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4" in namespace "pods-4064" to be "running and ready"
    Mar 23 18:35:58.976: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.211988ms
    Mar 23 18:35:58.976: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:00.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014484089s
    Mar 23 18:36:00.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:02.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013961303s
    Mar 23 18:36:02.980: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:04.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014620615s
    Mar 23 18:36:04.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:06.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013424158s
    Mar 23 18:36:06.980: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:08.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014210649s
    Mar 23 18:36:08.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:10.980: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.014123843s
    Mar 23 18:36:10.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:12.982: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.015355134s
    Mar 23 18:36:12.982: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:36:14.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4": Phase="Running", Reason="", readiness=true. Elapsed: 16.014596804s
    Mar 23 18:36:14.981: INFO: The phase of Pod pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4 is Running (Ready = true)
    Mar 23 18:36:14.981: INFO: Pod "pod-exec-websocket-b08278e4-0507-431a-bbe2-3a7596f734f4" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 18:36:15.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4064" for this suite. 03/23/23 18:36:15.127
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:36:15.138
Mar 23 18:36:15.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 18:36:15.14
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:15.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:15.17
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Mar 23 18:36:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:36:18.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1858" for this suite. 03/23/23 18:36:18.622
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":9,"skipped":159,"failed":0}
------------------------------
• [3.495 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:36:15.138
    Mar 23 18:36:15.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 18:36:15.14
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:15.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:15.17
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Mar 23 18:36:15.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:36:18.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1858" for this suite. 03/23/23 18:36:18.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:36:18.636
Mar 23 18:36:18.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 18:36:18.638
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:18.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:18.661
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-35bea862-7019-488b-9ef0-793827609cba 03/23/23 18:36:18.664
STEP: Creating a pod to test consume configMaps 03/23/23 18:36:18.672
Mar 23 18:36:18.699: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54" in namespace "projected-7154" to be "Succeeded or Failed"
Mar 23 18:36:18.704: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970488ms
Mar 23 18:36:20.710: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010224984s
Mar 23 18:36:22.711: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011336783s
STEP: Saw pod success 03/23/23 18:36:22.711
Mar 23 18:36:22.711: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54" satisfied condition "Succeeded or Failed"
Mar 23 18:36:22.714: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:36:22.72
Mar 23 18:36:22.743: INFO: Waiting for pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 to disappear
Mar 23 18:36:22.746: INFO: Pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 18:36:22.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7154" for this suite. 03/23/23 18:36:22.752
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":10,"skipped":192,"failed":0}
------------------------------
• [4.123 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:36:18.636
    Mar 23 18:36:18.637: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 18:36:18.638
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:18.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:18.661
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-35bea862-7019-488b-9ef0-793827609cba 03/23/23 18:36:18.664
    STEP: Creating a pod to test consume configMaps 03/23/23 18:36:18.672
    Mar 23 18:36:18.699: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54" in namespace "projected-7154" to be "Succeeded or Failed"
    Mar 23 18:36:18.704: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970488ms
    Mar 23 18:36:20.710: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010224984s
    Mar 23 18:36:22.711: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011336783s
    STEP: Saw pod success 03/23/23 18:36:22.711
    Mar 23 18:36:22.711: INFO: Pod "pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54" satisfied condition "Succeeded or Failed"
    Mar 23 18:36:22.714: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:36:22.72
    Mar 23 18:36:22.743: INFO: Waiting for pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 to disappear
    Mar 23 18:36:22.746: INFO: Pod pod-projected-configmaps-858848a0-2322-4ddc-a9c6-d242182d2d54 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 18:36:22.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7154" for this suite. 03/23/23 18:36:22.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:36:22.77
Mar 23 18:36:22.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 18:36:22.771
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:22.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:22.796
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 03/23/23 18:36:22.798
STEP: Ensuring ResourceQuota status is calculated 03/23/23 18:36:22.804
STEP: Creating a ResourceQuota with not terminating scope 03/23/23 18:36:24.82
STEP: Ensuring ResourceQuota status is calculated 03/23/23 18:36:24.825
STEP: Creating a long running pod 03/23/23 18:36:26.833
STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/23/23 18:36:26.862
STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/23/23 18:36:28.866
STEP: Deleting the pod 03/23/23 18:36:30.87
STEP: Ensuring resource quota status released the pod usage 03/23/23 18:36:30.893
STEP: Creating a terminating pod 03/23/23 18:36:32.898
STEP: Ensuring resource quota with terminating scope captures the pod usage 03/23/23 18:36:32.91
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/23/23 18:36:34.914
STEP: Deleting the pod 03/23/23 18:36:36.921
STEP: Ensuring resource quota status released the pod usage 03/23/23 18:36:36.934
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 18:36:38.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8228" for this suite. 03/23/23 18:36:38.943
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":11,"skipped":229,"failed":0}
------------------------------
• [SLOW TEST] [16.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:36:22.77
    Mar 23 18:36:22.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 18:36:22.771
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:22.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:22.796
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 03/23/23 18:36:22.798
    STEP: Ensuring ResourceQuota status is calculated 03/23/23 18:36:22.804
    STEP: Creating a ResourceQuota with not terminating scope 03/23/23 18:36:24.82
    STEP: Ensuring ResourceQuota status is calculated 03/23/23 18:36:24.825
    STEP: Creating a long running pod 03/23/23 18:36:26.833
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 03/23/23 18:36:26.862
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 03/23/23 18:36:28.866
    STEP: Deleting the pod 03/23/23 18:36:30.87
    STEP: Ensuring resource quota status released the pod usage 03/23/23 18:36:30.893
    STEP: Creating a terminating pod 03/23/23 18:36:32.898
    STEP: Ensuring resource quota with terminating scope captures the pod usage 03/23/23 18:36:32.91
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 03/23/23 18:36:34.914
    STEP: Deleting the pod 03/23/23 18:36:36.921
    STEP: Ensuring resource quota status released the pod usage 03/23/23 18:36:36.934
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 18:36:38.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8228" for this suite. 03/23/23 18:36:38.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:36:38.962
Mar 23 18:36:38.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 18:36:38.963
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:38.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:38.987
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 03/23/23 18:36:38.99
Mar 23 18:36:39.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed" in namespace "projected-4557" to be "Succeeded or Failed"
Mar 23 18:36:39.010: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133984ms
Mar 23 18:36:41.014: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012108577s
Mar 23 18:36:43.017: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014888872s
STEP: Saw pod success 03/23/23 18:36:43.017
Mar 23 18:36:43.017: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed" satisfied condition "Succeeded or Failed"
Mar 23 18:36:43.021: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed container client-container: <nil>
STEP: delete the pod 03/23/23 18:36:43.028
Mar 23 18:36:43.041: INFO: Waiting for pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed to disappear
Mar 23 18:36:43.044: INFO: Pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 18:36:43.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4557" for this suite. 03/23/23 18:36:43.049
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":12,"skipped":275,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:36:38.962
    Mar 23 18:36:38.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 18:36:38.963
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:38.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:38.987
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 03/23/23 18:36:38.99
    Mar 23 18:36:39.002: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed" in namespace "projected-4557" to be "Succeeded or Failed"
    Mar 23 18:36:39.010: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.133984ms
    Mar 23 18:36:41.014: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012108577s
    Mar 23 18:36:43.017: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014888872s
    STEP: Saw pod success 03/23/23 18:36:43.017
    Mar 23 18:36:43.017: INFO: Pod "downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed" satisfied condition "Succeeded or Failed"
    Mar 23 18:36:43.021: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed container client-container: <nil>
    STEP: delete the pod 03/23/23 18:36:43.028
    Mar 23 18:36:43.041: INFO: Waiting for pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed to disappear
    Mar 23 18:36:43.044: INFO: Pod downwardapi-volume-dea8d3e3-e36a-47c6-9ca5-d6c56cf8dfed no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 18:36:43.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4557" for this suite. 03/23/23 18:36:43.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:36:43.076
Mar 23 18:36:43.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename subpath 03/23/23 18:36:43.078
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:43.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:43.098
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/23/23 18:36:43.101
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-ffx9 03/23/23 18:36:43.13
STEP: Creating a pod to test atomic-volume-subpath 03/23/23 18:36:43.13
Mar 23 18:36:43.141: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ffx9" in namespace "subpath-9923" to be "Succeeded or Failed"
Mar 23 18:36:43.147: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279587ms
Mar 23 18:36:45.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011371678s
Mar 23 18:36:47.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 4.010968373s
Mar 23 18:36:49.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.011619802s
Mar 23 18:36:51.154: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 8.012661331s
Mar 23 18:36:53.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011193964s
Mar 23 18:36:55.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010799437s
Mar 23 18:36:57.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011753314s
Mar 23 18:36:59.151: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010217097s
Mar 23 18:37:01.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.012294372s
Mar 23 18:37:03.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 20.010814491s
Mar 23 18:37:05.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 22.011190565s
Mar 23 18:37:07.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=false. Elapsed: 24.011888439s
Mar 23 18:37:09.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010926316s
STEP: Saw pod success 03/23/23 18:37:09.152
Mar 23 18:37:09.152: INFO: Pod "pod-subpath-test-secret-ffx9" satisfied condition "Succeeded or Failed"
Mar 23 18:37:09.156: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-secret-ffx9 container test-container-subpath-secret-ffx9: <nil>
STEP: delete the pod 03/23/23 18:37:09.163
Mar 23 18:37:09.186: INFO: Waiting for pod pod-subpath-test-secret-ffx9 to disappear
Mar 23 18:37:09.190: INFO: Pod pod-subpath-test-secret-ffx9 no longer exists
STEP: Deleting pod pod-subpath-test-secret-ffx9 03/23/23 18:37:09.19
Mar 23 18:37:09.190: INFO: Deleting pod "pod-subpath-test-secret-ffx9" in namespace "subpath-9923"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 23 18:37:09.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9923" for this suite. 03/23/23 18:37:09.199
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":13,"skipped":293,"failed":0}
------------------------------
• [SLOW TEST] [26.128 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:36:43.076
    Mar 23 18:36:43.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename subpath 03/23/23 18:36:43.078
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:36:43.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:36:43.098
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/23/23 18:36:43.101
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-ffx9 03/23/23 18:36:43.13
    STEP: Creating a pod to test atomic-volume-subpath 03/23/23 18:36:43.13
    Mar 23 18:36:43.141: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ffx9" in namespace "subpath-9923" to be "Succeeded or Failed"
    Mar 23 18:36:43.147: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.279587ms
    Mar 23 18:36:45.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011371678s
    Mar 23 18:36:47.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 4.010968373s
    Mar 23 18:36:49.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 6.011619802s
    Mar 23 18:36:51.154: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 8.012661331s
    Mar 23 18:36:53.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 10.011193964s
    Mar 23 18:36:55.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010799437s
    Mar 23 18:36:57.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011753314s
    Mar 23 18:36:59.151: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 16.010217097s
    Mar 23 18:37:01.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 18.012294372s
    Mar 23 18:37:03.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 20.010814491s
    Mar 23 18:37:05.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=true. Elapsed: 22.011190565s
    Mar 23 18:37:07.153: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Running", Reason="", readiness=false. Elapsed: 24.011888439s
    Mar 23 18:37:09.152: INFO: Pod "pod-subpath-test-secret-ffx9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010926316s
    STEP: Saw pod success 03/23/23 18:37:09.152
    Mar 23 18:37:09.152: INFO: Pod "pod-subpath-test-secret-ffx9" satisfied condition "Succeeded or Failed"
    Mar 23 18:37:09.156: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-secret-ffx9 container test-container-subpath-secret-ffx9: <nil>
    STEP: delete the pod 03/23/23 18:37:09.163
    Mar 23 18:37:09.186: INFO: Waiting for pod pod-subpath-test-secret-ffx9 to disappear
    Mar 23 18:37:09.190: INFO: Pod pod-subpath-test-secret-ffx9 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-ffx9 03/23/23 18:37:09.19
    Mar 23 18:37:09.190: INFO: Deleting pod "pod-subpath-test-secret-ffx9" in namespace "subpath-9923"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 23 18:37:09.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9923" for this suite. 03/23/23 18:37:09.199
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:37:09.21
Mar 23 18:37:09.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 18:37:09.214
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:09.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:09.276
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 03/23/23 18:37:09.28
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:09.287
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:09.287
STEP: creating a pod to probe DNS 03/23/23 18:37:09.287
STEP: submitting the pod to kubernetes 03/23/23 18:37:09.288
Mar 23 18:37:09.304: INFO: Waiting up to 15m0s for pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423" in namespace "dns-1861" to be "running"
Mar 23 18:37:09.324: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Pending", Reason="", readiness=false. Elapsed: 19.586057ms
Mar 23 18:37:11.328: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023449224s
Mar 23 18:37:13.329: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Running", Reason="", readiness=true. Elapsed: 4.025199995s
Mar 23 18:37:13.329: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423" satisfied condition "running"
STEP: retrieving the pod 03/23/23 18:37:13.329
STEP: looking for the results for each expected name from probers 03/23/23 18:37:13.333
Mar 23 18:37:13.341: INFO: DNS probes using dns-test-d1f9c62a-f496-419d-84db-6209fb15e423 succeeded

STEP: deleting the pod 03/23/23 18:37:13.341
STEP: changing the externalName to bar.example.com 03/23/23 18:37:13.357
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:13.37
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:13.37
STEP: creating a second pod to probe DNS 03/23/23 18:37:13.37
STEP: submitting the pod to kubernetes 03/23/23 18:37:13.37
Mar 23 18:37:13.387: INFO: Waiting up to 15m0s for pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd" in namespace "dns-1861" to be "running"
Mar 23 18:37:13.397: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246077ms
Mar 23 18:37:15.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905642s
Mar 23 18:37:17.403: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015786116s
Mar 23 18:37:19.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Running", Reason="", readiness=true. Elapsed: 6.014982459s
Mar 23 18:37:19.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd" satisfied condition "running"
STEP: retrieving the pod 03/23/23 18:37:19.402
STEP: looking for the results for each expected name from probers 03/23/23 18:37:19.405
Mar 23 18:37:19.415: INFO: DNS probes using dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd succeeded

STEP: deleting the pod 03/23/23 18:37:19.415
STEP: changing the service to type=ClusterIP 03/23/23 18:37:19.434
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:19.452
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
 03/23/23 18:37:19.453
STEP: creating a third pod to probe DNS 03/23/23 18:37:19.453
STEP: submitting the pod to kubernetes 03/23/23 18:37:19.46
Mar 23 18:37:19.474: INFO: Waiting up to 15m0s for pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634" in namespace "dns-1861" to be "running"
Mar 23 18:37:19.483: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Pending", Reason="", readiness=false. Elapsed: 8.74718ms
Mar 23 18:37:21.487: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013536076s
Mar 23 18:37:23.488: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Running", Reason="", readiness=true. Elapsed: 4.013687683s
Mar 23 18:37:23.488: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634" satisfied condition "running"
STEP: retrieving the pod 03/23/23 18:37:23.488
STEP: looking for the results for each expected name from probers 03/23/23 18:37:23.492
Mar 23 18:37:23.503: INFO: DNS probes using dns-test-0a100132-1bad-4663-b05f-2ebb68e11634 succeeded

STEP: deleting the pod 03/23/23 18:37:23.503
STEP: deleting the test externalName service 03/23/23 18:37:23.531
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 18:37:23.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1861" for this suite. 03/23/23 18:37:23.567
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":14,"skipped":294,"failed":0}
------------------------------
• [SLOW TEST] [14.400 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:37:09.21
    Mar 23 18:37:09.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 18:37:09.214
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:09.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:09.276
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 03/23/23 18:37:09.28
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:09.287
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:09.287
    STEP: creating a pod to probe DNS 03/23/23 18:37:09.287
    STEP: submitting the pod to kubernetes 03/23/23 18:37:09.288
    Mar 23 18:37:09.304: INFO: Waiting up to 15m0s for pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423" in namespace "dns-1861" to be "running"
    Mar 23 18:37:09.324: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Pending", Reason="", readiness=false. Elapsed: 19.586057ms
    Mar 23 18:37:11.328: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023449224s
    Mar 23 18:37:13.329: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423": Phase="Running", Reason="", readiness=true. Elapsed: 4.025199995s
    Mar 23 18:37:13.329: INFO: Pod "dns-test-d1f9c62a-f496-419d-84db-6209fb15e423" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 18:37:13.329
    STEP: looking for the results for each expected name from probers 03/23/23 18:37:13.333
    Mar 23 18:37:13.341: INFO: DNS probes using dns-test-d1f9c62a-f496-419d-84db-6209fb15e423 succeeded

    STEP: deleting the pod 03/23/23 18:37:13.341
    STEP: changing the externalName to bar.example.com 03/23/23 18:37:13.357
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:13.37
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:13.37
    STEP: creating a second pod to probe DNS 03/23/23 18:37:13.37
    STEP: submitting the pod to kubernetes 03/23/23 18:37:13.37
    Mar 23 18:37:13.387: INFO: Waiting up to 15m0s for pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd" in namespace "dns-1861" to be "running"
    Mar 23 18:37:13.397: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246077ms
    Mar 23 18:37:15.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014905642s
    Mar 23 18:37:17.403: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015786116s
    Mar 23 18:37:19.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd": Phase="Running", Reason="", readiness=true. Elapsed: 6.014982459s
    Mar 23 18:37:19.402: INFO: Pod "dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 18:37:19.402
    STEP: looking for the results for each expected name from probers 03/23/23 18:37:19.405
    Mar 23 18:37:19.415: INFO: DNS probes using dns-test-06ce43df-0306-42e3-9b11-e8f143374fcd succeeded

    STEP: deleting the pod 03/23/23 18:37:19.415
    STEP: changing the service to type=ClusterIP 03/23/23 18:37:19.434
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:19.452
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1861.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1861.svc.cluster.local; sleep 1; done
     03/23/23 18:37:19.453
    STEP: creating a third pod to probe DNS 03/23/23 18:37:19.453
    STEP: submitting the pod to kubernetes 03/23/23 18:37:19.46
    Mar 23 18:37:19.474: INFO: Waiting up to 15m0s for pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634" in namespace "dns-1861" to be "running"
    Mar 23 18:37:19.483: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Pending", Reason="", readiness=false. Elapsed: 8.74718ms
    Mar 23 18:37:21.487: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013536076s
    Mar 23 18:37:23.488: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634": Phase="Running", Reason="", readiness=true. Elapsed: 4.013687683s
    Mar 23 18:37:23.488: INFO: Pod "dns-test-0a100132-1bad-4663-b05f-2ebb68e11634" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 18:37:23.488
    STEP: looking for the results for each expected name from probers 03/23/23 18:37:23.492
    Mar 23 18:37:23.503: INFO: DNS probes using dns-test-0a100132-1bad-4663-b05f-2ebb68e11634 succeeded

    STEP: deleting the pod 03/23/23 18:37:23.503
    STEP: deleting the test externalName service 03/23/23 18:37:23.531
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 18:37:23.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1861" for this suite. 03/23/23 18:37:23.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:37:23.613
Mar 23 18:37:23.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 18:37:23.615
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:23.64
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:23.642
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Mar 23 18:37:23.689: INFO: Create a RollingUpdate DaemonSet
Mar 23 18:37:23.701: INFO: Check that daemon pods launch on every node of the cluster
Mar 23 18:37:23.706: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:23.706: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:23.707: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:23.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:23.720: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:24.738: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:24.739: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:24.739: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:24.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:24.744: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:25.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:25.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:25.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:25.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:25.734: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:26.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:26.737: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:27.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:27.729: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:28.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:28.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:28.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:28.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:28.742: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:29.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 18:37:29.730: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 18:37:30.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:30.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:30.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:30.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 18:37:30.729: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:31.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 18:37:31.735: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:32.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 18:37:32.730: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:33.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 18:37:33.734: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:34.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 18:37:34.731: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Mar 23 18:37:34.731: INFO: Update the DaemonSet to trigger a rollout
Mar 23 18:37:34.742: INFO: Updating DaemonSet daemon-set
Mar 23 18:37:42.762: INFO: Roll back the DaemonSet before rollout is complete
Mar 23 18:37:42.773: INFO: Updating DaemonSet daemon-set
Mar 23 18:37:42.773: INFO: Make sure DaemonSet rollback is complete
Mar 23 18:37:42.779: INFO: Wrong image for pod: daemon-set-h69sr. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Mar 23 18:37:42.779: INFO: Pod daemon-set-h69sr is not available
Mar 23 18:37:42.784: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:42.784: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:42.785: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:44.790: INFO: Pod daemon-set-mvzt2 is not available
Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 18:37:44.813
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7483, will wait for the garbage collector to delete the pods 03/23/23 18:37:44.813
Mar 23 18:37:44.884: INFO: Deleting DaemonSet.extensions daemon-set took: 8.572481ms
Mar 23 18:37:44.985: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.726377ms
Mar 23 18:37:52.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 18:37:52.393: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 18:37:52.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3672"},"items":null}

Mar 23 18:37:52.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3672"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:37:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7483" for this suite. 03/23/23 18:37:52.424
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":15,"skipped":299,"failed":0}
------------------------------
• [SLOW TEST] [28.819 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:37:23.613
    Mar 23 18:37:23.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 18:37:23.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:23.64
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:23.642
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Mar 23 18:37:23.689: INFO: Create a RollingUpdate DaemonSet
    Mar 23 18:37:23.701: INFO: Check that daemon pods launch on every node of the cluster
    Mar 23 18:37:23.706: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:23.706: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:23.707: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:23.720: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:23.720: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:24.738: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:24.739: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:24.739: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:24.744: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:24.744: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:25.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:25.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:25.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:25.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:25.734: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:26.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:26.737: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:26.737: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:27.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:27.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:27.729: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:28.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:28.727: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:28.728: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:28.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:28.742: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:29.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:29.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 18:37:29.730: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 18:37:30.725: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:30.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:30.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:30.729: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 18:37:30.729: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:31.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:31.735: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 18:37:31.735: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:32.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:32.730: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 18:37:32.730: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:33.729: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:33.734: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 18:37:33.734: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:34.726: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:34.731: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 18:37:34.731: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Mar 23 18:37:34.731: INFO: Update the DaemonSet to trigger a rollout
    Mar 23 18:37:34.742: INFO: Updating DaemonSet daemon-set
    Mar 23 18:37:42.762: INFO: Roll back the DaemonSet before rollout is complete
    Mar 23 18:37:42.773: INFO: Updating DaemonSet daemon-set
    Mar 23 18:37:42.773: INFO: Make sure DaemonSet rollback is complete
    Mar 23 18:37:42.779: INFO: Wrong image for pod: daemon-set-h69sr. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Mar 23 18:37:42.779: INFO: Pod daemon-set-h69sr is not available
    Mar 23 18:37:42.784: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:42.784: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:42.785: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:43.794: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:44.790: INFO: Pod daemon-set-mvzt2 is not available
    Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 18:37:44.795: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 18:37:44.813
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7483, will wait for the garbage collector to delete the pods 03/23/23 18:37:44.813
    Mar 23 18:37:44.884: INFO: Deleting DaemonSet.extensions daemon-set took: 8.572481ms
    Mar 23 18:37:44.985: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.726377ms
    Mar 23 18:37:52.393: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 18:37:52.393: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 18:37:52.399: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3672"},"items":null}

    Mar 23 18:37:52.402: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3672"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:37:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7483" for this suite. 03/23/23 18:37:52.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:37:52.442
Mar 23 18:37:52.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 18:37:52.444
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:52.468
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:52.47
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 18:37:52.492
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:37:53.884
STEP: Deploying the webhook pod 03/23/23 18:37:53.895
STEP: Wait for the deployment to be ready 03/23/23 18:37:53.924
Mar 23 18:37:53.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 18:37:55.942
STEP: Verifying the service has paired with the endpoint 03/23/23 18:37:55.959
Mar 23 18:37:56.959: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 03/23/23 18:37:57.077
STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 18:37:57.13
STEP: Deleting the collection of validation webhooks 03/23/23 18:37:57.178
STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 18:37:57.23
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:37:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4797" for this suite. 03/23/23 18:37:57.25
STEP: Destroying namespace "webhook-4797-markers" for this suite. 03/23/23 18:37:57.257
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":16,"skipped":309,"failed":0}
------------------------------
• [4.933 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:37:52.442
    Mar 23 18:37:52.442: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 18:37:52.444
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:52.468
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:52.47
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 18:37:52.492
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:37:53.884
    STEP: Deploying the webhook pod 03/23/23 18:37:53.895
    STEP: Wait for the deployment to be ready 03/23/23 18:37:53.924
    Mar 23 18:37:53.932: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 18:37:55.942
    STEP: Verifying the service has paired with the endpoint 03/23/23 18:37:55.959
    Mar 23 18:37:56.959: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 03/23/23 18:37:57.077
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 18:37:57.13
    STEP: Deleting the collection of validation webhooks 03/23/23 18:37:57.178
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 18:37:57.23
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:37:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4797" for this suite. 03/23/23 18:37:57.25
    STEP: Destroying namespace "webhook-4797-markers" for this suite. 03/23/23 18:37:57.257
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:37:57.376
Mar 23 18:37:57.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename containers 03/23/23 18:37:57.377
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:57.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:57.448
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 03/23/23 18:37:57.451
Mar 23 18:37:57.462: INFO: Waiting up to 5m0s for pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00" in namespace "containers-1947" to be "Succeeded or Failed"
Mar 23 18:37:57.466: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487693ms
Mar 23 18:37:59.471: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008462768s
Mar 23 18:38:01.472: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010058076s
Mar 23 18:38:03.471: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008977091s
STEP: Saw pod success 03/23/23 18:38:03.471
Mar 23 18:38:03.472: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00" satisfied condition "Succeeded or Failed"
Mar 23 18:38:03.475: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:38:03.482
Mar 23 18:38:03.503: INFO: Waiting for pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 to disappear
Mar 23 18:38:03.508: INFO: Pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 23 18:38:03.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1947" for this suite. 03/23/23 18:38:03.515
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":17,"skipped":326,"failed":0}
------------------------------
• [SLOW TEST] [6.146 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:37:57.376
    Mar 23 18:37:57.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename containers 03/23/23 18:37:57.377
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:37:57.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:37:57.448
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 03/23/23 18:37:57.451
    Mar 23 18:37:57.462: INFO: Waiting up to 5m0s for pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00" in namespace "containers-1947" to be "Succeeded or Failed"
    Mar 23 18:37:57.466: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487693ms
    Mar 23 18:37:59.471: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008462768s
    Mar 23 18:38:01.472: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010058076s
    Mar 23 18:38:03.471: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008977091s
    STEP: Saw pod success 03/23/23 18:38:03.471
    Mar 23 18:38:03.472: INFO: Pod "client-containers-6469a117-9584-42b5-a102-4bade2558c00" satisfied condition "Succeeded or Failed"
    Mar 23 18:38:03.475: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:38:03.482
    Mar 23 18:38:03.503: INFO: Waiting for pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 to disappear
    Mar 23 18:38:03.508: INFO: Pod client-containers-6469a117-9584-42b5-a102-4bade2558c00 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 23 18:38:03.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1947" for this suite. 03/23/23 18:38:03.515
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:38:03.53
Mar 23 18:38:03.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 18:38:03.531
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:38:03.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:38:03.558
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 03/23/23 18:38:03.56
STEP: setting up watch 03/23/23 18:38:03.561
STEP: submitting the pod to kubernetes 03/23/23 18:38:03.665
STEP: verifying the pod is in kubernetes 03/23/23 18:38:03.676
STEP: verifying pod creation was observed 03/23/23 18:38:03.68
Mar 23 18:38:03.680: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b" in namespace "pods-2828" to be "running"
Mar 23 18:38:03.687: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.856584ms
Mar 23 18:38:05.691: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011209786s
Mar 23 18:38:05.691: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b" satisfied condition "running"
STEP: deleting the pod gracefully 03/23/23 18:38:05.695
STEP: verifying pod deletion was observed 03/23/23 18:38:05.705
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 18:38:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2828" for this suite. 03/23/23 18:38:09.417
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":18,"skipped":328,"failed":0}
------------------------------
• [SLOW TEST] [5.893 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:38:03.53
    Mar 23 18:38:03.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 18:38:03.531
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:38:03.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:38:03.558
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 03/23/23 18:38:03.56
    STEP: setting up watch 03/23/23 18:38:03.561
    STEP: submitting the pod to kubernetes 03/23/23 18:38:03.665
    STEP: verifying the pod is in kubernetes 03/23/23 18:38:03.676
    STEP: verifying pod creation was observed 03/23/23 18:38:03.68
    Mar 23 18:38:03.680: INFO: Waiting up to 5m0s for pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b" in namespace "pods-2828" to be "running"
    Mar 23 18:38:03.687: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.856584ms
    Mar 23 18:38:05.691: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011209786s
    Mar 23 18:38:05.691: INFO: Pod "pod-submit-remove-d913a6ba-b3ff-4742-ae29-dcc56e32a58b" satisfied condition "running"
    STEP: deleting the pod gracefully 03/23/23 18:38:05.695
    STEP: verifying pod deletion was observed 03/23/23 18:38:05.705
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 18:38:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2828" for this suite. 03/23/23 18:38:09.417
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:38:09.43
Mar 23 18:38:09.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 18:38:09.431
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:38:09.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:38:09.459
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3280 03/23/23 18:38:09.462
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-3280 03/23/23 18:38:09.467
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3280 03/23/23 18:38:09.485
Mar 23 18:38:09.494: INFO: Found 0 stateful pods, waiting for 1
Mar 23 18:38:19.499: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/23/23 18:38:19.499
Mar 23 18:38:19.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:38:19.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:38:19.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:38:19.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:38:19.723: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 18:38:29.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:38:29.728: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:38:29.750: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 23 18:38:29.750: INFO: ss-0  k8s-linuxpool-16392394-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
Mar 23 18:38:29.750: INFO: ss-1                            Pending         []
Mar 23 18:38:29.750: INFO: 
Mar 23 18:38:29.750: INFO: StatefulSet ss has not reached scale 3, at 2
Mar 23 18:38:30.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993909807s
Mar 23 18:38:31.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987763575s
Mar 23 18:38:32.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983256338s
Mar 23 18:38:33.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977412705s
Mar 23 18:38:34.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970811674s
Mar 23 18:38:35.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965825038s
Mar 23 18:38:36.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959028707s
Mar 23 18:38:37.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954679371s
Mar 23 18:38:38.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.169754ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3280 03/23/23 18:38:39.799
Mar 23 18:38:39.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:38:40.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 18:38:40.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:38:40.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:38:40.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:38:40.489: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 18:38:40.489: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:38:40.489: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:38:40.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 18:38:40.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Mar 23 18:38:40.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 18:38:40.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 18:38:40.716: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Mar 23 18:38:50.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:38:50.723: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:38:50.723: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 03/23/23 18:38:50.723
Mar 23 18:38:50.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:38:51.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:38:51.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:38:51.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:38:51.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:38:51.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:38:51.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:38:51.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:38:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 18:38:51.520: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 18:38:51.520: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 18:38:51.520: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 18:38:51.520: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:38:51.524: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 23 18:39:01.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:39:01.540: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:39:01.540: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 18:39:01.604: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 23 18:39:01.604: INFO: ss-0  k8s-linuxpool-16392394-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
Mar 23 18:39:01.604: INFO: ss-1  k8s-linuxpool-16392394-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
Mar 23 18:39:01.604: INFO: ss-2  k8s-linuxpool-16392394-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
Mar 23 18:39:01.604: INFO: 
Mar 23 18:39:01.605: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 18:39:02.612: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 23 18:39:02.612: INFO: ss-0  k8s-linuxpool-16392394-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
Mar 23 18:39:02.612: INFO: ss-1  k8s-linuxpool-16392394-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
Mar 23 18:39:02.612: INFO: ss-2  k8s-linuxpool-16392394-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
Mar 23 18:39:02.612: INFO: 
Mar 23 18:39:02.612: INFO: StatefulSet ss has not reached scale 0, at 3
Mar 23 18:39:03.618: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
Mar 23 18:39:03.618: INFO: ss-0  k8s-linuxpool-16392394-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
Mar 23 18:39:03.618: INFO: ss-1  k8s-linuxpool-16392394-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
Mar 23 18:39:03.618: INFO: 
Mar 23 18:39:03.618: INFO: StatefulSet ss has not reached scale 0, at 2
Mar 23 18:39:04.622: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.965777008s
Mar 23 18:39:05.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961602753s
Mar 23 18:39:06.636: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.956506101s
Mar 23 18:39:07.640: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.947429856s
Mar 23 18:39:08.646: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.943099701s
Mar 23 18:39:09.650: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.937638449s
Mar 23 18:39:10.655: INFO: Verifying statefulset ss doesn't scale past 0 for another 933.315308ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3280 03/23/23 18:39:11.655
Mar 23 18:39:11.659: INFO: Scaling statefulset ss to 0
Mar 23 18:39:11.669: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 18:39:11.672: INFO: Deleting all statefulset in ns statefulset-3280
Mar 23 18:39:11.676: INFO: Scaling statefulset ss to 0
Mar 23 18:39:11.686: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:39:11.689: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 18:39:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3280" for this suite. 03/23/23 18:39:11.725
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":19,"skipped":329,"failed":0}
------------------------------
• [SLOW TEST] [62.308 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:38:09.43
    Mar 23 18:38:09.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 18:38:09.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:38:09.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:38:09.459
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3280 03/23/23 18:38:09.462
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-3280 03/23/23 18:38:09.467
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3280 03/23/23 18:38:09.485
    Mar 23 18:38:09.494: INFO: Found 0 stateful pods, waiting for 1
    Mar 23 18:38:19.499: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 03/23/23 18:38:19.499
    Mar 23 18:38:19.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 18:38:19.720: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 18:38:19.720: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 18:38:19.720: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 18:38:19.723: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 23 18:38:29.728: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 18:38:29.728: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 18:38:29.750: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 18:38:29.750: INFO: ss-0  k8s-linuxpool-16392394-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:19 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
    Mar 23 18:38:29.750: INFO: ss-1                            Pending         []
    Mar 23 18:38:29.750: INFO: 
    Mar 23 18:38:29.750: INFO: StatefulSet ss has not reached scale 3, at 2
    Mar 23 18:38:30.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993909807s
    Mar 23 18:38:31.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.987763575s
    Mar 23 18:38:32.766: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983256338s
    Mar 23 18:38:33.772: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977412705s
    Mar 23 18:38:34.778: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.970811674s
    Mar 23 18:38:35.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965825038s
    Mar 23 18:38:36.789: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959028707s
    Mar 23 18:38:37.793: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.954679371s
    Mar 23 18:38:38.799: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.169754ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3280 03/23/23 18:38:39.799
    Mar 23 18:38:39.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 18:38:40.278: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 18:38:40.278: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 18:38:40.278: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 18:38:40.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 18:38:40.489: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 23 18:38:40.489: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 18:38:40.489: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 18:38:40.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 18:38:40.711: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Mar 23 18:38:40.711: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 18:38:40.711: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 18:38:40.716: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Mar 23 18:38:50.723: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:38:50.723: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:38:50.723: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 03/23/23 18:38:50.723
    Mar 23 18:38:50.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 18:38:51.034: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 18:38:51.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 18:38:51.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 18:38:51.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 18:38:51.275: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 18:38:51.275: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 18:38:51.275: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 18:38:51.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-3280 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 18:38:51.520: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 18:38:51.520: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 18:38:51.520: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 18:38:51.520: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 18:38:51.524: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 23 18:39:01.540: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 18:39:01.540: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 18:39:01.540: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 18:39:01.604: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 18:39:01.604: INFO: ss-0  k8s-linuxpool-16392394-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
    Mar 23 18:39:01.604: INFO: ss-1  k8s-linuxpool-16392394-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
    Mar 23 18:39:01.604: INFO: ss-2  k8s-linuxpool-16392394-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
    Mar 23 18:39:01.604: INFO: 
    Mar 23 18:39:01.605: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 23 18:39:02.612: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 18:39:02.612: INFO: ss-0  k8s-linuxpool-16392394-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
    Mar 23 18:39:02.612: INFO: ss-1  k8s-linuxpool-16392394-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
    Mar 23 18:39:02.612: INFO: ss-2  k8s-linuxpool-16392394-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
    Mar 23 18:39:02.612: INFO: 
    Mar 23 18:39:02.612: INFO: StatefulSet ss has not reached scale 0, at 3
    Mar 23 18:39:03.618: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 18:39:03.618: INFO: ss-0  k8s-linuxpool-16392394-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:51 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:09 +0000 UTC  }]
    Mar 23 18:39:03.618: INFO: ss-1  k8s-linuxpool-16392394-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:52 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 18:38:29 +0000 UTC  }]
    Mar 23 18:39:03.618: INFO: 
    Mar 23 18:39:03.618: INFO: StatefulSet ss has not reached scale 0, at 2
    Mar 23 18:39:04.622: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.965777008s
    Mar 23 18:39:05.627: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961602753s
    Mar 23 18:39:06.636: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.956506101s
    Mar 23 18:39:07.640: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.947429856s
    Mar 23 18:39:08.646: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.943099701s
    Mar 23 18:39:09.650: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.937638449s
    Mar 23 18:39:10.655: INFO: Verifying statefulset ss doesn't scale past 0 for another 933.315308ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3280 03/23/23 18:39:11.655
    Mar 23 18:39:11.659: INFO: Scaling statefulset ss to 0
    Mar 23 18:39:11.669: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 18:39:11.672: INFO: Deleting all statefulset in ns statefulset-3280
    Mar 23 18:39:11.676: INFO: Scaling statefulset ss to 0
    Mar 23 18:39:11.686: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 18:39:11.689: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 18:39:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3280" for this suite. 03/23/23 18:39:11.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:39:11.739
Mar 23 18:39:11.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 18:39:11.74
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:39:11.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:39:11.761
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 in namespace container-probe-662 03/23/23 18:39:11.765
Mar 23 18:39:11.777: INFO: Waiting up to 5m0s for pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609" in namespace "container-probe-662" to be "not pending"
Mar 23 18:39:11.781: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611291ms
Mar 23 18:39:13.786: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609": Phase="Running", Reason="", readiness=true. Elapsed: 2.009248488s
Mar 23 18:39:13.786: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609" satisfied condition "not pending"
Mar 23 18:39:13.786: INFO: Started pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 in namespace container-probe-662
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:39:13.786
Mar 23 18:39:13.790: INFO: Initial restart count of pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 is 0
STEP: deleting the pod 03/23/23 18:43:14.428
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 18:43:14.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-662" for this suite. 03/23/23 18:43:14.451
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":20,"skipped":368,"failed":0}
------------------------------
• [SLOW TEST] [242.718 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:39:11.739
    Mar 23 18:39:11.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 18:39:11.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:39:11.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:39:11.761
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 in namespace container-probe-662 03/23/23 18:39:11.765
    Mar 23 18:39:11.777: INFO: Waiting up to 5m0s for pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609" in namespace "container-probe-662" to be "not pending"
    Mar 23 18:39:11.781: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611291ms
    Mar 23 18:39:13.786: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609": Phase="Running", Reason="", readiness=true. Elapsed: 2.009248488s
    Mar 23 18:39:13.786: INFO: Pod "busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609" satisfied condition "not pending"
    Mar 23 18:39:13.786: INFO: Started pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 in namespace container-probe-662
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:39:13.786
    Mar 23 18:39:13.790: INFO: Initial restart count of pod busybox-5dfc8604-4f1e-470a-a307-a7c9661c4609 is 0
    STEP: deleting the pod 03/23/23 18:43:14.428
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 18:43:14.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-662" for this suite. 03/23/23 18:43:14.451
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:43:14.457
Mar 23 18:43:14.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 18:43:14.459
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:14.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:14.479
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-ce7bc1cd-f2d0-4ede-abfd-185c4ca3f4c8 03/23/23 18:43:14.482
STEP: Creating a pod to test consume configMaps 03/23/23 18:43:14.488
Mar 23 18:43:14.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557" in namespace "configmap-9430" to be "Succeeded or Failed"
Mar 23 18:43:14.510: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753985ms
Mar 23 18:43:16.514: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Running", Reason="", readiness=true. Elapsed: 2.010894144s
Mar 23 18:43:18.517: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Running", Reason="", readiness=false. Elapsed: 4.013822607s
Mar 23 18:43:20.514: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011309072s
STEP: Saw pod success 03/23/23 18:43:20.514
Mar 23 18:43:20.515: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557" satisfied condition "Succeeded or Failed"
Mar 23 18:43:20.517: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:43:20.55
Mar 23 18:43:20.563: INFO: Waiting for pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 to disappear
Mar 23 18:43:20.575: INFO: Pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 18:43:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9430" for this suite. 03/23/23 18:43:20.58
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":21,"skipped":369,"failed":0}
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:43:14.457
    Mar 23 18:43:14.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 18:43:14.459
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:14.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:14.479
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-ce7bc1cd-f2d0-4ede-abfd-185c4ca3f4c8 03/23/23 18:43:14.482
    STEP: Creating a pod to test consume configMaps 03/23/23 18:43:14.488
    Mar 23 18:43:14.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557" in namespace "configmap-9430" to be "Succeeded or Failed"
    Mar 23 18:43:14.510: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Pending", Reason="", readiness=false. Elapsed: 6.753985ms
    Mar 23 18:43:16.514: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Running", Reason="", readiness=true. Elapsed: 2.010894144s
    Mar 23 18:43:18.517: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Running", Reason="", readiness=false. Elapsed: 4.013822607s
    Mar 23 18:43:20.514: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011309072s
    STEP: Saw pod success 03/23/23 18:43:20.514
    Mar 23 18:43:20.515: INFO: Pod "pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557" satisfied condition "Succeeded or Failed"
    Mar 23 18:43:20.517: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:43:20.55
    Mar 23 18:43:20.563: INFO: Waiting for pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 to disappear
    Mar 23 18:43:20.575: INFO: Pod pod-configmaps-229d788b-d54a-4bbb-883d-dee1b19ba557 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 18:43:20.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9430" for this suite. 03/23/23 18:43:20.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:43:20.595
Mar 23 18:43:20.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-runtime 03/23/23 18:43:20.597
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:20.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:20.62
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/23/23 18:43:20.641
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/23/23 18:43:39.759
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/23/23 18:43:39.763
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/23/23 18:43:39.769
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/23/23 18:43:39.769
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/23/23 18:43:39.804
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/23/23 18:43:44.833
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/23/23 18:43:46.844
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/23/23 18:43:46.85
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/23/23 18:43:46.85
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/23/23 18:43:46.89
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/23/23 18:43:47.904
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/23/23 18:43:51.927
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/23/23 18:43:51.933
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/23/23 18:43:51.933
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 23 18:43:51.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1358" for this suite. 03/23/23 18:43:51.971
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":22,"skipped":379,"failed":0}
------------------------------
• [SLOW TEST] [31.383 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:43:20.595
    Mar 23 18:43:20.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-runtime 03/23/23 18:43:20.597
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:20.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:20.62
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 03/23/23 18:43:20.641
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 03/23/23 18:43:39.759
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 03/23/23 18:43:39.763
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 03/23/23 18:43:39.769
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 03/23/23 18:43:39.769
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 03/23/23 18:43:39.804
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 03/23/23 18:43:44.833
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 03/23/23 18:43:46.844
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 03/23/23 18:43:46.85
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 03/23/23 18:43:46.85
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 03/23/23 18:43:46.89
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 03/23/23 18:43:47.904
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 03/23/23 18:43:51.927
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 03/23/23 18:43:51.933
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 03/23/23 18:43:51.933
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 23 18:43:51.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1358" for this suite. 03/23/23 18:43:51.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:43:51.986
Mar 23 18:43:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubelet-test 03/23/23 18:43:51.988
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:52.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:52.012
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 03/23/23 18:43:52.034
Mar 23 18:43:52.035: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a" in namespace "kubelet-test-399" to be "completed"
Mar 23 18:43:52.066: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.088031ms
Mar 23 18:43:54.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036067788s
Mar 23 18:43:56.072: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Running", Reason="", readiness=false. Elapsed: 4.037038155s
Mar 23 18:43:58.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035711226s
Mar 23 18:43:58.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 23 18:43:58.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-399" for this suite. 03/23/23 18:43:58.083
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":23,"skipped":392,"failed":0}
------------------------------
• [SLOW TEST] [6.110 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:43:51.986
    Mar 23 18:43:51.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubelet-test 03/23/23 18:43:51.988
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:52.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:52.012
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 03/23/23 18:43:52.034
    Mar 23 18:43:52.035: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a" in namespace "kubelet-test-399" to be "completed"
    Mar 23 18:43:52.066: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.088031ms
    Mar 23 18:43:54.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Running", Reason="", readiness=true. Elapsed: 2.036067788s
    Mar 23 18:43:56.072: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Running", Reason="", readiness=false. Elapsed: 4.037038155s
    Mar 23 18:43:58.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035711226s
    Mar 23 18:43:58.071: INFO: Pod "agnhost-host-aliases4fa270de-14dd-4fe1-b0f6-0cf31a1be07a" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 23 18:43:58.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-399" for this suite. 03/23/23 18:43:58.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:43:58.098
Mar 23 18:43:58.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 18:43:58.1
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:58.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:58.133
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 18:43:58.201
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:43:58.856
STEP: Deploying the webhook pod 03/23/23 18:43:58.869
STEP: Wait for the deployment to be ready 03/23/23 18:43:58.881
Mar 23 18:43:58.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 18:44:00.904
STEP: Verifying the service has paired with the endpoint 03/23/23 18:44:00.926
Mar 23 18:44:01.927: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 03/23/23 18:44:01.93
STEP: create a pod that should be denied by the webhook 03/23/23 18:44:01.949
STEP: create a pod that causes the webhook to hang 03/23/23 18:44:01.962
STEP: create a configmap that should be denied by the webhook 03/23/23 18:44:11.969
STEP: create a configmap that should be admitted by the webhook 03/23/23 18:44:11.99
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/23/23 18:44:12.001
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/23/23 18:44:12.009
STEP: create a namespace that bypass the webhook 03/23/23 18:44:12.014
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/23/23 18:44:12.028
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:44:12.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-605" for this suite. 03/23/23 18:44:12.095
STEP: Destroying namespace "webhook-605-markers" for this suite. 03/23/23 18:44:12.103
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":24,"skipped":429,"failed":0}
------------------------------
• [SLOW TEST] [14.090 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:43:58.098
    Mar 23 18:43:58.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 18:43:58.1
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:43:58.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:43:58.133
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 18:43:58.201
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:43:58.856
    STEP: Deploying the webhook pod 03/23/23 18:43:58.869
    STEP: Wait for the deployment to be ready 03/23/23 18:43:58.881
    Mar 23 18:43:58.892: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 18:44:00.904
    STEP: Verifying the service has paired with the endpoint 03/23/23 18:44:00.926
    Mar 23 18:44:01.927: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 03/23/23 18:44:01.93
    STEP: create a pod that should be denied by the webhook 03/23/23 18:44:01.949
    STEP: create a pod that causes the webhook to hang 03/23/23 18:44:01.962
    STEP: create a configmap that should be denied by the webhook 03/23/23 18:44:11.969
    STEP: create a configmap that should be admitted by the webhook 03/23/23 18:44:11.99
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 03/23/23 18:44:12.001
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 03/23/23 18:44:12.009
    STEP: create a namespace that bypass the webhook 03/23/23 18:44:12.014
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 03/23/23 18:44:12.028
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:44:12.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-605" for this suite. 03/23/23 18:44:12.095
    STEP: Destroying namespace "webhook-605-markers" for this suite. 03/23/23 18:44:12.103
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:44:12.195
Mar 23 18:44:12.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sysctl 03/23/23 18:44:12.196
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:44:12.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:44:12.255
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 03/23/23 18:44:12.263
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 18:44:12.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-4946" for this suite. 03/23/23 18:44:12.307
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":25,"skipped":437,"failed":0}
------------------------------
• [0.129 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:44:12.195
    Mar 23 18:44:12.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sysctl 03/23/23 18:44:12.196
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:44:12.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:44:12.255
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 03/23/23 18:44:12.263
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 18:44:12.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-4946" for this suite. 03/23/23 18:44:12.307
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:44:12.325
Mar 23 18:44:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 18:44:12.326
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:44:12.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:44:12.351
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 in namespace container-probe-2979 03/23/23 18:44:12.354
Mar 23 18:44:12.364: INFO: Waiting up to 5m0s for pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6" in namespace "container-probe-2979" to be "not pending"
Mar 23 18:44:12.367: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171593ms
Mar 23 18:44:14.372: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007995432s
Mar 23 18:44:14.372: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6" satisfied condition "not pending"
Mar 23 18:44:14.372: INFO: Started pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 in namespace container-probe-2979
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:44:14.372
Mar 23 18:44:14.376: INFO: Initial restart count of pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is 0
Mar 23 18:44:34.429: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 1 (20.053099221s elapsed)
Mar 23 18:44:54.479: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 2 (40.102487969s elapsed)
Mar 23 18:45:14.536: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 3 (1m0.16032064s elapsed)
Mar 23 18:45:34.606: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 4 (1m20.230299287s elapsed)
Mar 23 18:46:46.780: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 5 (2m32.403792694s elapsed)
STEP: deleting the pod 03/23/23 18:46:46.78
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 18:46:46.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2979" for this suite. 03/23/23 18:46:46.804
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":26,"skipped":441,"failed":0}
------------------------------
• [SLOW TEST] [154.485 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:44:12.325
    Mar 23 18:44:12.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 18:44:12.326
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:44:12.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:44:12.351
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 in namespace container-probe-2979 03/23/23 18:44:12.354
    Mar 23 18:44:12.364: INFO: Waiting up to 5m0s for pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6" in namespace "container-probe-2979" to be "not pending"
    Mar 23 18:44:12.367: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171593ms
    Mar 23 18:44:14.372: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.007995432s
    Mar 23 18:44:14.372: INFO: Pod "liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6" satisfied condition "not pending"
    Mar 23 18:44:14.372: INFO: Started pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 in namespace container-probe-2979
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:44:14.372
    Mar 23 18:44:14.376: INFO: Initial restart count of pod liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is 0
    Mar 23 18:44:34.429: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 1 (20.053099221s elapsed)
    Mar 23 18:44:54.479: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 2 (40.102487969s elapsed)
    Mar 23 18:45:14.536: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 3 (1m0.16032064s elapsed)
    Mar 23 18:45:34.606: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 4 (1m20.230299287s elapsed)
    Mar 23 18:46:46.780: INFO: Restart count of pod container-probe-2979/liveness-adfb4197-a86b-4af9-a1ef-313f7216ecc6 is now 5 (2m32.403792694s elapsed)
    STEP: deleting the pod 03/23/23 18:46:46.78
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 18:46:46.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2979" for this suite. 03/23/23 18:46:46.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:46:46.816
Mar 23 18:46:46.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 18:46:46.819
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:46:46.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:46:46.845
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 18:46:46.864
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:46:47.886
STEP: Deploying the webhook pod 03/23/23 18:46:47.894
STEP: Wait for the deployment to be ready 03/23/23 18:46:47.908
Mar 23 18:46:47.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 18:46:49.937
STEP: Verifying the service has paired with the endpoint 03/23/23 18:46:49.953
Mar 23 18:46:50.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Mar 23 18:46:50.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8504-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 18:46:51.472
STEP: Creating a custom resource while v1 is storage version 03/23/23 18:46:51.491
STEP: Patching Custom Resource Definition to set v2 as storage 03/23/23 18:46:53.622
STEP: Patching the custom resource while v2 is storage version 03/23/23 18:46:53.641
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:46:54.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9750" for this suite. 03/23/23 18:46:54.301
STEP: Destroying namespace "webhook-9750-markers" for this suite. 03/23/23 18:46:54.318
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":27,"skipped":452,"failed":0}
------------------------------
• [SLOW TEST] [7.602 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:46:46.816
    Mar 23 18:46:46.816: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 18:46:46.819
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:46:46.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:46:46.845
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 18:46:46.864
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:46:47.886
    STEP: Deploying the webhook pod 03/23/23 18:46:47.894
    STEP: Wait for the deployment to be ready 03/23/23 18:46:47.908
    Mar 23 18:46:47.924: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 18:46:49.937
    STEP: Verifying the service has paired with the endpoint 03/23/23 18:46:49.953
    Mar 23 18:46:50.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Mar 23 18:46:50.957: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8504-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 18:46:51.472
    STEP: Creating a custom resource while v1 is storage version 03/23/23 18:46:51.491
    STEP: Patching Custom Resource Definition to set v2 as storage 03/23/23 18:46:53.622
    STEP: Patching the custom resource while v2 is storage version 03/23/23 18:46:53.641
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:46:54.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9750" for this suite. 03/23/23 18:46:54.301
    STEP: Destroying namespace "webhook-9750-markers" for this suite. 03/23/23 18:46:54.318
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:46:54.423
Mar 23 18:46:54.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 18:46:54.431
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:46:54.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:46:54.512
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 03/23/23 18:46:54.526
Mar 23 18:46:54.542: INFO: Waiting up to 5m0s for pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba" in namespace "downward-api-1713" to be "Succeeded or Failed"
Mar 23 18:46:54.548: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239584ms
Mar 23 18:46:56.553: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010965748s
Mar 23 18:46:58.556: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013884962s
Mar 23 18:47:00.558: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.01534588s
Mar 23 18:47:02.555: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Running", Reason="", readiness=false. Elapsed: 8.01241291s
Mar 23 18:47:04.553: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011129549s
STEP: Saw pod success 03/23/23 18:47:04.553
Mar 23 18:47:04.554: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba" satisfied condition "Succeeded or Failed"
Mar 23 18:47:04.558: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba container dapi-container: <nil>
STEP: delete the pod 03/23/23 18:47:04.596
Mar 23 18:47:04.612: INFO: Waiting for pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba to disappear
Mar 23 18:47:04.618: INFO: Pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 23 18:47:04.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1713" for this suite. 03/23/23 18:47:04.626
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":28,"skipped":468,"failed":0}
------------------------------
• [SLOW TEST] [10.211 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:46:54.423
    Mar 23 18:46:54.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 18:46:54.431
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:46:54.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:46:54.512
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 03/23/23 18:46:54.526
    Mar 23 18:46:54.542: INFO: Waiting up to 5m0s for pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba" in namespace "downward-api-1713" to be "Succeeded or Failed"
    Mar 23 18:46:54.548: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.239584ms
    Mar 23 18:46:56.553: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010965748s
    Mar 23 18:46:58.556: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013884962s
    Mar 23 18:47:00.558: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Running", Reason="", readiness=true. Elapsed: 6.01534588s
    Mar 23 18:47:02.555: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Running", Reason="", readiness=false. Elapsed: 8.01241291s
    Mar 23 18:47:04.553: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.011129549s
    STEP: Saw pod success 03/23/23 18:47:04.553
    Mar 23 18:47:04.554: INFO: Pod "downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba" satisfied condition "Succeeded or Failed"
    Mar 23 18:47:04.558: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba container dapi-container: <nil>
    STEP: delete the pod 03/23/23 18:47:04.596
    Mar 23 18:47:04.612: INFO: Waiting for pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba to disappear
    Mar 23 18:47:04.618: INFO: Pod downward-api-4c6d8ec2-ec8b-46d0-82b8-7e3f0f2744ba no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 23 18:47:04.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1713" for this suite. 03/23/23 18:47:04.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:04.642
Mar 23 18:47:04.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 18:47:04.643
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:04.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:04.663
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 in namespace container-probe-8788 03/23/23 18:47:04.665
Mar 23 18:47:04.687: INFO: Waiting up to 5m0s for pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784" in namespace "container-probe-8788" to be "not pending"
Mar 23 18:47:04.693: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711878ms
Mar 23 18:47:06.698: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784": Phase="Running", Reason="", readiness=true. Elapsed: 2.010889105s
Mar 23 18:47:06.698: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784" satisfied condition "not pending"
Mar 23 18:47:06.698: INFO: Started pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 in namespace container-probe-8788
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:47:06.698
Mar 23 18:47:06.701: INFO: Initial restart count of pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 is 0
Mar 23 18:47:28.757: INFO: Restart count of pod container-probe-8788/liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 is now 1 (22.056144998s elapsed)
STEP: deleting the pod 03/23/23 18:47:28.757
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 18:47:28.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8788" for this suite. 03/23/23 18:47:28.781
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":29,"skipped":502,"failed":0}
------------------------------
• [SLOW TEST] [24.151 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:04.642
    Mar 23 18:47:04.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 18:47:04.643
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:04.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:04.663
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 in namespace container-probe-8788 03/23/23 18:47:04.665
    Mar 23 18:47:04.687: INFO: Waiting up to 5m0s for pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784" in namespace "container-probe-8788" to be "not pending"
    Mar 23 18:47:04.693: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784": Phase="Pending", Reason="", readiness=false. Elapsed: 6.711878ms
    Mar 23 18:47:06.698: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784": Phase="Running", Reason="", readiness=true. Elapsed: 2.010889105s
    Mar 23 18:47:06.698: INFO: Pod "liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784" satisfied condition "not pending"
    Mar 23 18:47:06.698: INFO: Started pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 in namespace container-probe-8788
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 18:47:06.698
    Mar 23 18:47:06.701: INFO: Initial restart count of pod liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 is 0
    Mar 23 18:47:28.757: INFO: Restart count of pod container-probe-8788/liveness-eab5e2ec-177f-4771-a68d-8ae7b6d2d784 is now 1 (22.056144998s elapsed)
    STEP: deleting the pod 03/23/23 18:47:28.757
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 18:47:28.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8788" for this suite. 03/23/23 18:47:28.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:28.802
Mar 23 18:47:28.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 18:47:28.803
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:28.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:28.823
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-9683/configmap-test-7692b1ff-ad5f-4603-978c-26ff7f1697a0 03/23/23 18:47:28.826
STEP: Creating a pod to test consume configMaps 03/23/23 18:47:28.831
Mar 23 18:47:28.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54" in namespace "configmap-9683" to be "Succeeded or Failed"
Mar 23 18:47:28.852: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121986ms
Mar 23 18:47:30.859: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Running", Reason="", readiness=true. Elapsed: 2.012901567s
Mar 23 18:47:32.856: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Running", Reason="", readiness=false. Elapsed: 4.010085371s
Mar 23 18:47:34.857: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011206465s
STEP: Saw pod success 03/23/23 18:47:34.857
Mar 23 18:47:34.857: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54" satisfied condition "Succeeded or Failed"
Mar 23 18:47:34.860: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 container env-test: <nil>
STEP: delete the pod 03/23/23 18:47:34.891
Mar 23 18:47:34.909: INFO: Waiting for pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 to disappear
Mar 23 18:47:34.913: INFO: Pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 18:47:34.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9683" for this suite. 03/23/23 18:47:34.919
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":30,"skipped":553,"failed":0}
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:28.802
    Mar 23 18:47:28.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 18:47:28.803
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:28.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:28.823
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-9683/configmap-test-7692b1ff-ad5f-4603-978c-26ff7f1697a0 03/23/23 18:47:28.826
    STEP: Creating a pod to test consume configMaps 03/23/23 18:47:28.831
    Mar 23 18:47:28.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54" in namespace "configmap-9683" to be "Succeeded or Failed"
    Mar 23 18:47:28.852: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121986ms
    Mar 23 18:47:30.859: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Running", Reason="", readiness=true. Elapsed: 2.012901567s
    Mar 23 18:47:32.856: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Running", Reason="", readiness=false. Elapsed: 4.010085371s
    Mar 23 18:47:34.857: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011206465s
    STEP: Saw pod success 03/23/23 18:47:34.857
    Mar 23 18:47:34.857: INFO: Pod "pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54" satisfied condition "Succeeded or Failed"
    Mar 23 18:47:34.860: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 container env-test: <nil>
    STEP: delete the pod 03/23/23 18:47:34.891
    Mar 23 18:47:34.909: INFO: Waiting for pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 to disappear
    Mar 23 18:47:34.913: INFO: Pod pod-configmaps-d64177f0-cefe-4153-8913-836a10dcbf54 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 18:47:34.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9683" for this suite. 03/23/23 18:47:34.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:34.945
Mar 23 18:47:34.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename containers 03/23/23 18:47:34.946
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:34.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:34.968
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 03/23/23 18:47:34.972
Mar 23 18:47:35.010: INFO: Waiting up to 5m0s for pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848" in namespace "containers-1930" to be "Succeeded or Failed"
Mar 23 18:47:35.018: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076882ms
Mar 23 18:47:37.023: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Running", Reason="", readiness=true. Elapsed: 2.012928769s
Mar 23 18:47:39.022: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Running", Reason="", readiness=false. Elapsed: 4.012546752s
Mar 23 18:47:41.022: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012658434s
STEP: Saw pod success 03/23/23 18:47:41.023
Mar 23 18:47:41.023: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848" satisfied condition "Succeeded or Failed"
Mar 23 18:47:41.026: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:47:41.032
Mar 23 18:47:41.048: INFO: Waiting for pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 to disappear
Mar 23 18:47:41.051: INFO: Pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 23 18:47:41.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1930" for this suite. 03/23/23 18:47:41.06
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":31,"skipped":581,"failed":0}
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:34.945
    Mar 23 18:47:34.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename containers 03/23/23 18:47:34.946
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:34.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:34.968
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 03/23/23 18:47:34.972
    Mar 23 18:47:35.010: INFO: Waiting up to 5m0s for pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848" in namespace "containers-1930" to be "Succeeded or Failed"
    Mar 23 18:47:35.018: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076882ms
    Mar 23 18:47:37.023: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Running", Reason="", readiness=true. Elapsed: 2.012928769s
    Mar 23 18:47:39.022: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Running", Reason="", readiness=false. Elapsed: 4.012546752s
    Mar 23 18:47:41.022: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012658434s
    STEP: Saw pod success 03/23/23 18:47:41.023
    Mar 23 18:47:41.023: INFO: Pod "client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848" satisfied condition "Succeeded or Failed"
    Mar 23 18:47:41.026: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:47:41.032
    Mar 23 18:47:41.048: INFO: Waiting for pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 to disappear
    Mar 23 18:47:41.051: INFO: Pod client-containers-3be95832-e22b-45bb-8d05-ed6cffe7b848 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 23 18:47:41.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1930" for this suite. 03/23/23 18:47:41.06
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:41.085
Mar 23 18:47:41.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 18:47:41.086
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:41.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:41.109
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-d49350b5-1461-425a-8437-f9f833989386 03/23/23 18:47:41.111
STEP: Creating a pod to test consume configMaps 03/23/23 18:47:41.115
Mar 23 18:47:41.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500" in namespace "projected-8166" to be "Succeeded or Failed"
Mar 23 18:47:41.128: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490091ms
Mar 23 18:47:43.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Running", Reason="", readiness=true. Elapsed: 2.008255062s
Mar 23 18:47:45.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Running", Reason="", readiness=false. Elapsed: 4.008290684s
Mar 23 18:47:47.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008938104s
STEP: Saw pod success 03/23/23 18:47:47.133
Mar 23 18:47:47.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500" satisfied condition "Succeeded or Failed"
Mar 23 18:47:47.137: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:47:47.144
Mar 23 18:47:47.175: INFO: Waiting for pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 to disappear
Mar 23 18:47:47.178: INFO: Pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 18:47:47.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8166" for this suite. 03/23/23 18:47:47.183
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":32,"skipped":599,"failed":0}
------------------------------
• [SLOW TEST] [6.105 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:41.085
    Mar 23 18:47:41.085: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 18:47:41.086
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:41.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:41.109
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-d49350b5-1461-425a-8437-f9f833989386 03/23/23 18:47:41.111
    STEP: Creating a pod to test consume configMaps 03/23/23 18:47:41.115
    Mar 23 18:47:41.124: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500" in namespace "projected-8166" to be "Succeeded or Failed"
    Mar 23 18:47:41.128: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Pending", Reason="", readiness=false. Elapsed: 3.490091ms
    Mar 23 18:47:43.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Running", Reason="", readiness=true. Elapsed: 2.008255062s
    Mar 23 18:47:45.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Running", Reason="", readiness=false. Elapsed: 4.008290684s
    Mar 23 18:47:47.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008938104s
    STEP: Saw pod success 03/23/23 18:47:47.133
    Mar 23 18:47:47.133: INFO: Pod "pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500" satisfied condition "Succeeded or Failed"
    Mar 23 18:47:47.137: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:47:47.144
    Mar 23 18:47:47.175: INFO: Waiting for pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 to disappear
    Mar 23 18:47:47.178: INFO: Pod pod-projected-configmaps-b45de8df-4b12-48d3-bc02-fbaec14f3500 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 18:47:47.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8166" for this suite. 03/23/23 18:47:47.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:47.199
Mar 23 18:47:47.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 18:47:47.2
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:47.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:47.224
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Mar 23 18:47:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 18:47:50.344
Mar 23 18:47:50.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 create -f -'
Mar 23 18:47:51.111: INFO: stderr: ""
Mar 23 18:47:51.111: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 18:47:51.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 delete e2e-test-crd-publish-openapi-1912-crds test-cr'
Mar 23 18:47:51.260: INFO: stderr: ""
Mar 23 18:47:51.260: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Mar 23 18:47:51.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 apply -f -'
Mar 23 18:47:51.906: INFO: stderr: ""
Mar 23 18:47:51.906: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Mar 23 18:47:51.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 delete e2e-test-crd-publish-openapi-1912-crds test-cr'
Mar 23 18:47:51.999: INFO: stderr: ""
Mar 23 18:47:51.999: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/23/23 18:47:51.999
Mar 23 18:47:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 explain e2e-test-crd-publish-openapi-1912-crds'
Mar 23 18:47:52.244: INFO: stderr: ""
Mar 23 18:47:52.244: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1912-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:47:55.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1424" for this suite. 03/23/23 18:47:55.227
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":33,"skipped":622,"failed":0}
------------------------------
• [SLOW TEST] [8.033 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:47.199
    Mar 23 18:47:47.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 18:47:47.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:47.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:47.224
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Mar 23 18:47:47.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 18:47:50.344
    Mar 23 18:47:50.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 create -f -'
    Mar 23 18:47:51.111: INFO: stderr: ""
    Mar 23 18:47:51.111: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 23 18:47:51.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 delete e2e-test-crd-publish-openapi-1912-crds test-cr'
    Mar 23 18:47:51.260: INFO: stderr: ""
    Mar 23 18:47:51.260: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Mar 23 18:47:51.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 apply -f -'
    Mar 23 18:47:51.906: INFO: stderr: ""
    Mar 23 18:47:51.906: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Mar 23 18:47:51.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 --namespace=crd-publish-openapi-1424 delete e2e-test-crd-publish-openapi-1912-crds test-cr'
    Mar 23 18:47:51.999: INFO: stderr: ""
    Mar 23 18:47:51.999: INFO: stdout: "e2e-test-crd-publish-openapi-1912-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/23/23 18:47:51.999
    Mar 23 18:47:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-1424 explain e2e-test-crd-publish-openapi-1912-crds'
    Mar 23 18:47:52.244: INFO: stderr: ""
    Mar 23 18:47:52.244: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1912-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:47:55.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1424" for this suite. 03/23/23 18:47:55.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:47:55.24
Mar 23 18:47:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 18:47:55.241
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:55.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:55.261
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 18:47:55.28
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:47:56.026
STEP: Deploying the webhook pod 03/23/23 18:47:56.034
STEP: Wait for the deployment to be ready 03/23/23 18:47:56.051
Mar 23 18:47:56.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 18:47:58.08
STEP: Verifying the service has paired with the endpoint 03/23/23 18:47:58.09
Mar 23 18:47:59.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/23/23 18:47:59.094
STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:47:59.094
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/23/23 18:47:59.112
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/23/23 18:48:00.122
STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:00.122
STEP: Having no error when timeout is longer than webhook latency 03/23/23 18:48:01.157
STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:01.157
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/23/23 18:48:06.207
STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:06.207
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 18:48:11.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5654" for this suite. 03/23/23 18:48:11.242
STEP: Destroying namespace "webhook-5654-markers" for this suite. 03/23/23 18:48:11.249
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":34,"skipped":648,"failed":0}
------------------------------
• [SLOW TEST] [16.104 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:47:55.24
    Mar 23 18:47:55.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 18:47:55.241
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:47:55.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:47:55.261
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 18:47:55.28
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 18:47:56.026
    STEP: Deploying the webhook pod 03/23/23 18:47:56.034
    STEP: Wait for the deployment to be ready 03/23/23 18:47:56.051
    Mar 23 18:47:56.069: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 18:47:58.08
    STEP: Verifying the service has paired with the endpoint 03/23/23 18:47:58.09
    Mar 23 18:47:59.091: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 03/23/23 18:47:59.094
    STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:47:59.094
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 03/23/23 18:47:59.112
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 03/23/23 18:48:00.122
    STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:00.122
    STEP: Having no error when timeout is longer than webhook latency 03/23/23 18:48:01.157
    STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:01.157
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 03/23/23 18:48:06.207
    STEP: Registering slow webhook via the AdmissionRegistration API 03/23/23 18:48:06.207
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 18:48:11.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5654" for this suite. 03/23/23 18:48:11.242
    STEP: Destroying namespace "webhook-5654-markers" for this suite. 03/23/23 18:48:11.249
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:48:11.349
Mar 23 18:48:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename subpath 03/23/23 18:48:11.351
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:11.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:11.402
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/23/23 18:48:11.42
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-fsc9 03/23/23 18:48:11.449
STEP: Creating a pod to test atomic-volume-subpath 03/23/23 18:48:11.449
Mar 23 18:48:11.467: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fsc9" in namespace "subpath-1800" to be "Succeeded or Failed"
Mar 23 18:48:11.473: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.769183ms
Mar 23 18:48:13.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011350243s
Mar 23 18:48:15.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011405638s
Mar 23 18:48:17.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010642937s
Mar 23 18:48:19.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 8.010888335s
Mar 23 18:48:21.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 10.010859133s
Mar 23 18:48:23.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010488799s
Mar 23 18:48:25.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011455723s
Mar 23 18:48:27.479: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012048447s
Mar 23 18:48:29.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 18.010728476s
Mar 23 18:48:31.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 20.011234214s
Mar 23 18:48:33.479: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 22.01200864s
Mar 23 18:48:35.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 24.011603669s
Mar 23 18:48:37.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=false. Elapsed: 26.011826997s
Mar 23 18:48:39.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.011112684s
STEP: Saw pod success 03/23/23 18:48:39.478
Mar 23 18:48:39.478: INFO: Pod "pod-subpath-test-projected-fsc9" satisfied condition "Succeeded or Failed"
Mar 23 18:48:39.481: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-projected-fsc9 container test-container-subpath-projected-fsc9: <nil>
STEP: delete the pod 03/23/23 18:48:39.542
Mar 23 18:48:39.562: INFO: Waiting for pod pod-subpath-test-projected-fsc9 to disappear
Mar 23 18:48:39.571: INFO: Pod pod-subpath-test-projected-fsc9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-fsc9 03/23/23 18:48:39.571
Mar 23 18:48:39.571: INFO: Deleting pod "pod-subpath-test-projected-fsc9" in namespace "subpath-1800"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 23 18:48:39.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1800" for this suite. 03/23/23 18:48:39.58
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":35,"skipped":652,"failed":0}
------------------------------
• [SLOW TEST] [28.237 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:48:11.349
    Mar 23 18:48:11.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename subpath 03/23/23 18:48:11.351
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:11.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:11.402
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/23/23 18:48:11.42
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-fsc9 03/23/23 18:48:11.449
    STEP: Creating a pod to test atomic-volume-subpath 03/23/23 18:48:11.449
    Mar 23 18:48:11.467: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-fsc9" in namespace "subpath-1800" to be "Succeeded or Failed"
    Mar 23 18:48:11.473: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.769183ms
    Mar 23 18:48:13.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011350243s
    Mar 23 18:48:15.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011405638s
    Mar 23 18:48:17.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010642937s
    Mar 23 18:48:19.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 8.010888335s
    Mar 23 18:48:21.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 10.010859133s
    Mar 23 18:48:23.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 12.010488799s
    Mar 23 18:48:25.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 14.011455723s
    Mar 23 18:48:27.479: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 16.012048447s
    Mar 23 18:48:29.477: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 18.010728476s
    Mar 23 18:48:31.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 20.011234214s
    Mar 23 18:48:33.479: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 22.01200864s
    Mar 23 18:48:35.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=true. Elapsed: 24.011603669s
    Mar 23 18:48:37.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Running", Reason="", readiness=false. Elapsed: 26.011826997s
    Mar 23 18:48:39.478: INFO: Pod "pod-subpath-test-projected-fsc9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.011112684s
    STEP: Saw pod success 03/23/23 18:48:39.478
    Mar 23 18:48:39.478: INFO: Pod "pod-subpath-test-projected-fsc9" satisfied condition "Succeeded or Failed"
    Mar 23 18:48:39.481: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-projected-fsc9 container test-container-subpath-projected-fsc9: <nil>
    STEP: delete the pod 03/23/23 18:48:39.542
    Mar 23 18:48:39.562: INFO: Waiting for pod pod-subpath-test-projected-fsc9 to disappear
    Mar 23 18:48:39.571: INFO: Pod pod-subpath-test-projected-fsc9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-fsc9 03/23/23 18:48:39.571
    Mar 23 18:48:39.571: INFO: Deleting pod "pod-subpath-test-projected-fsc9" in namespace "subpath-1800"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 23 18:48:39.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1800" for this suite. 03/23/23 18:48:39.58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:48:39.59
Mar 23 18:48:39.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 18:48:39.591
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:39.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:39.608
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 03/23/23 18:48:39.619
STEP: waiting for Deployment to be created 03/23/23 18:48:39.625
STEP: waiting for all Replicas to be Ready 03/23/23 18:48:39.629
Mar 23 18:48:39.633: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.633: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.652: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.652: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:39.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Mar 23 18:48:41.022: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 23 18:48:41.022: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Mar 23 18:48:41.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 03/23/23 18:48:41.695
W0323 18:48:41.703087      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 23 18:48:41.705: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 03/23/23 18:48:41.705
Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.723: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.723: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.755: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.756: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.783: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.783: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:41.793: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:41.794: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:44.052: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:44.052: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:44.092: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
STEP: listing Deployments 03/23/23 18:48:44.092
Mar 23 18:48:44.097: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 03/23/23 18:48:44.097
Mar 23 18:48:44.120: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 03/23/23 18:48:44.12
Mar 23 18:48:44.133: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:44.138: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:44.171: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:44.213: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:45.918: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:45.967: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:45.983: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:45.997: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:46.009: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Mar 23 18:48:49.770: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 03/23/23 18:48:49.817
STEP: fetching the DeploymentStatus 03/23/23 18:48:49.825
Mar 23 18:48:49.829: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:49.831: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:49.831: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 3
STEP: deleting the Deployment 03/23/23 18:48:49.832
Mar 23 18:48:49.850: INFO: observed event type MODIFIED
Mar 23 18:48:49.850: INFO: observed event type MODIFIED
Mar 23 18:48:49.850: INFO: observed event type MODIFIED
Mar 23 18:48:49.850: INFO: observed event type MODIFIED
Mar 23 18:48:49.850: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
Mar 23 18:48:49.851: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 18:48:49.869: INFO: Log out all the ReplicaSets if there is no deployment created
Mar 23 18:48:49.873: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7897  cc265cc6-8ac2-4571-818c-8e21d5baa854 6992 4 2023-03-23 18:48:41 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f687 0xc004a5f688}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Mar 23 18:48:49.879: INFO: pod: "test-deployment-54cc775c4b-68dms":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-68dms test-deployment-54cc775c4b- deployment-7897  ce636ad7-fb8b-4166-bba6-27d504531aff 6988 0 2023-03-23 18:48:41 +0000 UTC 2023-03-23 18:48:50 +0000 UTC 0xc004a5fd40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b cc265cc6-8ac2-4571-818c-8e21d5baa854 0xc004a5fd77 0xc004a5fd78}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc265cc6-8ac2-4571-818c-8e21d5baa854\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kg67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kg67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.17,StartTime:2023-03-23 18:48:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://3c9dd076dbc4cd9b0f4cc7ba1c96b4880fe42180ff2d5a4d5970a9ac58d6ca69,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 23 18:48:49.879: INFO: pod: "test-deployment-54cc775c4b-lqwsc":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-lqwsc test-deployment-54cc775c4b- deployment-7897  6150240b-f74d-47fa-99c0-f825a0d3f77e 6976 0 2023-03-23 18:48:44 +0000 UTC 2023-03-23 18:48:46 +0000 UTC 0xc004cd0110 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b cc265cc6-8ac2-4571-818c-8e21d5baa854 0xc004cd0177 0xc004cd0178}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc265cc6-8ac2-4571-818c-8e21d5baa854\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xgwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xgwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.70,StartTime:2023-03-23 18:48:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://c564e94fe2b670342f8352e38e1de1ebd1a166d4678f276323f5a4f0014d2e03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 23 18:48:49.879: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-7897  39c7ba98-9f22-4609-9774-b6059789fc2f 6984 2 2023-03-23 18:48:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f787 0xc004a5f788}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f840 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Mar 23 18:48:49.886: INFO: pod: "test-deployment-7c7d8d58c8-qwdvv":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-qwdvv test-deployment-7c7d8d58c8- deployment-7897  239f0609-7289-47ee-9622-a12183680a33 6935 0 2023-03-23 18:48:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 39c7ba98-9f22-4609-9774-b6059789fc2f 0xc004cd1c67 0xc004cd1c68}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39c7ba98-9f22-4609-9774-b6059789fc2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g25d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g25d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.50,StartTime:2023-03-23 18:48:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e4325e6f83b4a8563a60b9fdd72fe9e2dad42c5f0033cf9c5af10b9cf224b119,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 23 18:48:49.886: INFO: pod: "test-deployment-7c7d8d58c8-sh5cg":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-sh5cg test-deployment-7c7d8d58c8- deployment-7897  014507b1-d436-4136-9e9e-5581eebf152a 6983 0 2023-03-23 18:48:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 39c7ba98-9f22-4609-9774-b6059789fc2f 0xc004cd1e47 0xc004cd1e48}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39c7ba98-9f22-4609-9774-b6059789fc2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwk4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwk4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.72,StartTime:2023-03-23 18:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://01e368a5e9d79a71b11ef882f79a8c57b94a4664c5ec9816d9d06590dd65e694,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Mar 23 18:48:49.887: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7897  4f7eed51-cb9b-410a-8a2f-a827255b7676 6894 3 2023-03-23 18:48:39 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f8b7 0xc004a5f8b8}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f970 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 18:48:49.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7897" for this suite. 03/23/23 18:48:49.899
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":36,"skipped":659,"failed":0}
------------------------------
• [SLOW TEST] [10.332 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:48:39.59
    Mar 23 18:48:39.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 18:48:39.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:39.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:39.608
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 03/23/23 18:48:39.619
    STEP: waiting for Deployment to be created 03/23/23 18:48:39.625
    STEP: waiting for all Replicas to be Ready 03/23/23 18:48:39.629
    Mar 23 18:48:39.633: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.633: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.652: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.652: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:39.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Mar 23 18:48:41.022: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 23 18:48:41.022: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Mar 23 18:48:41.695: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 03/23/23 18:48:41.695
    W0323 18:48:41.703087      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 23 18:48:41.705: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 03/23/23 18:48:41.705
    Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.707: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.708: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 0
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.709: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.723: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.723: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.755: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.756: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.783: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.783: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:41.793: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:41.794: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:44.052: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:44.052: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:44.092: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    STEP: listing Deployments 03/23/23 18:48:44.092
    Mar 23 18:48:44.097: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 03/23/23 18:48:44.097
    Mar 23 18:48:44.120: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 03/23/23 18:48:44.12
    Mar 23 18:48:44.133: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:44.138: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:44.171: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:44.213: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:45.918: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:45.967: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:45.983: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:45.997: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:46.009: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Mar 23 18:48:49.770: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 03/23/23 18:48:49.817
    STEP: fetching the DeploymentStatus 03/23/23 18:48:49.825
    Mar 23 18:48:49.829: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 1
    Mar 23 18:48:49.830: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:49.831: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:49.831: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 2
    Mar 23 18:48:49.832: INFO: observed Deployment test-deployment in namespace deployment-7897 with ReadyReplicas 3
    STEP: deleting the Deployment 03/23/23 18:48:49.832
    Mar 23 18:48:49.850: INFO: observed event type MODIFIED
    Mar 23 18:48:49.850: INFO: observed event type MODIFIED
    Mar 23 18:48:49.850: INFO: observed event type MODIFIED
    Mar 23 18:48:49.850: INFO: observed event type MODIFIED
    Mar 23 18:48:49.850: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    Mar 23 18:48:49.851: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 18:48:49.869: INFO: Log out all the ReplicaSets if there is no deployment created
    Mar 23 18:48:49.873: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-7897  cc265cc6-8ac2-4571-818c-8e21d5baa854 6992 4 2023-03-23 18:48:41 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f687 0xc004a5f688}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f720 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Mar 23 18:48:49.879: INFO: pod: "test-deployment-54cc775c4b-68dms":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-68dms test-deployment-54cc775c4b- deployment-7897  ce636ad7-fb8b-4166-bba6-27d504531aff 6988 0 2023-03-23 18:48:41 +0000 UTC 2023-03-23 18:48:50 +0000 UTC 0xc004a5fd40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b cc265cc6-8ac2-4571-818c-8e21d5baa854 0xc004a5fd77 0xc004a5fd78}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc265cc6-8ac2-4571-818c-8e21d5baa854\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kg67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kg67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.17,StartTime:2023-03-23 18:48:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://3c9dd076dbc4cd9b0f4cc7ba1c96b4880fe42180ff2d5a4d5970a9ac58d6ca69,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 23 18:48:49.879: INFO: pod: "test-deployment-54cc775c4b-lqwsc":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-lqwsc test-deployment-54cc775c4b- deployment-7897  6150240b-f74d-47fa-99c0-f825a0d3f77e 6976 0 2023-03-23 18:48:44 +0000 UTC 2023-03-23 18:48:46 +0000 UTC 0xc004cd0110 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b cc265cc6-8ac2-4571-818c-8e21d5baa854 0xc004cd0177 0xc004cd0178}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc265cc6-8ac2-4571-818c-8e21d5baa854\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4xgwf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4xgwf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.70,StartTime:2023-03-23 18:48:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://c564e94fe2b670342f8352e38e1de1ebd1a166d4678f276323f5a4f0014d2e03,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 23 18:48:49.879: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-7897  39c7ba98-9f22-4609-9774-b6059789fc2f 6984 2 2023-03-23 18:48:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f787 0xc004a5f788}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f840 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Mar 23 18:48:49.886: INFO: pod: "test-deployment-7c7d8d58c8-qwdvv":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-qwdvv test-deployment-7c7d8d58c8- deployment-7897  239f0609-7289-47ee-9622-a12183680a33 6935 0 2023-03-23 18:48:44 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 39c7ba98-9f22-4609-9774-b6059789fc2f 0xc004cd1c67 0xc004cd1c68}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39c7ba98-9f22-4609-9774-b6059789fc2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g25d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g25d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.50,StartTime:2023-03-23 18:48:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://e4325e6f83b4a8563a60b9fdd72fe9e2dad42c5f0033cf9c5af10b9cf224b119,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 23 18:48:49.886: INFO: pod: "test-deployment-7c7d8d58c8-sh5cg":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-sh5cg test-deployment-7c7d8d58c8- deployment-7897  014507b1-d436-4136-9e9e-5581eebf152a 6983 0 2023-03-23 18:48:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 39c7ba98-9f22-4609-9774-b6059789fc2f 0xc004cd1e47 0xc004cd1e48}] [] [{kube-controller-manager Update v1 2023-03-23 18:48:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39c7ba98-9f22-4609-9774-b6059789fc2f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 18:48:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zwk4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zwk4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 18:48:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.72,StartTime:2023-03-23 18:48:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 18:48:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://01e368a5e9d79a71b11ef882f79a8c57b94a4664c5ec9816d9d06590dd65e694,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Mar 23 18:48:49.887: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-7897  4f7eed51-cb9b-410a-8a2f-a827255b7676 6894 3 2023-03-23 18:48:39 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 427d1160-ed8c-45aa-8df7-5966ff9ea078 0xc004a5f8b7 0xc004a5f8b8}] [] [{kube-controller-manager Update apps/v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"427d1160-ed8c-45aa-8df7-5966ff9ea078\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 18:48:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004a5f970 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 18:48:49.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7897" for this suite. 03/23/23 18:48:49.899
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:48:49.922
Mar 23 18:48:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename init-container 03/23/23 18:48:49.924
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:49.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:49.939
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 03/23/23 18:48:49.945
Mar 23 18:48:49.946: INFO: PodSpec: initContainers in spec.initContainers
Mar 23 18:49:42.157: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b1f300f7-7f07-4c62-816d-4f7da19d3fde", GenerateName:"", Namespace:"init-container-9883", SelfLink:"", UID:"57a69f4f-f5c9-46b4-bbad-95e5300c8f9e", ResourceVersion:"7273", Generation:0, CreationTimestamp:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"946223743"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dc24c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 23, 18, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dc24f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lmjh4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003c9b7c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004d18f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-16392394-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000029a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d18f90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d18fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004d18fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004d18fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004dc6f70), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.4", PodIP:"10.240.0.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.240.0.29"}}, StartTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000029b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000029b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a3de4ee89c7d53e5218332ceb5a4dd113509b2384b802e38cd1d7b91b1ff4f74", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c9b840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c9b820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004d1904f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 18:49:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9883" for this suite. 03/23/23 18:49:42.165
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":37,"skipped":659,"failed":0}
------------------------------
• [SLOW TEST] [52.253 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:48:49.922
    Mar 23 18:48:49.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename init-container 03/23/23 18:48:49.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:48:49.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:48:49.939
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 03/23/23 18:48:49.945
    Mar 23 18:48:49.946: INFO: PodSpec: initContainers in spec.initContainers
    Mar 23 18:49:42.157: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b1f300f7-7f07-4c62-816d-4f7da19d3fde", GenerateName:"", Namespace:"init-container-9883", SelfLink:"", UID:"57a69f4f-f5c9-46b4-bbad-95e5300c8f9e", ResourceVersion:"7273", Generation:0, CreationTimestamp:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"946223743"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dc24c8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.March, 23, 18, 49, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dc24f8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lmjh4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc003c9b7c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lmjh4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004d18f18), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-linuxpool-16392394-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000029a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d18f90)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004d18fc0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004d18fc8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004d18fcc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004dc6f70), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.240.0.4", PodIP:"10.240.0.29", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.240.0.29"}}, StartTime:time.Date(2023, time.March, 23, 18, 48, 49, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000029b20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000029b90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://a3de4ee89c7d53e5218332ceb5a4dd113509b2384b802e38cd1d7b91b1ff4f74", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c9b840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003c9b820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004d1904f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 18:49:42.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9883" for this suite. 03/23/23 18:49:42.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:49:42.184
Mar 23 18:49:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 18:49:42.185
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:42.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:42.209
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 03/23/23 18:49:42.213
Mar 23 18:49:42.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa" in namespace "projected-324" to be "Succeeded or Failed"
Mar 23 18:49:42.230: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27319ms
Mar 23 18:49:44.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759262s
Mar 23 18:49:46.237: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Running", Reason="", readiness=false. Elapsed: 4.010601554s
Mar 23 18:49:48.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008929754s
STEP: Saw pod success 03/23/23 18:49:48.235
Mar 23 18:49:48.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa" satisfied condition "Succeeded or Failed"
Mar 23 18:49:48.238: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa container client-container: <nil>
STEP: delete the pod 03/23/23 18:49:48.276
Mar 23 18:49:48.289: INFO: Waiting for pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa to disappear
Mar 23 18:49:48.292: INFO: Pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 18:49:48.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-324" for this suite. 03/23/23 18:49:48.297
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":38,"skipped":694,"failed":0}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:49:42.184
    Mar 23 18:49:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 18:49:42.185
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:42.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:42.209
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 03/23/23 18:49:42.213
    Mar 23 18:49:42.226: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa" in namespace "projected-324" to be "Succeeded or Failed"
    Mar 23 18:49:42.230: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27319ms
    Mar 23 18:49:44.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Running", Reason="", readiness=true. Elapsed: 2.008759262s
    Mar 23 18:49:46.237: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Running", Reason="", readiness=false. Elapsed: 4.010601554s
    Mar 23 18:49:48.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008929754s
    STEP: Saw pod success 03/23/23 18:49:48.235
    Mar 23 18:49:48.235: INFO: Pod "downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa" satisfied condition "Succeeded or Failed"
    Mar 23 18:49:48.238: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa container client-container: <nil>
    STEP: delete the pod 03/23/23 18:49:48.276
    Mar 23 18:49:48.289: INFO: Waiting for pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa to disappear
    Mar 23 18:49:48.292: INFO: Pod downwardapi-volume-5801b729-a60c-494b-919f-6e9155f467aa no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 18:49:48.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-324" for this suite. 03/23/23 18:49:48.297
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:49:48.309
Mar 23 18:49:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption 03/23/23 18:49:48.313
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:48.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:48.329
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 03/23/23 18:49:48.337
STEP: Waiting for the pdb to be processed 03/23/23 18:49:48.344
STEP: updating the pdb 03/23/23 18:49:50.356
STEP: Waiting for the pdb to be processed 03/23/23 18:49:50.364
STEP: patching the pdb 03/23/23 18:49:50.37
STEP: Waiting for the pdb to be processed 03/23/23 18:49:50.389
STEP: Waiting for the pdb to be deleted 03/23/23 18:49:50.401
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 23 18:49:50.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2695" for this suite. 03/23/23 18:49:50.416
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":39,"skipped":694,"failed":0}
------------------------------
• [2.113 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:49:48.309
    Mar 23 18:49:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption 03/23/23 18:49:48.313
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:48.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:48.329
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 03/23/23 18:49:48.337
    STEP: Waiting for the pdb to be processed 03/23/23 18:49:48.344
    STEP: updating the pdb 03/23/23 18:49:50.356
    STEP: Waiting for the pdb to be processed 03/23/23 18:49:50.364
    STEP: patching the pdb 03/23/23 18:49:50.37
    STEP: Waiting for the pdb to be processed 03/23/23 18:49:50.389
    STEP: Waiting for the pdb to be deleted 03/23/23 18:49:50.401
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 23 18:49:50.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2695" for this suite. 03/23/23 18:49:50.416
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:49:50.427
Mar 23 18:49:50.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename namespaces 03/23/23 18:49:50.428
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:50.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:50.449
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 03/23/23 18:49:50.455
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:50.476
STEP: Creating a service in the namespace 03/23/23 18:49:50.481
STEP: Deleting the namespace 03/23/23 18:49:50.495
STEP: Waiting for the namespace to be removed. 03/23/23 18:49:50.526
STEP: Recreating the namespace 03/23/23 18:49:56.53
STEP: Verifying there is no service in the namespace 03/23/23 18:49:56.548
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:49:56.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5058" for this suite. 03/23/23 18:49:56.559
STEP: Destroying namespace "nsdeletetest-887" for this suite. 03/23/23 18:49:56.566
Mar 23 18:49:56.573: INFO: Namespace nsdeletetest-887 was already deleted
STEP: Destroying namespace "nsdeletetest-8597" for this suite. 03/23/23 18:49:56.573
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":40,"skipped":704,"failed":0}
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:49:50.427
    Mar 23 18:49:50.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename namespaces 03/23/23 18:49:50.428
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:50.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:50.449
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 03/23/23 18:49:50.455
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:50.476
    STEP: Creating a service in the namespace 03/23/23 18:49:50.481
    STEP: Deleting the namespace 03/23/23 18:49:50.495
    STEP: Waiting for the namespace to be removed. 03/23/23 18:49:50.526
    STEP: Recreating the namespace 03/23/23 18:49:56.53
    STEP: Verifying there is no service in the namespace 03/23/23 18:49:56.548
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:49:56.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5058" for this suite. 03/23/23 18:49:56.559
    STEP: Destroying namespace "nsdeletetest-887" for this suite. 03/23/23 18:49:56.566
    Mar 23 18:49:56.573: INFO: Namespace nsdeletetest-887 was already deleted
    STEP: Destroying namespace "nsdeletetest-8597" for this suite. 03/23/23 18:49:56.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:49:56.588
Mar 23 18:49:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 18:49:56.59
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:56.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:56.616
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/23/23 18:49:56.622
Mar 23 18:49:56.645: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-518  a8905872-8bfc-442d-a6cb-753af6cb2cc8 7422 0 2023-03-23 18:49:56 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-23 18:49:56 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s88x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s88x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 18:49:56.645: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-518" to be "running and ready"
Mar 23 18:49:56.649: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398391ms
Mar 23 18:49:56.649: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:49:58.654: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009166385s
Mar 23 18:49:58.654: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Mar 23 18:49:58.654: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 03/23/23 18:49:58.654
Mar 23 18:49:58.654: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-518 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:49:58.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:49:58.655: INFO: ExecWithOptions: Clientset creation
Mar 23 18:49:58.655: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-518/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 03/23/23 18:49:58.808
Mar 23 18:49:58.808: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-518 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:49:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:49:58.809: INFO: ExecWithOptions: Clientset creation
Mar 23 18:49:58.809: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-518/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 18:49:58.935: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 18:49:58.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-518" for this suite. 03/23/23 18:49:58.969
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":41,"skipped":741,"failed":0}
------------------------------
• [2.388 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:49:56.588
    Mar 23 18:49:56.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 18:49:56.59
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:56.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:56.616
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 03/23/23 18:49:56.622
    Mar 23 18:49:56.645: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-518  a8905872-8bfc-442d-a6cb-753af6cb2cc8 7422 0 2023-03-23 18:49:56 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-03-23 18:49:56 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s88x5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s88x5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 18:49:56.645: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-518" to be "running and ready"
    Mar 23 18:49:56.649: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 4.398391ms
    Mar 23 18:49:56.649: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:49:58.654: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.009166385s
    Mar 23 18:49:58.654: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Mar 23 18:49:58.654: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 03/23/23 18:49:58.654
    Mar 23 18:49:58.654: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-518 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:49:58.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:49:58.655: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:49:58.655: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-518/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 03/23/23 18:49:58.808
    Mar 23 18:49:58.808: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-518 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:49:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:49:58.809: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:49:58.809: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/dns-518/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 18:49:58.935: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 18:49:58.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-518" for this suite. 03/23/23 18:49:58.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:49:58.977
Mar 23 18:49:58.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 18:49:58.979
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:58.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:58.998
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5460 03/23/23 18:49:59.004
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 03/23/23 18:49:59.017
Mar 23 18:49:59.060: INFO: Found 0 stateful pods, waiting for 3
Mar 23 18:50:09.066: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:09.066: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:09.066: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/23/23 18:50:09.081
Mar 23 18:50:09.116: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/23/23 18:50:09.116
STEP: Not applying an update when the partition is greater than the number of replicas 03/23/23 18:50:19.141
STEP: Performing a canary update 03/23/23 18:50:19.141
Mar 23 18:50:19.166: INFO: Updating stateful set ss2
Mar 23 18:50:19.180: INFO: Waiting for Pod statefulset-5460/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 03/23/23 18:50:29.189
Mar 23 18:50:29.267: INFO: Found 2 stateful pods, waiting for 3
Mar 23 18:50:39.272: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:39.272: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:39.272: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Mar 23 18:50:49.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:49.273: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:50:49.273: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 03/23/23 18:50:49.281
Mar 23 18:50:49.305: INFO: Updating stateful set ss2
Mar 23 18:50:49.323: INFO: Waiting for Pod statefulset-5460/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Mar 23 18:50:59.358: INFO: Updating stateful set ss2
Mar 23 18:50:59.365: INFO: Waiting for StatefulSet statefulset-5460/ss2 to complete update
Mar 23 18:50:59.366: INFO: Waiting for Pod statefulset-5460/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 18:51:09.374: INFO: Deleting all statefulset in ns statefulset-5460
Mar 23 18:51:09.377: INFO: Scaling statefulset ss2 to 0
Mar 23 18:51:29.408: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 18:51:29.411: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 18:51:29.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5460" for this suite. 03/23/23 18:51:29.44
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":42,"skipped":748,"failed":0}
------------------------------
• [SLOW TEST] [90.475 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:49:58.977
    Mar 23 18:49:58.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 18:49:58.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:49:58.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:49:58.998
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5460 03/23/23 18:49:59.004
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 03/23/23 18:49:59.017
    Mar 23 18:49:59.060: INFO: Found 0 stateful pods, waiting for 3
    Mar 23 18:50:09.066: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:09.066: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:09.066: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/23/23 18:50:09.081
    Mar 23 18:50:09.116: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/23/23 18:50:09.116
    STEP: Not applying an update when the partition is greater than the number of replicas 03/23/23 18:50:19.141
    STEP: Performing a canary update 03/23/23 18:50:19.141
    Mar 23 18:50:19.166: INFO: Updating stateful set ss2
    Mar 23 18:50:19.180: INFO: Waiting for Pod statefulset-5460/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 03/23/23 18:50:29.189
    Mar 23 18:50:29.267: INFO: Found 2 stateful pods, waiting for 3
    Mar 23 18:50:39.272: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:39.272: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:39.272: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
    Mar 23 18:50:49.273: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:49.273: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:50:49.273: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 03/23/23 18:50:49.281
    Mar 23 18:50:49.305: INFO: Updating stateful set ss2
    Mar 23 18:50:49.323: INFO: Waiting for Pod statefulset-5460/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Mar 23 18:50:59.358: INFO: Updating stateful set ss2
    Mar 23 18:50:59.365: INFO: Waiting for StatefulSet statefulset-5460/ss2 to complete update
    Mar 23 18:50:59.366: INFO: Waiting for Pod statefulset-5460/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 18:51:09.374: INFO: Deleting all statefulset in ns statefulset-5460
    Mar 23 18:51:09.377: INFO: Scaling statefulset ss2 to 0
    Mar 23 18:51:29.408: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 18:51:29.411: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 18:51:29.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5460" for this suite. 03/23/23 18:51:29.44
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:51:29.453
Mar 23 18:51:29.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-pred 03/23/23 18:51:29.454
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:29.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:29.477
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 23 18:51:29.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 18:51:29.501: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 18:51:29.508: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
Mar 23 18:51:29.533: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:51:29.533: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 18:51:29.533: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:51:29.533: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:51:29.533: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:51:29.533: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:51:29.533: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:51:29.533: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 18:51:29.533: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container e2e ready: true, restart count 0
Mar 23 18:51:29.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:51:29.533: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:51:29.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:51:29.533: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 18:51:29.533: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
Mar 23 18:51:29.565: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:51:29.565: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 18:51:29.565: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:51:29.565: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:51:29.565: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:51:29.565: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:51:29.565: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:51:29.565: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container metrics-server ready: true, restart count 0
Mar 23 18:51:29.565: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:51:29.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:51:29.565: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 18:51:29.565: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
Mar 23 18:51:29.593: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:51:29.594: INFO: azure-npm-gzmll from kube-system started at 2023-03-23 18:30:41 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 18:51:29.594: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:51:29.594: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container coredns ready: true, restart count 0
Mar 23 18:51:29.594: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:51:29.594: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:51:29.594: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:51:29.594: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:51:29.594: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:51:29.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:51:29.594: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 03/23/23 18:51:29.594
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.174f21049890db74], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 03/23/23 18:51:29.661
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:51:30.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9954" for this suite. 03/23/23 18:51:30.663
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":43,"skipped":759,"failed":0}
------------------------------
• [1.215 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:51:29.453
    Mar 23 18:51:29.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-pred 03/23/23 18:51:29.454
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:29.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:29.477
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 23 18:51:29.485: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 23 18:51:29.501: INFO: Waiting for terminating namespaces to be deleted...
    Mar 23 18:51:29.508: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
    Mar 23 18:51:29.533: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container e2e ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:51:29.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 18:51:29.533: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
    Mar 23 18:51:29.565: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:51:29.565: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 18:51:29.565: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
    Mar 23 18:51:29.593: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: azure-npm-gzmll from kube-system started at 2023-03-23 18:30:41 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container coredns ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:51:29.594: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:51:29.594: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 03/23/23 18:51:29.594
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.174f21049890db74], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 03/23/23 18:51:29.661
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:51:30.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9954" for this suite. 03/23/23 18:51:30.663
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:51:30.67
Mar 23 18:51:30.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 18:51:30.672
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:30.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:30.7
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-9201/configmap-test-8c7be921-58e3-4a50-9548-570d85b9d3c3 03/23/23 18:51:30.703
STEP: Creating a pod to test consume configMaps 03/23/23 18:51:30.711
Mar 23 18:51:30.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f" in namespace "configmap-9201" to be "Succeeded or Failed"
Mar 23 18:51:30.733: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.686783ms
Mar 23 18:51:32.737: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011784968s
Mar 23 18:51:34.737: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Running", Reason="", readiness=false. Elapsed: 4.011705662s
Mar 23 18:51:36.736: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011129105s
STEP: Saw pod success 03/23/23 18:51:36.736
Mar 23 18:51:36.736: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f" satisfied condition "Succeeded or Failed"
Mar 23 18:51:36.739: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f container env-test: <nil>
STEP: delete the pod 03/23/23 18:51:36.771
Mar 23 18:51:36.788: INFO: Waiting for pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f to disappear
Mar 23 18:51:36.792: INFO: Pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 18:51:36.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9201" for this suite. 03/23/23 18:51:36.797
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":44,"skipped":810,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:51:30.67
    Mar 23 18:51:30.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 18:51:30.672
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:30.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:30.7
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-9201/configmap-test-8c7be921-58e3-4a50-9548-570d85b9d3c3 03/23/23 18:51:30.703
    STEP: Creating a pod to test consume configMaps 03/23/23 18:51:30.711
    Mar 23 18:51:30.725: INFO: Waiting up to 5m0s for pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f" in namespace "configmap-9201" to be "Succeeded or Failed"
    Mar 23 18:51:30.733: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.686783ms
    Mar 23 18:51:32.737: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Running", Reason="", readiness=true. Elapsed: 2.011784968s
    Mar 23 18:51:34.737: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Running", Reason="", readiness=false. Elapsed: 4.011705662s
    Mar 23 18:51:36.736: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011129105s
    STEP: Saw pod success 03/23/23 18:51:36.736
    Mar 23 18:51:36.736: INFO: Pod "pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f" satisfied condition "Succeeded or Failed"
    Mar 23 18:51:36.739: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f container env-test: <nil>
    STEP: delete the pod 03/23/23 18:51:36.771
    Mar 23 18:51:36.788: INFO: Waiting for pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f to disappear
    Mar 23 18:51:36.792: INFO: Pod pod-configmaps-e22418c6-ed03-4933-9949-f2fdc04d822f no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 18:51:36.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9201" for this suite. 03/23/23 18:51:36.797
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:51:36.803
Mar 23 18:51:36.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 18:51:36.804
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:36.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:36.823
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 03/23/23 18:51:36.829
STEP: Creating a ResourceQuota 03/23/23 18:51:41.835
STEP: Ensuring resource quota status is calculated 03/23/23 18:51:41.842
STEP: Creating a ReplicaSet 03/23/23 18:51:43.845
STEP: Ensuring resource quota status captures replicaset creation 03/23/23 18:51:43.857
STEP: Deleting a ReplicaSet 03/23/23 18:51:45.861
STEP: Ensuring resource quota status released usage 03/23/23 18:51:45.866
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 18:51:47.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2010" for this suite. 03/23/23 18:51:47.873
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":45,"skipped":814,"failed":0}
------------------------------
• [SLOW TEST] [11.076 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:51:36.803
    Mar 23 18:51:36.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 18:51:36.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:36.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:36.823
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 03/23/23 18:51:36.829
    STEP: Creating a ResourceQuota 03/23/23 18:51:41.835
    STEP: Ensuring resource quota status is calculated 03/23/23 18:51:41.842
    STEP: Creating a ReplicaSet 03/23/23 18:51:43.845
    STEP: Ensuring resource quota status captures replicaset creation 03/23/23 18:51:43.857
    STEP: Deleting a ReplicaSet 03/23/23 18:51:45.861
    STEP: Ensuring resource quota status released usage 03/23/23 18:51:45.866
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 18:51:47.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2010" for this suite. 03/23/23 18:51:47.873
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:51:47.886
Mar 23 18:51:47.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename containers 03/23/23 18:51:47.888
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:47.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:47.912
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 03/23/23 18:51:47.915
Mar 23 18:51:47.931: INFO: Waiting up to 5m0s for pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd" in namespace "containers-7427" to be "Succeeded or Failed"
Mar 23 18:51:47.947: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.292373ms
Mar 23 18:51:49.952: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.021034762s
Mar 23 18:51:51.952: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=true. Elapsed: 4.021245752s
Mar 23 18:51:53.951: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=false. Elapsed: 6.020442638s
Mar 23 18:51:55.953: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02223892s
STEP: Saw pod success 03/23/23 18:51:55.954
Mar 23 18:51:55.954: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd" satisfied condition "Succeeded or Failed"
Mar 23 18:51:55.959: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd container agnhost-container: <nil>
STEP: delete the pod 03/23/23 18:51:55.966
Mar 23 18:51:55.982: INFO: Waiting for pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd to disappear
Mar 23 18:51:55.984: INFO: Pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 23 18:51:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7427" for this suite. 03/23/23 18:51:55.991
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":46,"skipped":816,"failed":0}
------------------------------
• [SLOW TEST] [8.112 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:51:47.886
    Mar 23 18:51:47.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename containers 03/23/23 18:51:47.888
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:47.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:47.912
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 03/23/23 18:51:47.915
    Mar 23 18:51:47.931: INFO: Waiting up to 5m0s for pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd" in namespace "containers-7427" to be "Succeeded or Failed"
    Mar 23 18:51:47.947: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.292373ms
    Mar 23 18:51:49.952: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.021034762s
    Mar 23 18:51:51.952: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=true. Elapsed: 4.021245752s
    Mar 23 18:51:53.951: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Running", Reason="", readiness=false. Elapsed: 6.020442638s
    Mar 23 18:51:55.953: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02223892s
    STEP: Saw pod success 03/23/23 18:51:55.954
    Mar 23 18:51:55.954: INFO: Pod "client-containers-08548978-df38-432e-8175-c1dd12ceb1dd" satisfied condition "Succeeded or Failed"
    Mar 23 18:51:55.959: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 18:51:55.966
    Mar 23 18:51:55.982: INFO: Waiting for pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd to disappear
    Mar 23 18:51:55.984: INFO: Pod client-containers-08548978-df38-432e-8175-c1dd12ceb1dd no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 23 18:51:55.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7427" for this suite. 03/23/23 18:51:55.991
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:51:56.005
Mar 23 18:51:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 18:51:56.007
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:56.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:56.029
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 03/23/23 18:51:56.032
Mar 23 18:51:56.047: INFO: Waiting up to 5m0s for pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e" in namespace "var-expansion-7070" to be "Succeeded or Failed"
Mar 23 18:51:56.064: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.133777ms
Mar 23 18:51:58.068: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020942554s
Mar 23 18:52:00.069: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021626365s
Mar 23 18:52:02.070: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022376699s
STEP: Saw pod success 03/23/23 18:52:02.07
Mar 23 18:52:02.070: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e" satisfied condition "Succeeded or Failed"
Mar 23 18:52:02.074: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e container dapi-container: <nil>
STEP: delete the pod 03/23/23 18:52:02.118
Mar 23 18:52:02.144: INFO: Waiting for pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e to disappear
Mar 23 18:52:02.147: INFO: Pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 18:52:02.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7070" for this suite. 03/23/23 18:52:02.155
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":47,"skipped":816,"failed":0}
------------------------------
• [SLOW TEST] [6.158 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:51:56.005
    Mar 23 18:51:56.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 18:51:56.007
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:51:56.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:51:56.029
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 03/23/23 18:51:56.032
    Mar 23 18:51:56.047: INFO: Waiting up to 5m0s for pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e" in namespace "var-expansion-7070" to be "Succeeded or Failed"
    Mar 23 18:51:56.064: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.133777ms
    Mar 23 18:51:58.068: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Running", Reason="", readiness=true. Elapsed: 2.020942554s
    Mar 23 18:52:00.069: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Running", Reason="", readiness=false. Elapsed: 4.021626365s
    Mar 23 18:52:02.070: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022376699s
    STEP: Saw pod success 03/23/23 18:52:02.07
    Mar 23 18:52:02.070: INFO: Pod "var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e" satisfied condition "Succeeded or Failed"
    Mar 23 18:52:02.074: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e container dapi-container: <nil>
    STEP: delete the pod 03/23/23 18:52:02.118
    Mar 23 18:52:02.144: INFO: Waiting for pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e to disappear
    Mar 23 18:52:02.147: INFO: Pod var-expansion-8268be47-1f25-49ed-94ce-d235e721e15e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 18:52:02.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7070" for this suite. 03/23/23 18:52:02.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:52:02.166
Mar 23 18:52:02.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 18:52:02.168
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:52:02.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:52:02.212
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 03/23/23 18:52:02.219
Mar 23 18:52:02.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-206 api-versions'
Mar 23 18:52:02.321: INFO: stderr: ""
Mar 23 18:52:02.321: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 18:52:02.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-206" for this suite. 03/23/23 18:52:02.328
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":48,"skipped":824,"failed":0}
------------------------------
• [0.172 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:52:02.166
    Mar 23 18:52:02.166: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 18:52:02.168
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:52:02.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:52:02.212
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 03/23/23 18:52:02.219
    Mar 23 18:52:02.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-206 api-versions'
    Mar 23 18:52:02.321: INFO: stderr: ""
    Mar 23 18:52:02.321: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 18:52:02.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-206" for this suite. 03/23/23 18:52:02.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:52:02.34
Mar 23 18:52:02.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 18:52:02.341
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:52:02.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:52:02.37
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-6500 03/23/23 18:52:02.375
W0323 18:52:02.401988      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Mar 23 18:52:02.402: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6500" to be "running and ready"
Mar 23 18:52:02.409: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204189ms
Mar 23 18:52:02.409: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:52:04.414: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011948217s
Mar 23 18:52:04.414: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 23 18:52:04.414: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 23 18:52:04.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 23 18:52:04.622: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 23 18:52:04.622: INFO: stdout: "iptables"
Mar 23 18:52:04.622: INFO: proxyMode: iptables
Mar 23 18:52:04.637: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 23 18:52:04.641: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-6500 03/23/23 18:52:04.641
STEP: creating replication controller affinity-nodeport-timeout in namespace services-6500 03/23/23 18:52:04.673
I0323 18:52:04.694677      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6500, replica count: 3
I0323 18:52:07.745503      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 18:52:10.747354      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 18:52:10.766: INFO: Creating new exec pod
Mar 23 18:52:10.775: INFO: Waiting up to 5m0s for pod "execpod-affinityrj8cl" in namespace "services-6500" to be "running"
Mar 23 18:52:10.781: INFO: Pod "execpod-affinityrj8cl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.423486ms
Mar 23 18:52:12.784: INFO: Pod "execpod-affinityrj8cl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009158407s
Mar 23 18:52:12.784: INFO: Pod "execpod-affinityrj8cl" satisfied condition "running"
Mar 23 18:52:13.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Mar 23 18:52:13.999: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Mar 23 18:52:13.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 18:52:13.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.239.164 80'
Mar 23 18:52:14.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.239.164 80\nConnection to 10.0.239.164 80 port [tcp/http] succeeded!\n"
Mar 23 18:52:14.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 18:52:14.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 32185'
Mar 23 18:52:14.418: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 32185\nConnection to 10.240.0.56 32185 port [tcp/*] succeeded!\n"
Mar 23 18:52:14.418: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 18:52:14.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 32185'
Mar 23 18:52:14.618: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.0.30 32185\nConnection to 10.240.0.30 32185 port [tcp/*] succeeded!\n"
Mar 23 18:52:14.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 18:52:14.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:32185/ ; done'
Mar 23 18:52:14.904: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
Mar 23 18:52:14.904: INFO: stdout: "\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp"
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
Mar 23 18:52:14.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
Mar 23 18:52:15.119: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
Mar 23 18:52:15.119: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
Mar 23 18:52:35.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
Mar 23 18:52:35.308: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
Mar 23 18:52:35.308: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
Mar 23 18:52:55.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
Mar 23 18:52:55.510: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
Mar 23 18:52:55.510: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
Mar 23 18:53:15.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
Mar 23 18:53:15.716: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
Mar 23 18:53:15.716: INFO: stdout: "affinity-nodeport-timeout-bzxsp"
Mar 23 18:53:15.716: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6500, will wait for the garbage collector to delete the pods 03/23/23 18:53:15.734
Mar 23 18:53:15.803: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.158576ms
Mar 23 18:53:15.905: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.336718ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 18:53:21.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6500" for this suite. 03/23/23 18:53:21.044
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":49,"skipped":834,"failed":0}
------------------------------
• [SLOW TEST] [78.710 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:52:02.34
    Mar 23 18:52:02.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 18:52:02.341
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:52:02.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:52:02.37
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-6500 03/23/23 18:52:02.375
    W0323 18:52:02.401988      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Mar 23 18:52:02.402: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-6500" to be "running and ready"
    Mar 23 18:52:02.409: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 7.204189ms
    Mar 23 18:52:02.409: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:52:04.414: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.011948217s
    Mar 23 18:52:04.414: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 23 18:52:04.414: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 23 18:52:04.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 23 18:52:04.622: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 23 18:52:04.622: INFO: stdout: "iptables"
    Mar 23 18:52:04.622: INFO: proxyMode: iptables
    Mar 23 18:52:04.637: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 23 18:52:04.641: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-6500 03/23/23 18:52:04.641
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-6500 03/23/23 18:52:04.673
    I0323 18:52:04.694677      19 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-6500, replica count: 3
    I0323 18:52:07.745503      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 18:52:10.747354      19 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 18:52:10.766: INFO: Creating new exec pod
    Mar 23 18:52:10.775: INFO: Waiting up to 5m0s for pod "execpod-affinityrj8cl" in namespace "services-6500" to be "running"
    Mar 23 18:52:10.781: INFO: Pod "execpod-affinityrj8cl": Phase="Pending", Reason="", readiness=false. Elapsed: 5.423486ms
    Mar 23 18:52:12.784: INFO: Pod "execpod-affinityrj8cl": Phase="Running", Reason="", readiness=true. Elapsed: 2.009158407s
    Mar 23 18:52:12.784: INFO: Pod "execpod-affinityrj8cl" satisfied condition "running"
    Mar 23 18:52:13.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Mar 23 18:52:13.999: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Mar 23 18:52:13.999: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 18:52:13.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.239.164 80'
    Mar 23 18:52:14.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.239.164 80\nConnection to 10.0.239.164 80 port [tcp/http] succeeded!\n"
    Mar 23 18:52:14.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 18:52:14.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 32185'
    Mar 23 18:52:14.418: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 32185\nConnection to 10.240.0.56 32185 port [tcp/*] succeeded!\n"
    Mar 23 18:52:14.418: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 18:52:14.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 32185'
    Mar 23 18:52:14.618: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.0.30 32185\nConnection to 10.240.0.30 32185 port [tcp/*] succeeded!\n"
    Mar 23 18:52:14.619: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 18:52:14.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:32185/ ; done'
    Mar 23 18:52:14.904: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
    Mar 23 18:52:14.904: INFO: stdout: "\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp\naffinity-nodeport-timeout-ktkpp"
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Received response from host: affinity-nodeport-timeout-ktkpp
    Mar 23 18:52:14.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
    Mar 23 18:52:15.119: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
    Mar 23 18:52:15.119: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
    Mar 23 18:52:35.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
    Mar 23 18:52:35.308: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
    Mar 23 18:52:35.308: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
    Mar 23 18:52:55.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
    Mar 23 18:52:55.510: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
    Mar 23 18:52:55.510: INFO: stdout: "affinity-nodeport-timeout-ktkpp"
    Mar 23 18:53:15.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6500 exec execpod-affinityrj8cl -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.240.0.30:32185/'
    Mar 23 18:53:15.716: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.240.0.30:32185/\n"
    Mar 23 18:53:15.716: INFO: stdout: "affinity-nodeport-timeout-bzxsp"
    Mar 23 18:53:15.716: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-6500, will wait for the garbage collector to delete the pods 03/23/23 18:53:15.734
    Mar 23 18:53:15.803: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.158576ms
    Mar 23 18:53:15.905: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.336718ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 18:53:21.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6500" for this suite. 03/23/23 18:53:21.044
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:53:21.058
Mar 23 18:53:21.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename events 03/23/23 18:53:21.061
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:21.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:21.084
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 03/23/23 18:53:21.093
STEP: listing events in all namespaces 03/23/23 18:53:21.1
STEP: listing events in test namespace 03/23/23 18:53:21.122
STEP: listing events with field selection filtering on source 03/23/23 18:53:21.125
STEP: listing events with field selection filtering on reportingController 03/23/23 18:53:21.127
STEP: getting the test event 03/23/23 18:53:21.13
STEP: patching the test event 03/23/23 18:53:21.133
STEP: getting the test event 03/23/23 18:53:21.141
STEP: updating the test event 03/23/23 18:53:21.144
STEP: getting the test event 03/23/23 18:53:21.148
STEP: deleting the test event 03/23/23 18:53:21.151
STEP: listing events in all namespaces 03/23/23 18:53:21.157
STEP: listing events in test namespace 03/23/23 18:53:21.167
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 23 18:53:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8095" for this suite. 03/23/23 18:53:21.174
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":50,"skipped":840,"failed":0}
------------------------------
• [0.125 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:53:21.058
    Mar 23 18:53:21.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename events 03/23/23 18:53:21.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:21.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:21.084
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 03/23/23 18:53:21.093
    STEP: listing events in all namespaces 03/23/23 18:53:21.1
    STEP: listing events in test namespace 03/23/23 18:53:21.122
    STEP: listing events with field selection filtering on source 03/23/23 18:53:21.125
    STEP: listing events with field selection filtering on reportingController 03/23/23 18:53:21.127
    STEP: getting the test event 03/23/23 18:53:21.13
    STEP: patching the test event 03/23/23 18:53:21.133
    STEP: getting the test event 03/23/23 18:53:21.141
    STEP: updating the test event 03/23/23 18:53:21.144
    STEP: getting the test event 03/23/23 18:53:21.148
    STEP: deleting the test event 03/23/23 18:53:21.151
    STEP: listing events in all namespaces 03/23/23 18:53:21.157
    STEP: listing events in test namespace 03/23/23 18:53:21.167
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 23 18:53:21.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8095" for this suite. 03/23/23 18:53:21.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:53:21.189
Mar 23 18:53:21.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 18:53:21.191
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:21.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:21.212
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 03/23/23 18:53:21.215
Mar 23 18:53:21.235: INFO: Waiting up to 5m0s for pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678" in namespace "var-expansion-1854" to be "Succeeded or Failed"
Mar 23 18:53:21.247: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Pending", Reason="", readiness=false. Elapsed: 12.091473ms
Mar 23 18:53:23.251: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Running", Reason="", readiness=true. Elapsed: 2.015779413s
Mar 23 18:53:25.252: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Running", Reason="", readiness=false. Elapsed: 4.017422358s
Mar 23 18:53:27.256: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020696499s
STEP: Saw pod success 03/23/23 18:53:27.256
Mar 23 18:53:27.256: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678" satisfied condition "Succeeded or Failed"
Mar 23 18:53:27.259: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 container dapi-container: <nil>
STEP: delete the pod 03/23/23 18:53:27.268
Mar 23 18:53:27.281: INFO: Waiting for pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 to disappear
Mar 23 18:53:27.285: INFO: Pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 18:53:27.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1854" for this suite. 03/23/23 18:53:27.29
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":51,"skipped":853,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:53:21.189
    Mar 23 18:53:21.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 18:53:21.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:21.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:21.212
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 03/23/23 18:53:21.215
    Mar 23 18:53:21.235: INFO: Waiting up to 5m0s for pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678" in namespace "var-expansion-1854" to be "Succeeded or Failed"
    Mar 23 18:53:21.247: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Pending", Reason="", readiness=false. Elapsed: 12.091473ms
    Mar 23 18:53:23.251: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Running", Reason="", readiness=true. Elapsed: 2.015779413s
    Mar 23 18:53:25.252: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Running", Reason="", readiness=false. Elapsed: 4.017422358s
    Mar 23 18:53:27.256: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020696499s
    STEP: Saw pod success 03/23/23 18:53:27.256
    Mar 23 18:53:27.256: INFO: Pod "var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678" satisfied condition "Succeeded or Failed"
    Mar 23 18:53:27.259: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 18:53:27.268
    Mar 23 18:53:27.281: INFO: Waiting for pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 to disappear
    Mar 23 18:53:27.285: INFO: Pod var-expansion-1f64b867-3db8-4e2d-836f-838062dcb678 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 18:53:27.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1854" for this suite. 03/23/23 18:53:27.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:53:27.306
Mar 23 18:53:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename cronjob 03/23/23 18:53:27.307
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:27.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:27.326
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 03/23/23 18:53:27.329
STEP: Ensuring a job is scheduled 03/23/23 18:53:27.336
STEP: Ensuring exactly one is scheduled 03/23/23 18:54:01.348
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/23/23 18:54:01.359
STEP: Ensuring the job is replaced with a new one 03/23/23 18:54:01.366
STEP: Removing cronjob 03/23/23 18:55:01.371
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 23 18:55:01.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5136" for this suite. 03/23/23 18:55:01.407
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":52,"skipped":868,"failed":0}
------------------------------
• [SLOW TEST] [94.115 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:53:27.306
    Mar 23 18:53:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename cronjob 03/23/23 18:53:27.307
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:53:27.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:53:27.326
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 03/23/23 18:53:27.329
    STEP: Ensuring a job is scheduled 03/23/23 18:53:27.336
    STEP: Ensuring exactly one is scheduled 03/23/23 18:54:01.348
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/23/23 18:54:01.359
    STEP: Ensuring the job is replaced with a new one 03/23/23 18:54:01.366
    STEP: Removing cronjob 03/23/23 18:55:01.371
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 23 18:55:01.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5136" for this suite. 03/23/23 18:55:01.407
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:55:01.429
Mar 23 18:55:01.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename ingressclass 03/23/23 18:55:01.432
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:01.515
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 03/23/23 18:55:01.521
STEP: getting /apis/networking.k8s.io 03/23/23 18:55:01.53
STEP: getting /apis/networking.k8s.iov1 03/23/23 18:55:01.536
STEP: creating 03/23/23 18:55:01.539
STEP: getting 03/23/23 18:55:01.659
STEP: listing 03/23/23 18:55:01.695
STEP: watching 03/23/23 18:55:01.702
Mar 23 18:55:01.703: INFO: starting watch
STEP: patching 03/23/23 18:55:01.713
STEP: updating 03/23/23 18:55:01.723
Mar 23 18:55:01.727: INFO: waiting for watch events with expected annotations
Mar 23 18:55:01.728: INFO: saw patched and updated annotations
STEP: deleting 03/23/23 18:55:01.728
STEP: deleting a collection 03/23/23 18:55:01.753
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Mar 23 18:55:01.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-2355" for this suite. 03/23/23 18:55:01.812
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":53,"skipped":872,"failed":0}
------------------------------
• [0.389 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:55:01.429
    Mar 23 18:55:01.429: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename ingressclass 03/23/23 18:55:01.432
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:01.515
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 03/23/23 18:55:01.521
    STEP: getting /apis/networking.k8s.io 03/23/23 18:55:01.53
    STEP: getting /apis/networking.k8s.iov1 03/23/23 18:55:01.536
    STEP: creating 03/23/23 18:55:01.539
    STEP: getting 03/23/23 18:55:01.659
    STEP: listing 03/23/23 18:55:01.695
    STEP: watching 03/23/23 18:55:01.702
    Mar 23 18:55:01.703: INFO: starting watch
    STEP: patching 03/23/23 18:55:01.713
    STEP: updating 03/23/23 18:55:01.723
    Mar 23 18:55:01.727: INFO: waiting for watch events with expected annotations
    Mar 23 18:55:01.728: INFO: saw patched and updated annotations
    STEP: deleting 03/23/23 18:55:01.728
    STEP: deleting a collection 03/23/23 18:55:01.753
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Mar 23 18:55:01.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-2355" for this suite. 03/23/23 18:55:01.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:55:01.826
Mar 23 18:55:01.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename namespaces 03/23/23 18:55:01.828
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:01.859
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 03/23/23 18:55:01.873
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.905
STEP: Creating a pod in the namespace 03/23/23 18:55:01.91
STEP: Waiting for the pod to have running status 03/23/23 18:55:01.94
Mar 23 18:55:01.940: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5922" to be "running"
Mar 23 18:55:01.954: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050667ms
Mar 23 18:55:03.958: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018112011s
Mar 23 18:55:05.960: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019771165s
Mar 23 18:55:05.960: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 03/23/23 18:55:05.96
STEP: Waiting for the namespace to be removed. 03/23/23 18:55:05.971
STEP: Recreating the namespace 03/23/23 18:55:16.975
STEP: Verifying there are no pods in the namespace 03/23/23 18:55:16.993
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:55:17.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5854" for this suite. 03/23/23 18:55:17.011
STEP: Destroying namespace "nsdeletetest-5922" for this suite. 03/23/23 18:55:17.018
Mar 23 18:55:17.026: INFO: Namespace nsdeletetest-5922 was already deleted
STEP: Destroying namespace "nsdeletetest-8496" for this suite. 03/23/23 18:55:17.026
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":54,"skipped":906,"failed":0}
------------------------------
• [SLOW TEST] [15.206 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:55:01.826
    Mar 23 18:55:01.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename namespaces 03/23/23 18:55:01.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:01.859
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 03/23/23 18:55:01.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:01.905
    STEP: Creating a pod in the namespace 03/23/23 18:55:01.91
    STEP: Waiting for the pod to have running status 03/23/23 18:55:01.94
    Mar 23 18:55:01.940: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5922" to be "running"
    Mar 23 18:55:01.954: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.050667ms
    Mar 23 18:55:03.958: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018112011s
    Mar 23 18:55:05.960: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019771165s
    Mar 23 18:55:05.960: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 03/23/23 18:55:05.96
    STEP: Waiting for the namespace to be removed. 03/23/23 18:55:05.971
    STEP: Recreating the namespace 03/23/23 18:55:16.975
    STEP: Verifying there are no pods in the namespace 03/23/23 18:55:16.993
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:55:17.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5854" for this suite. 03/23/23 18:55:17.011
    STEP: Destroying namespace "nsdeletetest-5922" for this suite. 03/23/23 18:55:17.018
    Mar 23 18:55:17.026: INFO: Namespace nsdeletetest-5922 was already deleted
    STEP: Destroying namespace "nsdeletetest-8496" for this suite. 03/23/23 18:55:17.026
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:55:17.034
Mar 23 18:55:17.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 18:55:17.035
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:17.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:17.062
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2116 03/23/23 18:55:17.065
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Mar 23 18:55:17.094: INFO: Found 0 stateful pods, waiting for 1
Mar 23 18:55:27.099: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 03/23/23 18:55:27.105
W0323 18:55:27.119670      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 23 18:55:27.125: INFO: Found 1 stateful pods, waiting for 2
Mar 23 18:55:37.129: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 18:55:37.129: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 03/23/23 18:55:37.135
STEP: Delete all of the StatefulSets 03/23/23 18:55:37.138
STEP: Verify that StatefulSets have been deleted 03/23/23 18:55:37.145
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 18:55:37.152: INFO: Deleting all statefulset in ns statefulset-2116
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 18:55:37.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2116" for this suite. 03/23/23 18:55:37.181
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":55,"skipped":906,"failed":0}
------------------------------
• [SLOW TEST] [20.167 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:55:17.034
    Mar 23 18:55:17.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 18:55:17.035
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:17.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:17.062
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2116 03/23/23 18:55:17.065
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Mar 23 18:55:17.094: INFO: Found 0 stateful pods, waiting for 1
    Mar 23 18:55:27.099: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 03/23/23 18:55:27.105
    W0323 18:55:27.119670      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 23 18:55:27.125: INFO: Found 1 stateful pods, waiting for 2
    Mar 23 18:55:37.129: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 18:55:37.129: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 03/23/23 18:55:37.135
    STEP: Delete all of the StatefulSets 03/23/23 18:55:37.138
    STEP: Verify that StatefulSets have been deleted 03/23/23 18:55:37.145
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 18:55:37.152: INFO: Deleting all statefulset in ns statefulset-2116
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 18:55:37.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2116" for this suite. 03/23/23 18:55:37.181
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:55:37.208
Mar 23 18:55:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename taint-multiple-pods 03/23/23 18:55:37.21
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:37.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:37.231
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Mar 23 18:55:37.234: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 18:56:37.342: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Mar 23 18:56:37.345: INFO: Starting informer...
STEP: Starting pods... 03/23/23 18:56:37.346
Mar 23 18:56:37.583: INFO: Pod1 is running on k8s-linuxpool-16392394-2. Tainting Node
Mar 23 18:56:37.798: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4407" to be "running"
Mar 23 18:56:37.801: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625492ms
Mar 23 18:56:39.812: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014001673s
Mar 23 18:56:41.805: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007827493s
Mar 23 18:56:41.805: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Mar 23 18:56:41.805: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4407" to be "running"
Mar 23 18:56:41.808: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.828994ms
Mar 23 18:56:41.808: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Mar 23 18:56:41.808: INFO: Pod2 is running on k8s-linuxpool-16392394-2. Tainting Node
STEP: Trying to apply a taint on the Node 03/23/23 18:56:41.808
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 18:56:41.826
STEP: Waiting for Pod1 and Pod2 to be deleted 03/23/23 18:56:41.83
Mar 23 18:56:48.074: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Mar 23 18:57:09.141: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 18:57:09.161
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:57:09.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-4407" for this suite. 03/23/23 18:57:09.183
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":56,"skipped":909,"failed":0}
------------------------------
• [SLOW TEST] [91.985 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:55:37.208
    Mar 23 18:55:37.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename taint-multiple-pods 03/23/23 18:55:37.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:55:37.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:55:37.231
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Mar 23 18:55:37.234: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 18:56:37.342: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Mar 23 18:56:37.345: INFO: Starting informer...
    STEP: Starting pods... 03/23/23 18:56:37.346
    Mar 23 18:56:37.583: INFO: Pod1 is running on k8s-linuxpool-16392394-2. Tainting Node
    Mar 23 18:56:37.798: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-4407" to be "running"
    Mar 23 18:56:37.801: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.625492ms
    Mar 23 18:56:39.812: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014001673s
    Mar 23 18:56:41.805: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 4.007827493s
    Mar 23 18:56:41.805: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Mar 23 18:56:41.805: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-4407" to be "running"
    Mar 23 18:56:41.808: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.828994ms
    Mar 23 18:56:41.808: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Mar 23 18:56:41.808: INFO: Pod2 is running on k8s-linuxpool-16392394-2. Tainting Node
    STEP: Trying to apply a taint on the Node 03/23/23 18:56:41.808
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 18:56:41.826
    STEP: Waiting for Pod1 and Pod2 to be deleted 03/23/23 18:56:41.83
    Mar 23 18:56:48.074: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Mar 23 18:57:09.141: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 18:57:09.161
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:57:09.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-4407" for this suite. 03/23/23 18:57:09.183
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:09.205
Mar 23 18:57:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-pred 03/23/23 18:57:09.206
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:09.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:09.239
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 23 18:57:09.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 18:57:09.253: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 18:57:09.256: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
Mar 23 18:57:09.273: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:57:09.273: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 18:57:09.273: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:57:09.273: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:57:09.273: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:57:09.273: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:57:09.273: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:57:09.273: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 18:57:09.273: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:57:09.273: INFO: 	Container e2e ready: true, restart count 0
Mar 23 18:57:09.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:57:09.274: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:57:09.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:57:09.274: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 18:57:09.274: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
Mar 23 18:57:09.306: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.306: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:57:09.306: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.307: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 18:57:09.307: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.307: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:57:09.307: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 18:57:09.308: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:57:09.308: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:57:09.308: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:57:09.308: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.309: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:57:09.309: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.309: INFO: 	Container metrics-server ready: true, restart count 0
Mar 23 18:57:09.309: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:57:09.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:57:09.311: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 18:57:09.311: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
Mar 23 18:57:09.325: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.326: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 18:57:09.326: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.326: INFO: 	Container azure-npm ready: false, restart count 0
Mar 23 18:57:09.327: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.327: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 18:57:09.328: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.328: INFO: 	Container coredns ready: true, restart count 0
Mar 23 18:57:09.328: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
Mar 23 18:57:09.328: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 18:57:09.328: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 18:57:09.328: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 18:57:09.328: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 18:57:09.328: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 18:57:09.328: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 18:57:09.328: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 18:57:09.328: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 18:57:09.328
Mar 23 18:57:09.344: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9893" to be "running"
Mar 23 18:57:09.349: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46789ms
Mar 23 18:57:11.352: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008190361s
Mar 23 18:57:11.352: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 18:57:11.355
STEP: Trying to apply a random label on the found node. 03/23/23 18:57:11.364
STEP: verifying the node has the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 42 03/23/23 18:57:11.379
STEP: Trying to relaunch the pod, now with labels. 03/23/23 18:57:11.383
Mar 23 18:57:11.389: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9893" to be "not pending"
Mar 23 18:57:11.392: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920294ms
Mar 23 18:57:13.396: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007017003s
Mar 23 18:57:13.397: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 off the node k8s-linuxpool-16392394-1 03/23/23 18:57:13.4
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 03/23/23 18:57:13.413
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 23 18:57:13.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9893" for this suite. 03/23/23 18:57:13.424
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":57,"skipped":920,"failed":0}
------------------------------
• [4.231 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:09.205
    Mar 23 18:57:09.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-pred 03/23/23 18:57:09.206
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:09.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:09.239
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 23 18:57:09.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 23 18:57:09.253: INFO: Waiting for terminating namespaces to be deleted...
    Mar 23 18:57:09.256: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
    Mar 23 18:57:09.273: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:57:09.273: INFO: 	Container e2e ready: true, restart count 0
    Mar 23 18:57:09.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:57:09.274: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:57:09.274: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:57:09.274: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 18:57:09.274: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
    Mar 23 18:57:09.306: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.306: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:57:09.306: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.307: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 18:57:09.307: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.307: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:57:09.307: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 18:57:09.308: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:57:09.308: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:57:09.308: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:57:09.308: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.309: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:57:09.309: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.309: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 23 18:57:09.309: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:57:09.311: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:57:09.311: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 18:57:09.311: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
    Mar 23 18:57:09.325: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.326: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 18:57:09.326: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.326: INFO: 	Container azure-npm ready: false, restart count 0
    Mar 23 18:57:09.327: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.327: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.328: INFO: 	Container coredns ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
    Mar 23 18:57:09.328: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 18:57:09.328: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 18:57:09.328: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 18:57:09.328: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 18:57:09.328
    Mar 23 18:57:09.344: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9893" to be "running"
    Mar 23 18:57:09.349: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46789ms
    Mar 23 18:57:11.352: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008190361s
    Mar 23 18:57:11.352: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 18:57:11.355
    STEP: Trying to apply a random label on the found node. 03/23/23 18:57:11.364
    STEP: verifying the node has the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 42 03/23/23 18:57:11.379
    STEP: Trying to relaunch the pod, now with labels. 03/23/23 18:57:11.383
    Mar 23 18:57:11.389: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-9893" to be "not pending"
    Mar 23 18:57:11.392: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.920294ms
    Mar 23 18:57:13.396: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007017003s
    Mar 23 18:57:13.397: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 off the node k8s-linuxpool-16392394-1 03/23/23 18:57:13.4
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-2ee3a178-63e6-4170-9cc3-70a9a5a37d84 03/23/23 18:57:13.413
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 18:57:13.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9893" for this suite. 03/23/23 18:57:13.424
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:13.462
Mar 23 18:57:13.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 18:57:13.464
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:13.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:13.485
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-92c84cb3-8cf6-487e-a891-db580e9cee34 03/23/23 18:57:13.488
STEP: Creating a pod to test consume secrets 03/23/23 18:57:13.497
Mar 23 18:57:13.515: INFO: Waiting up to 5m0s for pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2" in namespace "secrets-4698" to be "Succeeded or Failed"
Mar 23 18:57:13.527: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.985773ms
Mar 23 18:57:15.532: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017115491s
Mar 23 18:57:17.531: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Running", Reason="", readiness=false. Elapsed: 4.016015924s
Mar 23 18:57:19.539: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024438835s
STEP: Saw pod success 03/23/23 18:57:19.539
Mar 23 18:57:19.539: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2" satisfied condition "Succeeded or Failed"
Mar 23 18:57:19.559: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 18:57:19.621
Mar 23 18:57:19.634: INFO: Waiting for pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 to disappear
Mar 23 18:57:19.639: INFO: Pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 18:57:19.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4698" for this suite. 03/23/23 18:57:19.644
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":58,"skipped":1042,"failed":0}
------------------------------
• [SLOW TEST] [6.189 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:13.462
    Mar 23 18:57:13.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 18:57:13.464
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:13.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:13.485
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-92c84cb3-8cf6-487e-a891-db580e9cee34 03/23/23 18:57:13.488
    STEP: Creating a pod to test consume secrets 03/23/23 18:57:13.497
    Mar 23 18:57:13.515: INFO: Waiting up to 5m0s for pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2" in namespace "secrets-4698" to be "Succeeded or Failed"
    Mar 23 18:57:13.527: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.985773ms
    Mar 23 18:57:15.532: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017115491s
    Mar 23 18:57:17.531: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Running", Reason="", readiness=false. Elapsed: 4.016015924s
    Mar 23 18:57:19.539: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024438835s
    STEP: Saw pod success 03/23/23 18:57:19.539
    Mar 23 18:57:19.539: INFO: Pod "pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2" satisfied condition "Succeeded or Failed"
    Mar 23 18:57:19.559: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 18:57:19.621
    Mar 23 18:57:19.634: INFO: Waiting for pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 to disappear
    Mar 23 18:57:19.639: INFO: Pod pod-secrets-4d473998-9669-4f3a-b68e-8ce937c9a1f2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 18:57:19.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4698" for this suite. 03/23/23 18:57:19.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:19.654
Mar 23 18:57:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replication-controller 03/23/23 18:57:19.658
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:19.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:19.702
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 03/23/23 18:57:19.754
STEP: waiting for RC to be added 03/23/23 18:57:19.766
STEP: waiting for available Replicas 03/23/23 18:57:19.766
STEP: patching ReplicationController 03/23/23 18:57:22.137
STEP: waiting for RC to be modified 03/23/23 18:57:22.145
STEP: patching ReplicationController status 03/23/23 18:57:22.145
STEP: waiting for RC to be modified 03/23/23 18:57:22.154
STEP: waiting for available Replicas 03/23/23 18:57:22.154
STEP: fetching ReplicationController status 03/23/23 18:57:22.158
STEP: patching ReplicationController scale 03/23/23 18:57:22.162
STEP: waiting for RC to be modified 03/23/23 18:57:22.171
STEP: waiting for ReplicationController's scale to be the max amount 03/23/23 18:57:22.171
STEP: fetching ReplicationController; ensuring that it's patched 03/23/23 18:57:24.763
STEP: updating ReplicationController status 03/23/23 18:57:24.767
STEP: waiting for RC to be modified 03/23/23 18:57:24.777
STEP: listing all ReplicationControllers 03/23/23 18:57:24.778
STEP: checking that ReplicationController has expected values 03/23/23 18:57:24.782
STEP: deleting ReplicationControllers by collection 03/23/23 18:57:24.782
STEP: waiting for ReplicationController to have a DELETED watchEvent 03/23/23 18:57:24.796
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 23 18:57:24.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5386" for this suite. 03/23/23 18:57:24.894
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":59,"skipped":1058,"failed":0}
------------------------------
• [SLOW TEST] [5.249 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:19.654
    Mar 23 18:57:19.655: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replication-controller 03/23/23 18:57:19.658
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:19.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:19.702
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 03/23/23 18:57:19.754
    STEP: waiting for RC to be added 03/23/23 18:57:19.766
    STEP: waiting for available Replicas 03/23/23 18:57:19.766
    STEP: patching ReplicationController 03/23/23 18:57:22.137
    STEP: waiting for RC to be modified 03/23/23 18:57:22.145
    STEP: patching ReplicationController status 03/23/23 18:57:22.145
    STEP: waiting for RC to be modified 03/23/23 18:57:22.154
    STEP: waiting for available Replicas 03/23/23 18:57:22.154
    STEP: fetching ReplicationController status 03/23/23 18:57:22.158
    STEP: patching ReplicationController scale 03/23/23 18:57:22.162
    STEP: waiting for RC to be modified 03/23/23 18:57:22.171
    STEP: waiting for ReplicationController's scale to be the max amount 03/23/23 18:57:22.171
    STEP: fetching ReplicationController; ensuring that it's patched 03/23/23 18:57:24.763
    STEP: updating ReplicationController status 03/23/23 18:57:24.767
    STEP: waiting for RC to be modified 03/23/23 18:57:24.777
    STEP: listing all ReplicationControllers 03/23/23 18:57:24.778
    STEP: checking that ReplicationController has expected values 03/23/23 18:57:24.782
    STEP: deleting ReplicationControllers by collection 03/23/23 18:57:24.782
    STEP: waiting for ReplicationController to have a DELETED watchEvent 03/23/23 18:57:24.796
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 23 18:57:24.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5386" for this suite. 03/23/23 18:57:24.894
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:24.908
Mar 23 18:57:24.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 18:57:24.91
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:24.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:24.933
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 03/23/23 18:57:24.938
Mar 23 18:57:24.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 create -f -'
Mar 23 18:57:25.239: INFO: stderr: ""
Mar 23 18:57:25.239: INFO: stdout: "pod/pause created\n"
Mar 23 18:57:25.240: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Mar 23 18:57:25.240: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9822" to be "running and ready"
Mar 23 18:57:25.246: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081286ms
Mar 23 18:57:25.246: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-linuxpool-16392394-1' to be 'Running' but was 'Pending'
Mar 23 18:57:27.250: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010445057s
Mar 23 18:57:27.250: INFO: Pod "pause" satisfied condition "running and ready"
Mar 23 18:57:27.251: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 03/23/23 18:57:27.251
Mar 23 18:57:27.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 label pods pause testing-label=testing-label-value'
Mar 23 18:57:27.359: INFO: stderr: ""
Mar 23 18:57:27.359: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 03/23/23 18:57:27.359
Mar 23 18:57:27.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pod pause -L testing-label'
Mar 23 18:57:27.497: INFO: stderr: ""
Mar 23 18:57:27.497: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 03/23/23 18:57:27.497
Mar 23 18:57:27.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 label pods pause testing-label-'
Mar 23 18:57:27.614: INFO: stderr: ""
Mar 23 18:57:27.614: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 03/23/23 18:57:27.614
Mar 23 18:57:27.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pod pause -L testing-label'
Mar 23 18:57:27.709: INFO: stderr: ""
Mar 23 18:57:27.709: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 03/23/23 18:57:27.709
Mar 23 18:57:27.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 delete --grace-period=0 --force -f -'
Mar 23 18:57:27.814: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 18:57:27.814: INFO: stdout: "pod \"pause\" force deleted\n"
Mar 23 18:57:27.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get rc,svc -l name=pause --no-headers'
Mar 23 18:57:27.916: INFO: stderr: "No resources found in kubectl-9822 namespace.\n"
Mar 23 18:57:27.916: INFO: stdout: ""
Mar 23 18:57:27.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 18:57:28.014: INFO: stderr: ""
Mar 23 18:57:28.014: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 18:57:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9822" for this suite. 03/23/23 18:57:28.019
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":60,"skipped":1060,"failed":0}
------------------------------
• [3.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:24.908
    Mar 23 18:57:24.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 18:57:24.91
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:24.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:24.933
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 03/23/23 18:57:24.938
    Mar 23 18:57:24.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 create -f -'
    Mar 23 18:57:25.239: INFO: stderr: ""
    Mar 23 18:57:25.239: INFO: stdout: "pod/pause created\n"
    Mar 23 18:57:25.240: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Mar 23 18:57:25.240: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9822" to be "running and ready"
    Mar 23 18:57:25.246: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081286ms
    Mar 23 18:57:25.246: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8s-linuxpool-16392394-1' to be 'Running' but was 'Pending'
    Mar 23 18:57:27.250: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010445057s
    Mar 23 18:57:27.250: INFO: Pod "pause" satisfied condition "running and ready"
    Mar 23 18:57:27.251: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 03/23/23 18:57:27.251
    Mar 23 18:57:27.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 label pods pause testing-label=testing-label-value'
    Mar 23 18:57:27.359: INFO: stderr: ""
    Mar 23 18:57:27.359: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 03/23/23 18:57:27.359
    Mar 23 18:57:27.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pod pause -L testing-label'
    Mar 23 18:57:27.497: INFO: stderr: ""
    Mar 23 18:57:27.497: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 03/23/23 18:57:27.497
    Mar 23 18:57:27.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 label pods pause testing-label-'
    Mar 23 18:57:27.614: INFO: stderr: ""
    Mar 23 18:57:27.614: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 03/23/23 18:57:27.614
    Mar 23 18:57:27.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pod pause -L testing-label'
    Mar 23 18:57:27.709: INFO: stderr: ""
    Mar 23 18:57:27.709: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 03/23/23 18:57:27.709
    Mar 23 18:57:27.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 delete --grace-period=0 --force -f -'
    Mar 23 18:57:27.814: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 18:57:27.814: INFO: stdout: "pod \"pause\" force deleted\n"
    Mar 23 18:57:27.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get rc,svc -l name=pause --no-headers'
    Mar 23 18:57:27.916: INFO: stderr: "No resources found in kubectl-9822 namespace.\n"
    Mar 23 18:57:27.916: INFO: stdout: ""
    Mar 23 18:57:27.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-9822 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 23 18:57:28.014: INFO: stderr: ""
    Mar 23 18:57:28.014: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 18:57:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9822" for this suite. 03/23/23 18:57:28.019
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:28.024
Mar 23 18:57:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 18:57:28.025
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:28.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:28.042
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 03/23/23 18:57:28.046
Mar 23 18:57:28.095: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f" in namespace "downward-api-5590" to be "Succeeded or Failed"
Mar 23 18:57:28.108: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.500069ms
Mar 23 18:57:30.114: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019447646s
Mar 23 18:57:32.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Running", Reason="", readiness=true. Elapsed: 4.017359142s
Mar 23 18:57:34.116: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Running", Reason="", readiness=false. Elapsed: 6.021031125s
Mar 23 18:57:36.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017469325s
STEP: Saw pod success 03/23/23 18:57:36.112
Mar 23 18:57:36.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f" satisfied condition "Succeeded or Failed"
Mar 23 18:57:36.116: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f container client-container: <nil>
STEP: delete the pod 03/23/23 18:57:36.122
Mar 23 18:57:36.134: INFO: Waiting for pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f to disappear
Mar 23 18:57:36.137: INFO: Pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 18:57:36.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5590" for this suite. 03/23/23 18:57:36.141
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":61,"skipped":1061,"failed":0}
------------------------------
• [SLOW TEST] [8.121 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:28.024
    Mar 23 18:57:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 18:57:28.025
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:28.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:28.042
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 03/23/23 18:57:28.046
    Mar 23 18:57:28.095: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f" in namespace "downward-api-5590" to be "Succeeded or Failed"
    Mar 23 18:57:28.108: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.500069ms
    Mar 23 18:57:30.114: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019447646s
    Mar 23 18:57:32.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Running", Reason="", readiness=true. Elapsed: 4.017359142s
    Mar 23 18:57:34.116: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Running", Reason="", readiness=false. Elapsed: 6.021031125s
    Mar 23 18:57:36.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.017469325s
    STEP: Saw pod success 03/23/23 18:57:36.112
    Mar 23 18:57:36.112: INFO: Pod "downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f" satisfied condition "Succeeded or Failed"
    Mar 23 18:57:36.116: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f container client-container: <nil>
    STEP: delete the pod 03/23/23 18:57:36.122
    Mar 23 18:57:36.134: INFO: Waiting for pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f to disappear
    Mar 23 18:57:36.137: INFO: Pod downwardapi-volume-2dbac01f-69bf-4a54-9693-bb57c4f2339f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 18:57:36.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5590" for this suite. 03/23/23 18:57:36.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:57:36.148
Mar 23 18:57:36.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 18:57:36.149
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:36.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:36.176
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 03/23/23 18:57:36.181
STEP: waiting for pod running 03/23/23 18:57:36.195
Mar 23 18:57:36.195: INFO: Waiting up to 2m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016" to be "running"
Mar 23 18:57:36.201: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233688ms
Mar 23 18:57:38.205: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Running", Reason="", readiness=true. Elapsed: 2.009264178s
Mar 23 18:57:38.205: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" satisfied condition "running"
STEP: creating a file in subpath 03/23/23 18:57:38.205
Mar 23 18:57:38.210: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1016 PodName:var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:57:38.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:57:38.210: INFO: ExecWithOptions: Clientset creation
Mar 23 18:57:38.210: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-1016/pods/var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 03/23/23 18:57:38.326
Mar 23 18:57:38.329: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1016 PodName:var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 18:57:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 18:57:38.330: INFO: ExecWithOptions: Clientset creation
Mar 23 18:57:38.330: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-1016/pods/var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 03/23/23 18:57:38.443
Mar 23 18:57:38.957: INFO: Successfully updated pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910"
STEP: waiting for annotated pod running 03/23/23 18:57:38.957
Mar 23 18:57:38.957: INFO: Waiting up to 2m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016" to be "running"
Mar 23 18:57:38.960: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Running", Reason="", readiness=true. Elapsed: 3.091793ms
Mar 23 18:57:38.960: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" satisfied condition "running"
STEP: deleting the pod gracefully 03/23/23 18:57:38.96
Mar 23 18:57:38.960: INFO: Deleting pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016"
Mar 23 18:57:38.967: INFO: Wait up to 5m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 18:58:12.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1016" for this suite. 03/23/23 18:58:12.979
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":62,"skipped":1077,"failed":0}
------------------------------
• [SLOW TEST] [36.848 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:57:36.148
    Mar 23 18:57:36.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 18:57:36.149
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:57:36.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:57:36.176
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 03/23/23 18:57:36.181
    STEP: waiting for pod running 03/23/23 18:57:36.195
    Mar 23 18:57:36.195: INFO: Waiting up to 2m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016" to be "running"
    Mar 23 18:57:36.201: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Pending", Reason="", readiness=false. Elapsed: 5.233688ms
    Mar 23 18:57:38.205: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Running", Reason="", readiness=true. Elapsed: 2.009264178s
    Mar 23 18:57:38.205: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" satisfied condition "running"
    STEP: creating a file in subpath 03/23/23 18:57:38.205
    Mar 23 18:57:38.210: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1016 PodName:var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:57:38.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:57:38.210: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:57:38.210: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-1016/pods/var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 03/23/23 18:57:38.326
    Mar 23 18:57:38.329: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1016 PodName:var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 18:57:38.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 18:57:38.330: INFO: ExecWithOptions: Clientset creation
    Mar 23 18:57:38.330: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/var-expansion-1016/pods/var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 03/23/23 18:57:38.443
    Mar 23 18:57:38.957: INFO: Successfully updated pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910"
    STEP: waiting for annotated pod running 03/23/23 18:57:38.957
    Mar 23 18:57:38.957: INFO: Waiting up to 2m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016" to be "running"
    Mar 23 18:57:38.960: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910": Phase="Running", Reason="", readiness=true. Elapsed: 3.091793ms
    Mar 23 18:57:38.960: INFO: Pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" satisfied condition "running"
    STEP: deleting the pod gracefully 03/23/23 18:57:38.96
    Mar 23 18:57:38.960: INFO: Deleting pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" in namespace "var-expansion-1016"
    Mar 23 18:57:38.967: INFO: Wait up to 5m0s for pod "var-expansion-4aa90cd7-2863-421f-b697-a7888dd09910" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 18:58:12.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1016" for this suite. 03/23/23 18:58:12.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:58:13.001
Mar 23 18:58:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-runtime 03/23/23 18:58:13.003
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:13.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:13.023
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 03/23/23 18:58:13.027
STEP: wait for the container to reach Succeeded 03/23/23 18:58:13.037
STEP: get the container status 03/23/23 18:58:19.076
STEP: the container should be terminated 03/23/23 18:58:19.078
STEP: the termination message should be set 03/23/23 18:58:19.078
Mar 23 18:58:19.079: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 03/23/23 18:58:19.079
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 23 18:58:19.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5548" for this suite. 03/23/23 18:58:19.117
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":63,"skipped":1088,"failed":0}
------------------------------
• [SLOW TEST] [6.125 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:58:13.001
    Mar 23 18:58:13.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-runtime 03/23/23 18:58:13.003
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:13.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:13.023
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 03/23/23 18:58:13.027
    STEP: wait for the container to reach Succeeded 03/23/23 18:58:13.037
    STEP: get the container status 03/23/23 18:58:19.076
    STEP: the container should be terminated 03/23/23 18:58:19.078
    STEP: the termination message should be set 03/23/23 18:58:19.078
    Mar 23 18:58:19.079: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 03/23/23 18:58:19.079
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 23 18:58:19.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5548" for this suite. 03/23/23 18:58:19.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:58:19.128
Mar 23 18:58:19.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename limitrange 03/23/23 18:58:19.129
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:19.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:19.145
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 03/23/23 18:58:19.148
STEP: Setting up watch 03/23/23 18:58:19.149
STEP: Submitting a LimitRange 03/23/23 18:58:19.257
STEP: Verifying LimitRange creation was observed 03/23/23 18:58:19.262
STEP: Fetching the LimitRange to ensure it has proper values 03/23/23 18:58:19.263
Mar 23 18:58:19.265: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 23 18:58:19.265: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 03/23/23 18:58:19.265
STEP: Ensuring Pod has resource requirements applied from LimitRange 03/23/23 18:58:19.27
Mar 23 18:58:19.275: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Mar 23 18:58:19.276: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 03/23/23 18:58:19.276
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/23/23 18:58:19.283
Mar 23 18:58:19.289: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Mar 23 18:58:19.289: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 03/23/23 18:58:19.289
STEP: Failing to create a Pod with more than max resources 03/23/23 18:58:19.292
STEP: Updating a LimitRange 03/23/23 18:58:19.295
STEP: Verifying LimitRange updating is effective 03/23/23 18:58:19.3
STEP: Creating a Pod with less than former min resources 03/23/23 18:58:21.303
STEP: Failing to create a Pod with more than max resources 03/23/23 18:58:21.316
STEP: Deleting a LimitRange 03/23/23 18:58:21.32
STEP: Verifying the LimitRange was deleted 03/23/23 18:58:21.336
Mar 23 18:58:26.339: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 03/23/23 18:58:26.339
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Mar 23 18:58:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-5531" for this suite. 03/23/23 18:58:26.363
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":64,"skipped":1102,"failed":0}
------------------------------
• [SLOW TEST] [7.243 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:58:19.128
    Mar 23 18:58:19.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename limitrange 03/23/23 18:58:19.129
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:19.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:19.145
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 03/23/23 18:58:19.148
    STEP: Setting up watch 03/23/23 18:58:19.149
    STEP: Submitting a LimitRange 03/23/23 18:58:19.257
    STEP: Verifying LimitRange creation was observed 03/23/23 18:58:19.262
    STEP: Fetching the LimitRange to ensure it has proper values 03/23/23 18:58:19.263
    Mar 23 18:58:19.265: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 23 18:58:19.265: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 03/23/23 18:58:19.265
    STEP: Ensuring Pod has resource requirements applied from LimitRange 03/23/23 18:58:19.27
    Mar 23 18:58:19.275: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Mar 23 18:58:19.276: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 03/23/23 18:58:19.276
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 03/23/23 18:58:19.283
    Mar 23 18:58:19.289: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Mar 23 18:58:19.289: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 03/23/23 18:58:19.289
    STEP: Failing to create a Pod with more than max resources 03/23/23 18:58:19.292
    STEP: Updating a LimitRange 03/23/23 18:58:19.295
    STEP: Verifying LimitRange updating is effective 03/23/23 18:58:19.3
    STEP: Creating a Pod with less than former min resources 03/23/23 18:58:21.303
    STEP: Failing to create a Pod with more than max resources 03/23/23 18:58:21.316
    STEP: Deleting a LimitRange 03/23/23 18:58:21.32
    STEP: Verifying the LimitRange was deleted 03/23/23 18:58:21.336
    Mar 23 18:58:26.339: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 03/23/23 18:58:26.339
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Mar 23 18:58:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-5531" for this suite. 03/23/23 18:58:26.363
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:58:26.376
Mar 23 18:58:26.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename endpointslice 03/23/23 18:58:26.378
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:26.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:26.399
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 03/23/23 18:58:31.6
STEP: referencing matching pods with named port 03/23/23 18:58:36.61
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/23/23 18:58:41.617
STEP: recreating EndpointSlices after they've been deleted 03/23/23 18:58:46.627
Mar 23 18:58:46.668: INFO: EndpointSlice for Service endpointslice-1544/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 23 18:58:56.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1544" for this suite. 03/23/23 18:58:56.685
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":65,"skipped":1102,"failed":0}
------------------------------
• [SLOW TEST] [30.314 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:58:26.376
    Mar 23 18:58:26.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename endpointslice 03/23/23 18:58:26.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:26.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:26.399
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 03/23/23 18:58:31.6
    STEP: referencing matching pods with named port 03/23/23 18:58:36.61
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 03/23/23 18:58:41.617
    STEP: recreating EndpointSlices after they've been deleted 03/23/23 18:58:46.627
    Mar 23 18:58:46.668: INFO: EndpointSlice for Service endpointslice-1544/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 23 18:58:56.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1544" for this suite. 03/23/23 18:58:56.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:58:56.699
Mar 23 18:58:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 18:58:56.7
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:56.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:56.726
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 03/23/23 18:58:56.73
Mar 23 18:58:56.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192" in namespace "downward-api-6755" to be "Succeeded or Failed"
Mar 23 18:58:56.745: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47009ms
Mar 23 18:58:58.749: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Running", Reason="", readiness=true. Elapsed: 2.008820078s
Mar 23 18:59:00.752: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Running", Reason="", readiness=false. Elapsed: 4.01134497s
Mar 23 18:59:02.753: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012431766s
STEP: Saw pod success 03/23/23 18:59:02.753
Mar 23 18:59:02.753: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192" satisfied condition "Succeeded or Failed"
Mar 23 18:59:02.757: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 container client-container: <nil>
STEP: delete the pod 03/23/23 18:59:02.777
Mar 23 18:59:02.800: INFO: Waiting for pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 to disappear
Mar 23 18:59:02.804: INFO: Pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 18:59:02.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6755" for this suite. 03/23/23 18:59:02.809
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":66,"skipped":1114,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:58:56.699
    Mar 23 18:58:56.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 18:58:56.7
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:58:56.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:58:56.726
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 03/23/23 18:58:56.73
    Mar 23 18:58:56.740: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192" in namespace "downward-api-6755" to be "Succeeded or Failed"
    Mar 23 18:58:56.745: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Pending", Reason="", readiness=false. Elapsed: 4.47009ms
    Mar 23 18:58:58.749: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Running", Reason="", readiness=true. Elapsed: 2.008820078s
    Mar 23 18:59:00.752: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Running", Reason="", readiness=false. Elapsed: 4.01134497s
    Mar 23 18:59:02.753: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012431766s
    STEP: Saw pod success 03/23/23 18:59:02.753
    Mar 23 18:59:02.753: INFO: Pod "downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192" satisfied condition "Succeeded or Failed"
    Mar 23 18:59:02.757: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 container client-container: <nil>
    STEP: delete the pod 03/23/23 18:59:02.777
    Mar 23 18:59:02.800: INFO: Waiting for pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 to disappear
    Mar 23 18:59:02.804: INFO: Pod downwardapi-volume-b9464c76-6af8-4693-99b8-0507b343f192 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 18:59:02.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6755" for this suite. 03/23/23 18:59:02.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:59:02.829
Mar 23 18:59:02.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 18:59:02.83
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:02.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:02.857
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 03/23/23 18:59:02.863
Mar 23 18:59:02.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 create -f -'
Mar 23 18:59:03.657: INFO: stderr: ""
Mar 23 18:59:03.657: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 03/23/23 18:59:03.657
Mar 23 18:59:03.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 diff -f -'
Mar 23 18:59:03.897: INFO: rc: 1
Mar 23 18:59:03.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 delete -f -'
Mar 23 18:59:03.999: INFO: stderr: ""
Mar 23 18:59:04.000: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 18:59:04.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7899" for this suite. 03/23/23 18:59:04.007
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":67,"skipped":1126,"failed":0}
------------------------------
• [1.189 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:59:02.829
    Mar 23 18:59:02.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 18:59:02.83
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:02.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:02.857
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 03/23/23 18:59:02.863
    Mar 23 18:59:02.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 create -f -'
    Mar 23 18:59:03.657: INFO: stderr: ""
    Mar 23 18:59:03.657: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 03/23/23 18:59:03.657
    Mar 23 18:59:03.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 diff -f -'
    Mar 23 18:59:03.897: INFO: rc: 1
    Mar 23 18:59:03.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7899 delete -f -'
    Mar 23 18:59:03.999: INFO: stderr: ""
    Mar 23 18:59:04.000: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 18:59:04.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7899" for this suite. 03/23/23 18:59:04.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:59:04.035
Mar 23 18:59:04.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename server-version 03/23/23 18:59:04.036
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.055
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 03/23/23 18:59:04.064
STEP: Confirm major version 03/23/23 18:59:04.066
Mar 23 18:59:04.066: INFO: Major version: 1
STEP: Confirm minor version 03/23/23 18:59:04.066
Mar 23 18:59:04.066: INFO: cleanMinorVersion: 25
Mar 23 18:59:04.066: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Mar 23 18:59:04.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9411" for this suite. 03/23/23 18:59:04.078
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":68,"skipped":1239,"failed":0}
------------------------------
• [0.051 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:59:04.035
    Mar 23 18:59:04.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename server-version 03/23/23 18:59:04.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.055
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 03/23/23 18:59:04.064
    STEP: Confirm major version 03/23/23 18:59:04.066
    Mar 23 18:59:04.066: INFO: Major version: 1
    STEP: Confirm minor version 03/23/23 18:59:04.066
    Mar 23 18:59:04.066: INFO: cleanMinorVersion: 25
    Mar 23 18:59:04.066: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Mar 23 18:59:04.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-9411" for this suite. 03/23/23 18:59:04.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:59:04.087
Mar 23 18:59:04.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 18:59:04.089
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.11
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 03/23/23 18:59:04.116
Mar 23 18:59:04.116: INFO: Creating e2e-svc-a-px4zr
Mar 23 18:59:04.148: INFO: Creating e2e-svc-b-fm25h
Mar 23 18:59:04.184: INFO: Creating e2e-svc-c-txzq8
STEP: deleting service collection 03/23/23 18:59:04.216
Mar 23 18:59:04.256: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 18:59:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-298" for this suite. 03/23/23 18:59:04.261
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":69,"skipped":1248,"failed":0}
------------------------------
• [0.181 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:59:04.087
    Mar 23 18:59:04.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 18:59:04.089
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.11
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 03/23/23 18:59:04.116
    Mar 23 18:59:04.116: INFO: Creating e2e-svc-a-px4zr
    Mar 23 18:59:04.148: INFO: Creating e2e-svc-b-fm25h
    Mar 23 18:59:04.184: INFO: Creating e2e-svc-c-txzq8
    STEP: deleting service collection 03/23/23 18:59:04.216
    Mar 23 18:59:04.256: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 18:59:04.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-298" for this suite. 03/23/23 18:59:04.261
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:59:04.271
Mar 23 18:59:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 18:59:04.272
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.287
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8622 03/23/23 18:59:04.293
STEP: changing the ExternalName service to type=ClusterIP 03/23/23 18:59:04.302
STEP: creating replication controller externalname-service in namespace services-8622 03/23/23 18:59:04.334
I0323 18:59:04.345804      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8622, replica count: 2
I0323 18:59:07.396121      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 18:59:10.396289      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 18:59:10.396: INFO: Creating new exec pod
Mar 23 18:59:10.402: INFO: Waiting up to 5m0s for pod "execpodkpln2" in namespace "services-8622" to be "running"
Mar 23 18:59:10.405: INFO: Pod "execpodkpln2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233893ms
Mar 23 18:59:12.409: INFO: Pod "execpodkpln2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006790182s
Mar 23 18:59:12.409: INFO: Pod "execpodkpln2" satisfied condition "running"
Mar 23 18:59:13.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 18:59:13.614: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:13.614: INFO: stdout: ""
Mar 23 18:59:14.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 18:59:14.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:14.801: INFO: stdout: ""
Mar 23 18:59:15.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 18:59:15.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:15.821: INFO: stdout: ""
Mar 23 18:59:16.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 18:59:16.813: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:16.813: INFO: stdout: ""
Mar 23 18:59:17.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 18:59:17.805: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:17.805: INFO: stdout: "externalname-service-2zp64"
Mar 23 18:59:17.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.176.14 80'
Mar 23 18:59:17.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.176.14 80\nConnection to 10.0.176.14 80 port [tcp/http] succeeded!\n"
Mar 23 18:59:17.992: INFO: stdout: "externalname-service-2zp64"
Mar 23 18:59:17.992: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 18:59:18.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8622" for this suite. 03/23/23 18:59:18.048
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":70,"skipped":1248,"failed":0}
------------------------------
• [SLOW TEST] [13.800 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:59:04.271
    Mar 23 18:59:04.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 18:59:04.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:04.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:04.287
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-8622 03/23/23 18:59:04.293
    STEP: changing the ExternalName service to type=ClusterIP 03/23/23 18:59:04.302
    STEP: creating replication controller externalname-service in namespace services-8622 03/23/23 18:59:04.334
    I0323 18:59:04.345804      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-8622, replica count: 2
    I0323 18:59:07.396121      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 0 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 18:59:10.396289      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 18:59:10.396: INFO: Creating new exec pod
    Mar 23 18:59:10.402: INFO: Waiting up to 5m0s for pod "execpodkpln2" in namespace "services-8622" to be "running"
    Mar 23 18:59:10.405: INFO: Pod "execpodkpln2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233893ms
    Mar 23 18:59:12.409: INFO: Pod "execpodkpln2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006790182s
    Mar 23 18:59:12.409: INFO: Pod "execpodkpln2" satisfied condition "running"
    Mar 23 18:59:13.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 18:59:13.614: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:13.614: INFO: stdout: ""
    Mar 23 18:59:14.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 18:59:14.801: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:14.801: INFO: stdout: ""
    Mar 23 18:59:15.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 18:59:15.821: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:15.821: INFO: stdout: ""
    Mar 23 18:59:16.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 18:59:16.813: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:16.813: INFO: stdout: ""
    Mar 23 18:59:17.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 18:59:17.805: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:17.805: INFO: stdout: "externalname-service-2zp64"
    Mar 23 18:59:17.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8622 exec execpodkpln2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.176.14 80'
    Mar 23 18:59:17.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.176.14 80\nConnection to 10.0.176.14 80 port [tcp/http] succeeded!\n"
    Mar 23 18:59:17.992: INFO: stdout: "externalname-service-2zp64"
    Mar 23 18:59:17.992: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 18:59:18.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8622" for this suite. 03/23/23 18:59:18.048
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 18:59:18.115
Mar 23 18:59:18.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 18:59:18.119
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:18.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:18.205
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-63cbce03-c576-4c93-a299-8c24948886ca 03/23/23 18:59:18.213
STEP: Creating secret with name s-test-opt-upd-cba94a49-de37-4c0e-bcb7-6cf63d45f7ee 03/23/23 18:59:18.218
STEP: Creating the pod 03/23/23 18:59:18.223
Mar 23 18:59:18.236: INFO: Waiting up to 5m0s for pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a" in namespace "secrets-982" to be "running and ready"
Mar 23 18:59:18.254: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.43456ms
Mar 23 18:59:18.254: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:59:20.258: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021659847s
Mar 23 18:59:20.258: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Pending, waiting for it to be Running (with Ready = true)
Mar 23 18:59:22.259: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.022669842s
Mar 23 18:59:22.259: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Running (Ready = true)
Mar 23 18:59:22.259: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-63cbce03-c576-4c93-a299-8c24948886ca 03/23/23 18:59:22.28
STEP: Updating secret s-test-opt-upd-cba94a49-de37-4c0e-bcb7-6cf63d45f7ee 03/23/23 18:59:22.288
STEP: Creating secret with name s-test-opt-create-c57dcedc-820b-422e-a58c-3eab3af5e5e1 03/23/23 18:59:22.292
STEP: waiting to observe update in volume 03/23/23 18:59:22.295
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:00:38.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-982" for this suite. 03/23/23 19:00:38.711
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":71,"skipped":1309,"failed":0}
------------------------------
• [SLOW TEST] [80.601 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 18:59:18.115
    Mar 23 18:59:18.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 18:59:18.119
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 18:59:18.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 18:59:18.205
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-63cbce03-c576-4c93-a299-8c24948886ca 03/23/23 18:59:18.213
    STEP: Creating secret with name s-test-opt-upd-cba94a49-de37-4c0e-bcb7-6cf63d45f7ee 03/23/23 18:59:18.218
    STEP: Creating the pod 03/23/23 18:59:18.223
    Mar 23 18:59:18.236: INFO: Waiting up to 5m0s for pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a" in namespace "secrets-982" to be "running and ready"
    Mar 23 18:59:18.254: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.43456ms
    Mar 23 18:59:18.254: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:59:20.258: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021659847s
    Mar 23 18:59:20.258: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 18:59:22.259: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.022669842s
    Mar 23 18:59:22.259: INFO: The phase of Pod pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a is Running (Ready = true)
    Mar 23 18:59:22.259: INFO: Pod "pod-secrets-6dbb428c-a64e-4114-af19-c2e9983a0c3a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-63cbce03-c576-4c93-a299-8c24948886ca 03/23/23 18:59:22.28
    STEP: Updating secret s-test-opt-upd-cba94a49-de37-4c0e-bcb7-6cf63d45f7ee 03/23/23 18:59:22.288
    STEP: Creating secret with name s-test-opt-create-c57dcedc-820b-422e-a58c-3eab3af5e5e1 03/23/23 18:59:22.292
    STEP: waiting to observe update in volume 03/23/23 18:59:22.295
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:00:38.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-982" for this suite. 03/23/23 19:00:38.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:00:38.724
Mar 23 19:00:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context-test 03/23/23 19:00:38.725
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:38.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:38.743
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Mar 23 19:00:38.762: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06" in namespace "security-context-test-3572" to be "Succeeded or Failed"
Mar 23 19:00:38.766: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133691ms
Mar 23 19:00:40.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Running", Reason="", readiness=true. Elapsed: 2.008957777s
Mar 23 19:00:42.770: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Running", Reason="", readiness=false. Elapsed: 4.008666676s
Mar 23 19:00:44.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009149073s
Mar 23 19:00:44.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 19:00:44.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3572" for this suite. 03/23/23 19:00:44.776
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":72,"skipped":1315,"failed":0}
------------------------------
• [SLOW TEST] [6.060 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:00:38.724
    Mar 23 19:00:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context-test 03/23/23 19:00:38.725
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:38.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:38.743
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Mar 23 19:00:38.762: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06" in namespace "security-context-test-3572" to be "Succeeded or Failed"
    Mar 23 19:00:38.766: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Pending", Reason="", readiness=false. Elapsed: 4.133691ms
    Mar 23 19:00:40.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Running", Reason="", readiness=true. Elapsed: 2.008957777s
    Mar 23 19:00:42.770: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Running", Reason="", readiness=false. Elapsed: 4.008666676s
    Mar 23 19:00:44.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009149073s
    Mar 23 19:00:44.771: INFO: Pod "busybox-readonly-false-808ba7cb-42ed-41c4-9289-6ce42d3a8d06" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 19:00:44.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3572" for this suite. 03/23/23 19:00:44.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:00:44.791
Mar 23 19:00:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:00:44.792
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:44.816
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:44.821
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-5000 03/23/23 19:00:44.825
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[] 03/23/23 19:00:44.869
Mar 23 19:00:44.913: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5000 03/23/23 19:00:44.914
Mar 23 19:00:44.964: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5000" to be "running and ready"
Mar 23 19:00:45.030: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 65.749655ms
Mar 23 19:00:45.030: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:00:47.034: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.069206746s
Mar 23 19:00:47.034: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 23 19:00:47.034: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod1:[80]] 03/23/23 19:00:47.036
Mar 23 19:00:47.046: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 03/23/23 19:00:47.046
Mar 23 19:00:47.047: INFO: Creating new exec pod
Mar 23 19:00:47.051: INFO: Waiting up to 5m0s for pod "execpodmrlc6" in namespace "services-5000" to be "running"
Mar 23 19:00:47.054: INFO: Pod "execpodmrlc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586194ms
Mar 23 19:00:49.057: INFO: Pod "execpodmrlc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006489983s
Mar 23 19:00:49.058: INFO: Pod "execpodmrlc6" satisfied condition "running"
Mar 23 19:00:50.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 23 19:00:50.285: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:50.285: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:00:50.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
Mar 23 19:00:50.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:50.497: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-5000 03/23/23 19:00:50.497
Mar 23 19:00:50.505: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5000" to be "running and ready"
Mar 23 19:00:50.511: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899887ms
Mar 23 19:00:50.511: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:00:52.516: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010276274s
Mar 23 19:00:52.516: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 23 19:00:52.516: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod1:[80] pod2:[80]] 03/23/23 19:00:52.52
Mar 23 19:00:52.532: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 03/23/23 19:00:52.532
Mar 23 19:00:53.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 23 19:00:53.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:53.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:00:53.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
Mar 23 19:00:53.955: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:53.955: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5000 03/23/23 19:00:53.955
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod2:[80]] 03/23/23 19:00:53.984
Mar 23 19:00:54.051: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 03/23/23 19:00:54.051
Mar 23 19:00:55.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Mar 23 19:00:55.275: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:55.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:00:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
Mar 23 19:00:55.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
Mar 23 19:00:55.499: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-5000 03/23/23 19:00:55.499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[] 03/23/23 19:00:55.519
Mar 23 19:00:56.544: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:00:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5000" for this suite. 03/23/23 19:00:56.605
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":73,"skipped":1331,"failed":0}
------------------------------
• [SLOW TEST] [11.836 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:00:44.791
    Mar 23 19:00:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:00:44.792
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:44.816
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:44.821
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-5000 03/23/23 19:00:44.825
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[] 03/23/23 19:00:44.869
    Mar 23 19:00:44.913: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5000 03/23/23 19:00:44.914
    Mar 23 19:00:44.964: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5000" to be "running and ready"
    Mar 23 19:00:45.030: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 65.749655ms
    Mar 23 19:00:45.030: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:00:47.034: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.069206746s
    Mar 23 19:00:47.034: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 23 19:00:47.034: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod1:[80]] 03/23/23 19:00:47.036
    Mar 23 19:00:47.046: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 03/23/23 19:00:47.046
    Mar 23 19:00:47.047: INFO: Creating new exec pod
    Mar 23 19:00:47.051: INFO: Waiting up to 5m0s for pod "execpodmrlc6" in namespace "services-5000" to be "running"
    Mar 23 19:00:47.054: INFO: Pod "execpodmrlc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.586194ms
    Mar 23 19:00:49.057: INFO: Pod "execpodmrlc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006489983s
    Mar 23 19:00:49.058: INFO: Pod "execpodmrlc6" satisfied condition "running"
    Mar 23 19:00:50.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 23 19:00:50.285: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:50.285: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:00:50.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
    Mar 23 19:00:50.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:50.497: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-5000 03/23/23 19:00:50.497
    Mar 23 19:00:50.505: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5000" to be "running and ready"
    Mar 23 19:00:50.511: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.899887ms
    Mar 23 19:00:50.511: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:00:52.516: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010276274s
    Mar 23 19:00:52.516: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 23 19:00:52.516: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod1:[80] pod2:[80]] 03/23/23 19:00:52.52
    Mar 23 19:00:52.532: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 03/23/23 19:00:52.532
    Mar 23 19:00:53.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 23 19:00:53.764: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:53.764: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:00:53.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
    Mar 23 19:00:53.955: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:53.955: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5000 03/23/23 19:00:53.955
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[pod2:[80]] 03/23/23 19:00:53.984
    Mar 23 19:00:54.051: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 03/23/23 19:00:54.051
    Mar 23 19:00:55.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Mar 23 19:00:55.275: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:55.275: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:00:55.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-5000 exec execpodmrlc6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.147.84 80'
    Mar 23 19:00:55.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.147.84 80\nConnection to 10.0.147.84 80 port [tcp/http] succeeded!\n"
    Mar 23 19:00:55.499: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-5000 03/23/23 19:00:55.499
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-5000 to expose endpoints map[] 03/23/23 19:00:55.519
    Mar 23 19:00:56.544: INFO: successfully validated that service endpoint-test2 in namespace services-5000 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:00:56.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5000" for this suite. 03/23/23 19:00:56.605
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:00:56.635
Mar 23 19:00:56.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:00:56.64
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:56.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:56.673
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-9be10562-3a19-4d8c-9d2d-768b754d927a 03/23/23 19:00:56.677
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:00:56.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1162" for this suite. 03/23/23 19:00:56.689
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":74,"skipped":1343,"failed":0}
------------------------------
• [0.062 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:00:56.635
    Mar 23 19:00:56.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:00:56.64
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:56.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:56.673
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-9be10562-3a19-4d8c-9d2d-768b754d927a 03/23/23 19:00:56.677
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:00:56.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1162" for this suite. 03/23/23 19:00:56.689
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:00:56.698
Mar 23 19:00:56.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:00:56.7
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:56.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:56.72
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:00:56.747
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:00:57.477
STEP: Deploying the webhook pod 03/23/23 19:00:57.487
STEP: Wait for the deployment to be ready 03/23/23 19:00:57.501
Mar 23 19:00:57.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:00:59.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/23/23 19:01:01.55
STEP: Verifying the service has paired with the endpoint 03/23/23 19:01:01.583
Mar 23 19:01:02.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/23/23 19:01:02.616
STEP: create a namespace for the webhook 03/23/23 19:01:02.638
STEP: create a configmap should be unconditionally rejected by the webhook 03/23/23 19:01:02.654
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:01:02.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7857" for this suite. 03/23/23 19:01:02.718
STEP: Destroying namespace "webhook-7857-markers" for this suite. 03/23/23 19:01:02.723
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":75,"skipped":1343,"failed":0}
------------------------------
• [SLOW TEST] [6.132 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:00:56.698
    Mar 23 19:00:56.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:00:56.7
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:00:56.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:00:56.72
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:00:56.747
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:00:57.477
    STEP: Deploying the webhook pod 03/23/23 19:00:57.487
    STEP: Wait for the deployment to be ready 03/23/23 19:00:57.501
    Mar 23 19:00:57.519: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 23 19:00:59.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 0, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/23/23 19:01:01.55
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:01:01.583
    Mar 23 19:01:02.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 03/23/23 19:01:02.616
    STEP: create a namespace for the webhook 03/23/23 19:01:02.638
    STEP: create a configmap should be unconditionally rejected by the webhook 03/23/23 19:01:02.654
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:01:02.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7857" for this suite. 03/23/23 19:01:02.718
    STEP: Destroying namespace "webhook-7857-markers" for this suite. 03/23/23 19:01:02.723
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:02.831
Mar 23 19:01:02.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:01:02.833
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:02.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:02.898
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5795/secret-test-293f3992-565c-4293-ace7-4b5398a83b04 03/23/23 19:01:02.906
STEP: Creating a pod to test consume secrets 03/23/23 19:01:02.913
Mar 23 19:01:03.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55" in namespace "secrets-5795" to be "Succeeded or Failed"
Mar 23 19:01:03.088: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77329ms
Mar 23 19:01:05.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=true. Elapsed: 2.00916197s
Mar 23 19:01:07.091: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=true. Elapsed: 4.008570061s
Mar 23 19:01:09.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=false. Elapsed: 6.008925649s
Mar 23 19:01:11.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009079837s
STEP: Saw pod success 03/23/23 19:01:11.092
Mar 23 19:01:11.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55" satisfied condition "Succeeded or Failed"
Mar 23 19:01:11.095: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 container env-test: <nil>
STEP: delete the pod 03/23/23 19:01:11.102
Mar 23 19:01:11.119: INFO: Waiting for pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 to disappear
Mar 23 19:01:11.122: INFO: Pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:01:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5795" for this suite. 03/23/23 19:01:11.128
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":76,"skipped":1344,"failed":0}
------------------------------
• [SLOW TEST] [8.303 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:02.831
    Mar 23 19:01:02.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:01:02.833
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:02.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:02.898
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5795/secret-test-293f3992-565c-4293-ace7-4b5398a83b04 03/23/23 19:01:02.906
    STEP: Creating a pod to test consume secrets 03/23/23 19:01:02.913
    Mar 23 19:01:03.083: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55" in namespace "secrets-5795" to be "Succeeded or Failed"
    Mar 23 19:01:03.088: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Pending", Reason="", readiness=false. Elapsed: 4.77329ms
    Mar 23 19:01:05.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=true. Elapsed: 2.00916197s
    Mar 23 19:01:07.091: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=true. Elapsed: 4.008570061s
    Mar 23 19:01:09.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Running", Reason="", readiness=false. Elapsed: 6.008925649s
    Mar 23 19:01:11.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.009079837s
    STEP: Saw pod success 03/23/23 19:01:11.092
    Mar 23 19:01:11.092: INFO: Pod "pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55" satisfied condition "Succeeded or Failed"
    Mar 23 19:01:11.095: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 container env-test: <nil>
    STEP: delete the pod 03/23/23 19:01:11.102
    Mar 23 19:01:11.119: INFO: Waiting for pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 to disappear
    Mar 23 19:01:11.122: INFO: Pod pod-configmaps-8ad437c3-27a8-4ef2-b683-44e06899ff55 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:01:11.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5795" for this suite. 03/23/23 19:01:11.128
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:11.142
Mar 23 19:01:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:01:11.143
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:11.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:11.164
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-0fe85306-66e5-4cd7-b720-d50a9797baf2 03/23/23 19:01:11.168
STEP: Creating a pod to test consume secrets 03/23/23 19:01:11.179
Mar 23 19:01:11.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357" in namespace "projected-7199" to be "Succeeded or Failed"
Mar 23 19:01:11.206: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Pending", Reason="", readiness=false. Elapsed: 14.639067ms
Mar 23 19:01:13.212: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021076142s
Mar 23 19:01:15.211: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Running", Reason="", readiness=true. Elapsed: 4.020066832s
Mar 23 19:01:17.211: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Running", Reason="", readiness=false. Elapsed: 6.020159419s
Mar 23 19:01:19.210: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018959309s
STEP: Saw pod success 03/23/23 19:01:19.21
Mar 23 19:01:19.210: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357" satisfied condition "Succeeded or Failed"
Mar 23 19:01:19.213: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:01:19.243
Mar 23 19:01:19.254: INFO: Waiting for pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 to disappear
Mar 23 19:01:19.257: INFO: Pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 19:01:19.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7199" for this suite. 03/23/23 19:01:19.261
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":77,"skipped":1344,"failed":0}
------------------------------
• [SLOW TEST] [8.124 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:11.142
    Mar 23 19:01:11.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:01:11.143
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:11.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:11.164
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-0fe85306-66e5-4cd7-b720-d50a9797baf2 03/23/23 19:01:11.168
    STEP: Creating a pod to test consume secrets 03/23/23 19:01:11.179
    Mar 23 19:01:11.191: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357" in namespace "projected-7199" to be "Succeeded or Failed"
    Mar 23 19:01:11.206: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Pending", Reason="", readiness=false. Elapsed: 14.639067ms
    Mar 23 19:01:13.212: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021076142s
    Mar 23 19:01:15.211: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Running", Reason="", readiness=true. Elapsed: 4.020066832s
    Mar 23 19:01:17.211: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Running", Reason="", readiness=false. Elapsed: 6.020159419s
    Mar 23 19:01:19.210: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018959309s
    STEP: Saw pod success 03/23/23 19:01:19.21
    Mar 23 19:01:19.210: INFO: Pod "pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357" satisfied condition "Succeeded or Failed"
    Mar 23 19:01:19.213: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:01:19.243
    Mar 23 19:01:19.254: INFO: Waiting for pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 to disappear
    Mar 23 19:01:19.257: INFO: Pod pod-projected-secrets-0f8733f3-61ef-4446-a857-b5626849b357 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 19:01:19.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7199" for this suite. 03/23/23 19:01:19.261
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:19.266
Mar 23 19:01:19.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:01:19.267
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:19.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:19.28
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 03/23/23 19:01:19.286
STEP: submitting the pod to kubernetes 03/23/23 19:01:19.287
Mar 23 19:01:19.298: INFO: Waiting up to 5m0s for pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" in namespace "pods-1951" to be "running and ready"
Mar 23 19:01:19.315: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.838063ms
Mar 23 19:01:19.315: INFO: The phase of Pod pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:01:21.320: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02170394s
Mar 23 19:01:21.320: INFO: The phase of Pod pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1 is Running (Ready = true)
Mar 23 19:01:21.320: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/23/23 19:01:21.322
STEP: updating the pod 03/23/23 19:01:21.325
Mar 23 19:01:21.840: INFO: Successfully updated pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1"
Mar 23 19:01:21.841: INFO: Waiting up to 5m0s for pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" in namespace "pods-1951" to be "running"
Mar 23 19:01:21.844: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Running", Reason="", readiness=true. Elapsed: 3.117993ms
Mar 23 19:01:21.846: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 03/23/23 19:01:21.846
Mar 23 19:01:21.849: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:01:21.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1951" for this suite. 03/23/23 19:01:21.856
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":78,"skipped":1347,"failed":0}
------------------------------
• [2.599 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:19.266
    Mar 23 19:01:19.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:01:19.267
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:19.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:19.28
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 03/23/23 19:01:19.286
    STEP: submitting the pod to kubernetes 03/23/23 19:01:19.287
    Mar 23 19:01:19.298: INFO: Waiting up to 5m0s for pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" in namespace "pods-1951" to be "running and ready"
    Mar 23 19:01:19.315: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.838063ms
    Mar 23 19:01:19.315: INFO: The phase of Pod pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:01:21.320: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.02170394s
    Mar 23 19:01:21.320: INFO: The phase of Pod pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1 is Running (Ready = true)
    Mar 23 19:01:21.320: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/23/23 19:01:21.322
    STEP: updating the pod 03/23/23 19:01:21.325
    Mar 23 19:01:21.840: INFO: Successfully updated pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1"
    Mar 23 19:01:21.841: INFO: Waiting up to 5m0s for pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" in namespace "pods-1951" to be "running"
    Mar 23 19:01:21.844: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1": Phase="Running", Reason="", readiness=true. Elapsed: 3.117993ms
    Mar 23 19:01:21.846: INFO: Pod "pod-update-c8c1d786-08fd-4a87-b99e-16a2a7d613d1" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 03/23/23 19:01:21.846
    Mar 23 19:01:21.849: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:01:21.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1951" for this suite. 03/23/23 19:01:21.856
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:21.868
Mar 23 19:01:21.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:01:21.869
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:21.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:21.889
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:01:21.909
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:01:22.395
STEP: Deploying the webhook pod 03/23/23 19:01:22.404
STEP: Wait for the deployment to be ready 03/23/23 19:01:22.429
Mar 23 19:01:22.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:01:24.457
STEP: Verifying the service has paired with the endpoint 03/23/23 19:01:24.469
Mar 23 19:01:25.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 03/23/23 19:01:25.476
STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/23/23 19:01:25.495
STEP: Creating a configMap that should not be mutated 03/23/23 19:01:25.507
STEP: Patching a mutating webhook configuration's rules to include the create operation 03/23/23 19:01:25.516
STEP: Creating a configMap that should be mutated 03/23/23 19:01:25.535
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:01:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9631" for this suite. 03/23/23 19:01:25.569
STEP: Destroying namespace "webhook-9631-markers" for this suite. 03/23/23 19:01:25.579
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":79,"skipped":1368,"failed":0}
------------------------------
• [3.849 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:21.868
    Mar 23 19:01:21.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:01:21.869
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:21.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:21.889
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:01:21.909
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:01:22.395
    STEP: Deploying the webhook pod 03/23/23 19:01:22.404
    STEP: Wait for the deployment to be ready 03/23/23 19:01:22.429
    Mar 23 19:01:22.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:01:24.457
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:01:24.469
    Mar 23 19:01:25.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 03/23/23 19:01:25.476
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 03/23/23 19:01:25.495
    STEP: Creating a configMap that should not be mutated 03/23/23 19:01:25.507
    STEP: Patching a mutating webhook configuration's rules to include the create operation 03/23/23 19:01:25.516
    STEP: Creating a configMap that should be mutated 03/23/23 19:01:25.535
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:01:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9631" for this suite. 03/23/23 19:01:25.569
    STEP: Destroying namespace "webhook-9631-markers" for this suite. 03/23/23 19:01:25.579
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:25.72
Mar 23 19:01:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-runtime 03/23/23 19:01:25.722
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:25.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:25.758
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 03/23/23 19:01:25.765
STEP: wait for the container to reach Failed 03/23/23 19:01:25.778
STEP: get the container status 03/23/23 19:01:33.82
STEP: the container should be terminated 03/23/23 19:01:33.823
STEP: the termination message should be set 03/23/23 19:01:33.823
Mar 23 19:01:33.823: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/23/23 19:01:33.823
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 23 19:01:33.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4200" for this suite. 03/23/23 19:01:33.841
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":80,"skipped":1368,"failed":0}
------------------------------
• [SLOW TEST] [8.136 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:25.72
    Mar 23 19:01:25.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-runtime 03/23/23 19:01:25.722
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:25.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:25.758
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 03/23/23 19:01:25.765
    STEP: wait for the container to reach Failed 03/23/23 19:01:25.778
    STEP: get the container status 03/23/23 19:01:33.82
    STEP: the container should be terminated 03/23/23 19:01:33.823
    STEP: the termination message should be set 03/23/23 19:01:33.823
    Mar 23 19:01:33.823: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/23/23 19:01:33.823
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 23 19:01:33.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4200" for this suite. 03/23/23 19:01:33.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:33.858
Mar 23 19:01:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:01:33.859
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:33.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:33.887
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 03/23/23 19:01:33.907
STEP: watching for the Service to be added 03/23/23 19:01:33.935
Mar 23 19:01:33.943: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Mar 23 19:01:33.944: INFO: Service test-service-7jdzw created
STEP: Getting /status 03/23/23 19:01:33.944
Mar 23 19:01:33.951: INFO: Service test-service-7jdzw has LoadBalancer: {[]}
STEP: patching the ServiceStatus 03/23/23 19:01:33.951
STEP: watching for the Service to be patched 03/23/23 19:01:33.962
Mar 23 19:01:33.966: INFO: observed Service test-service-7jdzw in namespace services-5497 with annotations: map[] & LoadBalancer: {[]}
Mar 23 19:01:33.967: INFO: Found Service test-service-7jdzw in namespace services-5497 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Mar 23 19:01:33.967: INFO: Service test-service-7jdzw has service status patched
STEP: updating the ServiceStatus 03/23/23 19:01:33.967
Mar 23 19:01:34.008: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 03/23/23 19:01:34.009
Mar 23 19:01:34.017: INFO: Observed Service test-service-7jdzw in namespace services-5497 with annotations: map[] & Conditions: {[]}
Mar 23 19:01:34.017: INFO: Observed event: &Service{ObjectMeta:{test-service-7jdzw  services-5497  dbd7417f-bc0e-4294-a971-5f9bbb3bd50f 11667 0 2023-03-23 19:01:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-23 19:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-23 19:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.0.215.83,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.0.215.83],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Mar 23 19:01:34.018: INFO: Found Service test-service-7jdzw in namespace services-5497 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 23 19:01:34.019: INFO: Service test-service-7jdzw has service status updated
STEP: patching the service 03/23/23 19:01:34.019
STEP: watching for the Service to be patched 03/23/23 19:01:34.051
Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
Mar 23 19:01:34.058: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service:patched test-service-static:true]
Mar 23 19:01:34.058: INFO: Service test-service-7jdzw patched
STEP: deleting the service 03/23/23 19:01:34.058
STEP: watching for the Service to be deleted 03/23/23 19:01:34.102
Mar 23 19:01:34.116: INFO: Observed event: ADDED
Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
Mar 23 19:01:34.116: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Mar 23 19:01:34.116: INFO: Service test-service-7jdzw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:01:34.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5497" for this suite. 03/23/23 19:01:34.131
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":81,"skipped":1393,"failed":0}
------------------------------
• [0.293 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:33.858
    Mar 23 19:01:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:01:33.859
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:33.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:33.887
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 03/23/23 19:01:33.907
    STEP: watching for the Service to be added 03/23/23 19:01:33.935
    Mar 23 19:01:33.943: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Mar 23 19:01:33.944: INFO: Service test-service-7jdzw created
    STEP: Getting /status 03/23/23 19:01:33.944
    Mar 23 19:01:33.951: INFO: Service test-service-7jdzw has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 03/23/23 19:01:33.951
    STEP: watching for the Service to be patched 03/23/23 19:01:33.962
    Mar 23 19:01:33.966: INFO: observed Service test-service-7jdzw in namespace services-5497 with annotations: map[] & LoadBalancer: {[]}
    Mar 23 19:01:33.967: INFO: Found Service test-service-7jdzw in namespace services-5497 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Mar 23 19:01:33.967: INFO: Service test-service-7jdzw has service status patched
    STEP: updating the ServiceStatus 03/23/23 19:01:33.967
    Mar 23 19:01:34.008: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 03/23/23 19:01:34.009
    Mar 23 19:01:34.017: INFO: Observed Service test-service-7jdzw in namespace services-5497 with annotations: map[] & Conditions: {[]}
    Mar 23 19:01:34.017: INFO: Observed event: &Service{ObjectMeta:{test-service-7jdzw  services-5497  dbd7417f-bc0e-4294-a971-5f9bbb3bd50f 11667 0 2023-03-23 19:01:33 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-03-23 19:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-03-23 19:01:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.0.215.83,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.0.215.83],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Mar 23 19:01:34.018: INFO: Found Service test-service-7jdzw in namespace services-5497 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 23 19:01:34.019: INFO: Service test-service-7jdzw has service status updated
    STEP: patching the service 03/23/23 19:01:34.019
    STEP: watching for the Service to be patched 03/23/23 19:01:34.051
    Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
    Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
    Mar 23 19:01:34.057: INFO: observed Service test-service-7jdzw in namespace services-5497 with labels: map[test-service-static:true]
    Mar 23 19:01:34.058: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service:patched test-service-static:true]
    Mar 23 19:01:34.058: INFO: Service test-service-7jdzw patched
    STEP: deleting the service 03/23/23 19:01:34.058
    STEP: watching for the Service to be deleted 03/23/23 19:01:34.102
    Mar 23 19:01:34.116: INFO: Observed event: ADDED
    Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
    Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
    Mar 23 19:01:34.116: INFO: Observed event: MODIFIED
    Mar 23 19:01:34.116: INFO: Found Service test-service-7jdzw in namespace services-5497 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Mar 23 19:01:34.116: INFO: Service test-service-7jdzw deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:01:34.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5497" for this suite. 03/23/23 19:01:34.131
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:34.154
Mar 23 19:01:34.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:01:34.155
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:34.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:34.182
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 03/23/23 19:01:34.185
Mar 23 19:01:34.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1095 cluster-info'
Mar 23 19:01:34.290: INFO: stderr: ""
Mar 23 19:01:34.290: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:01:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1095" for this suite. 03/23/23 19:01:34.296
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":82,"skipped":1412,"failed":0}
------------------------------
• [0.156 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:34.154
    Mar 23 19:01:34.154: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:01:34.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:34.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:34.182
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 03/23/23 19:01:34.185
    Mar 23 19:01:34.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1095 cluster-info'
    Mar 23 19:01:34.290: INFO: stderr: ""
    Mar 23 19:01:34.290: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.0.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:01:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1095" for this suite. 03/23/23 19:01:34.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:34.313
Mar 23 19:01:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 19:01:34.314
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:34.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:34.342
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3316 03/23/23 19:01:34.346
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 03/23/23 19:01:34.36
STEP: Creating pod with conflicting port in namespace statefulset-3316 03/23/23 19:01:34.377
W0323 19:01:34.390276      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until pod test-pod will start running in namespace statefulset-3316 03/23/23 19:01:34.39
Mar 23 19:01:34.390: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3316" to be "running"
Mar 23 19:01:34.396: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.334288ms
Mar 23 19:01:36.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558066s
Mar 23 19:01:38.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009439853s
Mar 23 19:01:40.400: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00971644s
Mar 23 19:01:40.400: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3316 03/23/23 19:01:40.4
W0323 19:01:40.408049      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3316 03/23/23 19:01:40.408
Mar 23 19:01:40.422: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Pending. Waiting for statefulset controller to delete.
Mar 23 19:01:40.448: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 19:01:40.458: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Failed. Waiting for statefulset controller to delete.
Mar 23 19:01:40.462: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3316
STEP: Removing pod with conflicting port in namespace statefulset-3316 03/23/23 19:01:40.462
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3316 and will be in running state 03/23/23 19:01:40.506
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 19:01:44.524: INFO: Deleting all statefulset in ns statefulset-3316
Mar 23 19:01:44.528: INFO: Scaling statefulset ss to 0
W0323 19:01:44.536547      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
Mar 23 19:01:54.547: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:01:54.549: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 19:01:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3316" for this suite. 03/23/23 19:01:54.565
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":83,"skipped":1434,"failed":0}
------------------------------
• [SLOW TEST] [20.258 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:34.313
    Mar 23 19:01:34.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 19:01:34.314
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:34.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:34.342
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3316 03/23/23 19:01:34.346
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 03/23/23 19:01:34.36
    STEP: Creating pod with conflicting port in namespace statefulset-3316 03/23/23 19:01:34.377
    W0323 19:01:34.390276      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3316 03/23/23 19:01:34.39
    Mar 23 19:01:34.390: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3316" to be "running"
    Mar 23 19:01:34.396: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.334288ms
    Mar 23 19:01:36.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009558066s
    Mar 23 19:01:38.400: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009439853s
    Mar 23 19:01:40.400: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00971644s
    Mar 23 19:01:40.400: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3316 03/23/23 19:01:40.4
    W0323 19:01:40.408049      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3316 03/23/23 19:01:40.408
    Mar 23 19:01:40.422: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Pending. Waiting for statefulset controller to delete.
    Mar 23 19:01:40.448: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 23 19:01:40.458: INFO: Observed stateful pod in namespace: statefulset-3316, name: ss-0, uid: 1980da5e-3371-485f-82fb-9adbb577bce0, status phase: Failed. Waiting for statefulset controller to delete.
    Mar 23 19:01:40.462: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3316
    STEP: Removing pod with conflicting port in namespace statefulset-3316 03/23/23 19:01:40.462
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3316 and will be in running state 03/23/23 19:01:40.506
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 19:01:44.524: INFO: Deleting all statefulset in ns statefulset-3316
    Mar 23 19:01:44.528: INFO: Scaling statefulset ss to 0
    W0323 19:01:44.536547      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "webserver" uses hostPort 21017)
    Mar 23 19:01:54.547: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 19:01:54.549: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 19:01:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3316" for this suite. 03/23/23 19:01:54.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:01:54.578
Mar 23 19:01:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:01:54.579
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:54.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:54.605
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 23 19:02:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7905" for this suite. 03/23/23 19:02:02.641
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":84,"skipped":1445,"failed":0}
------------------------------
• [SLOW TEST] [8.073 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:01:54.578
    Mar 23 19:01:54.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:01:54.579
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:01:54.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:01:54.605
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 23 19:02:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7905" for this suite. 03/23/23 19:02:02.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:02.661
Mar 23 19:02:02.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename proxy 03/23/23 19:02:02.662
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:02.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:02.697
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Mar 23 19:02:02.708: INFO: Creating pod...
Mar 23 19:02:02.740: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6458" to be "running"
Mar 23 19:02:02.747: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794282ms
Mar 23 19:02:04.752: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011542135s
Mar 23 19:02:06.751: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010951402s
Mar 23 19:02:08.752: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011300792s
Mar 23 19:02:10.751: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.01053606s
Mar 23 19:02:10.751: INFO: Pod "agnhost" satisfied condition "running"
Mar 23 19:02:10.751: INFO: Creating service...
Mar 23 19:02:10.766: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=DELETE
Mar 23 19:02:10.786: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 23 19:02:10.786: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=OPTIONS
Mar 23 19:02:10.798: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 23 19:02:10.798: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=PATCH
Mar 23 19:02:10.803: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 23 19:02:10.803: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=POST
Mar 23 19:02:10.809: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 23 19:02:10.810: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=PUT
Mar 23 19:02:10.815: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 23 19:02:10.815: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=DELETE
Mar 23 19:02:10.827: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 23 19:02:10.827: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=OPTIONS
Mar 23 19:02:10.836: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 23 19:02:10.837: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=PATCH
Mar 23 19:02:10.846: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 23 19:02:10.847: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=POST
Mar 23 19:02:10.853: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 23 19:02:10.853: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=PUT
Mar 23 19:02:10.859: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 23 19:02:10.859: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=GET
Mar 23 19:02:10.865: INFO: http.Client request:GET StatusCode:301
Mar 23 19:02:10.865: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=GET
Mar 23 19:02:10.871: INFO: http.Client request:GET StatusCode:301
Mar 23 19:02:10.872: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=HEAD
Mar 23 19:02:10.877: INFO: http.Client request:HEAD StatusCode:301
Mar 23 19:02:10.877: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=HEAD
Mar 23 19:02:10.885: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 23 19:02:10.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6458" for this suite. 03/23/23 19:02:10.893
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":85,"skipped":1476,"failed":0}
------------------------------
• [SLOW TEST] [8.246 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:02.661
    Mar 23 19:02:02.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename proxy 03/23/23 19:02:02.662
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:02.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:02.697
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Mar 23 19:02:02.708: INFO: Creating pod...
    Mar 23 19:02:02.740: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-6458" to be "running"
    Mar 23 19:02:02.747: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.794282ms
    Mar 23 19:02:04.752: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011542135s
    Mar 23 19:02:06.751: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010951402s
    Mar 23 19:02:08.752: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011300792s
    Mar 23 19:02:10.751: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.01053606s
    Mar 23 19:02:10.751: INFO: Pod "agnhost" satisfied condition "running"
    Mar 23 19:02:10.751: INFO: Creating service...
    Mar 23 19:02:10.766: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=DELETE
    Mar 23 19:02:10.786: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 23 19:02:10.786: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=OPTIONS
    Mar 23 19:02:10.798: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 23 19:02:10.798: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=PATCH
    Mar 23 19:02:10.803: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 23 19:02:10.803: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=POST
    Mar 23 19:02:10.809: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 23 19:02:10.810: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=PUT
    Mar 23 19:02:10.815: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 23 19:02:10.815: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=DELETE
    Mar 23 19:02:10.827: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 23 19:02:10.827: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Mar 23 19:02:10.836: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 23 19:02:10.837: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=PATCH
    Mar 23 19:02:10.846: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 23 19:02:10.847: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=POST
    Mar 23 19:02:10.853: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 23 19:02:10.853: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=PUT
    Mar 23 19:02:10.859: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 23 19:02:10.859: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=GET
    Mar 23 19:02:10.865: INFO: http.Client request:GET StatusCode:301
    Mar 23 19:02:10.865: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=GET
    Mar 23 19:02:10.871: INFO: http.Client request:GET StatusCode:301
    Mar 23 19:02:10.872: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/pods/agnhost/proxy?method=HEAD
    Mar 23 19:02:10.877: INFO: http.Client request:HEAD StatusCode:301
    Mar 23 19:02:10.877: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-6458/services/e2e-proxy-test-service/proxy?method=HEAD
    Mar 23 19:02:10.885: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 23 19:02:10.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6458" for this suite. 03/23/23 19:02:10.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:10.91
Mar 23 19:02:10.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:02:10.912
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:10.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:10.953
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:02:10.989
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:02:11.792
STEP: Deploying the webhook pod 03/23/23 19:02:11.797
STEP: Wait for the deployment to be ready 03/23/23 19:02:11.809
Mar 23 19:02:11.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:02:13.835
STEP: Verifying the service has paired with the endpoint 03/23/23 19:02:13.848
Mar 23 19:02:14.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/23/23 19:02:14.852
STEP: create a configmap that should be updated by the webhook 03/23/23 19:02:14.871
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:02:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3241" for this suite. 03/23/23 19:02:14.909
STEP: Destroying namespace "webhook-3241-markers" for this suite. 03/23/23 19:02:14.919
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":86,"skipped":1486,"failed":0}
------------------------------
• [4.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:10.91
    Mar 23 19:02:10.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:02:10.912
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:10.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:10.953
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:02:10.989
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:02:11.792
    STEP: Deploying the webhook pod 03/23/23 19:02:11.797
    STEP: Wait for the deployment to be ready 03/23/23 19:02:11.809
    Mar 23 19:02:11.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:02:13.835
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:02:13.848
    Mar 23 19:02:14.848: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 03/23/23 19:02:14.852
    STEP: create a configmap that should be updated by the webhook 03/23/23 19:02:14.871
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:02:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3241" for this suite. 03/23/23 19:02:14.909
    STEP: Destroying namespace "webhook-3241-markers" for this suite. 03/23/23 19:02:14.919
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:15.081
Mar 23 19:02:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:02:15.082
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:15.119
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:15.122
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/23/23 19:02:15.126
Mar 23 19:02:15.140: INFO: Waiting up to 5m0s for pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b" in namespace "emptydir-1125" to be "Succeeded or Failed"
Mar 23 19:02:15.145: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625588ms
Mar 23 19:02:17.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009647012s
Mar 23 19:02:19.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 4.009601735s
Mar 23 19:02:21.149: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 6.00901186s
Mar 23 19:02:23.151: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=false. Elapsed: 8.010707778s
Mar 23 19:02:25.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009700716s
STEP: Saw pod success 03/23/23 19:02:25.15
Mar 23 19:02:25.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b" satisfied condition "Succeeded or Failed"
Mar 23 19:02:25.154: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b container test-container: <nil>
STEP: delete the pod 03/23/23 19:02:25.16
Mar 23 19:02:25.175: INFO: Waiting for pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b to disappear
Mar 23 19:02:25.178: INFO: Pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:02:25.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1125" for this suite. 03/23/23 19:02:25.182
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":87,"skipped":1514,"failed":0}
------------------------------
• [SLOW TEST] [10.109 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:15.081
    Mar 23 19:02:15.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:02:15.082
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:15.119
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:15.122
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/23/23 19:02:15.126
    Mar 23 19:02:15.140: INFO: Waiting up to 5m0s for pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b" in namespace "emptydir-1125" to be "Succeeded or Failed"
    Mar 23 19:02:15.145: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.625588ms
    Mar 23 19:02:17.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009647012s
    Mar 23 19:02:19.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 4.009601735s
    Mar 23 19:02:21.149: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=true. Elapsed: 6.00901186s
    Mar 23 19:02:23.151: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Running", Reason="", readiness=false. Elapsed: 8.010707778s
    Mar 23 19:02:25.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.009700716s
    STEP: Saw pod success 03/23/23 19:02:25.15
    Mar 23 19:02:25.150: INFO: Pod "pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b" satisfied condition "Succeeded or Failed"
    Mar 23 19:02:25.154: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b container test-container: <nil>
    STEP: delete the pod 03/23/23 19:02:25.16
    Mar 23 19:02:25.175: INFO: Waiting for pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b to disappear
    Mar 23 19:02:25.178: INFO: Pod pod-54fe6e4b-cb64-471f-9e1c-1a538812a94b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:02:25.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1125" for this suite. 03/23/23 19:02:25.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:25.202
Mar 23 19:02:25.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename endpointslice 03/23/23 19:02:25.209
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:25.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:25.234
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 03/23/23 19:02:25.238
STEP: getting /apis/discovery.k8s.io 03/23/23 19:02:25.241
STEP: getting /apis/discovery.k8s.iov1 03/23/23 19:02:25.242
STEP: creating 03/23/23 19:02:25.243
STEP: getting 03/23/23 19:02:25.262
STEP: listing 03/23/23 19:02:25.265
STEP: watching 03/23/23 19:02:25.286
Mar 23 19:02:25.286: INFO: starting watch
STEP: cluster-wide listing 03/23/23 19:02:25.287
STEP: cluster-wide watching 03/23/23 19:02:25.291
Mar 23 19:02:25.291: INFO: starting watch
STEP: patching 03/23/23 19:02:25.292
STEP: updating 03/23/23 19:02:25.297
Mar 23 19:02:25.306: INFO: waiting for watch events with expected annotations
Mar 23 19:02:25.306: INFO: saw patched and updated annotations
STEP: deleting 03/23/23 19:02:25.306
STEP: deleting a collection 03/23/23 19:02:25.317
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 23 19:02:25.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7580" for this suite. 03/23/23 19:02:25.337
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":88,"skipped":1520,"failed":0}
------------------------------
• [0.141 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:25.202
    Mar 23 19:02:25.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename endpointslice 03/23/23 19:02:25.209
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:25.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:25.234
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 03/23/23 19:02:25.238
    STEP: getting /apis/discovery.k8s.io 03/23/23 19:02:25.241
    STEP: getting /apis/discovery.k8s.iov1 03/23/23 19:02:25.242
    STEP: creating 03/23/23 19:02:25.243
    STEP: getting 03/23/23 19:02:25.262
    STEP: listing 03/23/23 19:02:25.265
    STEP: watching 03/23/23 19:02:25.286
    Mar 23 19:02:25.286: INFO: starting watch
    STEP: cluster-wide listing 03/23/23 19:02:25.287
    STEP: cluster-wide watching 03/23/23 19:02:25.291
    Mar 23 19:02:25.291: INFO: starting watch
    STEP: patching 03/23/23 19:02:25.292
    STEP: updating 03/23/23 19:02:25.297
    Mar 23 19:02:25.306: INFO: waiting for watch events with expected annotations
    Mar 23 19:02:25.306: INFO: saw patched and updated annotations
    STEP: deleting 03/23/23 19:02:25.306
    STEP: deleting a collection 03/23/23 19:02:25.317
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 23 19:02:25.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7580" for this suite. 03/23/23 19:02:25.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:25.345
Mar 23 19:02:25.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:02:25.346
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:25.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:25.369
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:02:25.372
Mar 23 19:02:25.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3" in namespace "projected-953" to be "Succeeded or Failed"
Mar 23 19:02:25.394: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.173368ms
Mar 23 19:02:27.400: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018888005s
Mar 23 19:02:29.398: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Running", Reason="", readiness=true. Elapsed: 4.017241561s
Mar 23 19:02:31.399: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Running", Reason="", readiness=false. Elapsed: 6.018830108s
Mar 23 19:02:33.400: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019620243s
STEP: Saw pod success 03/23/23 19:02:33.4
Mar 23 19:02:33.401: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3" satisfied condition "Succeeded or Failed"
Mar 23 19:02:33.405: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 container client-container: <nil>
STEP: delete the pod 03/23/23 19:02:33.41
Mar 23 19:02:33.422: INFO: Waiting for pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 to disappear
Mar 23 19:02:33.425: INFO: Pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:02:33.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-953" for this suite. 03/23/23 19:02:33.43
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":89,"skipped":1527,"failed":0}
------------------------------
• [SLOW TEST] [8.090 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:25.345
    Mar 23 19:02:25.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:02:25.346
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:25.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:25.369
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:02:25.372
    Mar 23 19:02:25.381: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3" in namespace "projected-953" to be "Succeeded or Failed"
    Mar 23 19:02:25.394: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 13.173368ms
    Mar 23 19:02:27.400: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018888005s
    Mar 23 19:02:29.398: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Running", Reason="", readiness=true. Elapsed: 4.017241561s
    Mar 23 19:02:31.399: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Running", Reason="", readiness=false. Elapsed: 6.018830108s
    Mar 23 19:02:33.400: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019620243s
    STEP: Saw pod success 03/23/23 19:02:33.4
    Mar 23 19:02:33.401: INFO: Pod "downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3" satisfied condition "Succeeded or Failed"
    Mar 23 19:02:33.405: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 container client-container: <nil>
    STEP: delete the pod 03/23/23 19:02:33.41
    Mar 23 19:02:33.422: INFO: Waiting for pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 to disappear
    Mar 23 19:02:33.425: INFO: Pod downwardapi-volume-3d76790f-73a9-4687-985b-910f72a72aa3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:02:33.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-953" for this suite. 03/23/23 19:02:33.43
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:33.443
Mar 23 19:02:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:02:33.445
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:33.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:33.462
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:02:33.471
Mar 23 19:02:33.488: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6651" to be "running and ready"
Mar 23 19:02:33.499: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.29217ms
Mar 23 19:02:33.499: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:02:35.504: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015961508s
Mar 23 19:02:35.504: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 23 19:02:35.504: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 03/23/23 19:02:35.507
Mar 23 19:02:35.512: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6651" to be "running and ready"
Mar 23 19:02:35.516: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481091ms
Mar 23 19:02:35.516: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:02:37.524: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267218s
Mar 23 19:02:37.524: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Mar 23 19:02:37.524: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/23/23 19:02:37.533
STEP: delete the pod with lifecycle hook 03/23/23 19:02:37.542
Mar 23 19:02:37.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:02:37.574: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 19:02:39.574: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:02:39.593: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 19:02:41.574: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:02:41.581: INFO: Pod pod-with-poststart-http-hook still exists
Mar 23 19:02:43.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Mar 23 19:02:43.578: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 23 19:02:43.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6651" for this suite. 03/23/23 19:02:43.583
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":90,"skipped":1527,"failed":0}
------------------------------
• [SLOW TEST] [10.146 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:33.443
    Mar 23 19:02:33.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:02:33.445
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:33.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:33.462
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:02:33.471
    Mar 23 19:02:33.488: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6651" to be "running and ready"
    Mar 23 19:02:33.499: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.29217ms
    Mar 23 19:02:33.499: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:02:35.504: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015961508s
    Mar 23 19:02:35.504: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 23 19:02:35.504: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 03/23/23 19:02:35.507
    Mar 23 19:02:35.512: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6651" to be "running and ready"
    Mar 23 19:02:35.516: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.481091ms
    Mar 23 19:02:35.516: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:02:37.524: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267218s
    Mar 23 19:02:37.524: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Mar 23 19:02:37.524: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/23/23 19:02:37.533
    STEP: delete the pod with lifecycle hook 03/23/23 19:02:37.542
    Mar 23 19:02:37.561: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 23 19:02:37.574: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 23 19:02:39.574: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 23 19:02:39.593: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 23 19:02:41.574: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 23 19:02:41.581: INFO: Pod pod-with-poststart-http-hook still exists
    Mar 23 19:02:43.575: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Mar 23 19:02:43.578: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 23 19:02:43.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6651" for this suite. 03/23/23 19:02:43.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:43.597
Mar 23 19:02:43.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 19:02:43.598
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:43.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:43.618
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 03/23/23 19:02:43.657
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:02:43.665
Mar 23 19:02:43.676: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:43.677: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:43.677: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:43.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:02:43.681: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:02:44.686: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:44.687: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:44.687: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:44.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:02:44.690: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:02:45.688: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:45.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:45.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:45.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 19:02:45.692: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:02:46.688: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:46.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:46.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:02:46.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 19:02:46.696: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 03/23/23 19:02:46.702
Mar 23 19:02:46.705: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 03/23/23 19:02:46.705
Mar 23 19:02:46.716: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 03/23/23 19:02:46.716
Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: ADDED
Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.719: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.719: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.719: INFO: Found daemon set daemon-set in namespace daemonsets-514 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 23 19:02:46.719: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 03/23/23 19:02:46.719
STEP: watching for the daemon set status to be patched 03/23/23 19:02:46.726
Mar 23 19:02:46.728: INFO: Observed &DaemonSet event: ADDED
Mar 23 19:02:46.728: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.729: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.729: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.730: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.730: INFO: Observed daemon set daemon-set in namespace daemonsets-514 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 23 19:02:46.730: INFO: Observed &DaemonSet event: MODIFIED
Mar 23 19:02:46.730: INFO: Found daemon set daemon-set in namespace daemonsets-514 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Mar 23 19:02:46.730: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 19:02:46.743
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-514, will wait for the garbage collector to delete the pods 03/23/23 19:02:46.743
Mar 23 19:02:46.801: INFO: Deleting DaemonSet.extensions daemon-set took: 4.951387ms
Mar 23 19:02:46.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.334236ms
Mar 23 19:02:51.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:02:51.905: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 19:02:51.908: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12352"},"items":null}

Mar 23 19:02:51.911: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12352"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:02:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-514" for this suite. 03/23/23 19:02:51.929
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":91,"skipped":1537,"failed":0}
------------------------------
• [SLOW TEST] [8.341 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:43.597
    Mar 23 19:02:43.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 19:02:43.598
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:43.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:43.618
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 03/23/23 19:02:43.657
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:02:43.665
    Mar 23 19:02:43.676: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:43.677: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:43.677: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:43.681: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:02:43.681: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:02:44.686: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:44.687: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:44.687: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:44.690: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:02:44.690: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:02:45.688: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:45.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:45.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:45.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 19:02:45.692: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:02:46.688: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:46.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:46.689: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:02:46.696: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 19:02:46.696: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 03/23/23 19:02:46.702
    Mar 23 19:02:46.705: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 03/23/23 19:02:46.705
    Mar 23 19:02:46.716: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 03/23/23 19:02:46.716
    Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: ADDED
    Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.718: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.719: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.719: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.719: INFO: Found daemon set daemon-set in namespace daemonsets-514 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 23 19:02:46.719: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 03/23/23 19:02:46.719
    STEP: watching for the daemon set status to be patched 03/23/23 19:02:46.726
    Mar 23 19:02:46.728: INFO: Observed &DaemonSet event: ADDED
    Mar 23 19:02:46.728: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.729: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.729: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.730: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.730: INFO: Observed daemon set daemon-set in namespace daemonsets-514 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 23 19:02:46.730: INFO: Observed &DaemonSet event: MODIFIED
    Mar 23 19:02:46.730: INFO: Found daemon set daemon-set in namespace daemonsets-514 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Mar 23 19:02:46.730: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 19:02:46.743
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-514, will wait for the garbage collector to delete the pods 03/23/23 19:02:46.743
    Mar 23 19:02:46.801: INFO: Deleting DaemonSet.extensions daemon-set took: 4.951387ms
    Mar 23 19:02:46.902: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.334236ms
    Mar 23 19:02:51.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:02:51.905: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 19:02:51.908: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12352"},"items":null}

    Mar 23 19:02:51.911: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12352"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:02:51.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-514" for this suite. 03/23/23 19:02:51.929
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:02:51.945
Mar 23 19:02:51.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir-wrapper 03/23/23 19:02:51.947
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:51.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:51.986
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 03/23/23 19:02:51.989
STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:02:52.257
Mar 23 19:02:52.338: INFO: Pod name wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363: Found 1 pods out of 5
Mar 23 19:02:57.349: INFO: Pod name wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/23/23 19:02:57.349
Mar 23 19:02:57.349: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:02:57.353: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218289ms
Mar 23 19:02:59.358: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009206711s
Mar 23 19:03:01.362: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012526837s
Mar 23 19:03:03.369: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019784153s
Mar 23 19:03:05.360: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010542597s
Mar 23 19:03:07.359: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009329138s
Mar 23 19:03:09.361: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.011327066s
Mar 23 19:03:09.361: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x" satisfied condition "running"
Mar 23 19:03:09.361: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:09.367: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d": Phase="Running", Reason="", readiness=true. Elapsed: 5.765778ms
Mar 23 19:03:09.367: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d" satisfied condition "running"
Mar 23 19:03:09.367: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:09.371: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr": Phase="Running", Reason="", readiness=true. Elapsed: 4.064785ms
Mar 23 19:03:09.371: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr" satisfied condition "running"
Mar 23 19:03:09.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:09.375: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.215184ms
Mar 23 19:03:09.376: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd" satisfied condition "running"
Mar 23 19:03:09.376: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:09.380: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz": Phase="Running", Reason="", readiness=true. Elapsed: 4.661482ms
Mar 23 19:03:09.380: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:03:09.381
Mar 23 19:03:09.443: INFO: Deleting ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 took: 8.196968ms
Mar 23 19:03:09.646: INFO: Terminating ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 pods took: 202.104326ms
STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:03:17.451
Mar 23 19:03:17.471: INFO: Pod name wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4: Found 0 pods out of 5
Mar 23 19:03:22.480: INFO: Pod name wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/23/23 19:03:22.48
Mar 23 19:03:22.480: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:22.483: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121256ms
Mar 23 19:03:24.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008091498s
Mar 23 19:03:26.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008224438s
Mar 23 19:03:28.489: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008799811s
Mar 23 19:03:30.489: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008795775s
Mar 23 19:03:32.492: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011696331s
Mar 23 19:03:34.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008019304s
Mar 23 19:03:36.490: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Running", Reason="", readiness=true. Elapsed: 14.010003083s
Mar 23 19:03:36.490: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47" satisfied condition "running"
Mar 23 19:03:36.490: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:36.493: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k": Phase="Running", Reason="", readiness=true. Elapsed: 3.451521ms
Mar 23 19:03:36.493: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k" satisfied condition "running"
Mar 23 19:03:36.493: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:36.497: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4": Phase="Running", Reason="", readiness=true. Elapsed: 3.533921ms
Mar 23 19:03:36.497: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4" satisfied condition "running"
Mar 23 19:03:36.497: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:36.501: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h": Phase="Running", Reason="", readiness=true. Elapsed: 3.843323ms
Mar 23 19:03:36.501: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h" satisfied condition "running"
Mar 23 19:03:36.501: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:36.505: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8": Phase="Running", Reason="", readiness=true. Elapsed: 3.640321ms
Mar 23 19:03:36.505: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:03:36.505
Mar 23 19:03:36.564: INFO: Deleting ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 took: 5.240431ms
Mar 23 19:03:36.765: INFO: Terminating ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 pods took: 201.124385ms
STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:03:43.672
Mar 23 19:03:43.690: INFO: Pod name wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234: Found 0 pods out of 5
Mar 23 19:03:48.701: INFO: Pod name wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234: Found 5 pods out of 5
STEP: Ensuring each pod is running 03/23/23 19:03:48.702
Mar 23 19:03:48.702: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:48.706: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903991ms
Mar 23 19:03:50.713: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010831147s
Mar 23 19:03:52.712: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009811062s
Mar 23 19:03:54.711: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009195121s
Mar 23 19:03:56.712: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009813677s
Mar 23 19:03:58.714: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Running", Reason="", readiness=true. Elapsed: 10.011764232s
Mar 23 19:03:58.714: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np" satisfied condition "running"
Mar 23 19:03:58.714: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:03:58.724: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929": Phase="Pending", Reason="", readiness=false. Elapsed: 10.829983ms
Mar 23 19:04:00.741: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929": Phase="Running", Reason="", readiness=true. Elapsed: 2.027369062s
Mar 23 19:04:00.741: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929" satisfied condition "running"
Mar 23 19:04:00.741: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:04:00.746: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x": Phase="Running", Reason="", readiness=true. Elapsed: 4.993894ms
Mar 23 19:04:00.746: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x" satisfied condition "running"
Mar 23 19:04:00.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:04:00.752: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb": Phase="Running", Reason="", readiness=true. Elapsed: 5.646894ms
Mar 23 19:04:00.752: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb" satisfied condition "running"
Mar 23 19:04:00.752: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56" in namespace "emptydir-wrapper-6953" to be "running"
Mar 23 19:04:00.758: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56": Phase="Running", Reason="", readiness=true. Elapsed: 5.888693ms
Mar 23 19:04:00.758: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:04:00.758
Mar 23 19:04:00.821: INFO: Deleting ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 took: 8.76659ms
Mar 23 19:04:00.921: INFO: Terminating ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 pods took: 100.346885ms
STEP: Cleaning up the configMaps 03/23/23 19:04:10.721
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 23 19:04:11.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6953" for this suite. 03/23/23 19:04:11.096
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":92,"skipped":1540,"failed":0}
------------------------------
• [SLOW TEST] [79.156 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:02:51.945
    Mar 23 19:02:51.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir-wrapper 03/23/23 19:02:51.947
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:02:51.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:02:51.986
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 03/23/23 19:02:51.989
    STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:02:52.257
    Mar 23 19:02:52.338: INFO: Pod name wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363: Found 1 pods out of 5
    Mar 23 19:02:57.349: INFO: Pod name wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/23/23 19:02:57.349
    Mar 23 19:02:57.349: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:02:57.353: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218289ms
    Mar 23 19:02:59.358: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009206711s
    Mar 23 19:03:01.362: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012526837s
    Mar 23 19:03:03.369: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019784153s
    Mar 23 19:03:05.360: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 8.010542597s
    Mar 23 19:03:07.359: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009329138s
    Mar 23 19:03:09.361: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.011327066s
    Mar 23 19:03:09.361: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-2mx5x" satisfied condition "running"
    Mar 23 19:03:09.361: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:09.367: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d": Phase="Running", Reason="", readiness=true. Elapsed: 5.765778ms
    Mar 23 19:03:09.367: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-8mr6d" satisfied condition "running"
    Mar 23 19:03:09.367: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:09.371: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr": Phase="Running", Reason="", readiness=true. Elapsed: 4.064785ms
    Mar 23 19:03:09.371: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-bjjbr" satisfied condition "running"
    Mar 23 19:03:09.371: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:09.375: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd": Phase="Running", Reason="", readiness=true. Elapsed: 4.215184ms
    Mar 23 19:03:09.376: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-hr4cd" satisfied condition "running"
    Mar 23 19:03:09.376: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:09.380: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz": Phase="Running", Reason="", readiness=true. Elapsed: 4.661482ms
    Mar 23 19:03:09.380: INFO: Pod "wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363-m8kjz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:03:09.381
    Mar 23 19:03:09.443: INFO: Deleting ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 took: 8.196968ms
    Mar 23 19:03:09.646: INFO: Terminating ReplicationController wrapped-volume-race-1c43a387-6e1c-4bf9-8ceb-12750f379363 pods took: 202.104326ms
    STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:03:17.451
    Mar 23 19:03:17.471: INFO: Pod name wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4: Found 0 pods out of 5
    Mar 23 19:03:22.480: INFO: Pod name wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/23/23 19:03:22.48
    Mar 23 19:03:22.480: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:22.483: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.121256ms
    Mar 23 19:03:24.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008091498s
    Mar 23 19:03:26.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008224438s
    Mar 23 19:03:28.489: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008799811s
    Mar 23 19:03:30.489: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008795775s
    Mar 23 19:03:32.492: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011696331s
    Mar 23 19:03:34.488: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008019304s
    Mar 23 19:03:36.490: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47": Phase="Running", Reason="", readiness=true. Elapsed: 14.010003083s
    Mar 23 19:03:36.490: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-8dx47" satisfied condition "running"
    Mar 23 19:03:36.490: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:36.493: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k": Phase="Running", Reason="", readiness=true. Elapsed: 3.451521ms
    Mar 23 19:03:36.493: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-gfb4k" satisfied condition "running"
    Mar 23 19:03:36.493: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:36.497: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4": Phase="Running", Reason="", readiness=true. Elapsed: 3.533921ms
    Mar 23 19:03:36.497: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-knjk4" satisfied condition "running"
    Mar 23 19:03:36.497: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:36.501: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h": Phase="Running", Reason="", readiness=true. Elapsed: 3.843323ms
    Mar 23 19:03:36.501: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-pww9h" satisfied condition "running"
    Mar 23 19:03:36.501: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:36.505: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8": Phase="Running", Reason="", readiness=true. Elapsed: 3.640321ms
    Mar 23 19:03:36.505: INFO: Pod "wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4-tcxs8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:03:36.505
    Mar 23 19:03:36.564: INFO: Deleting ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 took: 5.240431ms
    Mar 23 19:03:36.765: INFO: Terminating ReplicationController wrapped-volume-race-f69a21a1-bc9a-4950-acf9-82a0108605c4 pods took: 201.124385ms
    STEP: Creating RC which spawns configmap-volume pods 03/23/23 19:03:43.672
    Mar 23 19:03:43.690: INFO: Pod name wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234: Found 0 pods out of 5
    Mar 23 19:03:48.701: INFO: Pod name wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234: Found 5 pods out of 5
    STEP: Ensuring each pod is running 03/23/23 19:03:48.702
    Mar 23 19:03:48.702: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:48.706: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 3.903991ms
    Mar 23 19:03:50.713: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010831147s
    Mar 23 19:03:52.712: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009811062s
    Mar 23 19:03:54.711: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009195121s
    Mar 23 19:03:56.712: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009813677s
    Mar 23 19:03:58.714: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np": Phase="Running", Reason="", readiness=true. Elapsed: 10.011764232s
    Mar 23 19:03:58.714: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-2b2np" satisfied condition "running"
    Mar 23 19:03:58.714: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:03:58.724: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929": Phase="Pending", Reason="", readiness=false. Elapsed: 10.829983ms
    Mar 23 19:04:00.741: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929": Phase="Running", Reason="", readiness=true. Elapsed: 2.027369062s
    Mar 23 19:04:00.741: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-f6929" satisfied condition "running"
    Mar 23 19:04:00.741: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:04:00.746: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x": Phase="Running", Reason="", readiness=true. Elapsed: 4.993894ms
    Mar 23 19:04:00.746: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-m6g2x" satisfied condition "running"
    Mar 23 19:04:00.746: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:04:00.752: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb": Phase="Running", Reason="", readiness=true. Elapsed: 5.646894ms
    Mar 23 19:04:00.752: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-rfgqb" satisfied condition "running"
    Mar 23 19:04:00.752: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56" in namespace "emptydir-wrapper-6953" to be "running"
    Mar 23 19:04:00.758: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56": Phase="Running", Reason="", readiness=true. Elapsed: 5.888693ms
    Mar 23 19:04:00.758: INFO: Pod "wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234-zmt56" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 in namespace emptydir-wrapper-6953, will wait for the garbage collector to delete the pods 03/23/23 19:04:00.758
    Mar 23 19:04:00.821: INFO: Deleting ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 took: 8.76659ms
    Mar 23 19:04:00.921: INFO: Terminating ReplicationController wrapped-volume-race-3207dd3f-2a6b-4c02-8a09-c0ffdc1e2234 pods took: 100.346885ms
    STEP: Cleaning up the configMaps 03/23/23 19:04:10.721
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:04:11.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-6953" for this suite. 03/23/23 19:04:11.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:04:11.106
Mar 23 19:04:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 19:04:11.107
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:11.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:11.132
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 03/23/23 19:04:11.136
STEP: Ensure pods equal to paralellism count is attached to the job 03/23/23 19:04:11.148
STEP: patching /status 03/23/23 19:04:13.154
STEP: updating /status 03/23/23 19:04:13.162
STEP: get /status 03/23/23 19:04:13.177
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 19:04:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5353" for this suite. 03/23/23 19:04:13.187
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":93,"skipped":1588,"failed":0}
------------------------------
• [2.087 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:04:11.106
    Mar 23 19:04:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 19:04:11.107
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:11.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:11.132
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 03/23/23 19:04:11.136
    STEP: Ensure pods equal to paralellism count is attached to the job 03/23/23 19:04:11.148
    STEP: patching /status 03/23/23 19:04:13.154
    STEP: updating /status 03/23/23 19:04:13.162
    STEP: get /status 03/23/23 19:04:13.177
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 19:04:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5353" for this suite. 03/23/23 19:04:13.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:04:13.199
Mar 23 19:04:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 19:04:13.2
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:13.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:13.218
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 03/23/23 19:04:13.222
Mar 23 19:04:13.236: INFO: Waiting up to 5m0s for pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085" in namespace "var-expansion-972" to be "Succeeded or Failed"
Mar 23 19:04:13.246: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Pending", Reason="", readiness=false. Elapsed: 9.875077ms
Mar 23 19:04:15.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=true. Elapsed: 2.013898427s
Mar 23 19:04:17.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=true. Elapsed: 4.014170056s
Mar 23 19:04:19.251: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=false. Elapsed: 6.014327885s
Mar 23 19:04:21.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014283674s
STEP: Saw pod success 03/23/23 19:04:21.251
Mar 23 19:04:21.251: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085" satisfied condition "Succeeded or Failed"
Mar 23 19:04:21.253: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 container dapi-container: <nil>
STEP: delete the pod 03/23/23 19:04:21.305
Mar 23 19:04:21.318: INFO: Waiting for pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 to disappear
Mar 23 19:04:21.320: INFO: Pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 19:04:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-972" for this suite. 03/23/23 19:04:21.325
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":94,"skipped":1597,"failed":0}
------------------------------
• [SLOW TEST] [8.131 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:04:13.199
    Mar 23 19:04:13.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 19:04:13.2
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:13.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:13.218
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 03/23/23 19:04:13.222
    Mar 23 19:04:13.236: INFO: Waiting up to 5m0s for pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085" in namespace "var-expansion-972" to be "Succeeded or Failed"
    Mar 23 19:04:13.246: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Pending", Reason="", readiness=false. Elapsed: 9.875077ms
    Mar 23 19:04:15.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=true. Elapsed: 2.013898427s
    Mar 23 19:04:17.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=true. Elapsed: 4.014170056s
    Mar 23 19:04:19.251: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Running", Reason="", readiness=false. Elapsed: 6.014327885s
    Mar 23 19:04:21.250: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.014283674s
    STEP: Saw pod success 03/23/23 19:04:21.251
    Mar 23 19:04:21.251: INFO: Pod "var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085" satisfied condition "Succeeded or Failed"
    Mar 23 19:04:21.253: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 19:04:21.305
    Mar 23 19:04:21.318: INFO: Waiting for pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 to disappear
    Mar 23 19:04:21.320: INFO: Pod var-expansion-a4e713df-b611-4ed0-b8ba-0eca0c80a085 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 19:04:21.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-972" for this suite. 03/23/23 19:04:21.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:04:21.331
Mar 23 19:04:21.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:04:21.338
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:21.361
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:21.365
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-92e9db3e-49c4-45f8-a845-11320890ddb5 03/23/23 19:04:21.373
STEP: Creating configMap with name cm-test-opt-upd-fba3ff04-5604-4599-834b-dc299d7ae911 03/23/23 19:04:21.377
STEP: Creating the pod 03/23/23 19:04:21.381
Mar 23 19:04:21.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d" in namespace "projected-2710" to be "running and ready"
Mar 23 19:04:21.415: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.557452ms
Mar 23 19:04:21.415: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:04:23.420: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024661967s
Mar 23 19:04:23.420: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:04:25.421: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Running", Reason="", readiness=true. Elapsed: 4.025492891s
Mar 23 19:04:25.421: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Running (Ready = true)
Mar 23 19:04:25.421: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-92e9db3e-49c4-45f8-a845-11320890ddb5 03/23/23 19:04:25.441
STEP: Updating configmap cm-test-opt-upd-fba3ff04-5604-4599-834b-dc299d7ae911 03/23/23 19:04:25.447
STEP: Creating configMap with name cm-test-opt-create-bdf7937e-b9b5-4054-8e23-f64e452b8680 03/23/23 19:04:25.451
STEP: waiting to observe update in volume 03/23/23 19:04:25.461
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 19:05:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2710" for this suite. 03/23/23 19:05:41.845
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":95,"skipped":1614,"failed":0}
------------------------------
• [SLOW TEST] [80.519 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:04:21.331
    Mar 23 19:04:21.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:04:21.338
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:04:21.361
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:04:21.365
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-92e9db3e-49c4-45f8-a845-11320890ddb5 03/23/23 19:04:21.373
    STEP: Creating configMap with name cm-test-opt-upd-fba3ff04-5604-4599-834b-dc299d7ae911 03/23/23 19:04:21.377
    STEP: Creating the pod 03/23/23 19:04:21.381
    Mar 23 19:04:21.395: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d" in namespace "projected-2710" to be "running and ready"
    Mar 23 19:04:21.415: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Pending", Reason="", readiness=false. Elapsed: 19.557452ms
    Mar 23 19:04:21.415: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:04:23.420: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024661967s
    Mar 23 19:04:23.420: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:04:25.421: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d": Phase="Running", Reason="", readiness=true. Elapsed: 4.025492891s
    Mar 23 19:04:25.421: INFO: The phase of Pod pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d is Running (Ready = true)
    Mar 23 19:04:25.421: INFO: Pod "pod-projected-configmaps-77d993d1-15c4-4c39-949e-79caf35cf92d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-92e9db3e-49c4-45f8-a845-11320890ddb5 03/23/23 19:04:25.441
    STEP: Updating configmap cm-test-opt-upd-fba3ff04-5604-4599-834b-dc299d7ae911 03/23/23 19:04:25.447
    STEP: Creating configMap with name cm-test-opt-create-bdf7937e-b9b5-4054-8e23-f64e452b8680 03/23/23 19:04:25.451
    STEP: waiting to observe update in volume 03/23/23 19:04:25.461
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 19:05:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2710" for this suite. 03/23/23 19:05:41.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:05:41.854
Mar 23 19:05:41.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:05:41.855
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:41.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:41.877
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:05:41.881
Mar 23 19:05:41.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031" in namespace "projected-9877" to be "Succeeded or Failed"
Mar 23 19:05:41.904: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167188ms
Mar 23 19:05:43.910: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Running", Reason="", readiness=true. Elapsed: 2.011293297s
Mar 23 19:05:45.908: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Running", Reason="", readiness=false. Elapsed: 4.008790409s
Mar 23 19:05:47.913: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013652696s
STEP: Saw pod success 03/23/23 19:05:47.913
Mar 23 19:05:47.913: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031" satisfied condition "Succeeded or Failed"
Mar 23 19:05:47.917: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 container client-container: <nil>
STEP: delete the pod 03/23/23 19:05:47.947
Mar 23 19:05:47.959: INFO: Waiting for pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 to disappear
Mar 23 19:05:47.962: INFO: Pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:05:47.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9877" for this suite. 03/23/23 19:05:47.967
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":96,"skipped":1662,"failed":0}
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:05:41.854
    Mar 23 19:05:41.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:05:41.855
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:41.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:41.877
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:05:41.881
    Mar 23 19:05:41.899: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031" in namespace "projected-9877" to be "Succeeded or Failed"
    Mar 23 19:05:41.904: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Pending", Reason="", readiness=false. Elapsed: 5.167188ms
    Mar 23 19:05:43.910: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Running", Reason="", readiness=true. Elapsed: 2.011293297s
    Mar 23 19:05:45.908: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Running", Reason="", readiness=false. Elapsed: 4.008790409s
    Mar 23 19:05:47.913: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013652696s
    STEP: Saw pod success 03/23/23 19:05:47.913
    Mar 23 19:05:47.913: INFO: Pod "downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031" satisfied condition "Succeeded or Failed"
    Mar 23 19:05:47.917: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 container client-container: <nil>
    STEP: delete the pod 03/23/23 19:05:47.947
    Mar 23 19:05:47.959: INFO: Waiting for pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 to disappear
    Mar 23 19:05:47.962: INFO: Pod downwardapi-volume-bcabf5cb-9037-4bfb-91a3-87ccd7f5e031 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:05:47.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9877" for this suite. 03/23/23 19:05:47.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:05:47.978
Mar 23 19:05:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:05:47.979
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:47.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:47.995
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/23/23 19:05:47.998
Mar 23 19:05:48.013: INFO: Waiting up to 5m0s for pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8" in namespace "emptydir-1123" to be "Succeeded or Failed"
Mar 23 19:05:48.026: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220368ms
Mar 23 19:05:50.031: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017835755s
Mar 23 19:05:52.031: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.017534354s
Mar 23 19:05:54.035: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022013622s
STEP: Saw pod success 03/23/23 19:05:54.035
Mar 23 19:05:54.035: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8" satisfied condition "Succeeded or Failed"
Mar 23 19:05:54.040: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 container test-container: <nil>
STEP: delete the pod 03/23/23 19:05:54.048
Mar 23 19:05:54.063: INFO: Waiting for pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 to disappear
Mar 23 19:05:54.067: INFO: Pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:05:54.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1123" for this suite. 03/23/23 19:05:54.072
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":97,"skipped":1682,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:05:47.978
    Mar 23 19:05:47.978: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:05:47.979
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:47.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:47.995
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/23/23 19:05:47.998
    Mar 23 19:05:48.013: INFO: Waiting up to 5m0s for pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8" in namespace "emptydir-1123" to be "Succeeded or Failed"
    Mar 23 19:05:48.026: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.220368ms
    Mar 23 19:05:50.031: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.017835755s
    Mar 23 19:05:52.031: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Running", Reason="", readiness=false. Elapsed: 4.017534354s
    Mar 23 19:05:54.035: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022013622s
    STEP: Saw pod success 03/23/23 19:05:54.035
    Mar 23 19:05:54.035: INFO: Pod "pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8" satisfied condition "Succeeded or Failed"
    Mar 23 19:05:54.040: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 container test-container: <nil>
    STEP: delete the pod 03/23/23 19:05:54.048
    Mar 23 19:05:54.063: INFO: Waiting for pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 to disappear
    Mar 23 19:05:54.067: INFO: Pod pod-5b27d334-dad0-4dde-9e3b-9b9f113662a8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:05:54.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1123" for this suite. 03/23/23 19:05:54.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:05:54.082
Mar 23 19:05:54.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:05:54.083
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:54.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:54.103
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-a377ceb1-b472-4fb9-9c2a-eca64579af0d 03/23/23 19:05:54.111
STEP: Creating a pod to test consume secrets 03/23/23 19:05:54.119
Mar 23 19:05:54.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482" in namespace "projected-9616" to be "Succeeded or Failed"
Mar 23 19:05:54.151: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Pending", Reason="", readiness=false. Elapsed: 12.309571ms
Mar 23 19:05:56.163: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Running", Reason="", readiness=true. Elapsed: 2.024249014s
Mar 23 19:05:58.156: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Running", Reason="", readiness=false. Elapsed: 4.016585604s
Mar 23 19:06:00.156: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017273374s
STEP: Saw pod success 03/23/23 19:06:00.156
Mar 23 19:06:00.157: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482" satisfied condition "Succeeded or Failed"
Mar 23 19:06:00.160: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:06:00.166
Mar 23 19:06:00.183: INFO: Waiting for pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 to disappear
Mar 23 19:06:00.186: INFO: Pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 19:06:00.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9616" for this suite. 03/23/23 19:06:00.19
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":98,"skipped":1698,"failed":0}
------------------------------
• [SLOW TEST] [6.115 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:05:54.082
    Mar 23 19:05:54.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:05:54.083
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:05:54.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:05:54.103
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-a377ceb1-b472-4fb9-9c2a-eca64579af0d 03/23/23 19:05:54.111
    STEP: Creating a pod to test consume secrets 03/23/23 19:05:54.119
    Mar 23 19:05:54.139: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482" in namespace "projected-9616" to be "Succeeded or Failed"
    Mar 23 19:05:54.151: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Pending", Reason="", readiness=false. Elapsed: 12.309571ms
    Mar 23 19:05:56.163: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Running", Reason="", readiness=true. Elapsed: 2.024249014s
    Mar 23 19:05:58.156: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Running", Reason="", readiness=false. Elapsed: 4.016585604s
    Mar 23 19:06:00.156: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017273374s
    STEP: Saw pod success 03/23/23 19:06:00.156
    Mar 23 19:06:00.157: INFO: Pod "pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482" satisfied condition "Succeeded or Failed"
    Mar 23 19:06:00.160: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:06:00.166
    Mar 23 19:06:00.183: INFO: Waiting for pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 to disappear
    Mar 23 19:06:00.186: INFO: Pod pod-projected-secrets-d3a3788d-9266-46ab-9403-dd7aaa62a482 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 19:06:00.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9616" for this suite. 03/23/23 19:06:00.19
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:00.204
Mar 23 19:06:00.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:06:00.205
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:00.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:00.226
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 03/23/23 19:06:00.228
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:06:00.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8254" for this suite. 03/23/23 19:06:00.238
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":99,"skipped":1702,"failed":0}
------------------------------
• [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:00.204
    Mar 23 19:06:00.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:06:00.205
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:00.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:00.226
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 03/23/23 19:06:00.228
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:06:00.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8254" for this suite. 03/23/23 19:06:00.238
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:00.251
Mar 23 19:06:00.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption 03/23/23 19:06:00.252
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:00.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:00.27
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 03/23/23 19:06:00.277
STEP: Waiting for all pods to be running 03/23/23 19:06:02.343
Mar 23 19:06:02.363: INFO: running pods: 0 < 3
Mar 23 19:06:04.368: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 23 19:06:06.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9285" for this suite. 03/23/23 19:06:06.374
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":100,"skipped":1729,"failed":0}
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:00.251
    Mar 23 19:06:00.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption 03/23/23 19:06:00.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:00.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:00.27
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 03/23/23 19:06:00.277
    STEP: Waiting for all pods to be running 03/23/23 19:06:02.343
    Mar 23 19:06:02.363: INFO: running pods: 0 < 3
    Mar 23 19:06:04.368: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 23 19:06:06.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9285" for this suite. 03/23/23 19:06:06.374
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:06.391
Mar 23 19:06:06.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename namespaces 03/23/23 19:06:06.393
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:06.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:06.454
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 03/23/23 19:06:06.459
STEP: patching the Namespace 03/23/23 19:06:06.488
STEP: get the Namespace and ensuring it has the label 03/23/23 19:06:06.499
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:06:06.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4597" for this suite. 03/23/23 19:06:06.509
STEP: Destroying namespace "nspatchtest-59baddac-11b8-4f7e-a58d-f5526822d8bb-2146" for this suite. 03/23/23 19:06:06.519
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":101,"skipped":1733,"failed":0}
------------------------------
• [0.141 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:06.391
    Mar 23 19:06:06.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename namespaces 03/23/23 19:06:06.393
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:06.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:06.454
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 03/23/23 19:06:06.459
    STEP: patching the Namespace 03/23/23 19:06:06.488
    STEP: get the Namespace and ensuring it has the label 03/23/23 19:06:06.499
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:06:06.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4597" for this suite. 03/23/23 19:06:06.509
    STEP: Destroying namespace "nspatchtest-59baddac-11b8-4f7e-a58d-f5526822d8bb-2146" for this suite. 03/23/23 19:06:06.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:06.54
Mar 23 19:06:06.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:06:06.541
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:06.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:06.573
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 03/23/23 19:06:06.577
Mar 23 19:06:06.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Mar 23 19:06:06.681: INFO: stderr: ""
Mar 23 19:06:06.681: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 03/23/23 19:06:06.681
Mar 23 19:06:06.682: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Mar 23 19:06:06.682: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-278" to be "running and ready, or succeeded"
Mar 23 19:06:06.688: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101085ms
Mar 23 19:06:06.688: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-linuxpool-16392394-2' to be 'Running' but was 'Pending'
Mar 23 19:06:08.692: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982275s
Mar 23 19:06:08.692: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Mar 23 19:06:08.692: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 03/23/23 19:06:08.692
Mar 23 19:06:08.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator'
Mar 23 19:06:08.826: INFO: stderr: ""
Mar 23 19:06:08.826: INFO: stdout: "I0323 19:06:07.898718       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/88t 435\nI0323 19:06:08.098839       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6ms 369\nI0323 19:06:08.299099       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/c5b 570\nI0323 19:06:08.499599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/r87 311\nI0323 19:06:08.698881       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d8rl 499\n"
STEP: limiting log lines 03/23/23 19:06:08.826
Mar 23 19:06:08.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --tail=1'
Mar 23 19:06:08.920: INFO: stderr: ""
Mar 23 19:06:08.920: INFO: stdout: "I0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\n"
Mar 23 19:06:08.920: INFO: got output "I0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\n"
STEP: limiting log bytes 03/23/23 19:06:08.92
Mar 23 19:06:08.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --limit-bytes=1'
Mar 23 19:06:09.017: INFO: stderr: ""
Mar 23 19:06:09.017: INFO: stdout: "I"
Mar 23 19:06:09.017: INFO: got output "I"
STEP: exposing timestamps 03/23/23 19:06:09.017
Mar 23 19:06:09.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --tail=1 --timestamps'
Mar 23 19:06:09.112: INFO: stderr: ""
Mar 23 19:06:09.113: INFO: stdout: "2023-03-23T19:06:09.099660175Z I0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\n"
Mar 23 19:06:09.113: INFO: got output "2023-03-23T19:06:09.099660175Z I0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\n"
STEP: restricting to a time range 03/23/23 19:06:09.113
Mar 23 19:06:11.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --since=1s'
Mar 23 19:06:11.738: INFO: stderr: ""
Mar 23 19:06:11.738: INFO: stdout: "I0323 19:06:10.899328       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nsf 275\nI0323 19:06:11.099657       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/2rtw 472\nI0323 19:06:11.298922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/qq6f 290\nI0323 19:06:11.499229       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/sh6 401\nI0323 19:06:11.699557       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/vcsv 543\n"
Mar 23 19:06:11.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --since=24h'
Mar 23 19:06:11.875: INFO: stderr: ""
Mar 23 19:06:11.875: INFO: stdout: "I0323 19:06:07.898718       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/88t 435\nI0323 19:06:08.098839       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6ms 369\nI0323 19:06:08.299099       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/c5b 570\nI0323 19:06:08.499599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/r87 311\nI0323 19:06:08.698881       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d8rl 499\nI0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\nI0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\nI0323 19:06:09.298865       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/szf 226\nI0323 19:06:09.499126       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/7nll 594\nI0323 19:06:09.699445       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/vwbq 440\nI0323 19:06:09.899742       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/7rjq 243\nI0323 19:06:10.099060       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/49v 531\nI0323 19:06:10.299367       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/pd6 301\nI0323 19:06:10.499716       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/gdt 210\nI0323 19:06:10.699024       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/nhnj 480\nI0323 19:06:10.899328       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nsf 275\nI0323 19:06:11.099657       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/2rtw 472\nI0323 19:06:11.298922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/qq6f 290\nI0323 19:06:11.499229       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/sh6 401\nI0323 19:06:11.699557       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/vcsv 543\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Mar 23 19:06:11.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 delete pod logs-generator'
Mar 23 19:06:16.657: INFO: stderr: ""
Mar 23 19:06:16.657: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:06:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-278" for this suite. 03/23/23 19:06:16.661
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":102,"skipped":1769,"failed":0}
------------------------------
• [SLOW TEST] [10.126 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:06.54
    Mar 23 19:06:06.540: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:06:06.541
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:06.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:06.573
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 03/23/23 19:06:06.577
    Mar 23 19:06:06.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Mar 23 19:06:06.681: INFO: stderr: ""
    Mar 23 19:06:06.681: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 03/23/23 19:06:06.681
    Mar 23 19:06:06.682: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Mar 23 19:06:06.682: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-278" to be "running and ready, or succeeded"
    Mar 23 19:06:06.688: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101085ms
    Mar 23 19:06:06.688: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8s-linuxpool-16392394-2' to be 'Running' but was 'Pending'
    Mar 23 19:06:08.692: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00982275s
    Mar 23 19:06:08.692: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Mar 23 19:06:08.692: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 03/23/23 19:06:08.692
    Mar 23 19:06:08.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator'
    Mar 23 19:06:08.826: INFO: stderr: ""
    Mar 23 19:06:08.826: INFO: stdout: "I0323 19:06:07.898718       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/88t 435\nI0323 19:06:08.098839       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6ms 369\nI0323 19:06:08.299099       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/c5b 570\nI0323 19:06:08.499599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/r87 311\nI0323 19:06:08.698881       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d8rl 499\n"
    STEP: limiting log lines 03/23/23 19:06:08.826
    Mar 23 19:06:08.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --tail=1'
    Mar 23 19:06:08.920: INFO: stderr: ""
    Mar 23 19:06:08.920: INFO: stdout: "I0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\n"
    Mar 23 19:06:08.920: INFO: got output "I0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\n"
    STEP: limiting log bytes 03/23/23 19:06:08.92
    Mar 23 19:06:08.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --limit-bytes=1'
    Mar 23 19:06:09.017: INFO: stderr: ""
    Mar 23 19:06:09.017: INFO: stdout: "I"
    Mar 23 19:06:09.017: INFO: got output "I"
    STEP: exposing timestamps 03/23/23 19:06:09.017
    Mar 23 19:06:09.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --tail=1 --timestamps'
    Mar 23 19:06:09.112: INFO: stderr: ""
    Mar 23 19:06:09.113: INFO: stdout: "2023-03-23T19:06:09.099660175Z I0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\n"
    Mar 23 19:06:09.113: INFO: got output "2023-03-23T19:06:09.099660175Z I0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\n"
    STEP: restricting to a time range 03/23/23 19:06:09.113
    Mar 23 19:06:11.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --since=1s'
    Mar 23 19:06:11.738: INFO: stderr: ""
    Mar 23 19:06:11.738: INFO: stdout: "I0323 19:06:10.899328       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nsf 275\nI0323 19:06:11.099657       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/2rtw 472\nI0323 19:06:11.298922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/qq6f 290\nI0323 19:06:11.499229       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/sh6 401\nI0323 19:06:11.699557       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/vcsv 543\n"
    Mar 23 19:06:11.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 logs logs-generator logs-generator --since=24h'
    Mar 23 19:06:11.875: INFO: stderr: ""
    Mar 23 19:06:11.875: INFO: stdout: "I0323 19:06:07.898718       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/88t 435\nI0323 19:06:08.098839       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6ms 369\nI0323 19:06:08.299099       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/c5b 570\nI0323 19:06:08.499599       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/r87 311\nI0323 19:06:08.698881       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/d8rl 499\nI0323 19:06:08.899203       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/dhj2 595\nI0323 19:06:09.099556       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/vdn7 561\nI0323 19:06:09.298865       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/szf 226\nI0323 19:06:09.499126       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/7nll 594\nI0323 19:06:09.699445       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/vwbq 440\nI0323 19:06:09.899742       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/7rjq 243\nI0323 19:06:10.099060       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/49v 531\nI0323 19:06:10.299367       1 logs_generator.go:76] 12 GET /api/v1/namespaces/ns/pods/pd6 301\nI0323 19:06:10.499716       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/gdt 210\nI0323 19:06:10.699024       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/nhnj 480\nI0323 19:06:10.899328       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/nsf 275\nI0323 19:06:11.099657       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/2rtw 472\nI0323 19:06:11.298922       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/qq6f 290\nI0323 19:06:11.499229       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/sh6 401\nI0323 19:06:11.699557       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/vcsv 543\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Mar 23 19:06:11.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-278 delete pod logs-generator'
    Mar 23 19:06:16.657: INFO: stderr: ""
    Mar 23 19:06:16.657: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:06:16.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-278" for this suite. 03/23/23 19:06:16.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:16.667
Mar 23 19:06:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:06:16.671
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:16.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:16.691
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 03/23/23 19:06:16.695
Mar 23 19:06:16.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-835 create -f -'
Mar 23 19:06:16.935: INFO: stderr: ""
Mar 23 19:06:16.935: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/23/23 19:06:16.935
Mar 23 19:06:17.939: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:06:17.939: INFO: Found 0 / 1
Mar 23 19:06:18.940: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:06:18.940: INFO: Found 1 / 1
Mar 23 19:06:18.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 03/23/23 19:06:18.94
Mar 23 19:06:18.943: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:06:18.943: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:06:18.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-835 patch pod agnhost-primary-5mh4x -p {"metadata":{"annotations":{"x":"y"}}}'
Mar 23 19:06:19.045: INFO: stderr: ""
Mar 23 19:06:19.045: INFO: stdout: "pod/agnhost-primary-5mh4x patched\n"
STEP: checking annotations 03/23/23 19:06:19.045
Mar 23 19:06:19.050: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:06:19.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:06:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-835" for this suite. 03/23/23 19:06:19.055
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":103,"skipped":1786,"failed":0}
------------------------------
• [2.395 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:16.667
    Mar 23 19:06:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:06:16.671
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:16.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:16.691
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 03/23/23 19:06:16.695
    Mar 23 19:06:16.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-835 create -f -'
    Mar 23 19:06:16.935: INFO: stderr: ""
    Mar 23 19:06:16.935: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/23/23 19:06:16.935
    Mar 23 19:06:17.939: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:06:17.939: INFO: Found 0 / 1
    Mar 23 19:06:18.940: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:06:18.940: INFO: Found 1 / 1
    Mar 23 19:06:18.940: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 03/23/23 19:06:18.94
    Mar 23 19:06:18.943: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:06:18.943: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 23 19:06:18.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-835 patch pod agnhost-primary-5mh4x -p {"metadata":{"annotations":{"x":"y"}}}'
    Mar 23 19:06:19.045: INFO: stderr: ""
    Mar 23 19:06:19.045: INFO: stdout: "pod/agnhost-primary-5mh4x patched\n"
    STEP: checking annotations 03/23/23 19:06:19.045
    Mar 23 19:06:19.050: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:06:19.050: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:06:19.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-835" for this suite. 03/23/23 19:06:19.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:19.07
Mar 23 19:06:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:06:19.071
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:19.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:19.1
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-cc0a77d0-c37f-49d7-b8b2-765f1b894d2e 03/23/23 19:06:19.103
STEP: Creating a pod to test consume secrets 03/23/23 19:06:19.108
Mar 23 19:06:19.133: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8" in namespace "projected-372" to be "Succeeded or Failed"
Mar 23 19:06:19.143: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.468975ms
Mar 23 19:06:21.147: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014513857s
Mar 23 19:06:23.149: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.015928744s
Mar 23 19:06:25.147: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=false. Elapsed: 6.014562946s
Mar 23 19:06:27.148: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01509806s
STEP: Saw pod success 03/23/23 19:06:27.148
Mar 23 19:06:27.148: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8" satisfied condition "Succeeded or Failed"
Mar 23 19:06:27.152: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:06:27.165
Mar 23 19:06:27.177: INFO: Waiting for pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 to disappear
Mar 23 19:06:27.180: INFO: Pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 19:06:27.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-372" for this suite. 03/23/23 19:06:27.184
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":104,"skipped":1832,"failed":0}
------------------------------
• [SLOW TEST] [8.120 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:19.07
    Mar 23 19:06:19.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:06:19.071
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:19.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:19.1
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-cc0a77d0-c37f-49d7-b8b2-765f1b894d2e 03/23/23 19:06:19.103
    STEP: Creating a pod to test consume secrets 03/23/23 19:06:19.108
    Mar 23 19:06:19.133: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8" in namespace "projected-372" to be "Succeeded or Failed"
    Mar 23 19:06:19.143: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.468975ms
    Mar 23 19:06:21.147: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.014513857s
    Mar 23 19:06:23.149: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=true. Elapsed: 4.015928744s
    Mar 23 19:06:25.147: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Running", Reason="", readiness=false. Elapsed: 6.014562946s
    Mar 23 19:06:27.148: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01509806s
    STEP: Saw pod success 03/23/23 19:06:27.148
    Mar 23 19:06:27.148: INFO: Pod "pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8" satisfied condition "Succeeded or Failed"
    Mar 23 19:06:27.152: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:06:27.165
    Mar 23 19:06:27.177: INFO: Waiting for pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 to disappear
    Mar 23 19:06:27.180: INFO: Pod pod-projected-secrets-b4e84107-76da-4888-bc81-dd717ed7a2a8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 19:06:27.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-372" for this suite. 03/23/23 19:06:27.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:27.196
Mar 23 19:06:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:06:27.197
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:27.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:27.222
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-9ed3bf2c-4f95-4596-9d8f-a0f3cd68d193 03/23/23 19:06:27.225
STEP: Creating a pod to test consume configMaps 03/23/23 19:06:27.231
Mar 23 19:06:27.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1" in namespace "configmap-8914" to be "Succeeded or Failed"
Mar 23 19:06:27.268: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.991943ms
Mar 23 19:06:29.291: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046690304s
Mar 23 19:06:31.273: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029239461s
STEP: Saw pod success 03/23/23 19:06:31.273
Mar 23 19:06:31.274: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1" satisfied condition "Succeeded or Failed"
Mar 23 19:06:31.276: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:06:31.284
Mar 23 19:06:31.296: INFO: Waiting for pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 to disappear
Mar 23 19:06:31.300: INFO: Pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:06:31.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8914" for this suite. 03/23/23 19:06:31.305
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":105,"skipped":1837,"failed":0}
------------------------------
• [4.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:27.196
    Mar 23 19:06:27.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:06:27.197
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:27.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:27.222
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-9ed3bf2c-4f95-4596-9d8f-a0f3cd68d193 03/23/23 19:06:27.225
    STEP: Creating a pod to test consume configMaps 03/23/23 19:06:27.231
    Mar 23 19:06:27.244: INFO: Waiting up to 5m0s for pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1" in namespace "configmap-8914" to be "Succeeded or Failed"
    Mar 23 19:06:27.268: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.991943ms
    Mar 23 19:06:29.291: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046690304s
    Mar 23 19:06:31.273: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029239461s
    STEP: Saw pod success 03/23/23 19:06:31.273
    Mar 23 19:06:31.274: INFO: Pod "pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1" satisfied condition "Succeeded or Failed"
    Mar 23 19:06:31.276: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:06:31.284
    Mar 23 19:06:31.296: INFO: Waiting for pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 to disappear
    Mar 23 19:06:31.300: INFO: Pod pod-configmaps-5233ecf9-a66e-4dd2-8574-826f37f4bdf1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:06:31.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8914" for this suite. 03/23/23 19:06:31.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:31.327
Mar 23 19:06:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename podtemplate 03/23/23 19:06:31.328
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:31.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:31.359
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 03/23/23 19:06:31.362
Mar 23 19:06:31.396: INFO: created test-podtemplate-1
Mar 23 19:06:31.408: INFO: created test-podtemplate-2
Mar 23 19:06:31.413: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 03/23/23 19:06:31.413
STEP: delete collection of pod templates 03/23/23 19:06:31.416
Mar 23 19:06:31.416: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 03/23/23 19:06:31.431
Mar 23 19:06:31.431: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 23 19:06:31.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1437" for this suite. 03/23/23 19:06:31.438
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":106,"skipped":1864,"failed":0}
------------------------------
• [0.118 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:31.327
    Mar 23 19:06:31.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename podtemplate 03/23/23 19:06:31.328
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:31.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:31.359
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 03/23/23 19:06:31.362
    Mar 23 19:06:31.396: INFO: created test-podtemplate-1
    Mar 23 19:06:31.408: INFO: created test-podtemplate-2
    Mar 23 19:06:31.413: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 03/23/23 19:06:31.413
    STEP: delete collection of pod templates 03/23/23 19:06:31.416
    Mar 23 19:06:31.416: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 03/23/23 19:06:31.431
    Mar 23 19:06:31.431: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 23 19:06:31.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1437" for this suite. 03/23/23 19:06:31.438
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:06:31.446
Mar 23 19:06:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 19:06:31.447
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:31.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:31.465
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1766 03/23/23 19:06:31.473
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 03/23/23 19:06:31.479
STEP: Creating stateful set ss in namespace statefulset-1766 03/23/23 19:06:31.493
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1766 03/23/23 19:06:31.501
Mar 23 19:06:31.508: INFO: Found 0 stateful pods, waiting for 1
Mar 23 19:06:41.513: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/23/23 19:06:41.513
Mar 23 19:06:41.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 19:06:41.722: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 19:06:41.722: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 19:06:41.722: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 19:06:41.726: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Mar 23 19:06:51.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 19:06:51.730: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:06:51.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
Mar 23 19:06:52.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993041386s
Mar 23 19:06:53.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988253966s
Mar 23 19:06:54.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983338748s
Mar 23 19:06:55.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969939149s
Mar 23 19:06:56.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.966260301s
Mar 23 19:06:57.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954257141s
Mar 23 19:06:58.803: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950161663s
Mar 23 19:06:59.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.946340184s
Mar 23 19:07:00.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.822205ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1766 03/23/23 19:07:01.812
Mar 23 19:07:01.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:07:02.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:07:02.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:07:02.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:07:02.024: INFO: Found 1 stateful pods, waiting for 3
Mar 23 19:07:12.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:07:12.039: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 19:07:12.039: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 03/23/23 19:07:12.039
STEP: Scale down will halt with unhealthy stateful pod 03/23/23 19:07:12.039
Mar 23 19:07:12.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 19:07:12.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 19:07:12.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 19:07:12.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 19:07:12.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 19:07:12.436: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 19:07:12.436: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 19:07:12.436: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 19:07:12.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 19:07:12.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 19:07:12.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 19:07:12.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 19:07:12.634: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:07:12.637: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Mar 23 19:07:22.647: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 19:07:22.647: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 19:07:22.647: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Mar 23 19:07:22.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Mar 23 19:07:23.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994192274s
Mar 23 19:07:24.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990447944s
Mar 23 19:07:25.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986421114s
Mar 23 19:07:26.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982441085s
Mar 23 19:07:27.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978188756s
Mar 23 19:07:28.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97408627s
Mar 23 19:07:29.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968472909s
Mar 23 19:07:30.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964774643s
Mar 23 19:07:31.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.570579ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1766 03/23/23 19:07:32.699
Mar 23 19:07:32.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:07:32.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:07:32.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:07:32.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:07:32.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:07:33.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:07:33.132: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:07:33.132: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:07:33.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 19:07:33.349: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 19:07:33.349: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 19:07:33.349: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 19:07:33.349: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 03/23/23 19:07:43.364
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 19:07:43.364: INFO: Deleting all statefulset in ns statefulset-1766
Mar 23 19:07:43.367: INFO: Scaling statefulset ss to 0
Mar 23 19:07:43.376: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:07:43.378: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 19:07:43.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1766" for this suite. 03/23/23 19:07:43.397
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":107,"skipped":1867,"failed":0}
------------------------------
• [SLOW TEST] [71.963 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:06:31.446
    Mar 23 19:06:31.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 19:06:31.447
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:06:31.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:06:31.465
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1766 03/23/23 19:06:31.473
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 03/23/23 19:06:31.479
    STEP: Creating stateful set ss in namespace statefulset-1766 03/23/23 19:06:31.493
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1766 03/23/23 19:06:31.501
    Mar 23 19:06:31.508: INFO: Found 0 stateful pods, waiting for 1
    Mar 23 19:06:41.513: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 03/23/23 19:06:41.513
    Mar 23 19:06:41.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 19:06:41.722: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 19:06:41.722: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 19:06:41.722: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 19:06:41.726: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Mar 23 19:06:51.730: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 19:06:51.730: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 19:06:51.756: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.9999996s
    Mar 23 19:06:52.761: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993041386s
    Mar 23 19:06:53.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.988253966s
    Mar 23 19:06:54.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.983338748s
    Mar 23 19:06:55.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.969939149s
    Mar 23 19:06:56.796: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.966260301s
    Mar 23 19:06:57.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.954257141s
    Mar 23 19:06:58.803: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.950161663s
    Mar 23 19:06:59.807: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.946340184s
    Mar 23 19:07:00.811: INFO: Verifying statefulset ss doesn't scale past 1 for another 942.822205ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1766 03/23/23 19:07:01.812
    Mar 23 19:07:01.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 19:07:02.020: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 19:07:02.020: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 19:07:02.020: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 19:07:02.024: INFO: Found 1 stateful pods, waiting for 3
    Mar 23 19:07:12.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 19:07:12.039: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 19:07:12.039: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 03/23/23 19:07:12.039
    STEP: Scale down will halt with unhealthy stateful pod 03/23/23 19:07:12.039
    Mar 23 19:07:12.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 19:07:12.247: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 19:07:12.247: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 19:07:12.247: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 19:07:12.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 19:07:12.436: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 19:07:12.436: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 19:07:12.436: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 19:07:12.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 19:07:12.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 19:07:12.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 19:07:12.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 19:07:12.634: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 19:07:12.637: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Mar 23 19:07:22.647: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 19:07:22.647: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 19:07:22.647: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Mar 23 19:07:22.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
    Mar 23 19:07:23.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994192274s
    Mar 23 19:07:24.669: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990447944s
    Mar 23 19:07:25.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986421114s
    Mar 23 19:07:26.677: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982441085s
    Mar 23 19:07:27.681: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978188756s
    Mar 23 19:07:28.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97408627s
    Mar 23 19:07:29.691: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968472909s
    Mar 23 19:07:30.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964774643s
    Mar 23 19:07:31.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.570579ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1766 03/23/23 19:07:32.699
    Mar 23 19:07:32.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 19:07:32.925: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 19:07:32.925: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 19:07:32.925: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 19:07:32.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 19:07:33.132: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 19:07:33.132: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 19:07:33.132: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 19:07:33.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-1766 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 19:07:33.349: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 19:07:33.349: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 19:07:33.349: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 19:07:33.349: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 03/23/23 19:07:43.364
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 19:07:43.364: INFO: Deleting all statefulset in ns statefulset-1766
    Mar 23 19:07:43.367: INFO: Scaling statefulset ss to 0
    Mar 23 19:07:43.376: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 19:07:43.378: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 19:07:43.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1766" for this suite. 03/23/23 19:07:43.397
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:07:43.409
Mar 23 19:07:43.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 19:07:43.413
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:43.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:43.432
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 03/23/23 19:07:43.441
STEP: Patching the Job 03/23/23 19:07:43.446
STEP: Watching for Job to be patched 03/23/23 19:07:43.471
Mar 23 19:07:43.473: INFO: Event ADDED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 23 19:07:43.473: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
Mar 23 19:07:43.473: INFO: Event MODIFIED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 03/23/23 19:07:43.473
STEP: Watching for Job to be updated 03/23/23 19:07:43.482
Mar 23 19:07:43.484: INFO: Event MODIFIED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:43.484: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 03/23/23 19:07:43.484
Mar 23 19:07:43.489: INFO: Job: e2e-g8cfv as labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv]
STEP: Waiting for job to complete 03/23/23 19:07:43.489
STEP: Delete a job collection with a labelselector 03/23/23 19:07:59.492
STEP: Watching for Job to be deleted 03/23/23 19:07:59.507
Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.513: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Mar 23 19:07:59.514: INFO: Event DELETED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 03/23/23 19:07:59.514
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 19:07:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2727" for this suite. 03/23/23 19:07:59.572
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":108,"skipped":1867,"failed":0}
------------------------------
• [SLOW TEST] [16.180 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:07:43.409
    Mar 23 19:07:43.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 19:07:43.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:43.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:43.432
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 03/23/23 19:07:43.441
    STEP: Patching the Job 03/23/23 19:07:43.446
    STEP: Watching for Job to be patched 03/23/23 19:07:43.471
    Mar 23 19:07:43.473: INFO: Event ADDED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 23 19:07:43.473: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
    Mar 23 19:07:43.473: INFO: Event MODIFIED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 03/23/23 19:07:43.473
    STEP: Watching for Job to be updated 03/23/23 19:07:43.482
    Mar 23 19:07:43.484: INFO: Event MODIFIED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:43.484: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 03/23/23 19:07:43.484
    Mar 23 19:07:43.489: INFO: Job: e2e-g8cfv as labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv]
    STEP: Waiting for job to complete 03/23/23 19:07:43.489
    STEP: Delete a job collection with a labelselector 03/23/23 19:07:59.492
    STEP: Watching for Job to be deleted 03/23/23 19:07:59.507
    Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.512: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.513: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event MODIFIED observed for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Mar 23 19:07:59.514: INFO: Event DELETED found for Job e2e-g8cfv in namespace job-2727 with labels: map[e2e-g8cfv:patched e2e-job-label:e2e-g8cfv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 03/23/23 19:07:59.514
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 19:07:59.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2727" for this suite. 03/23/23 19:07:59.572
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:07:59.59
Mar 23 19:07:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename podtemplate 03/23/23 19:07:59.593
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:59.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:59.619
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 03/23/23 19:07:59.622
STEP: Replace a pod template 03/23/23 19:07:59.638
Mar 23 19:07:59.659: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 23 19:07:59.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-434" for this suite. 03/23/23 19:07:59.665
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":109,"skipped":1867,"failed":0}
------------------------------
• [0.084 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:07:59.59
    Mar 23 19:07:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename podtemplate 03/23/23 19:07:59.593
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:59.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:59.619
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 03/23/23 19:07:59.622
    STEP: Replace a pod template 03/23/23 19:07:59.638
    Mar 23 19:07:59.659: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 23 19:07:59.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-434" for this suite. 03/23/23 19:07:59.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:07:59.675
Mar 23 19:07:59.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:07:59.677
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:59.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:59.695
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 03/23/23 19:08:16.702
STEP: Creating a ResourceQuota 03/23/23 19:08:21.707
STEP: Ensuring resource quota status is calculated 03/23/23 19:08:21.712
STEP: Creating a ConfigMap 03/23/23 19:08:23.716
STEP: Ensuring resource quota status captures configMap creation 03/23/23 19:08:23.727
STEP: Deleting a ConfigMap 03/23/23 19:08:25.732
STEP: Ensuring resource quota status released usage 03/23/23 19:08:25.737
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:08:27.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6297" for this suite. 03/23/23 19:08:27.745
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":110,"skipped":1876,"failed":0}
------------------------------
• [SLOW TEST] [28.080 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:07:59.675
    Mar 23 19:07:59.675: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:07:59.677
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:07:59.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:07:59.695
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 03/23/23 19:08:16.702
    STEP: Creating a ResourceQuota 03/23/23 19:08:21.707
    STEP: Ensuring resource quota status is calculated 03/23/23 19:08:21.712
    STEP: Creating a ConfigMap 03/23/23 19:08:23.716
    STEP: Ensuring resource quota status captures configMap creation 03/23/23 19:08:23.727
    STEP: Deleting a ConfigMap 03/23/23 19:08:25.732
    STEP: Ensuring resource quota status released usage 03/23/23 19:08:25.737
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:08:27.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6297" for this suite. 03/23/23 19:08:27.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:08:27.765
Mar 23 19:08:27.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 19:08:27.766
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:08:27.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:08:27.789
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 03/23/23 19:08:27.792
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/23/23 19:08:27.794
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/23/23 19:08:27.794
STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/23/23 19:08:27.794
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/23/23 19:08:27.796
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/23/23 19:08:27.796
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/23/23 19:08:27.797
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:08:27.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8079" for this suite. 03/23/23 19:08:27.807
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":111,"skipped":1925,"failed":0}
------------------------------
• [0.068 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:08:27.765
    Mar 23 19:08:27.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 19:08:27.766
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:08:27.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:08:27.789
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 03/23/23 19:08:27.792
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 03/23/23 19:08:27.794
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 03/23/23 19:08:27.794
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 03/23/23 19:08:27.794
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 03/23/23 19:08:27.796
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 03/23/23 19:08:27.796
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 03/23/23 19:08:27.797
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:08:27.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8079" for this suite. 03/23/23 19:08:27.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:08:27.839
Mar 23 19:08:27.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 19:08:27.84
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:08:27.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:08:27.858
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d in namespace container-probe-8359 03/23/23 19:08:27.86
Mar 23 19:08:27.874: INFO: Waiting up to 5m0s for pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d" in namespace "container-probe-8359" to be "not pending"
Mar 23 19:08:27.881: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.577286ms
Mar 23 19:08:29.886: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012150182s
Mar 23 19:08:29.886: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d" satisfied condition "not pending"
Mar 23 19:08:29.886: INFO: Started pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d in namespace container-probe-8359
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 19:08:29.886
Mar 23 19:08:29.891: INFO: Initial restart count of pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d is 0
STEP: deleting the pod 03/23/23 19:12:30.422
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 19:12:30.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8359" for this suite. 03/23/23 19:12:30.445
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":112,"skipped":1957,"failed":0}
------------------------------
• [SLOW TEST] [242.612 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:08:27.839
    Mar 23 19:08:27.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 19:08:27.84
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:08:27.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:08:27.858
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d in namespace container-probe-8359 03/23/23 19:08:27.86
    Mar 23 19:08:27.874: INFO: Waiting up to 5m0s for pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d" in namespace "container-probe-8359" to be "not pending"
    Mar 23 19:08:27.881: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.577286ms
    Mar 23 19:08:29.886: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012150182s
    Mar 23 19:08:29.886: INFO: Pod "liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d" satisfied condition "not pending"
    Mar 23 19:08:29.886: INFO: Started pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d in namespace container-probe-8359
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 19:08:29.886
    Mar 23 19:08:29.891: INFO: Initial restart count of pod liveness-1d8cdc35-f5c7-476f-b4e3-15b89eeb999d is 0
    STEP: deleting the pod 03/23/23 19:12:30.422
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 19:12:30.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8359" for this suite. 03/23/23 19:12:30.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:12:30.458
Mar 23 19:12:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:12:30.459
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:30.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:30.482
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-3070ba25-0ef1-4823-9271-1d64f839e656 03/23/23 19:12:30.486
STEP: Creating a pod to test consume configMaps 03/23/23 19:12:30.493
Mar 23 19:12:30.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a" in namespace "configmap-8529" to be "Succeeded or Failed"
Mar 23 19:12:30.528: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457682ms
Mar 23 19:12:32.533: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012475261s
Mar 23 19:12:34.532: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011080569s
Mar 23 19:12:36.534: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012815873s
STEP: Saw pod success 03/23/23 19:12:36.534
Mar 23 19:12:36.534: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a" satisfied condition "Succeeded or Failed"
Mar 23 19:12:36.537: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:12:36.573
Mar 23 19:12:36.584: INFO: Waiting for pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a to disappear
Mar 23 19:12:36.587: INFO: Pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:12:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8529" for this suite. 03/23/23 19:12:36.591
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":113,"skipped":1987,"failed":0}
------------------------------
• [SLOW TEST] [6.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:12:30.458
    Mar 23 19:12:30.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:12:30.459
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:30.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:30.482
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-3070ba25-0ef1-4823-9271-1d64f839e656 03/23/23 19:12:30.486
    STEP: Creating a pod to test consume configMaps 03/23/23 19:12:30.493
    Mar 23 19:12:30.520: INFO: Waiting up to 5m0s for pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a" in namespace "configmap-8529" to be "Succeeded or Failed"
    Mar 23 19:12:30.528: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.457682ms
    Mar 23 19:12:32.533: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012475261s
    Mar 23 19:12:34.532: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011080569s
    Mar 23 19:12:36.534: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012815873s
    STEP: Saw pod success 03/23/23 19:12:36.534
    Mar 23 19:12:36.534: INFO: Pod "pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a" satisfied condition "Succeeded or Failed"
    Mar 23 19:12:36.537: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:12:36.573
    Mar 23 19:12:36.584: INFO: Waiting for pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a to disappear
    Mar 23 19:12:36.587: INFO: Pod pod-configmaps-d10daca2-16ef-42c9-b259-8ede60cf418a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:12:36.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8529" for this suite. 03/23/23 19:12:36.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:12:36.603
Mar 23 19:12:36.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:12:36.605
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:36.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:36.631
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/23/23 19:12:36.638
Mar 23 19:12:36.646: INFO: Waiting up to 5m0s for pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d" in namespace "emptydir-873" to be "Succeeded or Failed"
Mar 23 19:12:36.661: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109065ms
Mar 23 19:12:38.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019304064s
Mar 23 19:12:40.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=true. Elapsed: 4.019085973s
Mar 23 19:12:42.667: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=false. Elapsed: 6.021386743s
Mar 23 19:12:44.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019596115s
STEP: Saw pod success 03/23/23 19:12:44.665
Mar 23 19:12:44.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d" satisfied condition "Succeeded or Failed"
Mar 23 19:12:44.668: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d container test-container: <nil>
STEP: delete the pod 03/23/23 19:12:44.704
Mar 23 19:12:44.721: INFO: Waiting for pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d to disappear
Mar 23 19:12:44.724: INFO: Pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:12:44.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-873" for this suite. 03/23/23 19:12:44.729
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":114,"skipped":1996,"failed":0}
------------------------------
• [SLOW TEST] [8.137 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:12:36.603
    Mar 23 19:12:36.604: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:12:36.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:36.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:36.631
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/23/23 19:12:36.638
    Mar 23 19:12:36.646: INFO: Waiting up to 5m0s for pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d" in namespace "emptydir-873" to be "Succeeded or Failed"
    Mar 23 19:12:36.661: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.109065ms
    Mar 23 19:12:38.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=true. Elapsed: 2.019304064s
    Mar 23 19:12:40.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=true. Elapsed: 4.019085973s
    Mar 23 19:12:42.667: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Running", Reason="", readiness=false. Elapsed: 6.021386743s
    Mar 23 19:12:44.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019596115s
    STEP: Saw pod success 03/23/23 19:12:44.665
    Mar 23 19:12:44.665: INFO: Pod "pod-546d206b-e9f5-423b-9aef-151c2533bc1d" satisfied condition "Succeeded or Failed"
    Mar 23 19:12:44.668: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d container test-container: <nil>
    STEP: delete the pod 03/23/23 19:12:44.704
    Mar 23 19:12:44.721: INFO: Waiting for pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d to disappear
    Mar 23 19:12:44.724: INFO: Pod pod-546d206b-e9f5-423b-9aef-151c2533bc1d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:12:44.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-873" for this suite. 03/23/23 19:12:44.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:12:44.762
Mar 23 19:12:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:12:44.763
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:44.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:44.781
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6582 03/23/23 19:12:44.784
STEP: changing the ExternalName service to type=NodePort 03/23/23 19:12:44.793
STEP: creating replication controller externalname-service in namespace services-6582 03/23/23 19:12:44.823
I0323 19:12:44.835436      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6582, replica count: 2
I0323 19:12:47.887753      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:12:47.887: INFO: Creating new exec pod
Mar 23 19:12:47.896: INFO: Waiting up to 5m0s for pod "execpodrmtwr" in namespace "services-6582" to be "running"
Mar 23 19:12:47.901: INFO: Pod "execpodrmtwr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825689ms
Mar 23 19:12:49.909: INFO: Pod "execpodrmtwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013185283s
Mar 23 19:12:49.909: INFO: Pod "execpodrmtwr" satisfied condition "running"
Mar 23 19:12:50.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 19:12:51.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:12:51.147: INFO: stdout: ""
Mar 23 19:12:52.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 19:12:52.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:12:52.368: INFO: stdout: ""
Mar 23 19:12:53.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 19:12:53.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:12:53.337: INFO: stdout: ""
Mar 23 19:12:54.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Mar 23 19:12:54.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Mar 23 19:12:54.362: INFO: stdout: "externalname-service-2dt8b"
Mar 23 19:12:54.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.19.13 80'
Mar 23 19:12:54.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.19.13 80\nConnection to 10.0.19.13 80 port [tcp/http] succeeded!\n"
Mar 23 19:12:54.604: INFO: stdout: "externalname-service-8lmr2"
Mar 23 19:12:54.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 32290'
Mar 23 19:12:54.792: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 32290\nConnection to 10.240.0.56 32290 port [tcp/*] succeeded!\n"
Mar 23 19:12:54.792: INFO: stdout: "externalname-service-8lmr2"
Mar 23 19:12:54.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 32290'
Mar 23 19:12:55.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 32290\nConnection to 10.240.0.4 32290 port [tcp/*] succeeded!\n"
Mar 23 19:12:55.004: INFO: stdout: ""
Mar 23 19:12:56.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 32290'
Mar 23 19:12:56.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 32290\nConnection to 10.240.0.4 32290 port [tcp/*] succeeded!\n"
Mar 23 19:12:56.201: INFO: stdout: "externalname-service-8lmr2"
Mar 23 19:12:56.201: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:12:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6582" for this suite. 03/23/23 19:12:56.282
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":115,"skipped":2093,"failed":0}
------------------------------
• [SLOW TEST] [11.533 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:12:44.762
    Mar 23 19:12:44.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:12:44.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:44.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:44.781
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6582 03/23/23 19:12:44.784
    STEP: changing the ExternalName service to type=NodePort 03/23/23 19:12:44.793
    STEP: creating replication controller externalname-service in namespace services-6582 03/23/23 19:12:44.823
    I0323 19:12:44.835436      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6582, replica count: 2
    I0323 19:12:47.887753      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:12:47.887: INFO: Creating new exec pod
    Mar 23 19:12:47.896: INFO: Waiting up to 5m0s for pod "execpodrmtwr" in namespace "services-6582" to be "running"
    Mar 23 19:12:47.901: INFO: Pod "execpodrmtwr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.825689ms
    Mar 23 19:12:49.909: INFO: Pod "execpodrmtwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.013185283s
    Mar 23 19:12:49.909: INFO: Pod "execpodrmtwr" satisfied condition "running"
    Mar 23 19:12:50.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 19:12:51.147: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 19:12:51.147: INFO: stdout: ""
    Mar 23 19:12:52.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 19:12:52.368: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 19:12:52.368: INFO: stdout: ""
    Mar 23 19:12:53.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 19:12:53.337: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 19:12:53.337: INFO: stdout: ""
    Mar 23 19:12:54.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Mar 23 19:12:54.362: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Mar 23 19:12:54.362: INFO: stdout: "externalname-service-2dt8b"
    Mar 23 19:12:54.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.19.13 80'
    Mar 23 19:12:54.604: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.19.13 80\nConnection to 10.0.19.13 80 port [tcp/http] succeeded!\n"
    Mar 23 19:12:54.604: INFO: stdout: "externalname-service-8lmr2"
    Mar 23 19:12:54.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 32290'
    Mar 23 19:12:54.792: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 32290\nConnection to 10.240.0.56 32290 port [tcp/*] succeeded!\n"
    Mar 23 19:12:54.792: INFO: stdout: "externalname-service-8lmr2"
    Mar 23 19:12:54.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 32290'
    Mar 23 19:12:55.004: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 32290\nConnection to 10.240.0.4 32290 port [tcp/*] succeeded!\n"
    Mar 23 19:12:55.004: INFO: stdout: ""
    Mar 23 19:12:56.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6582 exec execpodrmtwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 32290'
    Mar 23 19:12:56.201: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 32290\nConnection to 10.240.0.4 32290 port [tcp/*] succeeded!\n"
    Mar 23 19:12:56.201: INFO: stdout: "externalname-service-8lmr2"
    Mar 23 19:12:56.201: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:12:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6582" for this suite. 03/23/23 19:12:56.282
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:12:56.296
Mar 23 19:12:56.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption 03/23/23 19:12:56.297
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:56.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:56.342
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 03/23/23 19:12:56.373
STEP: Waiting for the pdb to be processed 03/23/23 19:12:56.411
STEP: First trying to evict a pod which shouldn't be evictable 03/23/23 19:12:56.425
STEP: Waiting for all pods to be running 03/23/23 19:12:56.426
Mar 23 19:12:56.433: INFO: pods: 0 < 3
STEP: locating a running pod 03/23/23 19:12:58.437
STEP: Updating the pdb to allow a pod to be evicted 03/23/23 19:12:58.447
STEP: Waiting for the pdb to be processed 03/23/23 19:12:58.454
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/23/23 19:12:58.458
STEP: Waiting for all pods to be running 03/23/23 19:12:58.458
STEP: Waiting for the pdb to observed all healthy pods 03/23/23 19:12:58.462
STEP: Patching the pdb to disallow a pod to be evicted 03/23/23 19:12:58.502
STEP: Waiting for the pdb to be processed 03/23/23 19:12:58.532
STEP: Waiting for all pods to be running 03/23/23 19:13:00.547
STEP: locating a running pod 03/23/23 19:13:00.551
STEP: Deleting the pdb to allow a pod to be evicted 03/23/23 19:13:00.56
STEP: Waiting for the pdb to be deleted 03/23/23 19:13:00.572
STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/23/23 19:13:00.575
STEP: Waiting for all pods to be running 03/23/23 19:13:00.575
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 23 19:13:00.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9114" for this suite. 03/23/23 19:13:00.597
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":116,"skipped":2095,"failed":0}
------------------------------
• [4.312 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:12:56.296
    Mar 23 19:12:56.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption 03/23/23 19:12:56.297
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:12:56.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:12:56.342
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 03/23/23 19:12:56.373
    STEP: Waiting for the pdb to be processed 03/23/23 19:12:56.411
    STEP: First trying to evict a pod which shouldn't be evictable 03/23/23 19:12:56.425
    STEP: Waiting for all pods to be running 03/23/23 19:12:56.426
    Mar 23 19:12:56.433: INFO: pods: 0 < 3
    STEP: locating a running pod 03/23/23 19:12:58.437
    STEP: Updating the pdb to allow a pod to be evicted 03/23/23 19:12:58.447
    STEP: Waiting for the pdb to be processed 03/23/23 19:12:58.454
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/23/23 19:12:58.458
    STEP: Waiting for all pods to be running 03/23/23 19:12:58.458
    STEP: Waiting for the pdb to observed all healthy pods 03/23/23 19:12:58.462
    STEP: Patching the pdb to disallow a pod to be evicted 03/23/23 19:12:58.502
    STEP: Waiting for the pdb to be processed 03/23/23 19:12:58.532
    STEP: Waiting for all pods to be running 03/23/23 19:13:00.547
    STEP: locating a running pod 03/23/23 19:13:00.551
    STEP: Deleting the pdb to allow a pod to be evicted 03/23/23 19:13:00.56
    STEP: Waiting for the pdb to be deleted 03/23/23 19:13:00.572
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 03/23/23 19:13:00.575
    STEP: Waiting for all pods to be running 03/23/23 19:13:00.575
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 23 19:13:00.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9114" for this suite. 03/23/23 19:13:00.597
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:00.608
Mar 23 19:13:00.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:13:00.61
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:00.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:00.671
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 03/23/23 19:13:00.677
Mar 23 19:13:00.711: INFO: Waiting up to 5m0s for pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b" in namespace "projected-8265" to be "running and ready"
Mar 23 19:13:00.727: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.496268ms
Mar 23 19:13:00.727: INFO: The phase of Pod labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:13:02.733: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021477756s
Mar 23 19:13:02.733: INFO: The phase of Pod labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b is Running (Ready = true)
Mar 23 19:13:02.733: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b" satisfied condition "running and ready"
Mar 23 19:13:03.366: INFO: Successfully updated pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:13:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8265" for this suite. 03/23/23 19:13:07.408
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":117,"skipped":2096,"failed":0}
------------------------------
• [SLOW TEST] [6.811 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:00.608
    Mar 23 19:13:00.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:13:00.61
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:00.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:00.671
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 03/23/23 19:13:00.677
    Mar 23 19:13:00.711: INFO: Waiting up to 5m0s for pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b" in namespace "projected-8265" to be "running and ready"
    Mar 23 19:13:00.727: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.496268ms
    Mar 23 19:13:00.727: INFO: The phase of Pod labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:13:02.733: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b": Phase="Running", Reason="", readiness=true. Elapsed: 2.021477756s
    Mar 23 19:13:02.733: INFO: The phase of Pod labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b is Running (Ready = true)
    Mar 23 19:13:02.733: INFO: Pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b" satisfied condition "running and ready"
    Mar 23 19:13:03.366: INFO: Successfully updated pod "labelsupdatede7ad2a4-a7a7-40ab-aff6-fd1c232c237b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:13:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8265" for this suite. 03/23/23 19:13:07.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:07.426
Mar 23 19:13:07.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:13:07.427
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:07.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:07.45
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-71281638-ce1b-433b-909f-b6c2c704b4ea 03/23/23 19:13:07.453
STEP: Creating a pod to test consume secrets 03/23/23 19:13:07.461
Mar 23 19:13:07.472: INFO: Waiting up to 5m0s for pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955" in namespace "secrets-180" to be "Succeeded or Failed"
Mar 23 19:13:07.477: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742991ms
Mar 23 19:13:09.481: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Running", Reason="", readiness=true. Elapsed: 2.008998371s
Mar 23 19:13:11.481: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Running", Reason="", readiness=false. Elapsed: 4.00862826s
Mar 23 19:13:13.480: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008326431s
STEP: Saw pod success 03/23/23 19:13:13.48
Mar 23 19:13:13.480: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955" satisfied condition "Succeeded or Failed"
Mar 23 19:13:13.483: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 container secret-env-test: <nil>
STEP: delete the pod 03/23/23 19:13:13.489
Mar 23 19:13:13.507: INFO: Waiting for pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 to disappear
Mar 23 19:13:13.510: INFO: Pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:13:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-180" for this suite. 03/23/23 19:13:13.515
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":118,"skipped":2135,"failed":0}
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:07.426
    Mar 23 19:13:07.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:13:07.427
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:07.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:07.45
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-71281638-ce1b-433b-909f-b6c2c704b4ea 03/23/23 19:13:07.453
    STEP: Creating a pod to test consume secrets 03/23/23 19:13:07.461
    Mar 23 19:13:07.472: INFO: Waiting up to 5m0s for pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955" in namespace "secrets-180" to be "Succeeded or Failed"
    Mar 23 19:13:07.477: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Pending", Reason="", readiness=false. Elapsed: 4.742991ms
    Mar 23 19:13:09.481: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Running", Reason="", readiness=true. Elapsed: 2.008998371s
    Mar 23 19:13:11.481: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Running", Reason="", readiness=false. Elapsed: 4.00862826s
    Mar 23 19:13:13.480: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008326431s
    STEP: Saw pod success 03/23/23 19:13:13.48
    Mar 23 19:13:13.480: INFO: Pod "pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955" satisfied condition "Succeeded or Failed"
    Mar 23 19:13:13.483: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 container secret-env-test: <nil>
    STEP: delete the pod 03/23/23 19:13:13.489
    Mar 23 19:13:13.507: INFO: Waiting for pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 to disappear
    Mar 23 19:13:13.510: INFO: Pod pod-secrets-9abb0cdc-d6c6-4e24-b3b2-4fb55a33f955 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:13:13.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-180" for this suite. 03/23/23 19:13:13.515
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:13.529
Mar 23 19:13:13.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:13:13.53
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:13.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:13.55
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 03/23/23 19:13:13.554
STEP: Creating a ResourceQuota 03/23/23 19:13:18.56
STEP: Ensuring resource quota status is calculated 03/23/23 19:13:18.57
STEP: Creating a Pod that fits quota 03/23/23 19:13:20.574
STEP: Ensuring ResourceQuota status captures the pod usage 03/23/23 19:13:20.592
STEP: Not allowing a pod to be created that exceeds remaining quota 03/23/23 19:13:22.597
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/23/23 19:13:22.602
STEP: Ensuring a pod cannot update its resource requirements 03/23/23 19:13:22.604
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/23/23 19:13:22.61
STEP: Deleting the pod 03/23/23 19:13:24.615
STEP: Ensuring resource quota status released the pod usage 03/23/23 19:13:24.639
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:13:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5363" for this suite. 03/23/23 19:13:26.648
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":119,"skipped":2135,"failed":0}
------------------------------
• [SLOW TEST] [13.128 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:13.529
    Mar 23 19:13:13.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:13:13.53
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:13.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:13.55
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 03/23/23 19:13:13.554
    STEP: Creating a ResourceQuota 03/23/23 19:13:18.56
    STEP: Ensuring resource quota status is calculated 03/23/23 19:13:18.57
    STEP: Creating a Pod that fits quota 03/23/23 19:13:20.574
    STEP: Ensuring ResourceQuota status captures the pod usage 03/23/23 19:13:20.592
    STEP: Not allowing a pod to be created that exceeds remaining quota 03/23/23 19:13:22.597
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 03/23/23 19:13:22.602
    STEP: Ensuring a pod cannot update its resource requirements 03/23/23 19:13:22.604
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 03/23/23 19:13:22.61
    STEP: Deleting the pod 03/23/23 19:13:24.615
    STEP: Ensuring resource quota status released the pod usage 03/23/23 19:13:24.639
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:13:26.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5363" for this suite. 03/23/23 19:13:26.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:26.668
Mar 23 19:13:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename ingress 03/23/23 19:13:26.669
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:26.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:26.688
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 03/23/23 19:13:26.691
STEP: getting /apis/networking.k8s.io 03/23/23 19:13:26.693
STEP: getting /apis/networking.k8s.iov1 03/23/23 19:13:26.694
STEP: creating 03/23/23 19:13:26.696
STEP: getting 03/23/23 19:13:26.72
STEP: listing 03/23/23 19:13:26.725
STEP: watching 03/23/23 19:13:26.728
Mar 23 19:13:26.728: INFO: starting watch
STEP: cluster-wide listing 03/23/23 19:13:26.729
STEP: cluster-wide watching 03/23/23 19:13:26.732
Mar 23 19:13:26.733: INFO: starting watch
STEP: patching 03/23/23 19:13:26.734
STEP: updating 03/23/23 19:13:26.738
Mar 23 19:13:26.749: INFO: waiting for watch events with expected annotations
Mar 23 19:13:26.749: INFO: saw patched and updated annotations
STEP: patching /status 03/23/23 19:13:26.749
STEP: updating /status 03/23/23 19:13:26.755
STEP: get /status 03/23/23 19:13:26.762
STEP: deleting 03/23/23 19:13:26.765
STEP: deleting a collection 03/23/23 19:13:26.778
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Mar 23 19:13:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2457" for this suite. 03/23/23 19:13:26.794
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":120,"skipped":2154,"failed":0}
------------------------------
• [0.132 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:26.668
    Mar 23 19:13:26.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename ingress 03/23/23 19:13:26.669
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:26.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:26.688
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 03/23/23 19:13:26.691
    STEP: getting /apis/networking.k8s.io 03/23/23 19:13:26.693
    STEP: getting /apis/networking.k8s.iov1 03/23/23 19:13:26.694
    STEP: creating 03/23/23 19:13:26.696
    STEP: getting 03/23/23 19:13:26.72
    STEP: listing 03/23/23 19:13:26.725
    STEP: watching 03/23/23 19:13:26.728
    Mar 23 19:13:26.728: INFO: starting watch
    STEP: cluster-wide listing 03/23/23 19:13:26.729
    STEP: cluster-wide watching 03/23/23 19:13:26.732
    Mar 23 19:13:26.733: INFO: starting watch
    STEP: patching 03/23/23 19:13:26.734
    STEP: updating 03/23/23 19:13:26.738
    Mar 23 19:13:26.749: INFO: waiting for watch events with expected annotations
    Mar 23 19:13:26.749: INFO: saw patched and updated annotations
    STEP: patching /status 03/23/23 19:13:26.749
    STEP: updating /status 03/23/23 19:13:26.755
    STEP: get /status 03/23/23 19:13:26.762
    STEP: deleting 03/23/23 19:13:26.765
    STEP: deleting a collection 03/23/23 19:13:26.778
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Mar 23 19:13:26.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2457" for this suite. 03/23/23 19:13:26.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:26.802
Mar 23 19:13:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:13:26.804
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:26.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:26.825
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Mar 23 19:13:26.892: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"195151e3-e821-4b10-8e88-39762d494bc4", Controller:(*bool)(0xc001285cc6), BlockOwnerDeletion:(*bool)(0xc001285cc7)}}
Mar 23 19:13:26.931: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"781303b4-49e5-40d6-a2a6-640718b81f59", Controller:(*bool)(0xc003d5ee9e), BlockOwnerDeletion:(*bool)(0xc003d5ee9f)}}
Mar 23 19:13:26.939: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"399bf350-a336-4bb1-8e85-672130fc54d9", Controller:(*bool)(0xc003d5f116), BlockOwnerDeletion:(*bool)(0xc003d5f117)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:13:31.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5199" for this suite. 03/23/23 19:13:31.966
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":121,"skipped":2165,"failed":0}
------------------------------
• [SLOW TEST] [5.179 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:26.802
    Mar 23 19:13:26.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:13:26.804
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:26.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:26.825
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Mar 23 19:13:26.892: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"195151e3-e821-4b10-8e88-39762d494bc4", Controller:(*bool)(0xc001285cc6), BlockOwnerDeletion:(*bool)(0xc001285cc7)}}
    Mar 23 19:13:26.931: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"781303b4-49e5-40d6-a2a6-640718b81f59", Controller:(*bool)(0xc003d5ee9e), BlockOwnerDeletion:(*bool)(0xc003d5ee9f)}}
    Mar 23 19:13:26.939: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"399bf350-a336-4bb1-8e85-672130fc54d9", Controller:(*bool)(0xc003d5f116), BlockOwnerDeletion:(*bool)(0xc003d5f117)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:13:31.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5199" for this suite. 03/23/23 19:13:31.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:31.995
Mar 23 19:13:31.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 19:13:31.997
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:32.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:32.074
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8998 03/23/23 19:13:32.079
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-8998 03/23/23 19:13:32.093
Mar 23 19:13:32.113: INFO: Found 0 stateful pods, waiting for 1
Mar 23 19:13:42.118: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 03/23/23 19:13:42.123
STEP: updating a scale subresource 03/23/23 19:13:42.126
STEP: verifying the statefulset Spec.Replicas was modified 03/23/23 19:13:42.133
STEP: Patch a scale subresource 03/23/23 19:13:42.137
STEP: verifying the statefulset Spec.Replicas was modified 03/23/23 19:13:42.147
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 19:13:42.159: INFO: Deleting all statefulset in ns statefulset-8998
Mar 23 19:13:42.168: INFO: Scaling statefulset ss to 0
Mar 23 19:13:52.200: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 19:13:52.203: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 19:13:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8998" for this suite. 03/23/23 19:13:52.221
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":122,"skipped":2189,"failed":0}
------------------------------
• [SLOW TEST] [20.235 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:31.995
    Mar 23 19:13:31.995: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 19:13:31.997
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:32.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:32.074
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8998 03/23/23 19:13:32.079
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-8998 03/23/23 19:13:32.093
    Mar 23 19:13:32.113: INFO: Found 0 stateful pods, waiting for 1
    Mar 23 19:13:42.118: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 03/23/23 19:13:42.123
    STEP: updating a scale subresource 03/23/23 19:13:42.126
    STEP: verifying the statefulset Spec.Replicas was modified 03/23/23 19:13:42.133
    STEP: Patch a scale subresource 03/23/23 19:13:42.137
    STEP: verifying the statefulset Spec.Replicas was modified 03/23/23 19:13:42.147
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 19:13:42.159: INFO: Deleting all statefulset in ns statefulset-8998
    Mar 23 19:13:42.168: INFO: Scaling statefulset ss to 0
    Mar 23 19:13:52.200: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 19:13:52.203: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 19:13:52.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8998" for this suite. 03/23/23 19:13:52.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:13:52.24
Mar 23 19:13:52.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:13:52.242
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:52.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:52.265
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 03/23/23 19:13:52.268
STEP: Ensuring ResourceQuota status is calculated 03/23/23 19:13:52.276
STEP: Creating a ResourceQuota with not best effort scope 03/23/23 19:13:54.28
STEP: Ensuring ResourceQuota status is calculated 03/23/23 19:13:54.292
STEP: Creating a best-effort pod 03/23/23 19:13:56.295
STEP: Ensuring resource quota with best effort scope captures the pod usage 03/23/23 19:13:56.307
STEP: Ensuring resource quota with not best effort ignored the pod usage 03/23/23 19:13:58.311
STEP: Deleting the pod 03/23/23 19:14:00.316
STEP: Ensuring resource quota status released the pod usage 03/23/23 19:14:00.328
STEP: Creating a not best-effort pod 03/23/23 19:14:02.333
STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/23/23 19:14:02.347
STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/23/23 19:14:04.351
STEP: Deleting the pod 03/23/23 19:14:06.355
STEP: Ensuring resource quota status released the pod usage 03/23/23 19:14:06.385
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:14:08.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9819" for this suite. 03/23/23 19:14:08.396
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":123,"skipped":2206,"failed":0}
------------------------------
• [SLOW TEST] [16.163 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:13:52.24
    Mar 23 19:13:52.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:13:52.242
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:13:52.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:13:52.265
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 03/23/23 19:13:52.268
    STEP: Ensuring ResourceQuota status is calculated 03/23/23 19:13:52.276
    STEP: Creating a ResourceQuota with not best effort scope 03/23/23 19:13:54.28
    STEP: Ensuring ResourceQuota status is calculated 03/23/23 19:13:54.292
    STEP: Creating a best-effort pod 03/23/23 19:13:56.295
    STEP: Ensuring resource quota with best effort scope captures the pod usage 03/23/23 19:13:56.307
    STEP: Ensuring resource quota with not best effort ignored the pod usage 03/23/23 19:13:58.311
    STEP: Deleting the pod 03/23/23 19:14:00.316
    STEP: Ensuring resource quota status released the pod usage 03/23/23 19:14:00.328
    STEP: Creating a not best-effort pod 03/23/23 19:14:02.333
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 03/23/23 19:14:02.347
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 03/23/23 19:14:04.351
    STEP: Deleting the pod 03/23/23 19:14:06.355
    STEP: Ensuring resource quota status released the pod usage 03/23/23 19:14:06.385
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:14:08.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9819" for this suite. 03/23/23 19:14:08.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:14:08.413
Mar 23 19:14:08.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:14:08.415
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:08.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:08.437
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:14:08.44
Mar 23 19:14:08.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 23 19:14:08.600: INFO: stderr: ""
Mar 23 19:14:08.600: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 03/23/23 19:14:08.6
STEP: verifying the pod e2e-test-httpd-pod was created 03/23/23 19:14:13.65
Mar 23 19:14:13.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 get pod e2e-test-httpd-pod -o json'
Mar 23 19:14:13.744: INFO: stderr: ""
Mar 23 19:14:13.744: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-23T19:14:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1721\",\n        \"resourceVersion\": \"16841\",\n        \"uid\": \"bb7e05bd-9be8-4037-b92f-7c3540a84124\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9p6pd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-linuxpool-16392394-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9p6pd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://703b8f7724b80ff6890fb8ca35df8e3ab432b03f97ba1fb19a5bc0710e8ecef7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-23T19:14:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.56\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.79\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.240.0.79\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-23T19:14:08Z\"\n    }\n}\n"
STEP: replace the image in the pod 03/23/23 19:14:13.744
Mar 23 19:14:13.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 replace -f -'
Mar 23 19:14:14.376: INFO: stderr: ""
Mar 23 19:14:14.376: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/23/23 19:14:14.376
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Mar 23 19:14:14.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 delete pods e2e-test-httpd-pod'
Mar 23 19:14:17.065: INFO: stderr: ""
Mar 23 19:14:17.065: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:14:17.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1721" for this suite. 03/23/23 19:14:17.071
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":124,"skipped":2237,"failed":0}
------------------------------
• [SLOW TEST] [8.664 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:14:08.413
    Mar 23 19:14:08.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:14:08.415
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:08.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:08.437
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:14:08.44
    Mar 23 19:14:08.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 23 19:14:08.600: INFO: stderr: ""
    Mar 23 19:14:08.600: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 03/23/23 19:14:08.6
    STEP: verifying the pod e2e-test-httpd-pod was created 03/23/23 19:14:13.65
    Mar 23 19:14:13.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 get pod e2e-test-httpd-pod -o json'
    Mar 23 19:14:13.744: INFO: stderr: ""
    Mar 23 19:14:13.744: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-03-23T19:14:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1721\",\n        \"resourceVersion\": \"16841\",\n        \"uid\": \"bb7e05bd-9be8-4037-b92f-7c3540a84124\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-9p6pd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-linuxpool-16392394-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-9p6pd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-03-23T19:14:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://703b8f7724b80ff6890fb8ca35df8e3ab432b03f97ba1fb19a5bc0710e8ecef7\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-03-23T19:14:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.240.0.56\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.240.0.79\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.240.0.79\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-03-23T19:14:08Z\"\n    }\n}\n"
    STEP: replace the image in the pod 03/23/23 19:14:13.744
    Mar 23 19:14:13.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 replace -f -'
    Mar 23 19:14:14.376: INFO: stderr: ""
    Mar 23 19:14:14.376: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 03/23/23 19:14:14.376
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Mar 23 19:14:14.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1721 delete pods e2e-test-httpd-pod'
    Mar 23 19:14:17.065: INFO: stderr: ""
    Mar 23 19:14:17.065: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:14:17.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1721" for this suite. 03/23/23 19:14:17.071
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:14:17.078
Mar 23 19:14:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:14:17.08
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:17.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:17.096
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-6173d852-3673-4463-8b55-1fa95d7fe31f 03/23/23 19:14:17.102
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:14:17.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-282" for this suite. 03/23/23 19:14:17.112
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":125,"skipped":2241,"failed":0}
------------------------------
• [0.046 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:14:17.078
    Mar 23 19:14:17.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:14:17.08
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:17.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:17.096
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-6173d852-3673-4463-8b55-1fa95d7fe31f 03/23/23 19:14:17.102
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:14:17.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-282" for this suite. 03/23/23 19:14:17.112
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:14:17.124
Mar 23 19:14:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:14:17.127
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:17.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:17.146
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Mar 23 19:14:17.168: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6909 to be scheduled
Mar 23 19:14:17.175: INFO: 1 pods are not scheduled: [runtimeclass-6909/test-runtimeclass-runtimeclass-6909-preconfigured-handler-zhbgg(3c0a410f-7c28-4f16-948a-774c1d2aa017)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 23 19:14:19.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6909" for this suite. 03/23/23 19:14:19.192
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":126,"skipped":2242,"failed":0}
------------------------------
• [2.072 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:14:17.124
    Mar 23 19:14:17.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:14:17.127
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:17.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:17.146
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Mar 23 19:14:17.168: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-6909 to be scheduled
    Mar 23 19:14:17.175: INFO: 1 pods are not scheduled: [runtimeclass-6909/test-runtimeclass-runtimeclass-6909-preconfigured-handler-zhbgg(3c0a410f-7c28-4f16-948a-774c1d2aa017)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 23 19:14:19.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6909" for this suite. 03/23/23 19:14:19.192
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:14:19.197
Mar 23 19:14:19.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:14:19.199
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:19.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:19.217
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 03/23/23 19:14:19.22
STEP: Wait for the Deployment to create new ReplicaSet 03/23/23 19:14:19.226
STEP: delete the deployment 03/23/23 19:14:19.735
STEP: wait for all rs to be garbage collected 03/23/23 19:14:19.741
STEP: expected 0 rs, got 1 rs 03/23/23 19:14:19.749
STEP: expected 0 pods, got 2 pods 03/23/23 19:14:19.753
STEP: Gathering metrics 03/23/23 19:14:20.262
Mar 23 19:14:20.336: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 19:14:20.340: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.366092ms
Mar 23 19:14:20.340: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 19:14:20.340: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 19:15:20.545: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:15:20.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3752" for this suite. 03/23/23 19:15:20.55
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":127,"skipped":2243,"failed":0}
------------------------------
• [SLOW TEST] [61.361 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:14:19.197
    Mar 23 19:14:19.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:14:19.199
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:14:19.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:14:19.217
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 03/23/23 19:14:19.22
    STEP: Wait for the Deployment to create new ReplicaSet 03/23/23 19:14:19.226
    STEP: delete the deployment 03/23/23 19:14:19.735
    STEP: wait for all rs to be garbage collected 03/23/23 19:14:19.741
    STEP: expected 0 rs, got 1 rs 03/23/23 19:14:19.749
    STEP: expected 0 pods, got 2 pods 03/23/23 19:14:19.753
    STEP: Gathering metrics 03/23/23 19:14:20.262
    Mar 23 19:14:20.336: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 19:14:20.340: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.366092ms
    Mar 23 19:14:20.340: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 19:14:20.340: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 19:15:20.545: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:15:20.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3752" for this suite. 03/23/23 19:15:20.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:15:20.568
Mar 23 19:15:20.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 19:15:20.569
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:20.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:20.589
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 03/23/23 19:15:20.622
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:15:20.628
Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:20.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:15:20.636: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:21.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:15:21.663: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:15:22.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 19:15:22.645: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 03/23/23 19:15:22.648
STEP: DeleteCollection of the DaemonSets 03/23/23 19:15:22.651
STEP: Verify that ReplicaSets have been deleted 03/23/23 19:15:22.658
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Mar 23 19:15:22.682: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17224"},"items":null}

Mar 23 19:15:22.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17225"},"items":[{"metadata":{"name":"daemon-set-4xfx2","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"54949152-3070-4fbf-8c08-3a04e2e8d303","resourceVersion":"17222","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r2mmk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r2mmk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.30","podIP":"10.240.0.31","podIPs":[{"ip":"10.240.0.31"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6bed6e11050e3143c9e8181be724b7d22caacde4447529207a9980814370dcff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hmppl","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"d37db072-a444-48a9-b06f-a3c19d26228a","resourceVersion":"17218","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cbl7c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cbl7c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.56","podIP":"10.240.0.62","podIPs":[{"ip":"10.240.0.62"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://364ccc427f86e32da632892ed27befd9fe7ca4f45ae941ea08eae18392e576af","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7745","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"876e464b-725d-48ae-93f5-70e287b1c3e0","resourceVersion":"17220","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zfqds","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zfqds","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.4","podIP":"10.240.0.18","podIPs":[{"ip":"10.240.0.18"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a39418ffce59e16f563d3bcc408ed5ffaa9c46bca392bd25e5c97406b7238721","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:15:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7588" for this suite. 03/23/23 19:15:22.722
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":128,"skipped":2275,"failed":0}
------------------------------
• [2.161 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:15:20.568
    Mar 23 19:15:20.568: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 19:15:20.569
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:20.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:20.589
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 03/23/23 19:15:20.622
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:15:20.628
    Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:20.632: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:20.636: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:15:20.636: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:21.658: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:21.663: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:15:21.663: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:22.641: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:15:22.645: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 19:15:22.645: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 03/23/23 19:15:22.648
    STEP: DeleteCollection of the DaemonSets 03/23/23 19:15:22.651
    STEP: Verify that ReplicaSets have been deleted 03/23/23 19:15:22.658
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Mar 23 19:15:22.682: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17224"},"items":null}

    Mar 23 19:15:22.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17225"},"items":[{"metadata":{"name":"daemon-set-4xfx2","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"54949152-3070-4fbf-8c08-3a04e2e8d303","resourceVersion":"17222","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-r2mmk","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-r2mmk","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.30","podIP":"10.240.0.31","podIPs":[{"ip":"10.240.0.31"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://6bed6e11050e3143c9e8181be724b7d22caacde4447529207a9980814370dcff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-hmppl","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"d37db072-a444-48a9-b06f-a3c19d26228a","resourceVersion":"17218","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-cbl7c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-cbl7c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.56","podIP":"10.240.0.62","podIPs":[{"ip":"10.240.0.62"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://364ccc427f86e32da632892ed27befd9fe7ca4f45ae941ea08eae18392e576af","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w7745","generateName":"daemon-set-","namespace":"daemonsets-7588","uid":"876e464b-725d-48ae-93f5-70e287b1c3e0","resourceVersion":"17220","creationTimestamp":"2023-03-23T19:15:20Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a5b1af1-5160-4678-933f-2d67cf45153d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:20Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a5b1af1-5160-4678-933f-2d67cf45153d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-03-23T19:15:22Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zfqds","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zfqds","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8s-linuxpool-16392394-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8s-linuxpool-16392394-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:22Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-03-23T19:15:20Z"}],"hostIP":"10.240.0.4","podIP":"10.240.0.18","podIPs":[{"ip":"10.240.0.18"}],"startTime":"2023-03-23T19:15:20Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-03-23T19:15:21Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://a39418ffce59e16f563d3bcc408ed5ffaa9c46bca392bd25e5c97406b7238721","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:15:22.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7588" for this suite. 03/23/23 19:15:22.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:15:22.734
Mar 23 19:15:22.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption 03/23/23 19:15:22.736
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:22.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:22.753
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:15:22.756
Mar 23 19:15:22.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename disruption-2 03/23/23 19:15:22.758
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:22.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:22.784
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 03/23/23 19:15:22.796
STEP: Waiting for the pdb to be processed 03/23/23 19:15:24.811
STEP: Waiting for the pdb to be processed 03/23/23 19:15:24.83
STEP: listing a collection of PDBs across all namespaces 03/23/23 19:15:26.841
STEP: listing a collection of PDBs in namespace disruption-9320 03/23/23 19:15:26.844
STEP: deleting a collection of PDBs 03/23/23 19:15:26.847
STEP: Waiting for the PDB collection to be deleted 03/23/23 19:15:26.859
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Mar 23 19:15:26.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-5232" for this suite. 03/23/23 19:15:26.869
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Mar 23 19:15:26.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9320" for this suite. 03/23/23 19:15:26.884
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":129,"skipped":2313,"failed":0}
------------------------------
• [4.157 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:15:22.734
    Mar 23 19:15:22.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption 03/23/23 19:15:22.736
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:22.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:22.753
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:15:22.756
    Mar 23 19:15:22.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename disruption-2 03/23/23 19:15:22.758
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:22.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:22.784
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 03/23/23 19:15:22.796
    STEP: Waiting for the pdb to be processed 03/23/23 19:15:24.811
    STEP: Waiting for the pdb to be processed 03/23/23 19:15:24.83
    STEP: listing a collection of PDBs across all namespaces 03/23/23 19:15:26.841
    STEP: listing a collection of PDBs in namespace disruption-9320 03/23/23 19:15:26.844
    STEP: deleting a collection of PDBs 03/23/23 19:15:26.847
    STEP: Waiting for the PDB collection to be deleted 03/23/23 19:15:26.859
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Mar 23 19:15:26.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-5232" for this suite. 03/23/23 19:15:26.869
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Mar 23 19:15:26.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9320" for this suite. 03/23/23 19:15:26.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:15:26.895
Mar 23 19:15:26.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename prestop 03/23/23 19:15:26.897
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:26.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:26.922
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6412 03/23/23 19:15:26.925
STEP: Waiting for pods to come up. 03/23/23 19:15:26.951
Mar 23 19:15:26.951: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6412" to be "running"
Mar 23 19:15:26.970: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.215654ms
Mar 23 19:15:28.975: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.023674692s
Mar 23 19:15:28.975: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6412 03/23/23 19:15:28.978
Mar 23 19:15:28.988: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6412" to be "running"
Mar 23 19:15:28.995: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235383ms
Mar 23 19:15:31.000: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011995386s
Mar 23 19:15:31.000: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 03/23/23 19:15:31
Mar 23 19:15:36.014: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 03/23/23 19:15:36.014
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Mar 23 19:15:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6412" for this suite. 03/23/23 19:15:36.039
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":130,"skipped":2326,"failed":0}
------------------------------
• [SLOW TEST] [9.149 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:15:26.895
    Mar 23 19:15:26.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename prestop 03/23/23 19:15:26.897
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:26.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:26.922
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6412 03/23/23 19:15:26.925
    STEP: Waiting for pods to come up. 03/23/23 19:15:26.951
    Mar 23 19:15:26.951: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6412" to be "running"
    Mar 23 19:15:26.970: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.215654ms
    Mar 23 19:15:28.975: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.023674692s
    Mar 23 19:15:28.975: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6412 03/23/23 19:15:28.978
    Mar 23 19:15:28.988: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6412" to be "running"
    Mar 23 19:15:28.995: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 7.235383ms
    Mar 23 19:15:31.000: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.011995386s
    Mar 23 19:15:31.000: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 03/23/23 19:15:31
    Mar 23 19:15:36.014: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 03/23/23 19:15:36.014
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Mar 23 19:15:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6412" for this suite. 03/23/23 19:15:36.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:15:36.045
Mar 23 19:15:36.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:15:36.046
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:36.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:36.111
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 03/23/23 19:15:36.126
STEP: delete the rc 03/23/23 19:15:41.145
STEP: wait for the rc to be deleted 03/23/23 19:15:41.165
Mar 23 19:15:42.177: INFO: 25 pods remaining
Mar 23 19:15:42.177: INFO: 25 pods has nil DeletionTimestamp
Mar 23 19:15:42.177: INFO: 
Mar 23 19:15:43.200: INFO: 17 pods remaining
Mar 23 19:15:43.200: INFO: 17 pods has nil DeletionTimestamp
Mar 23 19:15:43.200: INFO: 
STEP: Gathering metrics 03/23/23 19:15:44.172
Mar 23 19:15:44.210: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 19:15:44.218: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.949682ms
Mar 23 19:15:44.218: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 19:15:44.218: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 19:16:44.432: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:16:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4917" for this suite. 03/23/23 19:16:44.438
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":131,"skipped":2334,"failed":0}
------------------------------
• [SLOW TEST] [68.399 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:15:36.045
    Mar 23 19:15:36.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:15:36.046
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:15:36.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:15:36.111
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 03/23/23 19:15:36.126
    STEP: delete the rc 03/23/23 19:15:41.145
    STEP: wait for the rc to be deleted 03/23/23 19:15:41.165
    Mar 23 19:15:42.177: INFO: 25 pods remaining
    Mar 23 19:15:42.177: INFO: 25 pods has nil DeletionTimestamp
    Mar 23 19:15:42.177: INFO: 
    Mar 23 19:15:43.200: INFO: 17 pods remaining
    Mar 23 19:15:43.200: INFO: 17 pods has nil DeletionTimestamp
    Mar 23 19:15:43.200: INFO: 
    STEP: Gathering metrics 03/23/23 19:15:44.172
    Mar 23 19:15:44.210: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 19:15:44.218: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 7.949682ms
    Mar 23 19:15:44.218: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 19:15:44.218: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 19:16:44.432: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:16:44.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4917" for this suite. 03/23/23 19:16:44.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:16:44.452
Mar 23 19:16:44.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:16:44.453
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:16:44.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:16:44.478
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:16:44.496
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:16:45.902
STEP: Deploying the webhook pod 03/23/23 19:16:45.91
STEP: Wait for the deployment to be ready 03/23/23 19:16:45.925
Mar 23 19:16:45.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:16:47.942
STEP: Verifying the service has paired with the endpoint 03/23/23 19:16:47.953
Mar 23 19:16:48.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/23/23 19:16:48.957
STEP: create a pod that should be updated by the webhook 03/23/23 19:16:48.979
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:16:49.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-96" for this suite. 03/23/23 19:16:49.009
STEP: Destroying namespace "webhook-96-markers" for this suite. 03/23/23 19:16:49.016
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":132,"skipped":2362,"failed":0}
------------------------------
• [4.687 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:16:44.452
    Mar 23 19:16:44.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:16:44.453
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:16:44.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:16:44.478
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:16:44.496
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:16:45.902
    STEP: Deploying the webhook pod 03/23/23 19:16:45.91
    STEP: Wait for the deployment to be ready 03/23/23 19:16:45.925
    Mar 23 19:16:45.933: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:16:47.942
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:16:47.953
    Mar 23 19:16:48.953: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 03/23/23 19:16:48.957
    STEP: create a pod that should be updated by the webhook 03/23/23 19:16:48.979
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:16:49.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-96" for this suite. 03/23/23 19:16:49.009
    STEP: Destroying namespace "webhook-96-markers" for this suite. 03/23/23 19:16:49.016
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:16:49.153
Mar 23 19:16:49.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 19:16:49.155
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:16:49.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:16:49.18
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 03/23/23 19:16:49.184
STEP: Ensuring active pods == parallelism 03/23/23 19:16:49.193
STEP: delete a job 03/23/23 19:16:53.198
STEP: deleting Job.batch foo in namespace job-553, will wait for the garbage collector to delete the pods 03/23/23 19:16:53.198
Mar 23 19:16:53.257: INFO: Deleting Job.batch foo took: 5.424087ms
Mar 23 19:16:53.358: INFO: Terminating Job.batch foo pods took: 100.716267ms
STEP: Ensuring job was deleted 03/23/23 19:17:26.759
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 19:17:26.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-553" for this suite. 03/23/23 19:17:26.767
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":133,"skipped":2375,"failed":0}
------------------------------
• [SLOW TEST] [37.622 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:16:49.153
    Mar 23 19:16:49.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 19:16:49.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:16:49.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:16:49.18
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 03/23/23 19:16:49.184
    STEP: Ensuring active pods == parallelism 03/23/23 19:16:49.193
    STEP: delete a job 03/23/23 19:16:53.198
    STEP: deleting Job.batch foo in namespace job-553, will wait for the garbage collector to delete the pods 03/23/23 19:16:53.198
    Mar 23 19:16:53.257: INFO: Deleting Job.batch foo took: 5.424087ms
    Mar 23 19:16:53.358: INFO: Terminating Job.batch foo pods took: 100.716267ms
    STEP: Ensuring job was deleted 03/23/23 19:17:26.759
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 19:17:26.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-553" for this suite. 03/23/23 19:17:26.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:26.78
Mar 23 19:17:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:17:26.783
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:26.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:26.812
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 03/23/23 19:17:26.815
STEP: getting /apis/node.k8s.io 03/23/23 19:17:26.817
STEP: getting /apis/node.k8s.io/v1 03/23/23 19:17:26.818
STEP: creating 03/23/23 19:17:26.819
STEP: watching 03/23/23 19:17:26.842
Mar 23 19:17:26.842: INFO: starting watch
STEP: getting 03/23/23 19:17:26.857
STEP: listing 03/23/23 19:17:26.86
STEP: patching 03/23/23 19:17:26.862
STEP: updating 03/23/23 19:17:26.868
Mar 23 19:17:26.874: INFO: waiting for watch events with expected annotations
STEP: deleting 03/23/23 19:17:26.874
STEP: deleting a collection 03/23/23 19:17:26.889
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 23 19:17:26.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1770" for this suite. 03/23/23 19:17:26.915
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":134,"skipped":2382,"failed":0}
------------------------------
• [0.145 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:26.78
    Mar 23 19:17:26.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:17:26.783
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:26.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:26.812
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 03/23/23 19:17:26.815
    STEP: getting /apis/node.k8s.io 03/23/23 19:17:26.817
    STEP: getting /apis/node.k8s.io/v1 03/23/23 19:17:26.818
    STEP: creating 03/23/23 19:17:26.819
    STEP: watching 03/23/23 19:17:26.842
    Mar 23 19:17:26.842: INFO: starting watch
    STEP: getting 03/23/23 19:17:26.857
    STEP: listing 03/23/23 19:17:26.86
    STEP: patching 03/23/23 19:17:26.862
    STEP: updating 03/23/23 19:17:26.868
    Mar 23 19:17:26.874: INFO: waiting for watch events with expected annotations
    STEP: deleting 03/23/23 19:17:26.874
    STEP: deleting a collection 03/23/23 19:17:26.889
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 23 19:17:26.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1770" for this suite. 03/23/23 19:17:26.915
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:26.926
Mar 23 19:17:26.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 19:17:26.927
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:26.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:26.948
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/23/23 19:17:26.955
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 03/23/23 19:17:26.955
STEP: creating a pod to probe DNS 03/23/23 19:17:26.955
STEP: submitting the pod to kubernetes 03/23/23 19:17:26.955
Mar 23 19:17:26.975: INFO: Waiting up to 15m0s for pod "dns-test-38b4583b-7307-4838-accb-f090d231f927" in namespace "dns-6920" to be "running"
Mar 23 19:17:26.984: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925881ms
Mar 23 19:17:28.988: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927": Phase="Running", Reason="", readiness=true. Elapsed: 2.012470896s
Mar 23 19:17:28.988: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927" satisfied condition "running"
STEP: retrieving the pod 03/23/23 19:17:28.988
STEP: looking for the results for each expected name from probers 03/23/23 19:17:28.991
Mar 23 19:17:29.003: INFO: DNS probes using dns-6920/dns-test-38b4583b-7307-4838-accb-f090d231f927 succeeded

STEP: deleting the pod 03/23/23 19:17:29.003
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 19:17:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6920" for this suite. 03/23/23 19:17:29.024
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":135,"skipped":2385,"failed":0}
------------------------------
• [2.104 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:26.926
    Mar 23 19:17:26.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 19:17:26.927
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:26.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:26.948
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/23/23 19:17:26.955
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     03/23/23 19:17:26.955
    STEP: creating a pod to probe DNS 03/23/23 19:17:26.955
    STEP: submitting the pod to kubernetes 03/23/23 19:17:26.955
    Mar 23 19:17:26.975: INFO: Waiting up to 15m0s for pod "dns-test-38b4583b-7307-4838-accb-f090d231f927" in namespace "dns-6920" to be "running"
    Mar 23 19:17:26.984: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927": Phase="Pending", Reason="", readiness=false. Elapsed: 8.925881ms
    Mar 23 19:17:28.988: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927": Phase="Running", Reason="", readiness=true. Elapsed: 2.012470896s
    Mar 23 19:17:28.988: INFO: Pod "dns-test-38b4583b-7307-4838-accb-f090d231f927" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 19:17:28.988
    STEP: looking for the results for each expected name from probers 03/23/23 19:17:28.991
    Mar 23 19:17:29.003: INFO: DNS probes using dns-6920/dns-test-38b4583b-7307-4838-accb-f090d231f927 succeeded

    STEP: deleting the pod 03/23/23 19:17:29.003
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 19:17:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6920" for this suite. 03/23/23 19:17:29.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:29.035
Mar 23 19:17:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename endpointslicemirroring 03/23/23 19:17:29.036
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:29.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:29.09
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 03/23/23 19:17:29.107
Mar 23 19:17:29.140: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 03/23/23 19:17:31.145
Mar 23 19:17:31.152: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 03/23/23 19:17:33.155
Mar 23 19:17:33.177: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Mar 23 19:17:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-8541" for this suite. 03/23/23 19:17:35.185
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":136,"skipped":2399,"failed":0}
------------------------------
• [SLOW TEST] [6.157 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:29.035
    Mar 23 19:17:29.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename endpointslicemirroring 03/23/23 19:17:29.036
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:29.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:29.09
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 03/23/23 19:17:29.107
    Mar 23 19:17:29.140: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 03/23/23 19:17:31.145
    Mar 23 19:17:31.152: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 03/23/23 19:17:33.155
    Mar 23 19:17:33.177: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Mar 23 19:17:35.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-8541" for this suite. 03/23/23 19:17:35.185
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:35.196
Mar 23 19:17:35.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:17:35.199
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:35.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:35.222
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 03/23/23 19:17:35.227
STEP: fetching the ConfigMap 03/23/23 19:17:35.232
STEP: patching the ConfigMap 03/23/23 19:17:35.235
STEP: listing all ConfigMaps in all namespaces with a label selector 03/23/23 19:17:35.243
STEP: deleting the ConfigMap by collection with a label selector 03/23/23 19:17:35.249
STEP: listing all ConfigMaps in test namespace 03/23/23 19:17:35.26
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:17:35.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4407" for this suite. 03/23/23 19:17:35.267
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":137,"skipped":2399,"failed":0}
------------------------------
• [0.077 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:35.196
    Mar 23 19:17:35.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:17:35.199
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:35.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:35.222
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 03/23/23 19:17:35.227
    STEP: fetching the ConfigMap 03/23/23 19:17:35.232
    STEP: patching the ConfigMap 03/23/23 19:17:35.235
    STEP: listing all ConfigMaps in all namespaces with a label selector 03/23/23 19:17:35.243
    STEP: deleting the ConfigMap by collection with a label selector 03/23/23 19:17:35.249
    STEP: listing all ConfigMaps in test namespace 03/23/23 19:17:35.26
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:17:35.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4407" for this suite. 03/23/23 19:17:35.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:35.273
Mar 23 19:17:35.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 19:17:35.274
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:35.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:35.303
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 03/23/23 19:17:35.306
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6608;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6608;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +notcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_tcp@PTR;sleep 1; done
 03/23/23 19:17:35.359
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6608;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6608;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +notcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_tcp@PTR;sleep 1; done
 03/23/23 19:17:35.36
STEP: creating a pod to probe DNS 03/23/23 19:17:35.36
STEP: submitting the pod to kubernetes 03/23/23 19:17:35.361
Mar 23 19:17:35.391: INFO: Waiting up to 15m0s for pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998" in namespace "dns-6608" to be "running"
Mar 23 19:17:35.417: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Pending", Reason="", readiness=false. Elapsed: 25.712743ms
Mar 23 19:17:37.423: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031621264s
Mar 23 19:17:39.421: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Running", Reason="", readiness=true. Elapsed: 4.02974321s
Mar 23 19:17:39.421: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998" satisfied condition "running"
STEP: retrieving the pod 03/23/23 19:17:39.421
STEP: looking for the results for each expected name from probers 03/23/23 19:17:39.425
Mar 23 19:17:39.431: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.435: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.441: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.444: INFO: Unable to read wheezy_udp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.447: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.452: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.467: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.470: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.473: INFO: Unable to read jessie_udp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.477: INFO: Unable to read jessie_tcp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.480: INFO: Unable to read jessie_udp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.487: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.491: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
Mar 23 19:17:39.521: INFO: Lookups using dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6608 wheezy_tcp@dns-test-service.dns-6608 wheezy_udp@dns-test-service.dns-6608.svc wheezy_tcp@dns-test-service.dns-6608.svc wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6608 jessie_tcp@dns-test-service.dns-6608 jessie_udp@dns-test-service.dns-6608.svc jessie_tcp@dns-test-service.dns-6608.svc jessie_udp@_http._tcp.dns-test-service.dns-6608.svc jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc]

Mar 23 19:17:44.596: INFO: DNS probes using dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998 succeeded

STEP: deleting the pod 03/23/23 19:17:44.596
STEP: deleting the test service 03/23/23 19:17:44.649
STEP: deleting the test headless service 03/23/23 19:17:44.714
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 19:17:44.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6608" for this suite. 03/23/23 19:17:44.755
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":138,"skipped":2407,"failed":0}
------------------------------
• [SLOW TEST] [9.495 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:35.273
    Mar 23 19:17:35.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 19:17:35.274
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:35.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:35.303
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 03/23/23 19:17:35.306
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6608;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6608;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +notcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_tcp@PTR;sleep 1; done
     03/23/23 19:17:35.359
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6608;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6608;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6608.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6608.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6608.svc;check="$$(dig +notcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_udp@PTR;check="$$(dig +tcp +noall +answer +search 97.171.0.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.0.171.97_tcp@PTR;sleep 1; done
     03/23/23 19:17:35.36
    STEP: creating a pod to probe DNS 03/23/23 19:17:35.36
    STEP: submitting the pod to kubernetes 03/23/23 19:17:35.361
    Mar 23 19:17:35.391: INFO: Waiting up to 15m0s for pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998" in namespace "dns-6608" to be "running"
    Mar 23 19:17:35.417: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Pending", Reason="", readiness=false. Elapsed: 25.712743ms
    Mar 23 19:17:37.423: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031621264s
    Mar 23 19:17:39.421: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998": Phase="Running", Reason="", readiness=true. Elapsed: 4.02974321s
    Mar 23 19:17:39.421: INFO: Pod "dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 19:17:39.421
    STEP: looking for the results for each expected name from probers 03/23/23 19:17:39.425
    Mar 23 19:17:39.431: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.435: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.438: INFO: Unable to read wheezy_udp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.441: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.444: INFO: Unable to read wheezy_udp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.447: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.450: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.452: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.467: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.470: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.473: INFO: Unable to read jessie_udp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.477: INFO: Unable to read jessie_tcp@dns-test-service.dns-6608 from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.480: INFO: Unable to read jessie_udp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.483: INFO: Unable to read jessie_tcp@dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.487: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.491: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc from pod dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998: the server could not find the requested resource (get pods dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998)
    Mar 23 19:17:39.521: INFO: Lookups using dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6608 wheezy_tcp@dns-test-service.dns-6608 wheezy_udp@dns-test-service.dns-6608.svc wheezy_tcp@dns-test-service.dns-6608.svc wheezy_udp@_http._tcp.dns-test-service.dns-6608.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6608.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6608 jessie_tcp@dns-test-service.dns-6608 jessie_udp@dns-test-service.dns-6608.svc jessie_tcp@dns-test-service.dns-6608.svc jessie_udp@_http._tcp.dns-test-service.dns-6608.svc jessie_tcp@_http._tcp.dns-test-service.dns-6608.svc]

    Mar 23 19:17:44.596: INFO: DNS probes using dns-6608/dns-test-5a9f29e2-b11c-4a48-94b8-9e224432a998 succeeded

    STEP: deleting the pod 03/23/23 19:17:44.596
    STEP: deleting the test service 03/23/23 19:17:44.649
    STEP: deleting the test headless service 03/23/23 19:17:44.714
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 19:17:44.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6608" for this suite. 03/23/23 19:17:44.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:44.782
Mar 23 19:17:44.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:17:44.784
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:44.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:44.838
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-728be531-7865-4e81-a606-6fe9a6d881ca 03/23/23 19:17:44.85
STEP: Creating the pod 03/23/23 19:17:44.86
Mar 23 19:17:44.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e" in namespace "configmap-6362" to be "running"
Mar 23 19:17:44.890: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.723473ms
Mar 23 19:17:46.896: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017733549s
Mar 23 19:17:48.896: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017320646s
Mar 23 19:17:50.895: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Running", Reason="", readiness=true. Elapsed: 6.016201444s
Mar 23 19:17:50.895: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e" satisfied condition "running"
STEP: Waiting for pod with text data 03/23/23 19:17:50.895
STEP: Waiting for pod with binary data 03/23/23 19:17:50.932
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:17:50.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6362" for this suite. 03/23/23 19:17:50.942
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":139,"skipped":2412,"failed":0}
------------------------------
• [SLOW TEST] [6.169 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:44.782
    Mar 23 19:17:44.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:17:44.784
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:44.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:44.838
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-728be531-7865-4e81-a606-6fe9a6d881ca 03/23/23 19:17:44.85
    STEP: Creating the pod 03/23/23 19:17:44.86
    Mar 23 19:17:44.879: INFO: Waiting up to 5m0s for pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e" in namespace "configmap-6362" to be "running"
    Mar 23 19:17:44.890: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.723473ms
    Mar 23 19:17:46.896: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017733549s
    Mar 23 19:17:48.896: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017320646s
    Mar 23 19:17:50.895: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e": Phase="Running", Reason="", readiness=true. Elapsed: 6.016201444s
    Mar 23 19:17:50.895: INFO: Pod "pod-configmaps-73f51f5e-276a-4934-b50b-8ea95de5329e" satisfied condition "running"
    STEP: Waiting for pod with text data 03/23/23 19:17:50.895
    STEP: Waiting for pod with binary data 03/23/23 19:17:50.932
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:17:50.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6362" for this suite. 03/23/23 19:17:50.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:50.958
Mar 23 19:17:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:17:50.96
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:50.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:50.98
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Mar 23 19:17:50.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 create -f -'
Mar 23 19:17:51.292: INFO: stderr: ""
Mar 23 19:17:51.292: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Mar 23 19:17:51.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 create -f -'
Mar 23 19:17:51.606: INFO: stderr: ""
Mar 23 19:17:51.606: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/23/23 19:17:51.606
Mar 23 19:17:52.611: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:17:52.611: INFO: Found 1 / 1
Mar 23 19:17:52.611: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 19:17:52.614: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:17:52.614: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:17:52.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe pod agnhost-primary-8w8k8'
Mar 23 19:17:52.737: INFO: stderr: ""
Mar 23 19:17:52.737: INFO: stdout: "Name:             agnhost-primary-8w8k8\nNamespace:        kubectl-7734\nPriority:         0\nService Account:  default\nNode:             k8s-linuxpool-16392394-2/10.240.0.4\nStart Time:       Thu, 23 Mar 2023 19:17:51 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.240.0.12\nIPs:\n  IP:           10.240.0.12\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://da1eb7d667b53d6325e9eefbe0640f5b1f027cf993a2ae19c915356e85c8e9b8\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 23 Mar 2023 19:17:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cgf9d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-cgf9d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7734/agnhost-primary-8w8k8 to k8s-linuxpool-16392394-2\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Mar 23 19:17:52.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe rc agnhost-primary'
Mar 23 19:17:52.933: INFO: stderr: ""
Mar 23 19:17:52.933: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7734\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-8w8k8\n"
Mar 23 19:17:52.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe service agnhost-primary'
Mar 23 19:17:53.060: INFO: stderr: ""
Mar 23 19:17:53.060: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7734\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.0.243.48\nIPs:               10.0.243.48\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.240.0.12:6379\nSession Affinity:  None\nEvents:            <none>\n"
Mar 23 19:17:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe node k8s-linuxpool-16392394-0'
Mar 23 19:17:53.245: INFO: stderr: ""
Mar 23 19:17:53.245: INFO: stdout: "Name:               k8s-linuxpool-16392394-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=2\n                    kubernetes.azure.com/cluster=test125conformance\n                    kubernetes.azure.com/role=agent\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-linuxpool-16392394-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    node.kubernetes.io/instance-type=Standard_D2_v2\n                    storageprofile=managed\n                    storagetier=Standard_LRS\n                    topology.disk.csi.azure.com/zone=\n                    topology.kubernetes.io/region=redmond\n                    topology.kubernetes.io/zone=2\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"disk.csi.azure.com\":\"k8s-linuxpool-16392394-0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Mar 2023 18:29:37 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-linuxpool-16392394-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 23 Mar 2023 19:17:46 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:30:38 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.30\n  Hostname:    k8s-linuxpool-16392394-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30298176Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7076236Ki\n  pods:               30\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  27922798956\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6308236Ki\n  pods:               30\nSystem Info:\n  Machine ID:                 8e31c4aec0014a0682933e7e520ebb04\n  System UUID:                036026a7-6eec-dd44-9321-a6899110ead8\n  Boot ID:                    9106472c-9648-41df-be99-189578650a5d\n  Kernel Version:             5.15.0-1034-azure\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.16+azure-1\n  Kubelet Version:            v1.25.7\n  Kube-Proxy Version:         v1.25.7\nProviderID:                   azure:///subscriptions/0d276e41-73f1-4a04-be73-23a205c5271a/resourceGroups/test125conformance/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-16392394-0\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 azure-ip-masq-agent-zvvq5                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     47m\n  kube-system                 azure-npm-zmbq7                                            10m (0%)      100m (5%)   20Mi (0%)        200Mi (3%)     47m\n  kube-system                 cloud-node-manager-n24nn                                   50m (2%)      2 (100%)    50Mi (0%)        512Mi (8%)     47m\n  kube-system                 csi-azuredisk-node-vnzfb                                   30m (1%)      6 (300%)    60Mi (0%)        6Gi (99%)      47m\n  kube-system                 kube-proxy-zktpk                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                    sonobuoy-e2e-job-4ee5050bb09d4ac4                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                240m (12%)  8150m (407%)\n  memory             180Mi (2%)  7106Mi (115%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 47m   kube-proxy       \n  Normal   Starting                 48m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      48m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           48m   node-controller  Node k8s-linuxpool-16392394-0 event: Registered Node k8s-linuxpool-16392394-0 in Controller\n  Normal   NodeAllocatableEnforced  48m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                47m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeReady\n"
Mar 23 19:17:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe namespace kubectl-7734'
Mar 23 19:17:53.371: INFO: stderr: ""
Mar 23 19:17:53.371: INFO: stdout: "Name:         kubectl-7734\nLabels:       e2e-framework=kubectl\n              e2e-run=2e572e98-56a7-4dc0-ab28-a625aca4b3e9\n              kubernetes.io/metadata.name=kubectl-7734\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:17:53.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7734" for this suite. 03/23/23 19:17:53.375
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":140,"skipped":2429,"failed":0}
------------------------------
• [2.435 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:50.958
    Mar 23 19:17:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:17:50.96
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:50.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:50.98
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Mar 23 19:17:50.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 create -f -'
    Mar 23 19:17:51.292: INFO: stderr: ""
    Mar 23 19:17:51.292: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Mar 23 19:17:51.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 create -f -'
    Mar 23 19:17:51.606: INFO: stderr: ""
    Mar 23 19:17:51.606: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/23/23 19:17:51.606
    Mar 23 19:17:52.611: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:17:52.611: INFO: Found 1 / 1
    Mar 23 19:17:52.611: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 23 19:17:52.614: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:17:52.614: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 23 19:17:52.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe pod agnhost-primary-8w8k8'
    Mar 23 19:17:52.737: INFO: stderr: ""
    Mar 23 19:17:52.737: INFO: stdout: "Name:             agnhost-primary-8w8k8\nNamespace:        kubectl-7734\nPriority:         0\nService Account:  default\nNode:             k8s-linuxpool-16392394-2/10.240.0.4\nStart Time:       Thu, 23 Mar 2023 19:17:51 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.240.0.12\nIPs:\n  IP:           10.240.0.12\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://da1eb7d667b53d6325e9eefbe0640f5b1f027cf993a2ae19c915356e85c8e9b8\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 23 Mar 2023 19:17:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-cgf9d (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-cgf9d:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7734/agnhost-primary-8w8k8 to k8s-linuxpool-16392394-2\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Mar 23 19:17:52.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe rc agnhost-primary'
    Mar 23 19:17:52.933: INFO: stderr: ""
    Mar 23 19:17:52.933: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7734\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-8w8k8\n"
    Mar 23 19:17:52.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe service agnhost-primary'
    Mar 23 19:17:53.060: INFO: stderr: ""
    Mar 23 19:17:53.060: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7734\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.0.243.48\nIPs:               10.0.243.48\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.240.0.12:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Mar 23 19:17:53.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe node k8s-linuxpool-16392394-0'
    Mar 23 19:17:53.245: INFO: stderr: ""
    Mar 23 19:17:53.245: INFO: stdout: "Name:               k8s-linuxpool-16392394-0\nRoles:              agent\nLabels:             agentpool=linuxpool\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=Standard_D2_v2\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=redmond\n                    failure-domain.beta.kubernetes.io/zone=2\n                    kubernetes.azure.com/cluster=test125conformance\n                    kubernetes.azure.com/role=agent\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-linuxpool-16392394-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=agent\n                    node-role.kubernetes.io/agent=\n                    node.kubernetes.io/instance-type=Standard_D2_v2\n                    storageprofile=managed\n                    storagetier=Standard_LRS\n                    topology.disk.csi.azure.com/zone=\n                    topology.kubernetes.io/region=redmond\n                    topology.kubernetes.io/zone=2\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"disk.csi.azure.com\":\"k8s-linuxpool-16392394-0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 23 Mar 2023 18:29:37 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8s-linuxpool-16392394-0\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 23 Mar 2023 19:17:46 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:29:37 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 23 Mar 2023 19:16:32 +0000   Thu, 23 Mar 2023 18:30:38 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.240.0.30\n  Hostname:    k8s-linuxpool-16392394-0\nCapacity:\n  cpu:                2\n  ephemeral-storage:  30298176Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             7076236Ki\n  pods:               30\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  27922798956\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             6308236Ki\n  pods:               30\nSystem Info:\n  Machine ID:                 8e31c4aec0014a0682933e7e520ebb04\n  System UUID:                036026a7-6eec-dd44-9321-a6899110ead8\n  Boot ID:                    9106472c-9648-41df-be99-189578650a5d\n  Kernel Version:             5.15.0-1034-azure\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.5.16+azure-1\n  Kubelet Version:            v1.25.7\n  Kube-Proxy Version:         v1.25.7\nProviderID:                   azure:///subscriptions/0d276e41-73f1-4a04-be73-23a205c5271a/resourceGroups/test125conformance/providers/Microsoft.Compute/virtualMachines/k8s-linuxpool-16392394-0\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 azure-ip-masq-agent-zvvq5                                  50m (2%)      50m (2%)    50Mi (0%)        250Mi (4%)     47m\n  kube-system                 azure-npm-zmbq7                                            10m (0%)      100m (5%)   20Mi (0%)        200Mi (3%)     47m\n  kube-system                 cloud-node-manager-n24nn                                   50m (2%)      2 (100%)    50Mi (0%)        512Mi (8%)     47m\n  kube-system                 csi-azuredisk-node-vnzfb                                   30m (1%)      6 (300%)    60Mi (0%)        6Gi (99%)      47m\n  kube-system                 kube-proxy-zktpk                                           100m (5%)     0 (0%)      0 (0%)           0 (0%)         47m\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                    sonobuoy-e2e-job-4ee5050bb09d4ac4                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         44m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                240m (12%)  8150m (407%)\n  memory             180Mi (2%)  7106Mi (115%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age   From             Message\n  ----     ------                   ----  ----             -------\n  Normal   Starting                 47m   kube-proxy       \n  Normal   Starting                 48m   kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      48m   kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     48m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeHasSufficientPID\n  Normal   RegisteredNode           48m   node-controller  Node k8s-linuxpool-16392394-0 event: Registered Node k8s-linuxpool-16392394-0 in Controller\n  Normal   NodeAllocatableEnforced  48m   kubelet          Updated Node Allocatable limit across pods\n  Normal   NodeReady                47m   kubelet          Node k8s-linuxpool-16392394-0 status is now: NodeReady\n"
    Mar 23 19:17:53.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7734 describe namespace kubectl-7734'
    Mar 23 19:17:53.371: INFO: stderr: ""
    Mar 23 19:17:53.371: INFO: stdout: "Name:         kubectl-7734\nLabels:       e2e-framework=kubectl\n              e2e-run=2e572e98-56a7-4dc0-ab28-a625aca4b3e9\n              kubernetes.io/metadata.name=kubectl-7734\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:17:53.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7734" for this suite. 03/23/23 19:17:53.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:17:53.404
Mar 23 19:17:53.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:17:53.406
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:53.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:53.426
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-d38b7a0b-509f-4dbc-985e-9222ec53a90c 03/23/23 19:17:53.43
STEP: Creating a pod to test consume configMaps 03/23/23 19:17:53.45
Mar 23 19:17:53.462: INFO: Waiting up to 5m0s for pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874" in namespace "configmap-3796" to be "Succeeded or Failed"
Mar 23 19:17:53.472: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111378ms
Mar 23 19:17:55.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=true. Elapsed: 2.017977426s
Mar 23 19:17:57.476: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=true. Elapsed: 4.013951899s
Mar 23 19:17:59.477: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=false. Elapsed: 6.01523026s
Mar 23 19:18:01.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018231918s
STEP: Saw pod success 03/23/23 19:18:01.48
Mar 23 19:18:01.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874" satisfied condition "Succeeded or Failed"
Mar 23 19:18:01.483: INFO: Trying to get logs from node k8s-linuxpool-16392394-0 pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 container configmap-volume-test: <nil>
STEP: delete the pod 03/23/23 19:18:01.804
Mar 23 19:18:01.827: INFO: Waiting for pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 to disappear
Mar 23 19:18:01.840: INFO: Pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:18:01.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3796" for this suite. 03/23/23 19:18:01.847
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":141,"skipped":2465,"failed":0}
------------------------------
• [SLOW TEST] [8.450 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:17:53.404
    Mar 23 19:17:53.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:17:53.406
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:17:53.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:17:53.426
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-d38b7a0b-509f-4dbc-985e-9222ec53a90c 03/23/23 19:17:53.43
    STEP: Creating a pod to test consume configMaps 03/23/23 19:17:53.45
    Mar 23 19:17:53.462: INFO: Waiting up to 5m0s for pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874" in namespace "configmap-3796" to be "Succeeded or Failed"
    Mar 23 19:17:53.472: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111378ms
    Mar 23 19:17:55.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=true. Elapsed: 2.017977426s
    Mar 23 19:17:57.476: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=true. Elapsed: 4.013951899s
    Mar 23 19:17:59.477: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Running", Reason="", readiness=false. Elapsed: 6.01523026s
    Mar 23 19:18:01.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018231918s
    STEP: Saw pod success 03/23/23 19:18:01.48
    Mar 23 19:18:01.480: INFO: Pod "pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874" satisfied condition "Succeeded or Failed"
    Mar 23 19:18:01.483: INFO: Trying to get logs from node k8s-linuxpool-16392394-0 pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 container configmap-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:18:01.804
    Mar 23 19:18:01.827: INFO: Waiting for pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 to disappear
    Mar 23 19:18:01.840: INFO: Pod pod-configmaps-279ed2c9-c235-46aa-8c6c-980a0bddd874 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:18:01.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3796" for this suite. 03/23/23 19:18:01.847
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:18:01.863
Mar 23 19:18:01.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:18:01.865
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:01.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:01.887
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-ebe7b00f-296d-4347-a134-47807179e32b 03/23/23 19:18:01.89
STEP: Creating a pod to test consume secrets 03/23/23 19:18:01.898
Mar 23 19:18:01.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10" in namespace "projected-7310" to be "Succeeded or Failed"
Mar 23 19:18:01.927: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.491772ms
Mar 23 19:18:03.933: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Running", Reason="", readiness=true. Elapsed: 2.018048863s
Mar 23 19:18:05.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Running", Reason="", readiness=false. Elapsed: 4.016955268s
Mar 23 19:18:07.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016170772s
STEP: Saw pod success 03/23/23 19:18:07.931
Mar 23 19:18:07.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10" satisfied condition "Succeeded or Failed"
Mar 23 19:18:07.933: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:18:07.939
Mar 23 19:18:07.953: INFO: Waiting for pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 to disappear
Mar 23 19:18:07.956: INFO: Pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 19:18:07.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7310" for this suite. 03/23/23 19:18:07.961
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":142,"skipped":2468,"failed":0}
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:18:01.863
    Mar 23 19:18:01.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:18:01.865
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:01.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:01.887
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-ebe7b00f-296d-4347-a134-47807179e32b 03/23/23 19:18:01.89
    STEP: Creating a pod to test consume secrets 03/23/23 19:18:01.898
    Mar 23 19:18:01.914: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10" in namespace "projected-7310" to be "Succeeded or Failed"
    Mar 23 19:18:01.927: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Pending", Reason="", readiness=false. Elapsed: 12.491772ms
    Mar 23 19:18:03.933: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Running", Reason="", readiness=true. Elapsed: 2.018048863s
    Mar 23 19:18:05.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Running", Reason="", readiness=false. Elapsed: 4.016955268s
    Mar 23 19:18:07.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016170772s
    STEP: Saw pod success 03/23/23 19:18:07.931
    Mar 23 19:18:07.931: INFO: Pod "pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10" satisfied condition "Succeeded or Failed"
    Mar 23 19:18:07.933: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:18:07.939
    Mar 23 19:18:07.953: INFO: Waiting for pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 to disappear
    Mar 23 19:18:07.956: INFO: Pod pod-projected-secrets-b0b2252b-1a08-41a3-bcbd-370650f3dc10 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 19:18:07.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7310" for this suite. 03/23/23 19:18:07.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:18:07.975
Mar 23 19:18:07.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:18:07.976
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:07.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:07.998
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 03/23/23 19:18:08.002
Mar 23 19:18:08.015: INFO: Waiting up to 5m0s for pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81" in namespace "downward-api-8846" to be "Succeeded or Failed"
Mar 23 19:18:08.036: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Pending", Reason="", readiness=false. Elapsed: 20.103356ms
Mar 23 19:18:10.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024511333s
Mar 23 19:18:12.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Running", Reason="", readiness=true. Elapsed: 4.024414981s
Mar 23 19:18:14.043: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Running", Reason="", readiness=false. Elapsed: 6.027256223s
Mar 23 19:18:16.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024768276s
STEP: Saw pod success 03/23/23 19:18:16.04
Mar 23 19:18:16.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81" satisfied condition "Succeeded or Failed"
Mar 23 19:18:16.046: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 container dapi-container: <nil>
STEP: delete the pod 03/23/23 19:18:16.055
Mar 23 19:18:16.066: INFO: Waiting for pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 to disappear
Mar 23 19:18:16.069: INFO: Pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 23 19:18:16.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8846" for this suite. 03/23/23 19:18:16.075
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":143,"skipped":2479,"failed":0}
------------------------------
• [SLOW TEST] [8.109 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:18:07.975
    Mar 23 19:18:07.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:18:07.976
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:07.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:07.998
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 03/23/23 19:18:08.002
    Mar 23 19:18:08.015: INFO: Waiting up to 5m0s for pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81" in namespace "downward-api-8846" to be "Succeeded or Failed"
    Mar 23 19:18:08.036: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Pending", Reason="", readiness=false. Elapsed: 20.103356ms
    Mar 23 19:18:10.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024511333s
    Mar 23 19:18:12.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Running", Reason="", readiness=true. Elapsed: 4.024414981s
    Mar 23 19:18:14.043: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Running", Reason="", readiness=false. Elapsed: 6.027256223s
    Mar 23 19:18:16.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024768276s
    STEP: Saw pod success 03/23/23 19:18:16.04
    Mar 23 19:18:16.040: INFO: Pod "downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81" satisfied condition "Succeeded or Failed"
    Mar 23 19:18:16.046: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 19:18:16.055
    Mar 23 19:18:16.066: INFO: Waiting for pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 to disappear
    Mar 23 19:18:16.069: INFO: Pod downward-api-64917cb5-1bbc-4a85-ad5d-3a1dc478aa81 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 23 19:18:16.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8846" for this suite. 03/23/23 19:18:16.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:18:16.093
Mar 23 19:18:16.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pod-network-test 03/23/23 19:18:16.095
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:16.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:16.118
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8846 03/23/23 19:18:16.121
STEP: creating a selector 03/23/23 19:18:16.122
STEP: Creating the service pods in kubernetes 03/23/23 19:18:16.122
Mar 23 19:18:16.122: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 23 19:18:16.178: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8846" to be "running and ready"
Mar 23 19:18:16.189: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603976ms
Mar 23 19:18:16.189: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:18:18.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015958202s
Mar 23 19:18:18.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:20.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014873821s
Mar 23 19:18:20.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:22.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016072736s
Mar 23 19:18:22.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:24.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014954756s
Mar 23 19:18:24.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:26.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014860461s
Mar 23 19:18:26.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:28.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014521243s
Mar 23 19:18:28.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:30.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014846023s
Mar 23 19:18:30.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:32.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015309502s
Mar 23 19:18:32.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:34.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014598488s
Mar 23 19:18:34.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:36.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015862274s
Mar 23 19:18:36.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 19:18:38.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014942365s
Mar 23 19:18:38.193: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 23 19:18:38.193: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 23 19:18:38.196: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8846" to be "running and ready"
Mar 23 19:18:38.199: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.779394ms
Mar 23 19:18:38.199: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 23 19:18:38.199: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 23 19:18:38.201: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8846" to be "running and ready"
Mar 23 19:18:38.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.281392ms
Mar 23 19:18:38.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 23 19:18:38.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/23/23 19:18:38.209
W0323 19:18:38.221191      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Mar 23 19:18:38.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8846" to be "running"
Mar 23 19:18:38.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480086ms
Mar 23 19:18:40.232: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011043564s
Mar 23 19:18:40.232: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 23 19:18:40.235: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8846" to be "running"
Mar 23 19:18:40.237: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.460794ms
Mar 23 19:18:40.237: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 23 19:18:40.239: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 23 19:18:40.239: INFO: Going to poll 10.240.0.35 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 23 19:18:40.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.35:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:18:40.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:18:40.242: INFO: ExecWithOptions: Clientset creation
Mar 23 19:18:40.243: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.35%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 19:18:40.372: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 23 19:18:40.372: INFO: Going to poll 10.240.0.65 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 23 19:18:40.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:18:40.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:18:40.377: INFO: ExecWithOptions: Clientset creation
Mar 23 19:18:40.377: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 19:18:40.528: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 23 19:18:40.529: INFO: Going to poll 10.240.0.29 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Mar 23 19:18:40.532: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.29:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:18:40.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:18:40.533: INFO: ExecWithOptions: Clientset creation
Mar 23 19:18:40.533: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.29%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 19:18:40.639: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 23 19:18:40.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8846" for this suite. 03/23/23 19:18:40.645
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":144,"skipped":2497,"failed":0}
------------------------------
• [SLOW TEST] [24.559 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:18:16.093
    Mar 23 19:18:16.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pod-network-test 03/23/23 19:18:16.095
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:16.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:16.118
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8846 03/23/23 19:18:16.121
    STEP: creating a selector 03/23/23 19:18:16.122
    STEP: Creating the service pods in kubernetes 03/23/23 19:18:16.122
    Mar 23 19:18:16.122: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 23 19:18:16.178: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8846" to be "running and ready"
    Mar 23 19:18:16.189: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.603976ms
    Mar 23 19:18:16.189: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:18:18.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015958202s
    Mar 23 19:18:18.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:20.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014873821s
    Mar 23 19:18:20.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:22.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016072736s
    Mar 23 19:18:22.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:24.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014954756s
    Mar 23 19:18:24.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:26.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014860461s
    Mar 23 19:18:26.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:28.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014521243s
    Mar 23 19:18:28.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:30.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014846023s
    Mar 23 19:18:30.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:32.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015309502s
    Mar 23 19:18:32.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:34.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.014598488s
    Mar 23 19:18:34.193: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:36.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015862274s
    Mar 23 19:18:36.194: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 19:18:38.193: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014942365s
    Mar 23 19:18:38.193: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 23 19:18:38.193: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 23 19:18:38.196: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8846" to be "running and ready"
    Mar 23 19:18:38.199: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.779394ms
    Mar 23 19:18:38.199: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 23 19:18:38.199: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 23 19:18:38.201: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8846" to be "running and ready"
    Mar 23 19:18:38.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.281392ms
    Mar 23 19:18:38.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 23 19:18:38.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/23/23 19:18:38.209
    W0323 19:18:38.221191      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Mar 23 19:18:38.221: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8846" to be "running"
    Mar 23 19:18:38.227: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.480086ms
    Mar 23 19:18:40.232: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011043564s
    Mar 23 19:18:40.232: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 23 19:18:40.235: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8846" to be "running"
    Mar 23 19:18:40.237: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.460794ms
    Mar 23 19:18:40.237: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 23 19:18:40.239: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 23 19:18:40.239: INFO: Going to poll 10.240.0.35 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 19:18:40.242: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.35:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:18:40.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:18:40.242: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:18:40.243: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.35%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 19:18:40.372: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 23 19:18:40.372: INFO: Going to poll 10.240.0.65 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 19:18:40.376: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.65:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:18:40.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:18:40.377: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:18:40.377: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.65%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 19:18:40.528: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 23 19:18:40.529: INFO: Going to poll 10.240.0.29 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 19:18:40.532: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.240.0.29:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8846 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:18:40.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:18:40.533: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:18:40.533: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-8846/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.240.0.29%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 19:18:40.639: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 23 19:18:40.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8846" for this suite. 03/23/23 19:18:40.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:18:40.66
Mar 23 19:18:40.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:18:40.661
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:40.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:40.685
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/23/23 19:18:40.688
Mar 23 19:18:40.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:18:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:18:57.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8000" for this suite. 03/23/23 19:18:57.194
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":145,"skipped":2511,"failed":0}
------------------------------
• [SLOW TEST] [16.544 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:18:40.66
    Mar 23 19:18:40.660: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:18:40.661
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:40.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:40.685
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 03/23/23 19:18:40.688
    Mar 23 19:18:40.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:18:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:18:57.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8000" for this suite. 03/23/23 19:18:57.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:18:57.207
Mar 23 19:18:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:18:57.21
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:57.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:57.235
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:18:57.238
Mar 23 19:18:57.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed" in namespace "downward-api-6074" to be "Succeeded or Failed"
Mar 23 19:18:57.250: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192093ms
Mar 23 19:18:59.254: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.007105861s
Mar 23 19:19:01.256: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Running", Reason="", readiness=false. Elapsed: 4.008529538s
Mar 23 19:19:03.255: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007525821s
STEP: Saw pod success 03/23/23 19:19:03.255
Mar 23 19:19:03.255: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed" satisfied condition "Succeeded or Failed"
Mar 23 19:19:03.258: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed container client-container: <nil>
STEP: delete the pod 03/23/23 19:19:03.29
Mar 23 19:19:03.314: INFO: Waiting for pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed to disappear
Mar 23 19:19:03.317: INFO: Pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 19:19:03.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6074" for this suite. 03/23/23 19:19:03.323
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":146,"skipped":2548,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:18:57.207
    Mar 23 19:18:57.207: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:18:57.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:18:57.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:18:57.235
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:18:57.238
    Mar 23 19:18:57.247: INFO: Waiting up to 5m0s for pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed" in namespace "downward-api-6074" to be "Succeeded or Failed"
    Mar 23 19:18:57.250: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Pending", Reason="", readiness=false. Elapsed: 3.192093ms
    Mar 23 19:18:59.254: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.007105861s
    Mar 23 19:19:01.256: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Running", Reason="", readiness=false. Elapsed: 4.008529538s
    Mar 23 19:19:03.255: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007525821s
    STEP: Saw pod success 03/23/23 19:19:03.255
    Mar 23 19:19:03.255: INFO: Pod "downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed" satisfied condition "Succeeded or Failed"
    Mar 23 19:19:03.258: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed container client-container: <nil>
    STEP: delete the pod 03/23/23 19:19:03.29
    Mar 23 19:19:03.314: INFO: Waiting for pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed to disappear
    Mar 23 19:19:03.317: INFO: Pod downwardapi-volume-390a703d-7d0a-4999-a2de-a366971800ed no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 19:19:03.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6074" for this suite. 03/23/23 19:19:03.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:19:03.33
Mar 23 19:19:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-watch 03/23/23 19:19:03.332
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:19:03.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:19:03.37
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Mar 23 19:19:03.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Creating first CR  03/23/23 19:19:05.953
Mar 23 19:19:05.958: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:05Z]] name:name1 resourceVersion:19440 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 03/23/23 19:19:15.96
Mar 23 19:19:15.967: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:15Z]] name:name2 resourceVersion:19483 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 03/23/23 19:19:25.969
Mar 23 19:19:25.975: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:25Z]] name:name1 resourceVersion:19515 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 03/23/23 19:19:35.975
Mar 23 19:19:35.983: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:35Z]] name:name2 resourceVersion:19547 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 03/23/23 19:19:45.986
Mar 23 19:19:45.994: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:25Z]] name:name1 resourceVersion:19580 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 03/23/23 19:19:55.994
Mar 23 19:19:56.002: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:35Z]] name:name2 resourceVersion:19613 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:20:06.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2331" for this suite. 03/23/23 19:20:06.527
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":147,"skipped":2559,"failed":0}
------------------------------
• [SLOW TEST] [63.204 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:19:03.33
    Mar 23 19:19:03.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-watch 03/23/23 19:19:03.332
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:19:03.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:19:03.37
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Mar 23 19:19:03.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Creating first CR  03/23/23 19:19:05.953
    Mar 23 19:19:05.958: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:05Z]] name:name1 resourceVersion:19440 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 03/23/23 19:19:15.96
    Mar 23 19:19:15.967: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:15Z]] name:name2 resourceVersion:19483 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 03/23/23 19:19:25.969
    Mar 23 19:19:25.975: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:25Z]] name:name1 resourceVersion:19515 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 03/23/23 19:19:35.975
    Mar 23 19:19:35.983: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:35Z]] name:name2 resourceVersion:19547 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 03/23/23 19:19:45.986
    Mar 23 19:19:45.994: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:25Z]] name:name1 resourceVersion:19580 uid:95837f39-bf7c-4742-93f4-186661905881] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 03/23/23 19:19:55.994
    Mar 23 19:19:56.002: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-03-23T19:19:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-03-23T19:19:35Z]] name:name2 resourceVersion:19613 uid:ab753156-5081-48d9-a8b7-8337dbb5eae8] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:20:06.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2331" for this suite. 03/23/23 19:20:06.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:20:06.543
Mar 23 19:20:06.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:20:06.544
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:20:06.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:20:06.59
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 03/23/23 19:20:06.593
STEP: Wait for the Deployment to create new ReplicaSet 03/23/23 19:20:06.61
STEP: delete the deployment 03/23/23 19:20:07.123
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/23/23 19:20:07.13
STEP: Gathering metrics 03/23/23 19:20:07.653
Mar 23 19:20:07.714: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 19:20:07.719: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.762886ms
Mar 23 19:20:07.720: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 19:20:07.720: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 19:21:07.912: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:21:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8012" for this suite. 03/23/23 19:21:07.918
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":148,"skipped":2566,"failed":0}
------------------------------
• [SLOW TEST] [61.387 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:20:06.543
    Mar 23 19:20:06.543: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:20:06.544
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:20:06.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:20:06.59
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 03/23/23 19:20:06.593
    STEP: Wait for the Deployment to create new ReplicaSet 03/23/23 19:20:06.61
    STEP: delete the deployment 03/23/23 19:20:07.123
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 03/23/23 19:20:07.13
    STEP: Gathering metrics 03/23/23 19:20:07.653
    Mar 23 19:20:07.714: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 19:20:07.719: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 5.762886ms
    Mar 23 19:20:07.720: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 19:20:07.720: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 19:21:07.912: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:21:07.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8012" for this suite. 03/23/23 19:21:07.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:07.944
Mar 23 19:21:07.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 19:21:07.945
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:07.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:07.971
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 03/23/23 19:21:07.981
STEP: Ensuring job reaches completions 03/23/23 19:21:07.992
STEP: Ensuring pods with index for job exist 03/23/23 19:21:21.998
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 19:21:22.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-9554" for this suite. 03/23/23 19:21:22.008
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":149,"skipped":2603,"failed":0}
------------------------------
• [SLOW TEST] [14.070 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:07.944
    Mar 23 19:21:07.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 19:21:07.945
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:07.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:07.971
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 03/23/23 19:21:07.981
    STEP: Ensuring job reaches completions 03/23/23 19:21:07.992
    STEP: Ensuring pods with index for job exist 03/23/23 19:21:21.998
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 19:21:22.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-9554" for this suite. 03/23/23 19:21:22.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:22.022
Mar 23 19:21:22.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:21:22.024
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:22.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:22.045
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 03/23/23 19:21:22.048
Mar 23 19:21:22.058: INFO: Waiting up to 5m0s for pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b" in namespace "downward-api-5803" to be "Succeeded or Failed"
Mar 23 19:21:22.065: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.237482ms
Mar 23 19:21:24.075: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016706549s
Mar 23 19:21:26.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Running", Reason="", readiness=false. Elapsed: 4.01245245s
Mar 23 19:21:28.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012191041s
STEP: Saw pod success 03/23/23 19:21:28.07
Mar 23 19:21:28.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b" satisfied condition "Succeeded or Failed"
Mar 23 19:21:28.075: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b container dapi-container: <nil>
STEP: delete the pod 03/23/23 19:21:28.106
Mar 23 19:21:28.122: INFO: Waiting for pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b to disappear
Mar 23 19:21:28.125: INFO: Pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 23 19:21:28.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5803" for this suite. 03/23/23 19:21:28.131
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":150,"skipped":2633,"failed":0}
------------------------------
• [SLOW TEST] [6.123 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:22.022
    Mar 23 19:21:22.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:21:22.024
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:22.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:22.045
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 03/23/23 19:21:22.048
    Mar 23 19:21:22.058: INFO: Waiting up to 5m0s for pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b" in namespace "downward-api-5803" to be "Succeeded or Failed"
    Mar 23 19:21:22.065: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.237482ms
    Mar 23 19:21:24.075: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016706549s
    Mar 23 19:21:26.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Running", Reason="", readiness=false. Elapsed: 4.01245245s
    Mar 23 19:21:28.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012191041s
    STEP: Saw pod success 03/23/23 19:21:28.07
    Mar 23 19:21:28.070: INFO: Pod "downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b" satisfied condition "Succeeded or Failed"
    Mar 23 19:21:28.075: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b container dapi-container: <nil>
    STEP: delete the pod 03/23/23 19:21:28.106
    Mar 23 19:21:28.122: INFO: Waiting for pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b to disappear
    Mar 23 19:21:28.125: INFO: Pod downward-api-ef922971-1bd5-41ab-9d65-8e4f52faec1b no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 23 19:21:28.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5803" for this suite. 03/23/23 19:21:28.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:28.155
Mar 23 19:21:28.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:21:28.156
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:28.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:28.18
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Mar 23 19:21:28.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 19:21:31.161
Mar 23 19:21:31.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 create -f -'
Mar 23 19:21:31.930: INFO: stderr: ""
Mar 23 19:21:31.930: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 19:21:31.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 delete e2e-test-crd-publish-openapi-1484-crds test-cr'
Mar 23 19:21:32.033: INFO: stderr: ""
Mar 23 19:21:32.033: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Mar 23 19:21:32.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 apply -f -'
Mar 23 19:21:32.793: INFO: stderr: ""
Mar 23 19:21:32.793: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Mar 23 19:21:32.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 delete e2e-test-crd-publish-openapi-1484-crds test-cr'
Mar 23 19:21:32.890: INFO: stderr: ""
Mar 23 19:21:32.890: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 03/23/23 19:21:32.89
Mar 23 19:21:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 explain e2e-test-crd-publish-openapi-1484-crds'
Mar 23 19:21:33.776: INFO: stderr: ""
Mar 23 19:21:33.776: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1484-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:21:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6472" for this suite. 03/23/23 19:21:36.68
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":151,"skipped":2661,"failed":0}
------------------------------
• [SLOW TEST] [8.530 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:28.155
    Mar 23 19:21:28.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:21:28.156
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:28.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:28.18
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Mar 23 19:21:28.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 19:21:31.161
    Mar 23 19:21:31.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 create -f -'
    Mar 23 19:21:31.930: INFO: stderr: ""
    Mar 23 19:21:31.930: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 23 19:21:31.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 delete e2e-test-crd-publish-openapi-1484-crds test-cr'
    Mar 23 19:21:32.033: INFO: stderr: ""
    Mar 23 19:21:32.033: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Mar 23 19:21:32.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 apply -f -'
    Mar 23 19:21:32.793: INFO: stderr: ""
    Mar 23 19:21:32.793: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Mar 23 19:21:32.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 --namespace=crd-publish-openapi-6472 delete e2e-test-crd-publish-openapi-1484-crds test-cr'
    Mar 23 19:21:32.890: INFO: stderr: ""
    Mar 23 19:21:32.890: INFO: stdout: "e2e-test-crd-publish-openapi-1484-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 03/23/23 19:21:32.89
    Mar 23 19:21:32.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-6472 explain e2e-test-crd-publish-openapi-1484-crds'
    Mar 23 19:21:33.776: INFO: stderr: ""
    Mar 23 19:21:33.776: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1484-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:21:36.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6472" for this suite. 03/23/23 19:21:36.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:36.685
Mar 23 19:21:36.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir-wrapper 03/23/23 19:21:36.686
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:36.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:36.705
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Mar 23 19:21:36.730: INFO: Waiting up to 5m0s for pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006" in namespace "emptydir-wrapper-8749" to be "running and ready"
Mar 23 19:21:36.736: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159087ms
Mar 23 19:21:36.736: INFO: The phase of Pod pod-secrets-c6367218-6199-439f-8524-7c7422b51006 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:21:38.740: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006": Phase="Running", Reason="", readiness=true. Elapsed: 2.010408937s
Mar 23 19:21:38.740: INFO: The phase of Pod pod-secrets-c6367218-6199-439f-8524-7c7422b51006 is Running (Ready = true)
Mar 23 19:21:38.741: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006" satisfied condition "running and ready"
STEP: Cleaning up the secret 03/23/23 19:21:38.743
STEP: Cleaning up the configmap 03/23/23 19:21:38.75
STEP: Cleaning up the pod 03/23/23 19:21:38.755
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Mar 23 19:21:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8749" for this suite. 03/23/23 19:21:38.776
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":152,"skipped":2667,"failed":0}
------------------------------
• [2.097 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:36.685
    Mar 23 19:21:36.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir-wrapper 03/23/23 19:21:36.686
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:36.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:36.705
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Mar 23 19:21:36.730: INFO: Waiting up to 5m0s for pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006" in namespace "emptydir-wrapper-8749" to be "running and ready"
    Mar 23 19:21:36.736: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006": Phase="Pending", Reason="", readiness=false. Elapsed: 6.159087ms
    Mar 23 19:21:36.736: INFO: The phase of Pod pod-secrets-c6367218-6199-439f-8524-7c7422b51006 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:21:38.740: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006": Phase="Running", Reason="", readiness=true. Elapsed: 2.010408937s
    Mar 23 19:21:38.740: INFO: The phase of Pod pod-secrets-c6367218-6199-439f-8524-7c7422b51006 is Running (Ready = true)
    Mar 23 19:21:38.741: INFO: Pod "pod-secrets-c6367218-6199-439f-8524-7c7422b51006" satisfied condition "running and ready"
    STEP: Cleaning up the secret 03/23/23 19:21:38.743
    STEP: Cleaning up the configmap 03/23/23 19:21:38.75
    STEP: Cleaning up the pod 03/23/23 19:21:38.755
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:21:38.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-8749" for this suite. 03/23/23 19:21:38.776
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:38.783
Mar 23 19:21:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename conformance-tests 03/23/23 19:21:38.785
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.804
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 03/23/23 19:21:38.81
Mar 23 19:21:38.811: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Mar 23 19:21:38.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3677" for this suite. 03/23/23 19:21:38.823
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":153,"skipped":2668,"failed":0}
------------------------------
• [0.048 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:38.783
    Mar 23 19:21:38.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename conformance-tests 03/23/23 19:21:38.785
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.804
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 03/23/23 19:21:38.81
    Mar 23 19:21:38.811: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Mar 23 19:21:38.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3677" for this suite. 03/23/23 19:21:38.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:38.834
Mar 23 19:21:38.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename tables 03/23/23 19:21:38.836
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.858
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Mar 23 19:21:38.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1017" for this suite. 03/23/23 19:21:38.878
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":154,"skipped":2682,"failed":0}
------------------------------
• [0.054 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:38.834
    Mar 23 19:21:38.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename tables 03/23/23 19:21:38.836
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.858
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Mar 23 19:21:38.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-1017" for this suite. 03/23/23 19:21:38.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:38.892
Mar 23 19:21:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:21:38.894
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.915
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 03/23/23 19:21:38.918
Mar 23 19:21:38.933: INFO: Waiting up to 5m0s for pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58" in namespace "pods-9872" to be "running and ready"
Mar 23 19:21:38.940: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.596283ms
Mar 23 19:21:38.940: INFO: The phase of Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:21:40.944: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58": Phase="Running", Reason="", readiness=true. Elapsed: 2.011750439s
Mar 23 19:21:40.944: INFO: The phase of Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 is Running (Ready = true)
Mar 23 19:21:40.945: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58" satisfied condition "running and ready"
Mar 23 19:21:40.950: INFO: Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 has hostIP: 10.240.0.4
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:21:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9872" for this suite. 03/23/23 19:21:40.955
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":155,"skipped":2698,"failed":0}
------------------------------
• [2.068 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:38.892
    Mar 23 19:21:38.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:21:38.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:38.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:38.915
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 03/23/23 19:21:38.918
    Mar 23 19:21:38.933: INFO: Waiting up to 5m0s for pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58" in namespace "pods-9872" to be "running and ready"
    Mar 23 19:21:38.940: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58": Phase="Pending", Reason="", readiness=false. Elapsed: 7.596283ms
    Mar 23 19:21:38.940: INFO: The phase of Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:21:40.944: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58": Phase="Running", Reason="", readiness=true. Elapsed: 2.011750439s
    Mar 23 19:21:40.944: INFO: The phase of Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 is Running (Ready = true)
    Mar 23 19:21:40.945: INFO: Pod "pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58" satisfied condition "running and ready"
    Mar 23 19:21:40.950: INFO: Pod pod-hostip-1f6db7fd-ec0c-4f0b-8bd8-079608674f58 has hostIP: 10.240.0.4
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:21:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9872" for this suite. 03/23/23 19:21:40.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:40.965
Mar 23 19:21:40.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:21:40.966
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:40.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:40.989
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:21:41.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6601" for this suite. 03/23/23 19:21:41.049
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":156,"skipped":2711,"failed":0}
------------------------------
• [0.094 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:40.965
    Mar 23 19:21:40.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:21:40.966
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:40.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:40.989
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:21:41.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6601" for this suite. 03/23/23 19:21:41.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:41.066
Mar 23 19:21:41.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 19:21:41.068
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:41.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:41.087
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Mar 23 19:21:41.090: INFO: Creating deployment "webserver-deployment"
Mar 23 19:21:41.100: INFO: Waiting for observed generation 1
Mar 23 19:21:43.112: INFO: Waiting for all required pods to come up
Mar 23 19:21:43.128: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 03/23/23 19:21:43.128
Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xndb8" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-42x7n" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h6wcx" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.133: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hw4gv" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4s9ll" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-559h5" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b4x46" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mc8qr" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kxqsb" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r5smf" in namespace "deployment-1039" to be "running"
Mar 23 19:21:43.143: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Pending", Reason="", readiness=false. Elapsed: 11.218875ms
Mar 23 19:21:43.143: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.142475ms
Mar 23 19:21:43.144: INFO: Pod "webserver-deployment-845c8977d9-hw4gv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.669083ms
Mar 23 19:21:43.144: INFO: Pod "webserver-deployment-845c8977d9-xndb8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.763973ms
Mar 23 19:21:43.176: INFO: Pod "webserver-deployment-845c8977d9-4s9ll": Phase="Pending", Reason="", readiness=false. Elapsed: 32.272029ms
Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-mc8qr": Phase="Pending", Reason="", readiness=false. Elapsed: 31.63303ms
Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-559h5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.204128ms
Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-b4x46": Phase="Pending", Reason="", readiness=false. Elapsed: 32.065429ms
Mar 23 19:21:43.186: INFO: Pod "webserver-deployment-845c8977d9-kxqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 40.56501ms
Mar 23 19:21:43.191: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249988ms
Mar 23 19:21:45.191: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058417435s
Mar 23 19:21:45.192: INFO: Pod "webserver-deployment-845c8977d9-hw4gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.05622214s
Mar 23 19:21:45.192: INFO: Pod "webserver-deployment-845c8977d9-hw4gv" satisfied condition "running"
Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06075963s
Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-xndb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.061360829s
Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-xndb8" satisfied condition "running"
Mar 23 19:21:45.195: INFO: Pod "webserver-deployment-845c8977d9-4s9ll": Phase="Running", Reason="", readiness=true. Elapsed: 2.051037352s
Mar 23 19:21:45.195: INFO: Pod "webserver-deployment-845c8977d9-4s9ll" satisfied condition "running"
Mar 23 19:21:45.209: INFO: Pod "webserver-deployment-845c8977d9-mc8qr": Phase="Running", Reason="", readiness=true. Elapsed: 2.063712524s
Mar 23 19:21:45.209: INFO: Pod "webserver-deployment-845c8977d9-mc8qr" satisfied condition "running"
Mar 23 19:21:45.210: INFO: Pod "webserver-deployment-845c8977d9-b4x46": Phase="Running", Reason="", readiness=true. Elapsed: 2.064852422s
Mar 23 19:21:45.210: INFO: Pod "webserver-deployment-845c8977d9-b4x46" satisfied condition "running"
Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02494731s
Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-kxqsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.065732519s
Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-kxqsb" satisfied condition "running"
Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-559h5": Phase="Running", Reason="", readiness=true. Elapsed: 2.066635017s
Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-559h5" satisfied condition "running"
Mar 23 19:21:47.192: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Running", Reason="", readiness=true. Elapsed: 4.059257238s
Mar 23 19:21:47.192: INFO: Pod "webserver-deployment-845c8977d9-h6wcx" satisfied condition "running"
Mar 23 19:21:47.193: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Running", Reason="", readiness=true. Elapsed: 4.060897335s
Mar 23 19:21:47.193: INFO: Pod "webserver-deployment-845c8977d9-42x7n" satisfied condition "running"
Mar 23 19:21:47.208: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Running", Reason="", readiness=true. Elapsed: 4.02240692s
Mar 23 19:21:47.208: INFO: Pod "webserver-deployment-845c8977d9-r5smf" satisfied condition "running"
Mar 23 19:21:47.208: INFO: Waiting for deployment "webserver-deployment" to complete
Mar 23 19:21:47.215: INFO: Updating deployment "webserver-deployment" with a non-existent image
Mar 23 19:21:47.229: INFO: Updating deployment webserver-deployment
Mar 23 19:21:47.229: INFO: Waiting for observed generation 2
Mar 23 19:21:49.237: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Mar 23 19:21:49.240: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Mar 23 19:21:49.242: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:21:49.250: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Mar 23 19:21:49.250: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Mar 23 19:21:49.252: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:21:49.257: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Mar 23 19:21:49.257: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Mar 23 19:21:49.266: INFO: Updating deployment webserver-deployment
Mar 23 19:21:49.266: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Mar 23 19:21:49.293: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Mar 23 19:21:51.316: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 19:21:51.324: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1039  6ddb56a7-feba-4899-aaef-aada35b84b4e 20547 3 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f22558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-23 19:21:49 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-23 19:21:49 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Mar 23 19:21:51.327: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-1039  356a5d7a-5b6f-467b-b354-4180d573466a 20543 3 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6ddb56a7-feba-4899-aaef-aada35b84b4e 0xc009f22957 0xc009f22958}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ddb56a7-feba-4899-aaef-aada35b84b4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f229f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:21:51.327: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Mar 23 19:21:51.327: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-1039  b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 20534 3 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6ddb56a7-feba-4899-aaef-aada35b84b4e 0xc009f22a57 0xc009f22a58}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ddb56a7-feba-4899-aaef-aada35b84b4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f22ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-2bctb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2bctb webserver-deployment-69b7448995- deployment-1039  7a6aa481-8a66-43d1-bb69-b41c19856b2e 20495 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009de9f57 0xc009de9f58}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pfnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pfnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-4t2ng" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4t2ng webserver-deployment-69b7448995- deployment-1039  d8a2fdc4-0647-4dcf-ad69-01a8ea69d8c3 20454 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4130 0xc009ee4131}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmj78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmj78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-5krhb" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5krhb webserver-deployment-69b7448995- deployment-1039  0e3163dc-1ba5-4725-8215-c4ab06213c31 20551 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4300 0xc009ee4301}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4gr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4gr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-6wq4q" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6wq4q webserver-deployment-69b7448995- deployment-1039  6a55e274-c4d3-4ef8-aa8d-e90960d89763 20539 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee44d0 0xc009ee44d1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drxjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drxjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-cxz79" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-cxz79 webserver-deployment-69b7448995- deployment-1039  fdac14cf-7930-498b-b31f-fb959643210e 20452 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee46a0 0xc009ee46a1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfx5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfx5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-drdsz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-drdsz webserver-deployment-69b7448995- deployment-1039  90d0f885-0eca-4e1c-94bc-f3968cacc23e 20441 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4870 0xc009ee4871}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8g6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8g6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-lk7kx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lk7kx webserver-deployment-69b7448995- deployment-1039  7ccbe978-5a2a-4ade-a896-dd5ff5c01b36 20533 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4a40 0xc009ee4a41}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dcsvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dcsvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-mmdsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-mmdsk webserver-deployment-69b7448995- deployment-1039  78d8ef81-5e61-4fdd-9939-8c74a3613b55 20519 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4ba0 0xc009ee4ba1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtgvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtgvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-n82ww" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-n82ww webserver-deployment-69b7448995- deployment-1039  ed223c2f-e3c2-4832-b562-4556acedff45 20549 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4d70 0xc009ee4d71}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j52hc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j52hc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-s5bj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s5bj5 webserver-deployment-69b7448995- deployment-1039  65ef0a75-1649-49f9-8a0f-d1d299e806a3 20555 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4f40 0xc009ee4f41}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7qvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7qvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tcllv" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tcllv webserver-deployment-69b7448995- deployment-1039  d1011108-8f86-4b2a-a0c5-ebbd001334b1 20530 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee5110 0xc009ee5111}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6jjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6jjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tdvk8" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tdvk8 webserver-deployment-69b7448995- deployment-1039  72442bd9-0439-48db-b880-aea61a1c94c2 20434 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee52e0 0xc009ee52e1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wfnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wfnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tx4bs" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tx4bs webserver-deployment-69b7448995- deployment-1039  b0c013a2-b7a0-49d2-b65c-95e58276c81c 20424 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee54b0 0xc009ee54b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqmjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqmjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-845c8977d9-42x7n" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-42x7n webserver-deployment-845c8977d9- deployment-1039  cda68294-6eda-4573-ba4a-ee517496a410 20373 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5680 0xc009ee5681}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgd2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgd2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.11,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://927b92ac68e0a3a8dcd3fd41adf10ca8fc01db927b158ba8a4c6ab2141a06359,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-6b9pn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6b9pn webserver-deployment-845c8977d9- deployment-1039  95e1d401-8e97-42f6-9171-0bb0d3f9b2f0 20516 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5850 0xc009ee5851}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvswx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvswx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-97smw" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-97smw webserver-deployment-845c8977d9- deployment-1039  1530fc5d-de4e-4438-8c67-915eed81cb7a 20541 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5a07 0xc009ee5a08}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsnxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsnxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-b4x46" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b4x46 webserver-deployment-845c8977d9- deployment-1039  141b398f-7947-4681-bebe-5f14cdada262 20352 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5bc0 0xc009ee5bc1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhcmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhcmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.40,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2e6eb5735973c3acfa55195b4c5e0a66941406f3dfbe1c90acdc839f2caa40ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-h6wcx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h6wcx webserver-deployment-845c8977d9- deployment-1039  3bf1328c-5b63-41da-aeb9-f12ed363db4c 20392 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5d90 0xc009ee5d91}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr6j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr6j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.18,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ae604d22b8a8abe258e9e887e5c719fb7096ed3e43efba5dfe9a1411ccc0ab5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-hw4gv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hw4gv webserver-deployment-845c8977d9- deployment-1039  9ec0b33f-fa31-4fde-9b71-5b33b8be8123 20319 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5f60 0xc009ee5f61}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x4qwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x4qwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.59,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://461a16bb1aac3cafae0fed3e94e9717fed2c2f365129da466a91e63995fcb4c5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-jlfff" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jlfff webserver-deployment-845c8977d9- deployment-1039  ce808624-0bec-469a-914c-198987916781 20496 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe130 0xc009efe131}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmjth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmjth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-kxqsb" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-kxqsb webserver-deployment-845c8977d9- deployment-1039  f470b417-09a7-4f8d-942b-a03d9c2baa3e 20355 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe2e0 0xc009efe2e1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mq6nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mq6nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.39,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://199067cc5c97b158995f83a707d49acf2b4fc1f573c5a31a458afa624ceb964b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mc8qr" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mc8qr webserver-deployment-845c8977d9- deployment-1039  b0e22b07-82fd-4b20-b941-b559a358f510 20322 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe4b0 0xc009efe4b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbfws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbfws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.58,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://39c715a43ceffacdac94ee2d46fc681feed0839c36ee397a556bcfee77d85fec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mgrvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mgrvv webserver-deployment-845c8977d9- deployment-1039  142dd37b-0620-4d7e-a4f3-23fe0c6c84f8 20526 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe680 0xc009efe681}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tn7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tn7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mhdtp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhdtp webserver-deployment-845c8977d9- deployment-1039  82e96e9d-8b12-4975-9426-fc051787e2e9 20502 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe7d0 0xc009efe7d1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk26q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk26q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-nf6xz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nf6xz webserver-deployment-845c8977d9- deployment-1039  d7b70ef8-1acf-47b4-b1cf-c5414fef72cd 20483 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe987 0xc009efe988}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vdqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vdqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-ngpx6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ngpx6 webserver-deployment-845c8977d9- deployment-1039  0447ddba-8113-47a3-aaac-10f4489964eb 20525 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efeb47 0xc009efeb48}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6p2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6p2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-nrcj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nrcj9 webserver-deployment-845c8977d9- deployment-1039  23f49667-be2d-48d2-b1c6-b3af09a5aca1 20594 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efed07 0xc009efed08}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxg9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxg9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-pdr2j" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pdr2j webserver-deployment-845c8977d9- deployment-1039  3a47b667-6a5e-4ed0-90fe-7d11cf02def9 20593 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efeec7 0xc009efeec8}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5mqf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5mqf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-r5smf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-r5smf webserver-deployment-845c8977d9- deployment-1039  bcb6dff1-5cd0-4100-8bad-6ddec8102e6f 20394 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff087 0xc009eff088}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpnn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpnn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.16,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5d6ec65a7b67e650f2f0fc144026ecb2ae656b78594527c2394b25521b39314d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-rrn82" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rrn82 webserver-deployment-845c8977d9- deployment-1039  b93ef0d7-fd40-42c6-a75b-f299ab08ac09 20531 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff260 0xc009eff261}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2brm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2brm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-s6gxs" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-s6gxs webserver-deployment-845c8977d9- deployment-1039  973e2481-534b-4c3e-8dce-58284e7fdd61 20529 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff3b0 0xc009eff3b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2s2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2s2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.346: INFO: Pod "webserver-deployment-845c8977d9-xggt6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xggt6 webserver-deployment-845c8977d9- deployment-1039  140b01b6-526d-4789-afae-235c084af5d5 20544 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff500 0xc009eff501}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49p22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49p22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:21:51.346: INFO: Pod "webserver-deployment-845c8977d9-xndb8" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xndb8 webserver-deployment-845c8977d9- deployment-1039  33607b6e-44a9-4463-a78d-55768de0ed2f 20358 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff6b7 0xc009eff6b8}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55rwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55rwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.33,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://42de42d95447b785e4d5a2956806d6cbe75f0c421529877ed289d277d3d75ab1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 19:21:51.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1039" for this suite. 03/23/23 19:21:51.351
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":157,"skipped":2755,"failed":0}
------------------------------
• [SLOW TEST] [10.297 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:41.066
    Mar 23 19:21:41.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 19:21:41.068
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:41.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:41.087
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Mar 23 19:21:41.090: INFO: Creating deployment "webserver-deployment"
    Mar 23 19:21:41.100: INFO: Waiting for observed generation 1
    Mar 23 19:21:43.112: INFO: Waiting for all required pods to come up
    Mar 23 19:21:43.128: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 03/23/23 19:21:43.128
    Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-xndb8" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-42x7n" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.132: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h6wcx" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.133: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hw4gv" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4s9ll" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-559h5" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b4x46" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mc8qr" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-kxqsb" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.136: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-r5smf" in namespace "deployment-1039" to be "running"
    Mar 23 19:21:43.143: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Pending", Reason="", readiness=false. Elapsed: 11.218875ms
    Mar 23 19:21:43.143: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.142475ms
    Mar 23 19:21:43.144: INFO: Pod "webserver-deployment-845c8977d9-hw4gv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.669083ms
    Mar 23 19:21:43.144: INFO: Pod "webserver-deployment-845c8977d9-xndb8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.763973ms
    Mar 23 19:21:43.176: INFO: Pod "webserver-deployment-845c8977d9-4s9ll": Phase="Pending", Reason="", readiness=false. Elapsed: 32.272029ms
    Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-mc8qr": Phase="Pending", Reason="", readiness=false. Elapsed: 31.63303ms
    Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-559h5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.204128ms
    Mar 23 19:21:43.177: INFO: Pod "webserver-deployment-845c8977d9-b4x46": Phase="Pending", Reason="", readiness=false. Elapsed: 32.065429ms
    Mar 23 19:21:43.186: INFO: Pod "webserver-deployment-845c8977d9-kxqsb": Phase="Pending", Reason="", readiness=false. Elapsed: 40.56501ms
    Mar 23 19:21:43.191: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249988ms
    Mar 23 19:21:45.191: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058417435s
    Mar 23 19:21:45.192: INFO: Pod "webserver-deployment-845c8977d9-hw4gv": Phase="Running", Reason="", readiness=true. Elapsed: 2.05622214s
    Mar 23 19:21:45.192: INFO: Pod "webserver-deployment-845c8977d9-hw4gv" satisfied condition "running"
    Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.06075963s
    Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-xndb8": Phase="Running", Reason="", readiness=true. Elapsed: 2.061360829s
    Mar 23 19:21:45.193: INFO: Pod "webserver-deployment-845c8977d9-xndb8" satisfied condition "running"
    Mar 23 19:21:45.195: INFO: Pod "webserver-deployment-845c8977d9-4s9ll": Phase="Running", Reason="", readiness=true. Elapsed: 2.051037352s
    Mar 23 19:21:45.195: INFO: Pod "webserver-deployment-845c8977d9-4s9ll" satisfied condition "running"
    Mar 23 19:21:45.209: INFO: Pod "webserver-deployment-845c8977d9-mc8qr": Phase="Running", Reason="", readiness=true. Elapsed: 2.063712524s
    Mar 23 19:21:45.209: INFO: Pod "webserver-deployment-845c8977d9-mc8qr" satisfied condition "running"
    Mar 23 19:21:45.210: INFO: Pod "webserver-deployment-845c8977d9-b4x46": Phase="Running", Reason="", readiness=true. Elapsed: 2.064852422s
    Mar 23 19:21:45.210: INFO: Pod "webserver-deployment-845c8977d9-b4x46" satisfied condition "running"
    Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02494731s
    Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-kxqsb": Phase="Running", Reason="", readiness=true. Elapsed: 2.065732519s
    Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-kxqsb" satisfied condition "running"
    Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-559h5": Phase="Running", Reason="", readiness=true. Elapsed: 2.066635017s
    Mar 23 19:21:45.211: INFO: Pod "webserver-deployment-845c8977d9-559h5" satisfied condition "running"
    Mar 23 19:21:47.192: INFO: Pod "webserver-deployment-845c8977d9-h6wcx": Phase="Running", Reason="", readiness=true. Elapsed: 4.059257238s
    Mar 23 19:21:47.192: INFO: Pod "webserver-deployment-845c8977d9-h6wcx" satisfied condition "running"
    Mar 23 19:21:47.193: INFO: Pod "webserver-deployment-845c8977d9-42x7n": Phase="Running", Reason="", readiness=true. Elapsed: 4.060897335s
    Mar 23 19:21:47.193: INFO: Pod "webserver-deployment-845c8977d9-42x7n" satisfied condition "running"
    Mar 23 19:21:47.208: INFO: Pod "webserver-deployment-845c8977d9-r5smf": Phase="Running", Reason="", readiness=true. Elapsed: 4.02240692s
    Mar 23 19:21:47.208: INFO: Pod "webserver-deployment-845c8977d9-r5smf" satisfied condition "running"
    Mar 23 19:21:47.208: INFO: Waiting for deployment "webserver-deployment" to complete
    Mar 23 19:21:47.215: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Mar 23 19:21:47.229: INFO: Updating deployment webserver-deployment
    Mar 23 19:21:47.229: INFO: Waiting for observed generation 2
    Mar 23 19:21:49.237: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Mar 23 19:21:49.240: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Mar 23 19:21:49.242: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 23 19:21:49.250: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Mar 23 19:21:49.250: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Mar 23 19:21:49.252: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Mar 23 19:21:49.257: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Mar 23 19:21:49.257: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Mar 23 19:21:49.266: INFO: Updating deployment webserver-deployment
    Mar 23 19:21:49.266: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Mar 23 19:21:49.293: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Mar 23 19:21:51.316: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 19:21:51.324: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1039  6ddb56a7-feba-4899-aaef-aada35b84b4e 20547 3 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f22558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-23 19:21:49 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-03-23 19:21:49 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Mar 23 19:21:51.327: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-1039  356a5d7a-5b6f-467b-b354-4180d573466a 20543 3 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6ddb56a7-feba-4899-aaef-aada35b84b4e 0xc009f22957 0xc009f22958}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ddb56a7-feba-4899-aaef-aada35b84b4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f229f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:21:51.327: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Mar 23 19:21:51.327: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-1039  b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 20534 3 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6ddb56a7-feba-4899-aaef-aada35b84b4e 0xc009f22a57 0xc009f22a58}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ddb56a7-feba-4899-aaef-aada35b84b4e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009f22ae8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-2bctb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2bctb webserver-deployment-69b7448995- deployment-1039  7a6aa481-8a66-43d1-bb69-b41c19856b2e 20495 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009de9f57 0xc009de9f58}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pfnlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pfnlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-4t2ng" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4t2ng webserver-deployment-69b7448995- deployment-1039  d8a2fdc4-0647-4dcf-ad69-01a8ea69d8c3 20454 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4130 0xc009ee4131}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmj78,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmj78,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.337: INFO: Pod "webserver-deployment-69b7448995-5krhb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5krhb webserver-deployment-69b7448995- deployment-1039  0e3163dc-1ba5-4725-8215-c4ab06213c31 20551 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4300 0xc009ee4301}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c4gr4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c4gr4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-6wq4q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6wq4q webserver-deployment-69b7448995- deployment-1039  6a55e274-c4d3-4ef8-aa8d-e90960d89763 20539 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee44d0 0xc009ee44d1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drxjm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drxjm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-cxz79" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-cxz79 webserver-deployment-69b7448995- deployment-1039  fdac14cf-7930-498b-b31f-fb959643210e 20452 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee46a0 0xc009ee46a1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gfx5t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gfx5t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-drdsz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-drdsz webserver-deployment-69b7448995- deployment-1039  90d0f885-0eca-4e1c-94bc-f3968cacc23e 20441 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4870 0xc009ee4871}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8g6l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8g6l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.338: INFO: Pod "webserver-deployment-69b7448995-lk7kx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lk7kx webserver-deployment-69b7448995- deployment-1039  7ccbe978-5a2a-4ade-a896-dd5ff5c01b36 20533 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4a40 0xc009ee4a41}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dcsvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dcsvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-mmdsk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-mmdsk webserver-deployment-69b7448995- deployment-1039  78d8ef81-5e61-4fdd-9939-8c74a3613b55 20519 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4ba0 0xc009ee4ba1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gtgvr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gtgvr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-n82ww" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-n82ww webserver-deployment-69b7448995- deployment-1039  ed223c2f-e3c2-4832-b562-4556acedff45 20549 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4d70 0xc009ee4d71}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j52hc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j52hc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.339: INFO: Pod "webserver-deployment-69b7448995-s5bj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s5bj5 webserver-deployment-69b7448995- deployment-1039  65ef0a75-1649-49f9-8a0f-d1d299e806a3 20555 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee4f40 0xc009ee4f41}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m7qvq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m7qvq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tcllv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tcllv webserver-deployment-69b7448995- deployment-1039  d1011108-8f86-4b2a-a0c5-ebbd001334b1 20530 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee5110 0xc009ee5111}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b6jjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b6jjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tdvk8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tdvk8 webserver-deployment-69b7448995- deployment-1039  72442bd9-0439-48db-b880-aea61a1c94c2 20434 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee52e0 0xc009ee52e1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wfnt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wfnt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-69b7448995-tx4bs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tx4bs webserver-deployment-69b7448995- deployment-1039  b0c013a2-b7a0-49d2-b65c-95e58276c81c 20424 0 2023-03-23 19:21:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 356a5d7a-5b6f-467b-b354-4180d573466a 0xc009ee54b0 0xc009ee54b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"356a5d7a-5b6f-467b-b354-4180d573466a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zqmjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zqmjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.340: INFO: Pod "webserver-deployment-845c8977d9-42x7n" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-42x7n webserver-deployment-845c8977d9- deployment-1039  cda68294-6eda-4573-ba4a-ee517496a410 20373 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5680 0xc009ee5681}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgd2p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgd2p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.11,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://927b92ac68e0a3a8dcd3fd41adf10ca8fc01db927b158ba8a4c6ab2141a06359,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.11,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-6b9pn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6b9pn webserver-deployment-845c8977d9- deployment-1039  95e1d401-8e97-42f6-9171-0bb0d3f9b2f0 20516 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5850 0xc009ee5851}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvswx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvswx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-97smw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-97smw webserver-deployment-845c8977d9- deployment-1039  1530fc5d-de4e-4438-8c67-915eed81cb7a 20541 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5a07 0xc009ee5a08}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bsnxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bsnxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.341: INFO: Pod "webserver-deployment-845c8977d9-b4x46" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b4x46 webserver-deployment-845c8977d9- deployment-1039  141b398f-7947-4681-bebe-5f14cdada262 20352 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5bc0 0xc009ee5bc1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.40\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hhcmt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hhcmt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.40,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2e6eb5735973c3acfa55195b4c5e0a66941406f3dfbe1c90acdc839f2caa40ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.40,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-h6wcx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h6wcx webserver-deployment-845c8977d9- deployment-1039  3bf1328c-5b63-41da-aeb9-f12ed363db4c 20392 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5d90 0xc009ee5d91}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lr6j7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lr6j7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.18,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8ae604d22b8a8abe258e9e887e5c719fb7096ed3e43efba5dfe9a1411ccc0ab5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-hw4gv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hw4gv webserver-deployment-845c8977d9- deployment-1039  9ec0b33f-fa31-4fde-9b71-5b33b8be8123 20319 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009ee5f60 0xc009ee5f61}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-x4qwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-x4qwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.59,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://461a16bb1aac3cafae0fed3e94e9717fed2c2f365129da466a91e63995fcb4c5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.342: INFO: Pod "webserver-deployment-845c8977d9-jlfff" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jlfff webserver-deployment-845c8977d9- deployment-1039  ce808624-0bec-469a-914c-198987916781 20496 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe130 0xc009efe131}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nmjth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nmjth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-kxqsb" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-kxqsb webserver-deployment-845c8977d9- deployment-1039  f470b417-09a7-4f8d-942b-a03d9c2baa3e 20355 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe2e0 0xc009efe2e1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.39\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mq6nf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mq6nf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.39,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://199067cc5c97b158995f83a707d49acf2b4fc1f573c5a31a458afa624ceb964b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.39,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mc8qr" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mc8qr webserver-deployment-845c8977d9- deployment-1039  b0e22b07-82fd-4b20-b941-b559a358f510 20322 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe4b0 0xc009efe4b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.58\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kbfws,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kbfws,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.58,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://39c715a43ceffacdac94ee2d46fc681feed0839c36ee397a556bcfee77d85fec,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.58,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mgrvv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mgrvv webserver-deployment-845c8977d9- deployment-1039  142dd37b-0620-4d7e-a4f3-23fe0c6c84f8 20526 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe680 0xc009efe681}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8tn7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8tn7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.343: INFO: Pod "webserver-deployment-845c8977d9-mhdtp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mhdtp webserver-deployment-845c8977d9- deployment-1039  82e96e9d-8b12-4975-9426-fc051787e2e9 20502 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe7d0 0xc009efe7d1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pk26q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pk26q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-nf6xz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nf6xz webserver-deployment-845c8977d9- deployment-1039  d7b70ef8-1acf-47b4-b1cf-c5414fef72cd 20483 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efe987 0xc009efe988}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vdqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vdqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-ngpx6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ngpx6 webserver-deployment-845c8977d9- deployment-1039  0447ddba-8113-47a3-aaac-10f4489964eb 20525 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efeb47 0xc009efeb48}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j6p2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j6p2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.344: INFO: Pod "webserver-deployment-845c8977d9-nrcj9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nrcj9 webserver-deployment-845c8977d9- deployment-1039  23f49667-be2d-48d2-b1c6-b3af09a5aca1 20594 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efed07 0xc009efed08}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxg9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxg9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-pdr2j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pdr2j webserver-deployment-845c8977d9- deployment-1039  3a47b667-6a5e-4ed0-90fe-7d11cf02def9 20593 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009efeec7 0xc009efeec8}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5mqf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5mqf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-r5smf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-r5smf webserver-deployment-845c8977d9- deployment-1039  bcb6dff1-5cd0-4100-8bad-6ddec8102e6f 20394 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff087 0xc009eff088}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lpnn8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lpnn8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:10.240.0.16,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://5d6ec65a7b67e650f2f0fc144026ecb2ae656b78594527c2394b25521b39314d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-rrn82" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rrn82 webserver-deployment-845c8977d9- deployment-1039  b93ef0d7-fd40-42c6-a75b-f299ab08ac09 20531 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff260 0xc009eff261}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2brm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2brm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.345: INFO: Pod "webserver-deployment-845c8977d9-s6gxs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-s6gxs webserver-deployment-845c8977d9- deployment-1039  973e2481-534b-4c3e-8dce-58284e7fdd61 20529 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff3b0 0xc009eff3b1}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l2s2j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l2s2j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.346: INFO: Pod "webserver-deployment-845c8977d9-xggt6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xggt6 webserver-deployment-845c8977d9- deployment-1039  140b01b6-526d-4789-afae-235c084af5d5 20544 0 2023-03-23 19:21:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff500 0xc009eff501}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-49p22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-49p22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:21:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:21:51.346: INFO: Pod "webserver-deployment-845c8977d9-xndb8" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xndb8 webserver-deployment-845c8977d9- deployment-1039  33607b6e-44a9-4463-a78d-55768de0ed2f 20358 0 2023-03-23 19:21:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 b797efaa-f3cb-4b13-b35f-46cdf9a89f0a 0xc009eff6b7 0xc009eff6b8}] [] [{kube-controller-manager Update v1 2023-03-23 19:21:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b797efaa-f3cb-4b13-b35f-46cdf9a89f0a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:21:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-55rwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-55rwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:21:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.30,PodIP:10.240.0.33,StartTime:2023-03-23 19:21:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:21:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://42de42d95447b785e4d5a2956806d6cbe75f0c421529877ed289d277d3d75ab1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 19:21:51.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1039" for this suite. 03/23/23 19:21:51.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:21:51.377
Mar 23 19:21:51.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context-test 03/23/23 19:21:51.378
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:51.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:51.41
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Mar 23 19:21:51.435: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359" in namespace "security-context-test-6908" to be "Succeeded or Failed"
Mar 23 19:21:51.450: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 15.672564ms
Mar 23 19:21:53.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019980235s
Mar 23 19:21:55.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019522308s
Mar 23 19:21:57.472: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037244138s
Mar 23 19:21:59.456: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021087045s
Mar 23 19:22:01.470: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034970383s
Mar 23 19:22:03.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02028444s
Mar 23 19:22:05.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019680975s
Mar 23 19:22:07.460: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024935797s
Mar 23 19:22:09.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=true. Elapsed: 18.019440943s
Mar 23 19:22:11.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=true. Elapsed: 20.020534679s
Mar 23 19:22:13.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=false. Elapsed: 22.019954419s
Mar 23 19:22:15.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020003759s
Mar 23 19:22:15.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 19:22:15.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6908" for this suite. 03/23/23 19:22:15.461
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":2763,"failed":0}
------------------------------
• [SLOW TEST] [24.094 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:21:51.377
    Mar 23 19:21:51.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context-test 03/23/23 19:21:51.378
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:21:51.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:21:51.41
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Mar 23 19:21:51.435: INFO: Waiting up to 5m0s for pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359" in namespace "security-context-test-6908" to be "Succeeded or Failed"
    Mar 23 19:21:51.450: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 15.672564ms
    Mar 23 19:21:53.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019980235s
    Mar 23 19:21:55.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019522308s
    Mar 23 19:21:57.472: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037244138s
    Mar 23 19:21:59.456: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021087045s
    Mar 23 19:22:01.470: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 10.034970383s
    Mar 23 19:22:03.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02028444s
    Mar 23 19:22:05.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019680975s
    Mar 23 19:22:07.460: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024935797s
    Mar 23 19:22:09.454: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=true. Elapsed: 18.019440943s
    Mar 23 19:22:11.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=true. Elapsed: 20.020534679s
    Mar 23 19:22:13.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Running", Reason="", readiness=false. Elapsed: 22.019954419s
    Mar 23 19:22:15.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020003759s
    Mar 23 19:22:15.455: INFO: Pod "busybox-user-65534-0a08da8b-f10c-4cbb-bb49-b0d0d6b82359" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 19:22:15.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6908" for this suite. 03/23/23 19:22:15.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:15.479
Mar 23 19:22:15.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:22:15.481
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:15.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:15.5
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 03/23/23 19:22:15.508
Mar 23 19:22:15.523: INFO: Waiting up to 5m0s for pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538" in namespace "downward-api-9436" to be "Succeeded or Failed"
Mar 23 19:22:15.527: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240491ms
Mar 23 19:22:17.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Running", Reason="", readiness=true. Elapsed: 2.008008322s
Mar 23 19:22:19.534: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Running", Reason="", readiness=false. Elapsed: 4.010738256s
Mar 23 19:22:21.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007766504s
STEP: Saw pod success 03/23/23 19:22:21.531
Mar 23 19:22:21.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538" satisfied condition "Succeeded or Failed"
Mar 23 19:22:21.534: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 container dapi-container: <nil>
STEP: delete the pod 03/23/23 19:22:21.563
Mar 23 19:22:21.580: INFO: Waiting for pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 to disappear
Mar 23 19:22:21.583: INFO: Pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 23 19:22:21.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9436" for this suite. 03/23/23 19:22:21.599
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":159,"skipped":2768,"failed":0}
------------------------------
• [SLOW TEST] [6.128 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:15.479
    Mar 23 19:22:15.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:22:15.481
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:15.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:15.5
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 03/23/23 19:22:15.508
    Mar 23 19:22:15.523: INFO: Waiting up to 5m0s for pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538" in namespace "downward-api-9436" to be "Succeeded or Failed"
    Mar 23 19:22:15.527: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Pending", Reason="", readiness=false. Elapsed: 4.240491ms
    Mar 23 19:22:17.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Running", Reason="", readiness=true. Elapsed: 2.008008322s
    Mar 23 19:22:19.534: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Running", Reason="", readiness=false. Elapsed: 4.010738256s
    Mar 23 19:22:21.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007766504s
    STEP: Saw pod success 03/23/23 19:22:21.531
    Mar 23 19:22:21.531: INFO: Pod "downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538" satisfied condition "Succeeded or Failed"
    Mar 23 19:22:21.534: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 19:22:21.563
    Mar 23 19:22:21.580: INFO: Waiting for pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 to disappear
    Mar 23 19:22:21.583: INFO: Pod downward-api-b4a6a4d7-f839-4408-b66c-aa5e345e1538 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 23 19:22:21.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9436" for this suite. 03/23/23 19:22:21.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:21.62
Mar 23 19:22:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename events 03/23/23 19:22:21.622
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:21.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:21.651
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 03/23/23 19:22:21.654
STEP: get a list of Events with a label in the current namespace 03/23/23 19:22:21.696
STEP: delete a list of events 03/23/23 19:22:21.699
Mar 23 19:22:21.699: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/23/23 19:22:21.716
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Mar 23 19:22:21.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6466" for this suite. 03/23/23 19:22:21.725
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":160,"skipped":2780,"failed":0}
------------------------------
• [0.115 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:21.62
    Mar 23 19:22:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename events 03/23/23 19:22:21.622
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:21.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:21.651
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 03/23/23 19:22:21.654
    STEP: get a list of Events with a label in the current namespace 03/23/23 19:22:21.696
    STEP: delete a list of events 03/23/23 19:22:21.699
    Mar 23 19:22:21.699: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/23/23 19:22:21.716
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Mar 23 19:22:21.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6466" for this suite. 03/23/23 19:22:21.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:21.735
Mar 23 19:22:21.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:22:21.737
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:21.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:21.771
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:22:21.775
Mar 23 19:22:21.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d" in namespace "downward-api-2969" to be "Succeeded or Failed"
Mar 23 19:22:21.819: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037468ms
Mar 23 19:22:23.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017655501s
Mar 23 19:22:25.824: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Running", Reason="", readiness=false. Elapsed: 4.019165639s
Mar 23 19:22:27.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018323182s
STEP: Saw pod success 03/23/23 19:22:27.823
Mar 23 19:22:27.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d" satisfied condition "Succeeded or Failed"
Mar 23 19:22:27.826: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d container client-container: <nil>
STEP: delete the pod 03/23/23 19:22:27.86
Mar 23 19:22:27.875: INFO: Waiting for pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d to disappear
Mar 23 19:22:27.878: INFO: Pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 19:22:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2969" for this suite. 03/23/23 19:22:27.883
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":161,"skipped":2786,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:21.735
    Mar 23 19:22:21.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:22:21.737
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:21.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:21.771
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:22:21.775
    Mar 23 19:22:21.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d" in namespace "downward-api-2969" to be "Succeeded or Failed"
    Mar 23 19:22:21.819: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.037468ms
    Mar 23 19:22:23.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017655501s
    Mar 23 19:22:25.824: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Running", Reason="", readiness=false. Elapsed: 4.019165639s
    Mar 23 19:22:27.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018323182s
    STEP: Saw pod success 03/23/23 19:22:27.823
    Mar 23 19:22:27.823: INFO: Pod "downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d" satisfied condition "Succeeded or Failed"
    Mar 23 19:22:27.826: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d container client-container: <nil>
    STEP: delete the pod 03/23/23 19:22:27.86
    Mar 23 19:22:27.875: INFO: Waiting for pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d to disappear
    Mar 23 19:22:27.878: INFO: Pod downwardapi-volume-38b55c44-2fd4-4543-aee4-296dfa23182d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 19:22:27.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2969" for this suite. 03/23/23 19:22:27.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:27.893
Mar 23 19:22:27.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 19:22:27.894
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:27.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:27.918
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 03/23/23 19:22:27.921
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local;sleep 1; done
 03/23/23 19:22:27.927
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local;sleep 1; done
 03/23/23 19:22:27.927
STEP: creating a pod to probe DNS 03/23/23 19:22:27.927
STEP: submitting the pod to kubernetes 03/23/23 19:22:27.927
Mar 23 19:22:27.949: INFO: Waiting up to 15m0s for pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf" in namespace "dns-712" to be "running"
Mar 23 19:22:27.984: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Pending", Reason="", readiness=false. Elapsed: 35.324021ms
Mar 23 19:22:29.990: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04099365s
Mar 23 19:22:31.989: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Running", Reason="", readiness=true. Elapsed: 4.040233793s
Mar 23 19:22:31.989: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf" satisfied condition "running"
STEP: retrieving the pod 03/23/23 19:22:31.989
STEP: looking for the results for each expected name from probers 03/23/23 19:22:31.993
Mar 23 19:22:31.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.003: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.008: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.019: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.027: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.032: INFO: Unable to read jessie_udp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.043: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
Mar 23 19:22:32.043: INFO: Lookups using dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local jessie_udp@dns-test-service-2.dns-712.svc.cluster.local jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local]

Mar 23 19:22:37.086: INFO: DNS probes using dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf succeeded

STEP: deleting the pod 03/23/23 19:22:37.086
STEP: deleting the test headless service 03/23/23 19:22:37.128
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 19:22:37.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-712" for this suite. 03/23/23 19:22:37.18
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":162,"skipped":2802,"failed":0}
------------------------------
• [SLOW TEST] [9.296 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:27.893
    Mar 23 19:22:27.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 19:22:27.894
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:27.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:27.918
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 03/23/23 19:22:27.921
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local;sleep 1; done
     03/23/23 19:22:27.927
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-712.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-712.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local;sleep 1; done
     03/23/23 19:22:27.927
    STEP: creating a pod to probe DNS 03/23/23 19:22:27.927
    STEP: submitting the pod to kubernetes 03/23/23 19:22:27.927
    Mar 23 19:22:27.949: INFO: Waiting up to 15m0s for pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf" in namespace "dns-712" to be "running"
    Mar 23 19:22:27.984: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Pending", Reason="", readiness=false. Elapsed: 35.324021ms
    Mar 23 19:22:29.990: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04099365s
    Mar 23 19:22:31.989: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf": Phase="Running", Reason="", readiness=true. Elapsed: 4.040233793s
    Mar 23 19:22:31.989: INFO: Pod "dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 19:22:31.989
    STEP: looking for the results for each expected name from probers 03/23/23 19:22:31.993
    Mar 23 19:22:31.998: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.003: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.008: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.019: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.027: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.032: INFO: Unable to read jessie_udp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.043: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local from pod dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf: the server could not find the requested resource (get pods dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf)
    Mar 23 19:22:32.043: INFO: Lookups using dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local wheezy_udp@dns-test-service-2.dns-712.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-712.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-712.svc.cluster.local jessie_udp@dns-test-service-2.dns-712.svc.cluster.local jessie_tcp@dns-test-service-2.dns-712.svc.cluster.local]

    Mar 23 19:22:37.086: INFO: DNS probes using dns-712/dns-test-78401f77-4fb7-49b5-a084-a9cdf8856caf succeeded

    STEP: deleting the pod 03/23/23 19:22:37.086
    STEP: deleting the test headless service 03/23/23 19:22:37.128
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 19:22:37.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-712" for this suite. 03/23/23 19:22:37.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:37.192
Mar 23 19:22:37.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:22:37.193
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:37.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:37.233
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-b4bfa183-4cf7-4223-be2d-70c05e4f0335 03/23/23 19:22:37.238
STEP: Creating a pod to test consume configMaps 03/23/23 19:22:37.244
Mar 23 19:22:37.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c" in namespace "configmap-2374" to be "Succeeded or Failed"
Mar 23 19:22:37.262: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764282ms
Mar 23 19:22:39.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013309512s
Mar 23 19:22:41.267: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Running", Reason="", readiness=false. Elapsed: 4.012688556s
Mar 23 19:22:43.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013549097s
STEP: Saw pod success 03/23/23 19:22:43.268
Mar 23 19:22:43.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c" satisfied condition "Succeeded or Failed"
Mar 23 19:22:43.271: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:22:43.278
Mar 23 19:22:43.290: INFO: Waiting for pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c to disappear
Mar 23 19:22:43.293: INFO: Pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:22:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2374" for this suite. 03/23/23 19:22:43.297
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":163,"skipped":2807,"failed":0}
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:37.192
    Mar 23 19:22:37.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:22:37.193
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:37.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:37.233
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-b4bfa183-4cf7-4223-be2d-70c05e4f0335 03/23/23 19:22:37.238
    STEP: Creating a pod to test consume configMaps 03/23/23 19:22:37.244
    Mar 23 19:22:37.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c" in namespace "configmap-2374" to be "Succeeded or Failed"
    Mar 23 19:22:37.262: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.764282ms
    Mar 23 19:22:39.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Running", Reason="", readiness=true. Elapsed: 2.013309512s
    Mar 23 19:22:41.267: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Running", Reason="", readiness=false. Elapsed: 4.012688556s
    Mar 23 19:22:43.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013549097s
    STEP: Saw pod success 03/23/23 19:22:43.268
    Mar 23 19:22:43.268: INFO: Pod "pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c" satisfied condition "Succeeded or Failed"
    Mar 23 19:22:43.271: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:22:43.278
    Mar 23 19:22:43.290: INFO: Waiting for pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c to disappear
    Mar 23 19:22:43.293: INFO: Pod pod-configmaps-550eab3d-0f08-4ec8-aabe-3937038f4b7c no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:22:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2374" for this suite. 03/23/23 19:22:43.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:22:43.311
Mar 23 19:22:43.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:22:43.312
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:43.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:43.331
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 03/23/23 19:22:43.337
Mar 23 19:22:43.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: mark a version not serverd 03/23/23 19:22:52.473
STEP: check the unserved version gets removed 03/23/23 19:22:52.51
STEP: check the other version is not changed 03/23/23 19:22:56.417
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:23:04.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3731" for this suite. 03/23/23 19:23:04.95
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":164,"skipped":2812,"failed":0}
------------------------------
• [SLOW TEST] [21.681 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:22:43.311
    Mar 23 19:22:43.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:22:43.312
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:22:43.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:22:43.331
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 03/23/23 19:22:43.337
    Mar 23 19:22:43.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: mark a version not serverd 03/23/23 19:22:52.473
    STEP: check the unserved version gets removed 03/23/23 19:22:52.51
    STEP: check the other version is not changed 03/23/23 19:22:56.417
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:23:04.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3731" for this suite. 03/23/23 19:23:04.95
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:04.996
Mar 23 19:23:04.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:23:04.998
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:05.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:05.02
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 03/23/23 19:23:05.023
STEP: watching for the ServiceAccount to be added 03/23/23 19:23:05.037
STEP: patching the ServiceAccount 03/23/23 19:23:05.04
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/23/23 19:23:05.048
STEP: deleting the ServiceAccount 03/23/23 19:23:05.053
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 19:23:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1701" for this suite. 03/23/23 19:23:05.1
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":165,"skipped":2843,"failed":0}
------------------------------
• [0.114 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:04.996
    Mar 23 19:23:04.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:23:04.998
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:05.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:05.02
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 03/23/23 19:23:05.023
    STEP: watching for the ServiceAccount to be added 03/23/23 19:23:05.037
    STEP: patching the ServiceAccount 03/23/23 19:23:05.04
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 03/23/23 19:23:05.048
    STEP: deleting the ServiceAccount 03/23/23 19:23:05.053
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 19:23:05.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-1701" for this suite. 03/23/23 19:23:05.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:05.115
Mar 23 19:23:05.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:23:05.116
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:05.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:05.161
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:23:05.177
Mar 23 19:23:05.193: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4050" to be "running and ready"
Mar 23 19:23:05.198: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62149ms
Mar 23 19:23:05.198: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:23:07.202: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008587026s
Mar 23 19:23:07.202: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 23 19:23:07.202: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 03/23/23 19:23:07.205
Mar 23 19:23:07.211: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4050" to be "running and ready"
Mar 23 19:23:07.215: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457092ms
Mar 23 19:23:07.215: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:23:09.219: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007720329s
Mar 23 19:23:09.219: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Mar 23 19:23:09.219: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 03/23/23 19:23:09.222
STEP: delete the pod with lifecycle hook 03/23/23 19:23:09.228
Mar 23 19:23:09.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 19:23:09.241: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 19:23:11.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 19:23:11.246: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 19:23:13.243: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 19:23:13.247: INFO: Pod pod-with-poststart-exec-hook still exists
Mar 23 19:23:15.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Mar 23 19:23:15.248: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 23 19:23:15.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4050" for this suite. 03/23/23 19:23:15.255
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":166,"skipped":2870,"failed":0}
------------------------------
• [SLOW TEST] [10.148 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:05.115
    Mar 23 19:23:05.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:23:05.116
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:05.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:05.161
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:23:05.177
    Mar 23 19:23:05.193: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4050" to be "running and ready"
    Mar 23 19:23:05.198: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62149ms
    Mar 23 19:23:05.198: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:23:07.202: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008587026s
    Mar 23 19:23:07.202: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 23 19:23:07.202: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 03/23/23 19:23:07.205
    Mar 23 19:23:07.211: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4050" to be "running and ready"
    Mar 23 19:23:07.215: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457092ms
    Mar 23 19:23:07.215: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:23:09.219: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007720329s
    Mar 23 19:23:09.219: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Mar 23 19:23:09.219: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 03/23/23 19:23:09.222
    STEP: delete the pod with lifecycle hook 03/23/23 19:23:09.228
    Mar 23 19:23:09.237: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 23 19:23:09.241: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 23 19:23:11.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 23 19:23:11.246: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 23 19:23:13.243: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 23 19:23:13.247: INFO: Pod pod-with-poststart-exec-hook still exists
    Mar 23 19:23:15.241: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Mar 23 19:23:15.248: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 23 19:23:15.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4050" for this suite. 03/23/23 19:23:15.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:15.264
Mar 23 19:23:15.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:23:15.265
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:15.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:15.29
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Mar 23 19:23:15.301: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337" in namespace "kubelet-test-1211" to be "running and ready"
Mar 23 19:23:15.314: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337": Phase="Pending", Reason="", readiness=false. Elapsed: 12.853071ms
Mar 23 19:23:15.314: INFO: The phase of Pod busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:23:17.318: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337": Phase="Running", Reason="", readiness=true. Elapsed: 2.016917609s
Mar 23 19:23:17.318: INFO: The phase of Pod busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337 is Running (Ready = true)
Mar 23 19:23:17.318: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 23 19:23:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1211" for this suite. 03/23/23 19:23:17.334
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":167,"skipped":2877,"failed":0}
------------------------------
• [2.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:15.264
    Mar 23 19:23:15.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:23:15.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:15.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:15.29
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Mar 23 19:23:15.301: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337" in namespace "kubelet-test-1211" to be "running and ready"
    Mar 23 19:23:15.314: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337": Phase="Pending", Reason="", readiness=false. Elapsed: 12.853071ms
    Mar 23 19:23:15.314: INFO: The phase of Pod busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:23:17.318: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337": Phase="Running", Reason="", readiness=true. Elapsed: 2.016917609s
    Mar 23 19:23:17.318: INFO: The phase of Pod busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337 is Running (Ready = true)
    Mar 23 19:23:17.318: INFO: Pod "busybox-readonly-fsd0c68e5e-561d-4357-b39e-af6cd779c337" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 23 19:23:17.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-1211" for this suite. 03/23/23 19:23:17.334
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:17.353
Mar 23 19:23:17.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 19:23:17.355
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:17.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:17.37
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 03/23/23 19:23:17.373
STEP: Verify that the required pods have come up 03/23/23 19:23:17.38
Mar 23 19:23:17.385: INFO: Pod name sample-pod: Found 0 pods out of 3
Mar 23 19:23:22.399: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 03/23/23 19:23:22.399
Mar 23 19:23:22.417: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 03/23/23 19:23:22.417
STEP: DeleteCollection of the ReplicaSets 03/23/23 19:23:22.423
STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/23/23 19:23:22.453
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 19:23:22.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8051" for this suite. 03/23/23 19:23:22.508
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":168,"skipped":2907,"failed":0}
------------------------------
• [SLOW TEST] [5.211 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:17.353
    Mar 23 19:23:17.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 19:23:17.355
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:17.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:17.37
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 03/23/23 19:23:17.373
    STEP: Verify that the required pods have come up 03/23/23 19:23:17.38
    Mar 23 19:23:17.385: INFO: Pod name sample-pod: Found 0 pods out of 3
    Mar 23 19:23:22.399: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 03/23/23 19:23:22.399
    Mar 23 19:23:22.417: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 03/23/23 19:23:22.417
    STEP: DeleteCollection of the ReplicaSets 03/23/23 19:23:22.423
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 03/23/23 19:23:22.453
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 19:23:22.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8051" for this suite. 03/23/23 19:23:22.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:22.565
Mar 23 19:23:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:23:22.566
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:22.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:22.677
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 03/23/23 19:23:22.683
Mar 23 19:23:22.708: INFO: Waiting up to 5m0s for pod "pod-8ssv9" in namespace "pods-2058" to be "running"
Mar 23 19:23:22.737: INFO: Pod "pod-8ssv9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.983035ms
Mar 23 19:23:24.740: INFO: Pod "pod-8ssv9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032259076s
Mar 23 19:23:26.752: INFO: Pod "pod-8ssv9": Phase="Running", Reason="", readiness=true. Elapsed: 4.044798397s
Mar 23 19:23:26.753: INFO: Pod "pod-8ssv9" satisfied condition "running"
STEP: patching /status 03/23/23 19:23:26.753
Mar 23 19:23:26.769: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:23:26.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2058" for this suite. 03/23/23 19:23:26.775
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":169,"skipped":2914,"failed":0}
------------------------------
• [4.217 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:22.565
    Mar 23 19:23:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:23:22.566
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:22.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:22.677
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 03/23/23 19:23:22.683
    Mar 23 19:23:22.708: INFO: Waiting up to 5m0s for pod "pod-8ssv9" in namespace "pods-2058" to be "running"
    Mar 23 19:23:22.737: INFO: Pod "pod-8ssv9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.983035ms
    Mar 23 19:23:24.740: INFO: Pod "pod-8ssv9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032259076s
    Mar 23 19:23:26.752: INFO: Pod "pod-8ssv9": Phase="Running", Reason="", readiness=true. Elapsed: 4.044798397s
    Mar 23 19:23:26.753: INFO: Pod "pod-8ssv9" satisfied condition "running"
    STEP: patching /status 03/23/23 19:23:26.753
    Mar 23 19:23:26.769: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:23:26.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2058" for this suite. 03/23/23 19:23:26.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:23:26.792
Mar 23 19:23:26.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:23:26.793
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:26.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:26.816
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 03/23/23 19:23:26.833
STEP: delete the rc 03/23/23 19:23:31.853
STEP: wait for the rc to be deleted 03/23/23 19:23:31.859
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/23/23 19:23:36.867
STEP: Gathering metrics 03/23/23 19:24:06.879
Mar 23 19:24:06.934: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 19:24:06.938: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.765491ms
Mar 23 19:24:06.938: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 19:24:06.938: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 19:25:07.141: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 23 19:25:07.141: INFO: Deleting pod "simpletest.rc-28bkb" in namespace "gc-568"
Mar 23 19:25:07.163: INFO: Deleting pod "simpletest.rc-4v5s4" in namespace "gc-568"
Mar 23 19:25:07.176: INFO: Deleting pod "simpletest.rc-58rh2" in namespace "gc-568"
Mar 23 19:25:07.193: INFO: Deleting pod "simpletest.rc-64l6t" in namespace "gc-568"
Mar 23 19:25:07.203: INFO: Deleting pod "simpletest.rc-6vnkn" in namespace "gc-568"
Mar 23 19:25:07.225: INFO: Deleting pod "simpletest.rc-74fdw" in namespace "gc-568"
Mar 23 19:25:07.234: INFO: Deleting pod "simpletest.rc-7hzl8" in namespace "gc-568"
Mar 23 19:25:07.251: INFO: Deleting pod "simpletest.rc-8blsn" in namespace "gc-568"
Mar 23 19:25:07.288: INFO: Deleting pod "simpletest.rc-9hp67" in namespace "gc-568"
Mar 23 19:25:07.354: INFO: Deleting pod "simpletest.rc-b6bcl" in namespace "gc-568"
Mar 23 19:25:07.400: INFO: Deleting pod "simpletest.rc-bd9pz" in namespace "gc-568"
Mar 23 19:25:07.432: INFO: Deleting pod "simpletest.rc-bxx6d" in namespace "gc-568"
Mar 23 19:25:07.457: INFO: Deleting pod "simpletest.rc-cbg9n" in namespace "gc-568"
Mar 23 19:25:07.482: INFO: Deleting pod "simpletest.rc-dwrnl" in namespace "gc-568"
Mar 23 19:25:07.519: INFO: Deleting pod "simpletest.rc-fbftz" in namespace "gc-568"
Mar 23 19:25:07.543: INFO: Deleting pod "simpletest.rc-fn4z4" in namespace "gc-568"
Mar 23 19:25:07.568: INFO: Deleting pod "simpletest.rc-fnhzq" in namespace "gc-568"
Mar 23 19:25:07.592: INFO: Deleting pod "simpletest.rc-gzk8c" in namespace "gc-568"
Mar 23 19:25:07.632: INFO: Deleting pod "simpletest.rc-hg8ds" in namespace "gc-568"
Mar 23 19:25:07.677: INFO: Deleting pod "simpletest.rc-jktwg" in namespace "gc-568"
Mar 23 19:25:07.691: INFO: Deleting pod "simpletest.rc-k4tkw" in namespace "gc-568"
Mar 23 19:25:07.708: INFO: Deleting pod "simpletest.rc-kjzs7" in namespace "gc-568"
Mar 23 19:25:07.730: INFO: Deleting pod "simpletest.rc-knr28" in namespace "gc-568"
Mar 23 19:25:07.747: INFO: Deleting pod "simpletest.rc-l2vw9" in namespace "gc-568"
Mar 23 19:25:07.760: INFO: Deleting pod "simpletest.rc-l8qs8" in namespace "gc-568"
Mar 23 19:25:07.787: INFO: Deleting pod "simpletest.rc-lb722" in namespace "gc-568"
Mar 23 19:25:07.801: INFO: Deleting pod "simpletest.rc-lrdfm" in namespace "gc-568"
Mar 23 19:25:07.820: INFO: Deleting pod "simpletest.rc-ndd8k" in namespace "gc-568"
Mar 23 19:25:07.844: INFO: Deleting pod "simpletest.rc-p6js7" in namespace "gc-568"
Mar 23 19:25:07.865: INFO: Deleting pod "simpletest.rc-p75sp" in namespace "gc-568"
Mar 23 19:25:07.887: INFO: Deleting pod "simpletest.rc-pzjbc" in namespace "gc-568"
Mar 23 19:25:07.916: INFO: Deleting pod "simpletest.rc-qqj8b" in namespace "gc-568"
Mar 23 19:25:07.936: INFO: Deleting pod "simpletest.rc-rt528" in namespace "gc-568"
Mar 23 19:25:07.958: INFO: Deleting pod "simpletest.rc-sp9h8" in namespace "gc-568"
Mar 23 19:25:07.980: INFO: Deleting pod "simpletest.rc-spphp" in namespace "gc-568"
Mar 23 19:25:08.009: INFO: Deleting pod "simpletest.rc-tj66s" in namespace "gc-568"
Mar 23 19:25:08.030: INFO: Deleting pod "simpletest.rc-tnnhj" in namespace "gc-568"
Mar 23 19:25:08.056: INFO: Deleting pod "simpletest.rc-tvf7n" in namespace "gc-568"
Mar 23 19:25:08.084: INFO: Deleting pod "simpletest.rc-vhhwp" in namespace "gc-568"
Mar 23 19:25:08.110: INFO: Deleting pod "simpletest.rc-vxrq8" in namespace "gc-568"
Mar 23 19:25:08.130: INFO: Deleting pod "simpletest.rc-w29s9" in namespace "gc-568"
Mar 23 19:25:08.157: INFO: Deleting pod "simpletest.rc-wb2dm" in namespace "gc-568"
Mar 23 19:25:08.189: INFO: Deleting pod "simpletest.rc-whtkv" in namespace "gc-568"
Mar 23 19:25:08.212: INFO: Deleting pod "simpletest.rc-wjrrw" in namespace "gc-568"
Mar 23 19:25:08.233: INFO: Deleting pod "simpletest.rc-wkp42" in namespace "gc-568"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:25:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-568" for this suite. 03/23/23 19:25:08.264
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":170,"skipped":2938,"failed":0}
------------------------------
• [SLOW TEST] [101.479 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:23:26.792
    Mar 23 19:23:26.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:23:26.793
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:23:26.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:23:26.816
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 03/23/23 19:23:26.833
    STEP: delete the rc 03/23/23 19:23:31.853
    STEP: wait for the rc to be deleted 03/23/23 19:23:31.859
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 03/23/23 19:23:36.867
    STEP: Gathering metrics 03/23/23 19:24:06.879
    Mar 23 19:24:06.934: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 19:24:06.938: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.765491ms
    Mar 23 19:24:06.938: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 19:24:06.938: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 19:25:07.141: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    Mar 23 19:25:07.141: INFO: Deleting pod "simpletest.rc-28bkb" in namespace "gc-568"
    Mar 23 19:25:07.163: INFO: Deleting pod "simpletest.rc-4v5s4" in namespace "gc-568"
    Mar 23 19:25:07.176: INFO: Deleting pod "simpletest.rc-58rh2" in namespace "gc-568"
    Mar 23 19:25:07.193: INFO: Deleting pod "simpletest.rc-64l6t" in namespace "gc-568"
    Mar 23 19:25:07.203: INFO: Deleting pod "simpletest.rc-6vnkn" in namespace "gc-568"
    Mar 23 19:25:07.225: INFO: Deleting pod "simpletest.rc-74fdw" in namespace "gc-568"
    Mar 23 19:25:07.234: INFO: Deleting pod "simpletest.rc-7hzl8" in namespace "gc-568"
    Mar 23 19:25:07.251: INFO: Deleting pod "simpletest.rc-8blsn" in namespace "gc-568"
    Mar 23 19:25:07.288: INFO: Deleting pod "simpletest.rc-9hp67" in namespace "gc-568"
    Mar 23 19:25:07.354: INFO: Deleting pod "simpletest.rc-b6bcl" in namespace "gc-568"
    Mar 23 19:25:07.400: INFO: Deleting pod "simpletest.rc-bd9pz" in namespace "gc-568"
    Mar 23 19:25:07.432: INFO: Deleting pod "simpletest.rc-bxx6d" in namespace "gc-568"
    Mar 23 19:25:07.457: INFO: Deleting pod "simpletest.rc-cbg9n" in namespace "gc-568"
    Mar 23 19:25:07.482: INFO: Deleting pod "simpletest.rc-dwrnl" in namespace "gc-568"
    Mar 23 19:25:07.519: INFO: Deleting pod "simpletest.rc-fbftz" in namespace "gc-568"
    Mar 23 19:25:07.543: INFO: Deleting pod "simpletest.rc-fn4z4" in namespace "gc-568"
    Mar 23 19:25:07.568: INFO: Deleting pod "simpletest.rc-fnhzq" in namespace "gc-568"
    Mar 23 19:25:07.592: INFO: Deleting pod "simpletest.rc-gzk8c" in namespace "gc-568"
    Mar 23 19:25:07.632: INFO: Deleting pod "simpletest.rc-hg8ds" in namespace "gc-568"
    Mar 23 19:25:07.677: INFO: Deleting pod "simpletest.rc-jktwg" in namespace "gc-568"
    Mar 23 19:25:07.691: INFO: Deleting pod "simpletest.rc-k4tkw" in namespace "gc-568"
    Mar 23 19:25:07.708: INFO: Deleting pod "simpletest.rc-kjzs7" in namespace "gc-568"
    Mar 23 19:25:07.730: INFO: Deleting pod "simpletest.rc-knr28" in namespace "gc-568"
    Mar 23 19:25:07.747: INFO: Deleting pod "simpletest.rc-l2vw9" in namespace "gc-568"
    Mar 23 19:25:07.760: INFO: Deleting pod "simpletest.rc-l8qs8" in namespace "gc-568"
    Mar 23 19:25:07.787: INFO: Deleting pod "simpletest.rc-lb722" in namespace "gc-568"
    Mar 23 19:25:07.801: INFO: Deleting pod "simpletest.rc-lrdfm" in namespace "gc-568"
    Mar 23 19:25:07.820: INFO: Deleting pod "simpletest.rc-ndd8k" in namespace "gc-568"
    Mar 23 19:25:07.844: INFO: Deleting pod "simpletest.rc-p6js7" in namespace "gc-568"
    Mar 23 19:25:07.865: INFO: Deleting pod "simpletest.rc-p75sp" in namespace "gc-568"
    Mar 23 19:25:07.887: INFO: Deleting pod "simpletest.rc-pzjbc" in namespace "gc-568"
    Mar 23 19:25:07.916: INFO: Deleting pod "simpletest.rc-qqj8b" in namespace "gc-568"
    Mar 23 19:25:07.936: INFO: Deleting pod "simpletest.rc-rt528" in namespace "gc-568"
    Mar 23 19:25:07.958: INFO: Deleting pod "simpletest.rc-sp9h8" in namespace "gc-568"
    Mar 23 19:25:07.980: INFO: Deleting pod "simpletest.rc-spphp" in namespace "gc-568"
    Mar 23 19:25:08.009: INFO: Deleting pod "simpletest.rc-tj66s" in namespace "gc-568"
    Mar 23 19:25:08.030: INFO: Deleting pod "simpletest.rc-tnnhj" in namespace "gc-568"
    Mar 23 19:25:08.056: INFO: Deleting pod "simpletest.rc-tvf7n" in namespace "gc-568"
    Mar 23 19:25:08.084: INFO: Deleting pod "simpletest.rc-vhhwp" in namespace "gc-568"
    Mar 23 19:25:08.110: INFO: Deleting pod "simpletest.rc-vxrq8" in namespace "gc-568"
    Mar 23 19:25:08.130: INFO: Deleting pod "simpletest.rc-w29s9" in namespace "gc-568"
    Mar 23 19:25:08.157: INFO: Deleting pod "simpletest.rc-wb2dm" in namespace "gc-568"
    Mar 23 19:25:08.189: INFO: Deleting pod "simpletest.rc-whtkv" in namespace "gc-568"
    Mar 23 19:25:08.212: INFO: Deleting pod "simpletest.rc-wjrrw" in namespace "gc-568"
    Mar 23 19:25:08.233: INFO: Deleting pod "simpletest.rc-wkp42" in namespace "gc-568"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:25:08.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-568" for this suite. 03/23/23 19:25:08.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:25:08.274
Mar 23 19:25:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename cronjob 03/23/23 19:25:08.278
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:25:08.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:25:08.302
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 03/23/23 19:25:08.308
STEP: Ensuring a job is scheduled 03/23/23 19:25:08.316
STEP: Ensuring exactly one is scheduled 03/23/23 19:26:00.32
STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/23/23 19:26:00.323
STEP: Ensuring no more jobs are scheduled 03/23/23 19:26:00.326
STEP: Removing cronjob 03/23/23 19:31:00.334
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 23 19:31:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1346" for this suite. 03/23/23 19:31:00.364
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":171,"skipped":2959,"failed":0}
------------------------------
• [SLOW TEST] [352.101 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:25:08.274
    Mar 23 19:25:08.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename cronjob 03/23/23 19:25:08.278
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:25:08.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:25:08.302
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 03/23/23 19:25:08.308
    STEP: Ensuring a job is scheduled 03/23/23 19:25:08.316
    STEP: Ensuring exactly one is scheduled 03/23/23 19:26:00.32
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 03/23/23 19:26:00.323
    STEP: Ensuring no more jobs are scheduled 03/23/23 19:26:00.326
    STEP: Removing cronjob 03/23/23 19:31:00.334
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 23 19:31:00.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1346" for this suite. 03/23/23 19:31:00.364
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:00.376
Mar 23 19:31:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:31:00.38
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:00.412
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:00.415
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:31:00.463
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:31:01.694
STEP: Deploying the webhook pod 03/23/23 19:31:01.702
STEP: Wait for the deployment to be ready 03/23/23 19:31:01.717
Mar 23 19:31:01.733: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:31:03.754
STEP: Verifying the service has paired with the endpoint 03/23/23 19:31:03.789
Mar 23 19:31:04.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Mar 23 19:31:04.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/23/23 19:31:05.311
STEP: Creating a custom resource that should be denied by the webhook 03/23/23 19:31:05.33
STEP: Creating a custom resource whose deletion would be denied by the webhook 03/23/23 19:31:07.473
STEP: Updating the custom resource with disallowed data should be denied 03/23/23 19:31:07.489
STEP: Deleting the custom resource should be denied 03/23/23 19:31:07.497
STEP: Remove the offending key and value from the custom resource data 03/23/23 19:31:07.503
STEP: Deleting the updated custom resource should be successful 03/23/23 19:31:07.513
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:31:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1908" for this suite. 03/23/23 19:31:08.072
STEP: Destroying namespace "webhook-1908-markers" for this suite. 03/23/23 19:31:08.08
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":172,"skipped":2963,"failed":0}
------------------------------
• [SLOW TEST] [7.840 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:00.376
    Mar 23 19:31:00.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:31:00.38
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:00.412
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:00.415
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:31:00.463
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:31:01.694
    STEP: Deploying the webhook pod 03/23/23 19:31:01.702
    STEP: Wait for the deployment to be ready 03/23/23 19:31:01.717
    Mar 23 19:31:01.733: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:31:03.754
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:31:03.789
    Mar 23 19:31:04.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Mar 23 19:31:04.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 03/23/23 19:31:05.311
    STEP: Creating a custom resource that should be denied by the webhook 03/23/23 19:31:05.33
    STEP: Creating a custom resource whose deletion would be denied by the webhook 03/23/23 19:31:07.473
    STEP: Updating the custom resource with disallowed data should be denied 03/23/23 19:31:07.489
    STEP: Deleting the custom resource should be denied 03/23/23 19:31:07.497
    STEP: Remove the offending key and value from the custom resource data 03/23/23 19:31:07.503
    STEP: Deleting the updated custom resource should be successful 03/23/23 19:31:07.513
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:31:08.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1908" for this suite. 03/23/23 19:31:08.072
    STEP: Destroying namespace "webhook-1908-markers" for this suite. 03/23/23 19:31:08.08
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:08.251
Mar 23 19:31:08.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:31:08.252
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:08.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:08.324
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 03/23/23 19:31:08.328
Mar 23 19:31:08.329: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7572 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 03/23/23 19:31:08.383
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:31:08.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7572" for this suite. 03/23/23 19:31:08.411
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":173,"skipped":3021,"failed":0}
------------------------------
• [0.165 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:08.251
    Mar 23 19:31:08.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:31:08.252
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:08.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:08.324
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 03/23/23 19:31:08.328
    Mar 23 19:31:08.329: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-7572 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 03/23/23 19:31:08.383
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:31:08.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7572" for this suite. 03/23/23 19:31:08.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:08.418
Mar 23 19:31:08.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:31:08.419
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:08.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:08.441
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:31:08.449
Mar 23 19:31:08.471: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6" in namespace "downward-api-4209" to be "Succeeded or Failed"
Mar 23 19:31:08.476: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.072689ms
Mar 23 19:31:10.481: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009682402s
Mar 23 19:31:12.482: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010636223s
Mar 23 19:31:14.483: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Running", Reason="", readiness=true. Elapsed: 6.011899843s
Mar 23 19:31:16.480: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Running", Reason="", readiness=false. Elapsed: 8.009219091s
Mar 23 19:31:18.483: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012223727s
STEP: Saw pod success 03/23/23 19:31:18.484
Mar 23 19:31:18.484: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6" satisfied condition "Succeeded or Failed"
Mar 23 19:31:18.487: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 container client-container: <nil>
STEP: delete the pod 03/23/23 19:31:18.535
Mar 23 19:31:18.551: INFO: Waiting for pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 to disappear
Mar 23 19:31:18.555: INFO: Pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 19:31:18.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4209" for this suite. 03/23/23 19:31:18.563
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":174,"skipped":3071,"failed":0}
------------------------------
• [SLOW TEST] [10.155 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:08.418
    Mar 23 19:31:08.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:31:08.419
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:08.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:08.441
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:31:08.449
    Mar 23 19:31:08.471: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6" in namespace "downward-api-4209" to be "Succeeded or Failed"
    Mar 23 19:31:08.476: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.072689ms
    Mar 23 19:31:10.481: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009682402s
    Mar 23 19:31:12.482: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010636223s
    Mar 23 19:31:14.483: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Running", Reason="", readiness=true. Elapsed: 6.011899843s
    Mar 23 19:31:16.480: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Running", Reason="", readiness=false. Elapsed: 8.009219091s
    Mar 23 19:31:18.483: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.012223727s
    STEP: Saw pod success 03/23/23 19:31:18.484
    Mar 23 19:31:18.484: INFO: Pod "downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6" satisfied condition "Succeeded or Failed"
    Mar 23 19:31:18.487: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 container client-container: <nil>
    STEP: delete the pod 03/23/23 19:31:18.535
    Mar 23 19:31:18.551: INFO: Waiting for pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 to disappear
    Mar 23 19:31:18.555: INFO: Pod downwardapi-volume-0cb2b699-ad82-49b4-b696-f9687deb97d6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 19:31:18.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4209" for this suite. 03/23/23 19:31:18.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:18.578
Mar 23 19:31:18.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:31:18.58
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:18.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:18.609
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 03/23/23 19:31:18.617
STEP: submitting the pod to kubernetes 03/23/23 19:31:18.617
Mar 23 19:31:18.628: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" in namespace "pods-7939" to be "running and ready"
Mar 23 19:31:18.639: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.553576ms
Mar 23 19:31:18.640: INFO: The phase of Pod pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:31:20.644: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.016117006s
Mar 23 19:31:20.645: INFO: The phase of Pod pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab is Running (Ready = true)
Mar 23 19:31:20.645: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 03/23/23 19:31:20.648
STEP: updating the pod 03/23/23 19:31:20.653
Mar 23 19:31:21.175: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab"
Mar 23 19:31:21.175: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" in namespace "pods-7939" to be "terminated with reason DeadlineExceeded"
Mar 23 19:31:21.178: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.955793ms
Mar 23 19:31:23.184: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.008146224s
Mar 23 19:31:25.184: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=false. Elapsed: 4.008095867s
Mar 23 19:31:27.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=false. Elapsed: 6.007282811s
Mar 23 19:31:29.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 8.007684553s
Mar 23 19:31:29.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:31:29.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7939" for this suite. 03/23/23 19:31:29.19
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":175,"skipped":3085,"failed":0}
------------------------------
• [SLOW TEST] [10.625 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:18.578
    Mar 23 19:31:18.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:31:18.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:18.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:18.609
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 03/23/23 19:31:18.617
    STEP: submitting the pod to kubernetes 03/23/23 19:31:18.617
    Mar 23 19:31:18.628: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" in namespace "pods-7939" to be "running and ready"
    Mar 23 19:31:18.639: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.553576ms
    Mar 23 19:31:18.640: INFO: The phase of Pod pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:31:20.644: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.016117006s
    Mar 23 19:31:20.645: INFO: The phase of Pod pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab is Running (Ready = true)
    Mar 23 19:31:20.645: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 03/23/23 19:31:20.648
    STEP: updating the pod 03/23/23 19:31:20.653
    Mar 23 19:31:21.175: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab"
    Mar 23 19:31:21.175: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" in namespace "pods-7939" to be "terminated with reason DeadlineExceeded"
    Mar 23 19:31:21.178: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.955793ms
    Mar 23 19:31:23.184: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=true. Elapsed: 2.008146224s
    Mar 23 19:31:25.184: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=false. Elapsed: 4.008095867s
    Mar 23 19:31:27.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Running", Reason="", readiness=false. Elapsed: 6.007282811s
    Mar 23 19:31:29.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 8.007684553s
    Mar 23 19:31:29.183: INFO: Pod "pod-update-activedeadlineseconds-ea97798e-365d-4b88-9ac8-05aa94a91aab" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:31:29.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7939" for this suite. 03/23/23 19:31:29.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:29.208
Mar 23 19:31:29.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename watch 03/23/23 19:31:29.21
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:29.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:29.23
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 03/23/23 19:31:29.238
STEP: creating a new configmap 03/23/23 19:31:29.24
STEP: modifying the configmap once 03/23/23 19:31:29.246
STEP: changing the label value of the configmap 03/23/23 19:31:29.261
STEP: Expecting to observe a delete notification for the watched object 03/23/23 19:31:29.283
Mar 23 19:31:29.283: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24112 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:31:29.284: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24113 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:31:29.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24115 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 03/23/23 19:31:29.284
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/23/23 19:31:29.295
STEP: changing the label value of the configmap back 03/23/23 19:31:39.296
STEP: modifying the configmap a third time 03/23/23 19:31:39.306
STEP: deleting the configmap 03/23/23 19:31:39.313
STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/23/23 19:31:39.317
Mar 23 19:31:39.318: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24166 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:31:39.318: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24167 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:31:39.318: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24168 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 23 19:31:39.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1242" for this suite. 03/23/23 19:31:39.323
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":176,"skipped":3093,"failed":0}
------------------------------
• [SLOW TEST] [10.121 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:29.208
    Mar 23 19:31:29.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename watch 03/23/23 19:31:29.21
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:29.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:29.23
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 03/23/23 19:31:29.238
    STEP: creating a new configmap 03/23/23 19:31:29.24
    STEP: modifying the configmap once 03/23/23 19:31:29.246
    STEP: changing the label value of the configmap 03/23/23 19:31:29.261
    STEP: Expecting to observe a delete notification for the watched object 03/23/23 19:31:29.283
    Mar 23 19:31:29.283: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24112 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:31:29.284: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24113 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:31:29.284: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24115 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 03/23/23 19:31:29.284
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 03/23/23 19:31:29.295
    STEP: changing the label value of the configmap back 03/23/23 19:31:39.296
    STEP: modifying the configmap a third time 03/23/23 19:31:39.306
    STEP: deleting the configmap 03/23/23 19:31:39.313
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 03/23/23 19:31:39.317
    Mar 23 19:31:39.318: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24166 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:31:39.318: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24167 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:31:39.318: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1242  71bf6c7d-a25f-475d-b9ec-2cc29b417fe0 24168 0 2023-03-23 19:31:29 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-03-23 19:31:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 23 19:31:39.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1242" for this suite. 03/23/23 19:31:39.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:39.342
Mar 23 19:31:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename lease-test 03/23/23 19:31:39.343
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:39.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:39.37
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Mar 23 19:31:39.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-5276" for this suite. 03/23/23 19:31:39.446
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":177,"skipped":3128,"failed":0}
------------------------------
• [0.112 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:39.342
    Mar 23 19:31:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename lease-test 03/23/23 19:31:39.343
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:39.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:39.37
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Mar 23 19:31:39.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-5276" for this suite. 03/23/23 19:31:39.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:39.454
Mar 23 19:31:39.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 19:31:39.455
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:39.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:39.487
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 03/23/23 19:31:39.492
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 03/23/23 19:31:39.514
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 03/23/23 19:31:39.514
STEP: creating a pod to probe DNS 03/23/23 19:31:39.514
STEP: submitting the pod to kubernetes 03/23/23 19:31:39.515
Mar 23 19:31:39.574: INFO: Waiting up to 15m0s for pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1" in namespace "dns-1054" to be "running"
Mar 23 19:31:39.593: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.04396ms
Mar 23 19:31:41.597: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022284311s
Mar 23 19:31:43.598: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Running", Reason="", readiness=true. Elapsed: 4.023975467s
Mar 23 19:31:43.598: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1" satisfied condition "running"
STEP: retrieving the pod 03/23/23 19:31:43.599
STEP: looking for the results for each expected name from probers 03/23/23 19:31:43.602
Mar 23 19:31:43.618: INFO: DNS probes using dns-1054/dns-test-79312444-8433-4775-a69b-3f9c780530d1 succeeded

STEP: deleting the pod 03/23/23 19:31:43.618
STEP: deleting the test headless service 03/23/23 19:31:43.639
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 19:31:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1054" for this suite. 03/23/23 19:31:43.697
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":178,"skipped":3136,"failed":0}
------------------------------
• [4.250 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:39.454
    Mar 23 19:31:39.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 19:31:39.455
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:39.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:39.487
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 03/23/23 19:31:39.492
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     03/23/23 19:31:39.514
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1054.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     03/23/23 19:31:39.514
    STEP: creating a pod to probe DNS 03/23/23 19:31:39.514
    STEP: submitting the pod to kubernetes 03/23/23 19:31:39.515
    Mar 23 19:31:39.574: INFO: Waiting up to 15m0s for pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1" in namespace "dns-1054" to be "running"
    Mar 23 19:31:39.593: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.04396ms
    Mar 23 19:31:41.597: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022284311s
    Mar 23 19:31:43.598: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1": Phase="Running", Reason="", readiness=true. Elapsed: 4.023975467s
    Mar 23 19:31:43.598: INFO: Pod "dns-test-79312444-8433-4775-a69b-3f9c780530d1" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 19:31:43.599
    STEP: looking for the results for each expected name from probers 03/23/23 19:31:43.602
    Mar 23 19:31:43.618: INFO: DNS probes using dns-1054/dns-test-79312444-8433-4775-a69b-3f9c780530d1 succeeded

    STEP: deleting the pod 03/23/23 19:31:43.618
    STEP: deleting the test headless service 03/23/23 19:31:43.639
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 19:31:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1054" for this suite. 03/23/23 19:31:43.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:43.714
Mar 23 19:31:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename init-container 03/23/23 19:31:43.723
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:43.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:43.747
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 03/23/23 19:31:43.75
Mar 23 19:31:43.750: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 19:31:51.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5309" for this suite. 03/23/23 19:31:51.789
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":179,"skipped":3147,"failed":0}
------------------------------
• [SLOW TEST] [8.081 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:43.714
    Mar 23 19:31:43.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename init-container 03/23/23 19:31:43.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:43.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:43.747
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 03/23/23 19:31:43.75
    Mar 23 19:31:43.750: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 19:31:51.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5309" for this suite. 03/23/23 19:31:51.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:51.801
Mar 23 19:31:51.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-webhook 03/23/23 19:31:51.803
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:51.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:51.821
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/23/23 19:31:51.826
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/23/23 19:31:52.109
STEP: Deploying the custom resource conversion webhook pod 03/23/23 19:31:52.116
STEP: Wait for the deployment to be ready 03/23/23 19:31:52.13
Mar 23 19:31:52.171: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:31:54.187
STEP: Verifying the service has paired with the endpoint 03/23/23 19:31:54.219
Mar 23 19:31:55.219: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Mar 23 19:31:55.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Creating a v1 custom resource 03/23/23 19:31:57.954
STEP: Create a v2 custom resource 03/23/23 19:31:57.971
STEP: List CRs in v1 03/23/23 19:31:58.106
STEP: List CRs in v2 03/23/23 19:31:58.116
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:31:58.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7660" for this suite. 03/23/23 19:31:58.652
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":180,"skipped":3194,"failed":0}
------------------------------
• [SLOW TEST] [6.988 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:51.801
    Mar 23 19:31:51.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-webhook 03/23/23 19:31:51.803
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:51.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:51.821
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/23/23 19:31:51.826
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/23/23 19:31:52.109
    STEP: Deploying the custom resource conversion webhook pod 03/23/23 19:31:52.116
    STEP: Wait for the deployment to be ready 03/23/23 19:31:52.13
    Mar 23 19:31:52.171: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:31:54.187
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:31:54.219
    Mar 23 19:31:55.219: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Mar 23 19:31:55.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Creating a v1 custom resource 03/23/23 19:31:57.954
    STEP: Create a v2 custom resource 03/23/23 19:31:57.971
    STEP: List CRs in v1 03/23/23 19:31:58.106
    STEP: List CRs in v2 03/23/23 19:31:58.116
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:31:58.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-7660" for this suite. 03/23/23 19:31:58.652
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:31:58.883
Mar 23 19:31:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replication-controller 03/23/23 19:31:58.888
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:58.969
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:58.977
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Mar 23 19:31:58.981: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/23/23 19:31:59.998
STEP: Checking rc "condition-test" has the desired failure condition set 03/23/23 19:32:00.005
STEP: Scaling down rc "condition-test" to satisfy pod quota 03/23/23 19:32:01.021
Mar 23 19:32:01.031: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 03/23/23 19:32:01.031
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 23 19:32:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9402" for this suite. 03/23/23 19:32:02.052
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":181,"skipped":3252,"failed":0}
------------------------------
• [3.179 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:31:58.883
    Mar 23 19:31:58.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replication-controller 03/23/23 19:31:58.888
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:31:58.969
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:31:58.977
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Mar 23 19:31:58.981: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 03/23/23 19:31:59.998
    STEP: Checking rc "condition-test" has the desired failure condition set 03/23/23 19:32:00.005
    STEP: Scaling down rc "condition-test" to satisfy pod quota 03/23/23 19:32:01.021
    Mar 23 19:32:01.031: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 03/23/23 19:32:01.031
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 23 19:32:02.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9402" for this suite. 03/23/23 19:32:02.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:32:02.071
Mar 23 19:32:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:32:02.072
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:02.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:02.097
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-1eb57815-b55a-4f5e-9170-72d1f1660b86 03/23/23 19:32:02.101
STEP: Creating a pod to test consume configMaps 03/23/23 19:32:02.11
Mar 23 19:32:02.127: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90" in namespace "projected-1819" to be "Succeeded or Failed"
Mar 23 19:32:02.150: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 22.27255ms
Mar 23 19:32:04.180: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052346562s
Mar 23 19:32:06.157: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02961929s
Mar 23 19:32:08.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Running", Reason="", readiness=true. Elapsed: 6.027373074s
Mar 23 19:32:10.156: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Running", Reason="", readiness=false. Elapsed: 8.02850155s
Mar 23 19:32:12.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027483141s
STEP: Saw pod success 03/23/23 19:32:12.155
Mar 23 19:32:12.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90" satisfied condition "Succeeded or Failed"
Mar 23 19:32:12.158: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:32:12.195
Mar 23 19:32:12.207: INFO: Waiting for pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 to disappear
Mar 23 19:32:12.209: INFO: Pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 19:32:12.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1819" for this suite. 03/23/23 19:32:12.214
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":182,"skipped":3283,"failed":0}
------------------------------
• [SLOW TEST] [10.153 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:32:02.071
    Mar 23 19:32:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:32:02.072
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:02.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:02.097
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-1eb57815-b55a-4f5e-9170-72d1f1660b86 03/23/23 19:32:02.101
    STEP: Creating a pod to test consume configMaps 03/23/23 19:32:02.11
    Mar 23 19:32:02.127: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90" in namespace "projected-1819" to be "Succeeded or Failed"
    Mar 23 19:32:02.150: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 22.27255ms
    Mar 23 19:32:04.180: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052346562s
    Mar 23 19:32:06.157: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02961929s
    Mar 23 19:32:08.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Running", Reason="", readiness=true. Elapsed: 6.027373074s
    Mar 23 19:32:10.156: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Running", Reason="", readiness=false. Elapsed: 8.02850155s
    Mar 23 19:32:12.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027483141s
    STEP: Saw pod success 03/23/23 19:32:12.155
    Mar 23 19:32:12.155: INFO: Pod "pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90" satisfied condition "Succeeded or Failed"
    Mar 23 19:32:12.158: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:32:12.195
    Mar 23 19:32:12.207: INFO: Waiting for pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 to disappear
    Mar 23 19:32:12.209: INFO: Pod pod-projected-configmaps-43396740-c8ee-45d3-aa7c-ca657bd52e90 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 19:32:12.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1819" for this suite. 03/23/23 19:32:12.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:32:12.23
Mar 23 19:32:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename aggregator 03/23/23 19:32:12.232
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:12.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:12.255
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Mar 23 19:32:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 03/23/23 19:32:12.261
Mar 23 19:32:12.887: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Mar 23 19:32:15.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:17.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:19.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:21.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:23.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:25.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:27.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:29.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:31.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:33.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:35.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:32:37.141: INFO: Waited 121.888469ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 03/23/23 19:32:37.189
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/23/23 19:32:37.192
STEP: List APIServices 03/23/23 19:32:37.204
Mar 23 19:32:37.212: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Mar 23 19:32:37.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5605" for this suite. 03/23/23 19:32:37.534
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":183,"skipped":3291,"failed":0}
------------------------------
• [SLOW TEST] [25.329 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:32:12.23
    Mar 23 19:32:12.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename aggregator 03/23/23 19:32:12.232
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:12.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:12.255
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Mar 23 19:32:12.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 03/23/23 19:32:12.261
    Mar 23 19:32:12.887: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Mar 23 19:32:15.007: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:17.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:19.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:21.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:23.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:25.013: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:27.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:29.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:31.011: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:33.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:35.012: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 32, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:32:37.141: INFO: Waited 121.888469ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 03/23/23 19:32:37.189
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 03/23/23 19:32:37.192
    STEP: List APIServices 03/23/23 19:32:37.204
    Mar 23 19:32:37.212: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Mar 23 19:32:37.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5605" for this suite. 03/23/23 19:32:37.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:32:37.563
Mar 23 19:32:37.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 19:32:37.565
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:37.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:37.631
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 03/23/23 19:32:37.636
Mar 23 19:32:37.662: INFO: Waiting up to 2m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609" to be "running"
Mar 23 19:32:37.677: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 14.891647ms
Mar 23 19:32:39.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019545556s
Mar 23 19:32:41.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019190483s
Mar 23 19:32:43.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018895738s
Mar 23 19:32:45.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018962339s
Mar 23 19:32:47.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018950241s
Mar 23 19:32:49.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019624541s
Mar 23 19:32:51.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019018906s
Mar 23 19:32:53.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019728292s
Mar 23 19:32:55.684: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021588075s
Mar 23 19:32:57.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019231169s
Mar 23 19:32:59.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01955157s
Mar 23 19:33:01.695: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 24.033268929s
Mar 23 19:33:03.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019994399s
Mar 23 19:33:05.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018792146s
Mar 23 19:33:07.686: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02398298s
Mar 23 19:33:09.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019992383s
Mar 23 19:33:11.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018929581s
Mar 23 19:33:13.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019806575s
Mar 23 19:33:15.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01852475s
Mar 23 19:33:17.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 40.020822872s
Mar 23 19:33:19.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019631203s
Mar 23 19:33:21.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020201102s
Mar 23 19:33:23.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01967696s
Mar 23 19:33:25.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 48.020210319s
Mar 23 19:33:27.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01967248s
Mar 23 19:33:29.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019658941s
Mar 23 19:33:31.685: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023022628s
Mar 23 19:33:33.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 56.019540679s
Mar 23 19:33:35.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020002322s
Mar 23 19:33:37.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020622964s
Mar 23 19:33:39.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019854047s
Mar 23 19:33:41.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02138412s
Mar 23 19:33:43.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.019546809s
Mar 23 19:33:45.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018686094s
Mar 23 19:33:47.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019908522s
Mar 23 19:33:49.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.019937223s
Mar 23 19:33:51.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021301022s
Mar 23 19:33:53.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019815127s
Mar 23 19:33:55.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019200051s
Mar 23 19:33:57.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019679308s
Mar 23 19:33:59.685: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023397759s
Mar 23 19:34:01.694: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031788901s
Mar 23 19:34:03.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018515275s
Mar 23 19:34:05.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018496348s
Mar 23 19:34:07.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.019046821s
Mar 23 19:34:09.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019823693s
Mar 23 19:34:11.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021240129s
Mar 23 19:34:13.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019448411s
Mar 23 19:34:15.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018817291s
Mar 23 19:34:17.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020204567s
Mar 23 19:34:19.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018777331s
Mar 23 19:34:21.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018762827s
Mar 23 19:34:23.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019418722s
Mar 23 19:34:25.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019529518s
Mar 23 19:34:27.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.020293301s
Mar 23 19:34:29.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019313413s
Mar 23 19:34:31.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019991799s
Mar 23 19:34:33.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019515425s
Mar 23 19:34:35.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019812381s
Mar 23 19:34:37.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019821617s
Mar 23 19:34:37.684: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022457911s
STEP: updating the pod 03/23/23 19:34:37.684
Mar 23 19:34:38.196: INFO: Successfully updated pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db"
STEP: waiting for pod running 03/23/23 19:34:38.196
Mar 23 19:34:38.196: INFO: Waiting up to 2m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609" to be "running"
Mar 23 19:34:38.201: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819289ms
Mar 23 19:34:40.206: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Running", Reason="", readiness=true. Elapsed: 2.009565814s
Mar 23 19:34:40.206: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" satisfied condition "running"
STEP: deleting the pod gracefully 03/23/23 19:34:40.206
Mar 23 19:34:40.206: INFO: Deleting pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609"
Mar 23 19:34:40.214: INFO: Wait up to 5m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 19:35:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4609" for this suite. 03/23/23 19:35:12.23
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":184,"skipped":3318,"failed":0}
------------------------------
• [SLOW TEST] [154.676 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:32:37.563
    Mar 23 19:32:37.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 19:32:37.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:32:37.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:32:37.631
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 03/23/23 19:32:37.636
    Mar 23 19:32:37.662: INFO: Waiting up to 2m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609" to be "running"
    Mar 23 19:32:37.677: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 14.891647ms
    Mar 23 19:32:39.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019545556s
    Mar 23 19:32:41.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019190483s
    Mar 23 19:32:43.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018895738s
    Mar 23 19:32:45.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018962339s
    Mar 23 19:32:47.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018950241s
    Mar 23 19:32:49.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 12.019624541s
    Mar 23 19:32:51.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 14.019018906s
    Mar 23 19:32:53.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 16.019728292s
    Mar 23 19:32:55.684: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 18.021588075s
    Mar 23 19:32:57.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019231169s
    Mar 23 19:32:59.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01955157s
    Mar 23 19:33:01.695: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 24.033268929s
    Mar 23 19:33:03.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019994399s
    Mar 23 19:33:05.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018792146s
    Mar 23 19:33:07.686: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02398298s
    Mar 23 19:33:09.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 32.019992383s
    Mar 23 19:33:11.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 34.018929581s
    Mar 23 19:33:13.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 36.019806575s
    Mar 23 19:33:15.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01852475s
    Mar 23 19:33:17.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 40.020822872s
    Mar 23 19:33:19.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 42.019631203s
    Mar 23 19:33:21.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 44.020201102s
    Mar 23 19:33:23.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 46.01967696s
    Mar 23 19:33:25.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 48.020210319s
    Mar 23 19:33:27.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 50.01967248s
    Mar 23 19:33:29.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019658941s
    Mar 23 19:33:31.685: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023022628s
    Mar 23 19:33:33.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 56.019540679s
    Mar 23 19:33:35.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 58.020002322s
    Mar 23 19:33:37.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.020622964s
    Mar 23 19:33:39.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.019854047s
    Mar 23 19:33:41.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.02138412s
    Mar 23 19:33:43.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.019546809s
    Mar 23 19:33:45.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.018686094s
    Mar 23 19:33:47.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.019908522s
    Mar 23 19:33:49.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.019937223s
    Mar 23 19:33:51.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.021301022s
    Mar 23 19:33:53.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.019815127s
    Mar 23 19:33:55.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.019200051s
    Mar 23 19:33:57.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.019679308s
    Mar 23 19:33:59.685: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.023397759s
    Mar 23 19:34:01.694: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031788901s
    Mar 23 19:34:03.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.018515275s
    Mar 23 19:34:05.680: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.018496348s
    Mar 23 19:34:07.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.019046821s
    Mar 23 19:34:09.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019823693s
    Mar 23 19:34:11.683: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021240129s
    Mar 23 19:34:13.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.019448411s
    Mar 23 19:34:15.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018817291s
    Mar 23 19:34:17.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.020204567s
    Mar 23 19:34:19.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.018777331s
    Mar 23 19:34:21.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.018762827s
    Mar 23 19:34:23.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.019418722s
    Mar 23 19:34:25.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.019529518s
    Mar 23 19:34:27.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.020293301s
    Mar 23 19:34:29.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.019313413s
    Mar 23 19:34:31.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.019991799s
    Mar 23 19:34:33.681: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019515425s
    Mar 23 19:34:35.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.019812381s
    Mar 23 19:34:37.682: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.019821617s
    Mar 23 19:34:37.684: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022457911s
    STEP: updating the pod 03/23/23 19:34:37.684
    Mar 23 19:34:38.196: INFO: Successfully updated pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db"
    STEP: waiting for pod running 03/23/23 19:34:38.196
    Mar 23 19:34:38.196: INFO: Waiting up to 2m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609" to be "running"
    Mar 23 19:34:38.201: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.819289ms
    Mar 23 19:34:40.206: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db": Phase="Running", Reason="", readiness=true. Elapsed: 2.009565814s
    Mar 23 19:34:40.206: INFO: Pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" satisfied condition "running"
    STEP: deleting the pod gracefully 03/23/23 19:34:40.206
    Mar 23 19:34:40.206: INFO: Deleting pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" in namespace "var-expansion-4609"
    Mar 23 19:34:40.214: INFO: Wait up to 5m0s for pod "var-expansion-dfc43b29-8af5-4b89-b2cc-e51b65f701db" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 19:35:12.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4609" for this suite. 03/23/23 19:35:12.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:35:12.258
Mar 23 19:35:12.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:35:12.26
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:12.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:12.282
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:35:12.31
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:35:13.165
STEP: Deploying the webhook pod 03/23/23 19:35:13.171
STEP: Wait for the deployment to be ready 03/23/23 19:35:13.188
Mar 23 19:35:13.206: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:35:15.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/23/23 19:35:17.226
STEP: Verifying the service has paired with the endpoint 03/23/23 19:35:17.312
Mar 23 19:35:18.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 03/23/23 19:35:18.317
STEP: create a pod 03/23/23 19:35:18.335
Mar 23 19:35:18.343: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4219" to be "running"
Mar 23 19:35:18.347: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354693ms
Mar 23 19:35:20.352: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008493564s
Mar 23 19:35:20.352: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 03/23/23 19:35:20.352
Mar 23 19:35:20.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=webhook-4219 attach --namespace=webhook-4219 to-be-attached-pod -i -c=container1'
Mar 23 19:35:20.487: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:35:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4219" for this suite. 03/23/23 19:35:20.498
STEP: Destroying namespace "webhook-4219-markers" for this suite. 03/23/23 19:35:20.503
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":185,"skipped":3323,"failed":0}
------------------------------
• [SLOW TEST] [8.360 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:35:12.258
    Mar 23 19:35:12.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:35:12.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:12.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:12.282
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:35:12.31
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:35:13.165
    STEP: Deploying the webhook pod 03/23/23 19:35:13.171
    STEP: Wait for the deployment to be ready 03/23/23 19:35:13.188
    Mar 23 19:35:13.206: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 23 19:35:15.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/23/23 19:35:17.226
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:35:17.312
    Mar 23 19:35:18.313: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 03/23/23 19:35:18.317
    STEP: create a pod 03/23/23 19:35:18.335
    Mar 23 19:35:18.343: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4219" to be "running"
    Mar 23 19:35:18.347: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354693ms
    Mar 23 19:35:20.352: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008493564s
    Mar 23 19:35:20.352: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 03/23/23 19:35:20.352
    Mar 23 19:35:20.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=webhook-4219 attach --namespace=webhook-4219 to-be-attached-pod -i -c=container1'
    Mar 23 19:35:20.487: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:35:20.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4219" for this suite. 03/23/23 19:35:20.498
    STEP: Destroying namespace "webhook-4219-markers" for this suite. 03/23/23 19:35:20.503
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:35:20.627
Mar 23 19:35:20.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 19:35:20.641
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:20.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:20.759
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Mar 23 19:35:20.763: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Mar 23 19:35:20.794: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 23 19:35:25.804: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 19:35:25.805
Mar 23 19:35:25.805: INFO: Creating deployment "test-rolling-update-deployment"
Mar 23 19:35:25.813: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Mar 23 19:35:25.827: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Mar 23 19:35:27.834: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Mar 23 19:35:27.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 19:35:29.841: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 19:35:29.853: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5671  6214f3e3-68c9-4035-8b4b-86ccb5831286 25457 1 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00be5c338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 19:35:25 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-23 19:35:28 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 19:35:29.857: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5671  85115948-27c6-4b51-a06c-b7db1347a3e6 25446 1 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6214f3e3-68c9-4035-8b4b-86ccb5831286 0xc00a56df17 0xc00a56df18}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6214f3e3-68c9-4035-8b4b-86ccb5831286\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a56dfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:35:29.857: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Mar 23 19:35:29.857: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5671  79331cfe-697a-4db8-aa29-cc6a539cc3be 25456 2 2023-03-23 19:35:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6214f3e3-68c9-4035-8b4b-86ccb5831286 0xc00a56dde7 0xc00a56dde8}] [] [{e2e.test Update apps/v1 2023-03-23 19:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6214f3e3-68c9-4035-8b4b-86ccb5831286\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a56dea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:35:29.862: INFO: Pod "test-rolling-update-deployment-78f575d8ff-986fk" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-986fk test-rolling-update-deployment-78f575d8ff- deployment-5671  13062b1a-f800-49e4-b90a-97de05f654f5 25445 0 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 85115948-27c6-4b51-a06c-b7db1347a3e6 0xc00b838417 0xc00b838418}] [] [{kube-controller-manager Update v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85115948-27c6-4b51-a06c-b7db1347a3e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s99tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s99tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.69,StartTime:2023-03-23 19:35:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:35:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://8021a49dfd64df50bece679981820c1747619c7873b1b821e0edfd063120d316,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 19:35:29.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5671" for this suite. 03/23/23 19:35:29.866
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":186,"skipped":3353,"failed":0}
------------------------------
• [SLOW TEST] [9.252 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:35:20.627
    Mar 23 19:35:20.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 19:35:20.641
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:20.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:20.759
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Mar 23 19:35:20.763: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Mar 23 19:35:20.794: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 23 19:35:25.804: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 19:35:25.805
    Mar 23 19:35:25.805: INFO: Creating deployment "test-rolling-update-deployment"
    Mar 23 19:35:25.813: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Mar 23 19:35:25.827: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Mar 23 19:35:27.834: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Mar 23 19:35:27.836: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 35, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 35, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-78f575d8ff\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 19:35:29.841: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 19:35:29.853: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5671  6214f3e3-68c9-4035-8b4b-86ccb5831286 25457 1 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00be5c338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 19:35:25 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-03-23 19:35:28 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 23 19:35:29.857: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5671  85115948-27c6-4b51-a06c-b7db1347a3e6 25446 1 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6214f3e3-68c9-4035-8b4b-86ccb5831286 0xc00a56df17 0xc00a56df18}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6214f3e3-68c9-4035-8b4b-86ccb5831286\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a56dfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:35:29.857: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Mar 23 19:35:29.857: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5671  79331cfe-697a-4db8-aa29-cc6a539cc3be 25456 2 2023-03-23 19:35:20 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6214f3e3-68c9-4035-8b4b-86ccb5831286 0xc00a56dde7 0xc00a56dde8}] [] [{e2e.test Update apps/v1 2023-03-23 19:35:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6214f3e3-68c9-4035-8b4b-86ccb5831286\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00a56dea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:35:29.862: INFO: Pod "test-rolling-update-deployment-78f575d8ff-986fk" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-986fk test-rolling-update-deployment-78f575d8ff- deployment-5671  13062b1a-f800-49e4-b90a-97de05f654f5 25445 0 2023-03-23 19:35:25 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 85115948-27c6-4b51-a06c-b7db1347a3e6 0xc00b838417 0xc00b838418}] [] [{kube-controller-manager Update v1 2023-03-23 19:35:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85115948-27c6-4b51-a06c-b7db1347a3e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:35:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.69\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s99tk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s99tk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:35:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.69,StartTime:2023-03-23 19:35:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:35:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://8021a49dfd64df50bece679981820c1747619c7873b1b821e0edfd063120d316,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.69,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 19:35:29.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5671" for this suite. 03/23/23 19:35:29.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:35:29.9
Mar 23 19:35:29.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:35:29.902
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:29.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:29.958
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:35:30.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-548" for this suite. 03/23/23 19:35:30.03
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":187,"skipped":3362,"failed":0}
------------------------------
• [0.137 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:35:29.9
    Mar 23 19:35:29.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:35:29.902
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:29.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:29.958
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:35:30.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-548" for this suite. 03/23/23 19:35:30.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:35:30.038
Mar 23 19:35:30.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:35:30.04
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:30.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:30.074
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Mar 23 19:35:30.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: creating the pod 03/23/23 19:35:30.078
STEP: submitting the pod to kubernetes 03/23/23 19:35:30.078
Mar 23 19:35:30.100: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6" in namespace "pods-9812" to be "running and ready"
Mar 23 19:35:30.116: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.875764ms
Mar 23 19:35:30.116: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:35:32.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020796287s
Mar 23 19:35:32.121: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:35:34.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.02070382s
Mar 23 19:35:34.121: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Running (Ready = true)
Mar 23 19:35:34.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:35:34.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9812" for this suite. 03/23/23 19:35:34.2
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":188,"skipped":3367,"failed":0}
------------------------------
• [4.178 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:35:30.038
    Mar 23 19:35:30.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:35:30.04
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:30.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:30.074
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Mar 23 19:35:30.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: creating the pod 03/23/23 19:35:30.078
    STEP: submitting the pod to kubernetes 03/23/23 19:35:30.078
    Mar 23 19:35:30.100: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6" in namespace "pods-9812" to be "running and ready"
    Mar 23 19:35:30.116: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 15.875764ms
    Mar 23 19:35:30.116: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:35:32.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020796287s
    Mar 23 19:35:32.121: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:35:34.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6": Phase="Running", Reason="", readiness=true. Elapsed: 4.02070382s
    Mar 23 19:35:34.121: INFO: The phase of Pod pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6 is Running (Ready = true)
    Mar 23 19:35:34.121: INFO: Pod "pod-logs-websocket-234657be-799c-4a36-8a3c-74be2d749cd6" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:35:34.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9812" for this suite. 03/23/23 19:35:34.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:35:34.226
Mar 23 19:35:34.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename subpath 03/23/23 19:35:34.227
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:34.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:34.249
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/23/23 19:35:34.252
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-255w 03/23/23 19:35:34.28
STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:35:34.28
Mar 23 19:35:34.298: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-255w" in namespace "subpath-1307" to be "Succeeded or Failed"
Mar 23 19:35:34.309: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Pending", Reason="", readiness=false. Elapsed: 11.005975ms
Mar 23 19:35:36.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 2.016180397s
Mar 23 19:35:38.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 4.015617631s
Mar 23 19:35:40.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 6.016520246s
Mar 23 19:35:42.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 8.015478281s
Mar 23 19:35:44.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 10.016320012s
Mar 23 19:35:46.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 12.015182983s
Mar 23 19:35:48.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 14.015538806s
Mar 23 19:35:50.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 16.01557373s
Mar 23 19:35:52.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 18.016213452s
Mar 23 19:35:54.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 20.016536974s
Mar 23 19:35:56.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 22.015441859s
Mar 23 19:35:58.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 24.01658707s
Mar 23 19:36:00.349: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=false. Elapsed: 26.049658317s
Mar 23 19:36:02.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.015674701s
STEP: Saw pod success 03/23/23 19:36:02.314
Mar 23 19:36:02.314: INFO: Pod "pod-subpath-test-configmap-255w" satisfied condition "Succeeded or Failed"
Mar 23 19:36:02.320: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-configmap-255w container test-container-subpath-configmap-255w: <nil>
STEP: delete the pod 03/23/23 19:36:02.329
Mar 23 19:36:02.380: INFO: Waiting for pod pod-subpath-test-configmap-255w to disappear
Mar 23 19:36:02.389: INFO: Pod pod-subpath-test-configmap-255w no longer exists
STEP: Deleting pod pod-subpath-test-configmap-255w 03/23/23 19:36:02.39
Mar 23 19:36:02.390: INFO: Deleting pod "pod-subpath-test-configmap-255w" in namespace "subpath-1307"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 23 19:36:02.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1307" for this suite. 03/23/23 19:36:02.398
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":189,"skipped":3395,"failed":0}
------------------------------
• [SLOW TEST] [28.199 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:35:34.226
    Mar 23 19:35:34.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename subpath 03/23/23 19:35:34.227
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:35:34.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:35:34.249
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/23/23 19:35:34.252
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-255w 03/23/23 19:35:34.28
    STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:35:34.28
    Mar 23 19:35:34.298: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-255w" in namespace "subpath-1307" to be "Succeeded or Failed"
    Mar 23 19:35:34.309: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Pending", Reason="", readiness=false. Elapsed: 11.005975ms
    Mar 23 19:35:36.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 2.016180397s
    Mar 23 19:35:38.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 4.015617631s
    Mar 23 19:35:40.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 6.016520246s
    Mar 23 19:35:42.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 8.015478281s
    Mar 23 19:35:44.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 10.016320012s
    Mar 23 19:35:46.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 12.015182983s
    Mar 23 19:35:48.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 14.015538806s
    Mar 23 19:35:50.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 16.01557373s
    Mar 23 19:35:52.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 18.016213452s
    Mar 23 19:35:54.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 20.016536974s
    Mar 23 19:35:56.313: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 22.015441859s
    Mar 23 19:35:58.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=true. Elapsed: 24.01658707s
    Mar 23 19:36:00.349: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Running", Reason="", readiness=false. Elapsed: 26.049658317s
    Mar 23 19:36:02.314: INFO: Pod "pod-subpath-test-configmap-255w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.015674701s
    STEP: Saw pod success 03/23/23 19:36:02.314
    Mar 23 19:36:02.314: INFO: Pod "pod-subpath-test-configmap-255w" satisfied condition "Succeeded or Failed"
    Mar 23 19:36:02.320: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-configmap-255w container test-container-subpath-configmap-255w: <nil>
    STEP: delete the pod 03/23/23 19:36:02.329
    Mar 23 19:36:02.380: INFO: Waiting for pod pod-subpath-test-configmap-255w to disappear
    Mar 23 19:36:02.389: INFO: Pod pod-subpath-test-configmap-255w no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-255w 03/23/23 19:36:02.39
    Mar 23 19:36:02.390: INFO: Deleting pod "pod-subpath-test-configmap-255w" in namespace "subpath-1307"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 23 19:36:02.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1307" for this suite. 03/23/23 19:36:02.398
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:02.425
Mar 23 19:36:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:36:02.427
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:02.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:02.486
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 03/23/23 19:36:02.494
Mar 23 19:36:02.495: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-6012 proxy --unix-socket=/tmp/kubectl-proxy-unix2134770021/test'
STEP: retrieving proxy /api/ output 03/23/23 19:36:02.594
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:36:02.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6012" for this suite. 03/23/23 19:36:02.612
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":190,"skipped":3399,"failed":0}
------------------------------
• [0.204 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:02.425
    Mar 23 19:36:02.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:36:02.427
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:02.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:02.486
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 03/23/23 19:36:02.494
    Mar 23 19:36:02.495: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-6012 proxy --unix-socket=/tmp/kubectl-proxy-unix2134770021/test'
    STEP: retrieving proxy /api/ output 03/23/23 19:36:02.594
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:36:02.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6012" for this suite. 03/23/23 19:36:02.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:02.633
Mar 23 19:36:02.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replication-controller 03/23/23 19:36:02.634
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:02.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:02.669
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 03/23/23 19:36:02.675
Mar 23 19:36:02.692: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7880" to be "running and ready"
Mar 23 19:36:02.695: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284093ms
Mar 23 19:36:02.695: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:36:04.699: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307555s
Mar 23 19:36:04.699: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Mar 23 19:36:04.699: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 03/23/23 19:36:04.708
STEP: Then the orphan pod is adopted 03/23/23 19:36:04.717
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 23 19:36:04.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7880" for this suite. 03/23/23 19:36:04.736
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":191,"skipped":3428,"failed":0}
------------------------------
• [2.120 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:02.633
    Mar 23 19:36:02.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replication-controller 03/23/23 19:36:02.634
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:02.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:02.669
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 03/23/23 19:36:02.675
    Mar 23 19:36:02.692: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7880" to be "running and ready"
    Mar 23 19:36:02.695: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.284093ms
    Mar 23 19:36:02.695: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:36:04.699: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.007307555s
    Mar 23 19:36:04.699: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Mar 23 19:36:04.699: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 03/23/23 19:36:04.708
    STEP: Then the orphan pod is adopted 03/23/23 19:36:04.717
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 23 19:36:04.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7880" for this suite. 03/23/23 19:36:04.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:04.762
Mar 23 19:36:04.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:36:04.763
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:04.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:04.783
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-12f298e7-4157-4d68-846f-75a1b9b513b1 03/23/23 19:36:04.785
STEP: Creating a pod to test consume configMaps 03/23/23 19:36:04.804
Mar 23 19:36:04.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60" in namespace "configmap-2875" to be "Succeeded or Failed"
Mar 23 19:36:04.833: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Pending", Reason="", readiness=false. Elapsed: 11.680272ms
Mar 23 19:36:06.838: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=true. Elapsed: 2.016725673s
Mar 23 19:36:08.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=true. Elapsed: 4.018584381s
Mar 23 19:36:10.837: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=false. Elapsed: 6.015651901s
Mar 23 19:36:12.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018576526s
STEP: Saw pod success 03/23/23 19:36:12.84
Mar 23 19:36:12.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60" satisfied condition "Succeeded or Failed"
Mar 23 19:36:12.844: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:36:12.884
Mar 23 19:36:12.907: INFO: Waiting for pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 to disappear
Mar 23 19:36:12.911: INFO: Pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:36:12.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2875" for this suite. 03/23/23 19:36:12.916
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":192,"skipped":3457,"failed":0}
------------------------------
• [SLOW TEST] [8.160 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:04.762
    Mar 23 19:36:04.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:36:04.763
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:04.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:04.783
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-12f298e7-4157-4d68-846f-75a1b9b513b1 03/23/23 19:36:04.785
    STEP: Creating a pod to test consume configMaps 03/23/23 19:36:04.804
    Mar 23 19:36:04.821: INFO: Waiting up to 5m0s for pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60" in namespace "configmap-2875" to be "Succeeded or Failed"
    Mar 23 19:36:04.833: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Pending", Reason="", readiness=false. Elapsed: 11.680272ms
    Mar 23 19:36:06.838: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=true. Elapsed: 2.016725673s
    Mar 23 19:36:08.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=true. Elapsed: 4.018584381s
    Mar 23 19:36:10.837: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Running", Reason="", readiness=false. Elapsed: 6.015651901s
    Mar 23 19:36:12.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018576526s
    STEP: Saw pod success 03/23/23 19:36:12.84
    Mar 23 19:36:12.840: INFO: Pod "pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60" satisfied condition "Succeeded or Failed"
    Mar 23 19:36:12.844: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:36:12.884
    Mar 23 19:36:12.907: INFO: Waiting for pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 to disappear
    Mar 23 19:36:12.911: INFO: Pod pod-configmaps-07078d6d-0457-4e27-9db4-c3019d6e9a60 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:36:12.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2875" for this suite. 03/23/23 19:36:12.916
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:12.93
Mar 23 19:36:12.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:36:12.932
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:12.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:12.963
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:36:12.97
Mar 23 19:36:12.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af" in namespace "projected-6397" to be "Succeeded or Failed"
Mar 23 19:36:13.008: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Pending", Reason="", readiness=false. Elapsed: 14.052766ms
Mar 23 19:36:15.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Running", Reason="", readiness=true. Elapsed: 2.019845284s
Mar 23 19:36:17.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Running", Reason="", readiness=false. Elapsed: 4.019940916s
Mar 23 19:36:19.013: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018865939s
STEP: Saw pod success 03/23/23 19:36:19.013
Mar 23 19:36:19.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af" satisfied condition "Succeeded or Failed"
Mar 23 19:36:19.017: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af container client-container: <nil>
STEP: delete the pod 03/23/23 19:36:19.023
Mar 23 19:36:19.037: INFO: Waiting for pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af to disappear
Mar 23 19:36:19.040: INFO: Pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:36:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6397" for this suite. 03/23/23 19:36:19.05
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":193,"skipped":3457,"failed":0}
------------------------------
• [SLOW TEST] [6.129 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:12.93
    Mar 23 19:36:12.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:36:12.932
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:12.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:12.963
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:36:12.97
    Mar 23 19:36:12.994: INFO: Waiting up to 5m0s for pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af" in namespace "projected-6397" to be "Succeeded or Failed"
    Mar 23 19:36:13.008: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Pending", Reason="", readiness=false. Elapsed: 14.052766ms
    Mar 23 19:36:15.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Running", Reason="", readiness=true. Elapsed: 2.019845284s
    Mar 23 19:36:17.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Running", Reason="", readiness=false. Elapsed: 4.019940916s
    Mar 23 19:36:19.013: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018865939s
    STEP: Saw pod success 03/23/23 19:36:19.013
    Mar 23 19:36:19.014: INFO: Pod "downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af" satisfied condition "Succeeded or Failed"
    Mar 23 19:36:19.017: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af container client-container: <nil>
    STEP: delete the pod 03/23/23 19:36:19.023
    Mar 23 19:36:19.037: INFO: Waiting for pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af to disappear
    Mar 23 19:36:19.040: INFO: Pod downwardapi-volume-82301f83-87ea-4fcb-88cd-71edfb1ad7af no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:36:19.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6397" for this suite. 03/23/23 19:36:19.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:19.062
Mar 23 19:36:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename controllerrevisions 03/23/23 19:36:19.063
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:19.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:19.087
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-ss62d-daemon-set" 03/23/23 19:36:19.139
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:36:19.158
Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:19.174: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
Mar 23 19:36:19.174: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:36:20.180: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:20.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:20.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:20.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
Mar 23 19:36:20.186: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:21.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 1
Mar 23 19:36:21.185: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:36:22.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 3
Mar 23 19:36:22.185: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-ss62d-daemon-set
STEP: Confirm DaemonSet "e2e-ss62d-daemon-set" successfully created with "daemonset-name=e2e-ss62d-daemon-set" label 03/23/23 19:36:22.188
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ss62d-daemon-set" 03/23/23 19:36:22.193
Mar 23 19:36:22.197: INFO: Located ControllerRevision: "e2e-ss62d-daemon-set-7cbdbfd7f7"
STEP: Patching ControllerRevision "e2e-ss62d-daemon-set-7cbdbfd7f7" 03/23/23 19:36:22.199
Mar 23 19:36:22.205: INFO: e2e-ss62d-daemon-set-7cbdbfd7f7 has been patched
STEP: Create a new ControllerRevision 03/23/23 19:36:22.205
Mar 23 19:36:22.212: INFO: Created ControllerRevision: e2e-ss62d-daemon-set-c55574f97
STEP: Confirm that there are two ControllerRevisions 03/23/23 19:36:22.212
Mar 23 19:36:22.212: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 23 19:36:22.214: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-ss62d-daemon-set-7cbdbfd7f7" 03/23/23 19:36:22.215
STEP: Confirm that there is only one ControllerRevision 03/23/23 19:36:22.22
Mar 23 19:36:22.220: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 23 19:36:22.223: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-ss62d-daemon-set-c55574f97" 03/23/23 19:36:22.226
Mar 23 19:36:22.233: INFO: e2e-ss62d-daemon-set-c55574f97 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 03/23/23 19:36:22.233
W0323 19:36:22.243273      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 03/23/23 19:36:22.243
Mar 23 19:36:22.243: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 23 19:36:23.255: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 23 19:36:23.260: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ss62d-daemon-set-c55574f97=updated" 03/23/23 19:36:23.26
STEP: Confirm that there is only one ControllerRevision 03/23/23 19:36:23.266
Mar 23 19:36:23.266: INFO: Requesting list of ControllerRevisions to confirm quantity
Mar 23 19:36:23.269: INFO: Found 1 ControllerRevisions
Mar 23 19:36:23.271: INFO: ControllerRevision "e2e-ss62d-daemon-set-7b977dbfb" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-ss62d-daemon-set" 03/23/23 19:36:23.274
STEP: deleting DaemonSet.extensions e2e-ss62d-daemon-set in namespace controllerrevisions-4759, will wait for the garbage collector to delete the pods 03/23/23 19:36:23.274
Mar 23 19:36:23.336: INFO: Deleting DaemonSet.extensions e2e-ss62d-daemon-set took: 8.938878ms
Mar 23 19:36:23.437: INFO: Terminating DaemonSet.extensions e2e-ss62d-daemon-set pods took: 100.775551ms
Mar 23 19:36:27.442: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
Mar 23 19:36:27.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ss62d-daemon-set
Mar 23 19:36:27.445: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25927"},"items":null}

Mar 23 19:36:27.449: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25927"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:36:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-4759" for this suite. 03/23/23 19:36:27.467
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":194,"skipped":3497,"failed":0}
------------------------------
• [SLOW TEST] [8.412 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:19.062
    Mar 23 19:36:19.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename controllerrevisions 03/23/23 19:36:19.063
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:19.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:19.087
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-ss62d-daemon-set" 03/23/23 19:36:19.139
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:36:19.158
    Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:19.166: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:19.174: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
    Mar 23 19:36:19.174: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:36:20.180: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:20.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:20.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:20.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
    Mar 23 19:36:20.186: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:21.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:21.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 1
    Mar 23 19:36:21.185: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:22.181: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:36:22.185: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 3
    Mar 23 19:36:22.185: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-ss62d-daemon-set
    STEP: Confirm DaemonSet "e2e-ss62d-daemon-set" successfully created with "daemonset-name=e2e-ss62d-daemon-set" label 03/23/23 19:36:22.188
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-ss62d-daemon-set" 03/23/23 19:36:22.193
    Mar 23 19:36:22.197: INFO: Located ControllerRevision: "e2e-ss62d-daemon-set-7cbdbfd7f7"
    STEP: Patching ControllerRevision "e2e-ss62d-daemon-set-7cbdbfd7f7" 03/23/23 19:36:22.199
    Mar 23 19:36:22.205: INFO: e2e-ss62d-daemon-set-7cbdbfd7f7 has been patched
    STEP: Create a new ControllerRevision 03/23/23 19:36:22.205
    Mar 23 19:36:22.212: INFO: Created ControllerRevision: e2e-ss62d-daemon-set-c55574f97
    STEP: Confirm that there are two ControllerRevisions 03/23/23 19:36:22.212
    Mar 23 19:36:22.212: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 23 19:36:22.214: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-ss62d-daemon-set-7cbdbfd7f7" 03/23/23 19:36:22.215
    STEP: Confirm that there is only one ControllerRevision 03/23/23 19:36:22.22
    Mar 23 19:36:22.220: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 23 19:36:22.223: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-ss62d-daemon-set-c55574f97" 03/23/23 19:36:22.226
    Mar 23 19:36:22.233: INFO: e2e-ss62d-daemon-set-c55574f97 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 03/23/23 19:36:22.233
    W0323 19:36:22.243273      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 03/23/23 19:36:22.243
    Mar 23 19:36:22.243: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 23 19:36:23.255: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 23 19:36:23.260: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-ss62d-daemon-set-c55574f97=updated" 03/23/23 19:36:23.26
    STEP: Confirm that there is only one ControllerRevision 03/23/23 19:36:23.266
    Mar 23 19:36:23.266: INFO: Requesting list of ControllerRevisions to confirm quantity
    Mar 23 19:36:23.269: INFO: Found 1 ControllerRevisions
    Mar 23 19:36:23.271: INFO: ControllerRevision "e2e-ss62d-daemon-set-7b977dbfb" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-ss62d-daemon-set" 03/23/23 19:36:23.274
    STEP: deleting DaemonSet.extensions e2e-ss62d-daemon-set in namespace controllerrevisions-4759, will wait for the garbage collector to delete the pods 03/23/23 19:36:23.274
    Mar 23 19:36:23.336: INFO: Deleting DaemonSet.extensions e2e-ss62d-daemon-set took: 8.938878ms
    Mar 23 19:36:23.437: INFO: Terminating DaemonSet.extensions e2e-ss62d-daemon-set pods took: 100.775551ms
    Mar 23 19:36:27.442: INFO: Number of nodes with available pods controlled by daemonset e2e-ss62d-daemon-set: 0
    Mar 23 19:36:27.442: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-ss62d-daemon-set
    Mar 23 19:36:27.445: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"25927"},"items":null}

    Mar 23 19:36:27.449: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"25927"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:36:27.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-4759" for this suite. 03/23/23 19:36:27.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:27.486
Mar 23 19:36:27.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:36:27.487
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:27.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:27.507
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 03/23/23 19:36:27.509
Mar 23 19:36:27.527: INFO: Waiting up to 5m0s for pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd" in namespace "emptydir-4551" to be "Succeeded or Failed"
Mar 23 19:36:27.533: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.864286ms
Mar 23 19:36:29.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011693629s
Mar 23 19:36:31.537: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.009836191s
Mar 23 19:36:33.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011146445s
STEP: Saw pod success 03/23/23 19:36:33.538
Mar 23 19:36:33.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd" satisfied condition "Succeeded or Failed"
Mar 23 19:36:33.545: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-0e762ea7-54ce-410c-adcb-959d954509bd container test-container: <nil>
STEP: delete the pod 03/23/23 19:36:33.552
Mar 23 19:36:33.584: INFO: Waiting for pod pod-0e762ea7-54ce-410c-adcb-959d954509bd to disappear
Mar 23 19:36:33.588: INFO: Pod pod-0e762ea7-54ce-410c-adcb-959d954509bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:36:33.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4551" for this suite. 03/23/23 19:36:33.594
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":195,"skipped":3538,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:27.486
    Mar 23 19:36:27.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:36:27.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:27.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:27.507
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 03/23/23 19:36:27.509
    Mar 23 19:36:27.527: INFO: Waiting up to 5m0s for pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd" in namespace "emptydir-4551" to be "Succeeded or Failed"
    Mar 23 19:36:27.533: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.864286ms
    Mar 23 19:36:29.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011693629s
    Mar 23 19:36:31.537: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Running", Reason="", readiness=false. Elapsed: 4.009836191s
    Mar 23 19:36:33.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011146445s
    STEP: Saw pod success 03/23/23 19:36:33.538
    Mar 23 19:36:33.538: INFO: Pod "pod-0e762ea7-54ce-410c-adcb-959d954509bd" satisfied condition "Succeeded or Failed"
    Mar 23 19:36:33.545: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-0e762ea7-54ce-410c-adcb-959d954509bd container test-container: <nil>
    STEP: delete the pod 03/23/23 19:36:33.552
    Mar 23 19:36:33.584: INFO: Waiting for pod pod-0e762ea7-54ce-410c-adcb-959d954509bd to disappear
    Mar 23 19:36:33.588: INFO: Pod pod-0e762ea7-54ce-410c-adcb-959d954509bd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:36:33.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4551" for this suite. 03/23/23 19:36:33.594
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:33.612
Mar 23 19:36:33.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svc-latency 03/23/23 19:36:33.613
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:33.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:33.643
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Mar 23 19:36:33.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: creating replication controller svc-latency-rc in namespace svc-latency-856 03/23/23 19:36:33.651
I0323 19:36:33.657545      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-856, replica count: 1
I0323 19:36:34.709003      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:36:35.709375      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:36:35.831: INFO: Created: latency-svc-cfwdg
Mar 23 19:36:35.859: INFO: Got endpoints: latency-svc-cfwdg [49.701358ms]
Mar 23 19:36:35.908: INFO: Created: latency-svc-7lsd5
Mar 23 19:36:35.920: INFO: Created: latency-svc-57txd
Mar 23 19:36:35.972: INFO: Created: latency-svc-qhmqz
Mar 23 19:36:35.990: INFO: Created: latency-svc-zlm7s
Mar 23 19:36:35.997: INFO: Got endpoints: latency-svc-7lsd5 [136.778734ms]
Mar 23 19:36:35.998: INFO: Got endpoints: latency-svc-57txd [135.788339ms]
Mar 23 19:36:36.003: INFO: Got endpoints: latency-svc-qhmqz [141.160413ms]
Mar 23 19:36:36.019: INFO: Got endpoints: latency-svc-zlm7s [157.136136ms]
Mar 23 19:36:36.028: INFO: Created: latency-svc-4f76q
Mar 23 19:36:36.045: INFO: Got endpoints: latency-svc-4f76q [183.393808ms]
Mar 23 19:36:36.081: INFO: Created: latency-svc-r4xrm
Mar 23 19:36:36.093: INFO: Created: latency-svc-qfkwk
Mar 23 19:36:36.103: INFO: Got endpoints: latency-svc-r4xrm [241.388625ms]
Mar 23 19:36:36.107: INFO: Got endpoints: latency-svc-qfkwk [245.018707ms]
Mar 23 19:36:36.116: INFO: Created: latency-svc-vflxj
Mar 23 19:36:36.132: INFO: Got endpoints: latency-svc-vflxj [270.056386ms]
Mar 23 19:36:36.136: INFO: Created: latency-svc-2zmcr
Mar 23 19:36:36.182: INFO: Got endpoints: latency-svc-2zmcr [321.915434ms]
Mar 23 19:36:36.187: INFO: Created: latency-svc-k8bpz
Mar 23 19:36:36.199: INFO: Got endpoints: latency-svc-k8bpz [336.836661ms]
Mar 23 19:36:36.210: INFO: Created: latency-svc-2vzlq
Mar 23 19:36:36.226: INFO: Got endpoints: latency-svc-2vzlq [363.73453ms]
Mar 23 19:36:36.237: INFO: Created: latency-svc-5n9gv
Mar 23 19:36:36.247: INFO: Got endpoints: latency-svc-5n9gv [385.370624ms]
Mar 23 19:36:36.252: INFO: Created: latency-svc-rfssl
Mar 23 19:36:36.272: INFO: Got endpoints: latency-svc-rfssl [409.842306ms]
Mar 23 19:36:36.291: INFO: Created: latency-svc-s852h
Mar 23 19:36:36.298: INFO: Created: latency-svc-qjbcb
Mar 23 19:36:36.322: INFO: Created: latency-svc-vbfqt
Mar 23 19:36:36.323: INFO: Got endpoints: latency-svc-s852h [460.844957ms]
Mar 23 19:36:36.323: INFO: Got endpoints: latency-svc-qjbcb [461.043856ms]
Mar 23 19:36:36.351: INFO: Got endpoints: latency-svc-vbfqt [353.222481ms]
Mar 23 19:36:36.352: INFO: Created: latency-svc-l6hp6
Mar 23 19:36:36.386: INFO: Got endpoints: latency-svc-l6hp6 [388.49521ms]
Mar 23 19:36:36.401: INFO: Created: latency-svc-np5z7
Mar 23 19:36:36.429: INFO: Got endpoints: latency-svc-np5z7 [426.237426ms]
Mar 23 19:36:36.431: INFO: Created: latency-svc-dtdf4
Mar 23 19:36:36.491: INFO: Got endpoints: latency-svc-dtdf4 [471.598105ms]
Mar 23 19:36:36.501: INFO: Created: latency-svc-b62d6
Mar 23 19:36:36.533: INFO: Got endpoints: latency-svc-b62d6 [488.385523ms]
Mar 23 19:36:36.547: INFO: Created: latency-svc-tffpw
Mar 23 19:36:36.558: INFO: Got endpoints: latency-svc-tffpw [454.219289ms]
Mar 23 19:36:36.563: INFO: Created: latency-svc-lb7tm
Mar 23 19:36:36.577: INFO: Got endpoints: latency-svc-lb7tm [469.337016ms]
Mar 23 19:36:36.590: INFO: Created: latency-svc-5gdlj
Mar 23 19:36:36.594: INFO: Got endpoints: latency-svc-5gdlj [461.962152ms]
Mar 23 19:36:36.601: INFO: Created: latency-svc-m6kwp
Mar 23 19:36:36.609: INFO: Created: latency-svc-sl8q2
Mar 23 19:36:36.641: INFO: Got endpoints: latency-svc-sl8q2 [441.88255ms]
Mar 23 19:36:36.641: INFO: Got endpoints: latency-svc-m6kwp [459.054465ms]
Mar 23 19:36:36.644: INFO: Created: latency-svc-wnppl
Mar 23 19:36:36.664: INFO: Got endpoints: latency-svc-wnppl [438.284767ms]
Mar 23 19:36:36.669: INFO: Created: latency-svc-t5grt
Mar 23 19:36:36.682: INFO: Got endpoints: latency-svc-t5grt [434.397686ms]
Mar 23 19:36:36.683: INFO: Created: latency-svc-bpxm9
Mar 23 19:36:36.701: INFO: Got endpoints: latency-svc-bpxm9 [429.43631ms]
Mar 23 19:36:36.708: INFO: Created: latency-svc-6vfjb
Mar 23 19:36:36.726: INFO: Got endpoints: latency-svc-6vfjb [403.340837ms]
Mar 23 19:36:36.727: INFO: Created: latency-svc-bkhpz
Mar 23 19:36:36.733: INFO: Got endpoints: latency-svc-bkhpz [410.142504ms]
Mar 23 19:36:36.741: INFO: Created: latency-svc-8ww4t
Mar 23 19:36:36.755: INFO: Got endpoints: latency-svc-8ww4t [404.290932ms]
Mar 23 19:36:36.766: INFO: Created: latency-svc-kgqz9
Mar 23 19:36:36.775: INFO: Got endpoints: latency-svc-kgqz9 [388.815308ms]
Mar 23 19:36:36.785: INFO: Created: latency-svc-c2kp4
Mar 23 19:36:36.793: INFO: Got endpoints: latency-svc-c2kp4 [363.591331ms]
Mar 23 19:36:36.802: INFO: Created: latency-svc-hxzjx
Mar 23 19:36:36.815: INFO: Got endpoints: latency-svc-hxzjx [324.154522ms]
Mar 23 19:36:36.967: INFO: Created: latency-svc-l6wjp
Mar 23 19:36:36.997: INFO: Got endpoints: latency-svc-l6wjp [463.844343ms]
Mar 23 19:36:37.029: INFO: Created: latency-svc-rss5q
Mar 23 19:36:37.029: INFO: Created: latency-svc-nfxql
Mar 23 19:36:37.030: INFO: Created: latency-svc-5k2pw
Mar 23 19:36:37.030: INFO: Created: latency-svc-q5t9f
Mar 23 19:36:37.030: INFO: Created: latency-svc-w7x2p
Mar 23 19:36:37.030: INFO: Created: latency-svc-ng2gr
Mar 23 19:36:37.030: INFO: Created: latency-svc-79826
Mar 23 19:36:37.031: INFO: Created: latency-svc-9g9r9
Mar 23 19:36:37.032: INFO: Created: latency-svc-hlxpl
Mar 23 19:36:37.032: INFO: Created: latency-svc-f296j
Mar 23 19:36:37.033: INFO: Created: latency-svc-lgt97
Mar 23 19:36:37.033: INFO: Created: latency-svc-qz4wf
Mar 23 19:36:37.034: INFO: Created: latency-svc-jzr9x
Mar 23 19:36:37.034: INFO: Created: latency-svc-dnlcb
Mar 23 19:36:37.065: INFO: Got endpoints: latency-svc-lgt97 [272.062476ms]
Mar 23 19:36:37.088: INFO: Got endpoints: latency-svc-f296j [530.461018ms]
Mar 23 19:36:37.101: INFO: Got endpoints: latency-svc-jzr9x [524.450547ms]
Mar 23 19:36:37.102: INFO: Got endpoints: latency-svc-hlxpl [508.283226ms]
Mar 23 19:36:37.103: INFO: Got endpoints: latency-svc-rss5q [288.089798ms]
Mar 23 19:36:37.114: INFO: Got endpoints: latency-svc-9g9r9 [473.395496ms]
Mar 23 19:36:37.114: INFO: Got endpoints: latency-svc-79826 [473.158397ms]
Mar 23 19:36:37.115: INFO: Created: latency-svc-glhmv
Mar 23 19:36:37.120: INFO: Got endpoints: latency-svc-ng2gr [456.294779ms]
Mar 23 19:36:37.123: INFO: Got endpoints: latency-svc-w7x2p [441.70555ms]
Mar 23 19:36:37.127: INFO: Got endpoints: latency-svc-q5t9f [424.971532ms]
Mar 23 19:36:37.129: INFO: Created: latency-svc-c29nq
Mar 23 19:36:37.140: INFO: Got endpoints: latency-svc-5k2pw [414.029485ms]
Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-dnlcb [419.225359ms]
Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-glhmv [155.634543ms]
Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-nfxql [377.845561ms]
Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-qz4wf [398.071963ms]
Mar 23 19:36:37.167: INFO: Got endpoints: latency-svc-c29nq [102.016303ms]
Mar 23 19:36:37.302: INFO: Created: latency-svc-8psfq
Mar 23 19:36:37.303: INFO: Created: latency-svc-k2h8m
Mar 23 19:36:37.305: INFO: Created: latency-svc-swwfx
Mar 23 19:36:37.315: INFO: Got endpoints: latency-svc-swwfx [227.260294ms]
Mar 23 19:36:37.324: INFO: Got endpoints: latency-svc-k2h8m [171.022268ms]
Mar 23 19:36:37.325: INFO: Got endpoints: latency-svc-8psfq [157.363934ms]
Mar 23 19:36:37.325: INFO: Created: latency-svc-xcxfs
Mar 23 19:36:37.333: INFO: Created: latency-svc-c795r
Mar 23 19:36:37.333: INFO: Created: latency-svc-qqcj4
Mar 23 19:36:37.333: INFO: Created: latency-svc-2w5z9
Mar 23 19:36:37.333: INFO: Created: latency-svc-bpwkj
Mar 23 19:36:37.334: INFO: Created: latency-svc-h25zq
Mar 23 19:36:37.334: INFO: Created: latency-svc-lwshs
Mar 23 19:36:37.334: INFO: Created: latency-svc-z2fgl
Mar 23 19:36:37.334: INFO: Created: latency-svc-x5w7q
Mar 23 19:36:37.334: INFO: Got endpoints: latency-svc-x5w7q [233.025166ms]
Mar 23 19:36:37.334: INFO: Created: latency-svc-qxpk5
Mar 23 19:36:37.334: INFO: Created: latency-svc-9tnf5
Mar 23 19:36:37.335: INFO: Created: latency-svc-hrgnw
Mar 23 19:36:37.348: INFO: Got endpoints: latency-svc-xcxfs [233.429264ms]
Mar 23 19:36:37.351: INFO: Got endpoints: latency-svc-z2fgl [248.233292ms]
Mar 23 19:36:37.362: INFO: Created: latency-svc-l4kz4
Mar 23 19:36:37.367: INFO: Got endpoints: latency-svc-h25zq [252.234073ms]
Mar 23 19:36:37.378: INFO: Got endpoints: latency-svc-9tnf5 [254.75626ms]
Mar 23 19:36:37.385: INFO: Created: latency-svc-jhptg
Mar 23 19:36:37.401: INFO: Created: latency-svc-tsp6q
Mar 23 19:36:37.418: INFO: Created: latency-svc-p59nn
Mar 23 19:36:37.418: INFO: Got endpoints: latency-svc-bpwkj [291.83608ms]
Mar 23 19:36:37.438: INFO: Created: latency-svc-sss4d
Mar 23 19:36:37.452: INFO: Created: latency-svc-k2snh
Mar 23 19:36:37.465: INFO: Got endpoints: latency-svc-lwshs [361.378341ms]
Mar 23 19:36:37.474: INFO: Created: latency-svc-zcxpp
Mar 23 19:36:37.488: INFO: Created: latency-svc-58g74
Mar 23 19:36:37.498: INFO: Created: latency-svc-hpwxd
Mar 23 19:36:37.522: INFO: Got endpoints: latency-svc-hrgnw [401.906244ms]
Mar 23 19:36:37.536: INFO: Created: latency-svc-xfwbh
Mar 23 19:36:37.549: INFO: Created: latency-svc-qzb5w
Mar 23 19:36:37.562: INFO: Got endpoints: latency-svc-2w5z9 [421.425049ms]
Mar 23 19:36:37.585: INFO: Created: latency-svc-njpfj
Mar 23 19:36:37.613: INFO: Got endpoints: latency-svc-c795r [460.192562ms]
Mar 23 19:36:37.629: INFO: Created: latency-svc-cv67d
Mar 23 19:36:37.665: INFO: Got endpoints: latency-svc-qxpk5 [511.797142ms]
Mar 23 19:36:37.682: INFO: Created: latency-svc-mjmbj
Mar 23 19:36:37.718: INFO: Got endpoints: latency-svc-qqcj4 [564.369119ms]
Mar 23 19:36:37.734: INFO: Created: latency-svc-4zv7w
Mar 23 19:36:37.763: INFO: Got endpoints: latency-svc-l4kz4 [447.340201ms]
Mar 23 19:36:37.781: INFO: Created: latency-svc-g7m7j
Mar 23 19:36:37.813: INFO: Got endpoints: latency-svc-jhptg [488.726027ms]
Mar 23 19:36:37.827: INFO: Created: latency-svc-dkrpk
Mar 23 19:36:37.872: INFO: Got endpoints: latency-svc-tsp6q [547.467488ms]
Mar 23 19:36:37.898: INFO: Created: latency-svc-lknz5
Mar 23 19:36:37.919: INFO: Got endpoints: latency-svc-p59nn [584.873224ms]
Mar 23 19:36:37.951: INFO: Created: latency-svc-nqhk4
Mar 23 19:36:37.986: INFO: Got endpoints: latency-svc-sss4d [634.930848ms]
Mar 23 19:36:38.008: INFO: Created: latency-svc-74wkq
Mar 23 19:36:38.016: INFO: Got endpoints: latency-svc-k2snh [668.092064ms]
Mar 23 19:36:38.037: INFO: Created: latency-svc-7qrcc
Mar 23 19:36:38.067: INFO: Got endpoints: latency-svc-zcxpp [699.346239ms]
Mar 23 19:36:38.107: INFO: Created: latency-svc-72lsd
Mar 23 19:36:38.112: INFO: Got endpoints: latency-svc-58g74 [733.652086ms]
Mar 23 19:36:38.137: INFO: Created: latency-svc-cn8cw
Mar 23 19:36:38.170: INFO: Got endpoints: latency-svc-hpwxd [751.299745ms]
Mar 23 19:36:38.182: INFO: Created: latency-svc-24gnj
Mar 23 19:36:38.221: INFO: Got endpoints: latency-svc-xfwbh [756.17175ms]
Mar 23 19:36:38.231: INFO: Created: latency-svc-gwp2h
Mar 23 19:36:38.263: INFO: Got endpoints: latency-svc-qzb5w [740.655831ms]
Mar 23 19:36:38.276: INFO: Created: latency-svc-plxkg
Mar 23 19:36:38.313: INFO: Got endpoints: latency-svc-njpfj [750.524708ms]
Mar 23 19:36:38.331: INFO: Created: latency-svc-sfmtt
Mar 23 19:36:38.364: INFO: Got endpoints: latency-svc-cv67d [751.099532ms]
Mar 23 19:36:38.377: INFO: Created: latency-svc-7mn9c
Mar 23 19:36:38.433: INFO: Got endpoints: latency-svc-mjmbj [767.594293ms]
Mar 23 19:36:38.452: INFO: Created: latency-svc-pjwfg
Mar 23 19:36:38.462: INFO: Got endpoints: latency-svc-4zv7w [744.023049ms]
Mar 23 19:36:38.477: INFO: Created: latency-svc-ds5fb
Mar 23 19:36:38.511: INFO: Got endpoints: latency-svc-g7m7j [748.236439ms]
Mar 23 19:36:38.527: INFO: Created: latency-svc-vszrv
Mar 23 19:36:38.570: INFO: Got endpoints: latency-svc-dkrpk [755.91982ms]
Mar 23 19:36:38.593: INFO: Created: latency-svc-qdwvr
Mar 23 19:36:38.616: INFO: Got endpoints: latency-svc-lknz5 [741.977554ms]
Mar 23 19:36:38.631: INFO: Created: latency-svc-fmh2l
Mar 23 19:36:38.666: INFO: Got endpoints: latency-svc-nqhk4 [745.028646ms]
Mar 23 19:36:38.686: INFO: Created: latency-svc-fkd6m
Mar 23 19:36:38.719: INFO: Got endpoints: latency-svc-74wkq [733.215575ms]
Mar 23 19:36:38.737: INFO: Created: latency-svc-q8r5f
Mar 23 19:36:38.769: INFO: Got endpoints: latency-svc-7qrcc [752.530628ms]
Mar 23 19:36:38.789: INFO: Created: latency-svc-mgffx
Mar 23 19:36:38.812: INFO: Got endpoints: latency-svc-72lsd [744.909146ms]
Mar 23 19:36:38.837: INFO: Created: latency-svc-74wxq
Mar 23 19:36:38.891: INFO: Got endpoints: latency-svc-cn8cw [778.249668ms]
Mar 23 19:36:38.924: INFO: Got endpoints: latency-svc-24gnj [754.001725ms]
Mar 23 19:36:38.964: INFO: Created: latency-svc-qnz6m
Mar 23 19:36:38.977: INFO: Got endpoints: latency-svc-gwp2h [755.869821ms]
Mar 23 19:36:38.982: INFO: Created: latency-svc-bdkws
Mar 23 19:36:38.998: INFO: Created: latency-svc-p7nlx
Mar 23 19:36:39.014: INFO: Got endpoints: latency-svc-plxkg [750.489033ms]
Mar 23 19:36:39.036: INFO: Created: latency-svc-6qg7x
Mar 23 19:36:39.064: INFO: Got endpoints: latency-svc-sfmtt [750.963932ms]
Mar 23 19:36:39.079: INFO: Created: latency-svc-rcshn
Mar 23 19:36:39.112: INFO: Got endpoints: latency-svc-7mn9c [747.321541ms]
Mar 23 19:36:39.132: INFO: Created: latency-svc-vbmkl
Mar 23 19:36:39.172: INFO: Got endpoints: latency-svc-pjwfg [739.13906ms]
Mar 23 19:36:39.183: INFO: Created: latency-svc-8d9h9
Mar 23 19:36:39.217: INFO: Got endpoints: latency-svc-ds5fb [755.265922ms]
Mar 23 19:36:39.235: INFO: Created: latency-svc-m86st
Mar 23 19:36:39.265: INFO: Got endpoints: latency-svc-vszrv [753.738926ms]
Mar 23 19:36:39.286: INFO: Created: latency-svc-rtjv7
Mar 23 19:36:39.313: INFO: Got endpoints: latency-svc-qdwvr [743.338651ms]
Mar 23 19:36:39.330: INFO: Created: latency-svc-hg2f9
Mar 23 19:36:39.369: INFO: Got endpoints: latency-svc-fmh2l [753.122228ms]
Mar 23 19:36:39.382: INFO: Created: latency-svc-xzphx
Mar 23 19:36:39.417: INFO: Got endpoints: latency-svc-fkd6m [750.327334ms]
Mar 23 19:36:39.427: INFO: Created: latency-svc-zvw75
Mar 23 19:36:39.462: INFO: Got endpoints: latency-svc-q8r5f [743.273751ms]
Mar 23 19:36:39.483: INFO: Created: latency-svc-4jfcg
Mar 23 19:36:39.528: INFO: Got endpoints: latency-svc-mgffx [758.681315ms]
Mar 23 19:36:39.558: INFO: Created: latency-svc-86nzj
Mar 23 19:36:39.564: INFO: Got endpoints: latency-svc-74wxq [751.99753ms]
Mar 23 19:36:39.584: INFO: Created: latency-svc-x6tzm
Mar 23 19:36:39.619: INFO: Got endpoints: latency-svc-qnz6m [728.832584ms]
Mar 23 19:36:39.638: INFO: Created: latency-svc-rz5n5
Mar 23 19:36:39.668: INFO: Got endpoints: latency-svc-bdkws [740.242758ms]
Mar 23 19:36:39.679: INFO: Created: latency-svc-dplvj
Mar 23 19:36:39.712: INFO: Got endpoints: latency-svc-p7nlx [734.80137ms]
Mar 23 19:36:39.724: INFO: Created: latency-svc-z4tqw
Mar 23 19:36:39.765: INFO: Got endpoints: latency-svc-6qg7x [751.190532ms]
Mar 23 19:36:39.788: INFO: Created: latency-svc-z4kbp
Mar 23 19:36:39.813: INFO: Got endpoints: latency-svc-rcshn [748.619038ms]
Mar 23 19:36:39.826: INFO: Created: latency-svc-lhvmx
Mar 23 19:36:39.872: INFO: Got endpoints: latency-svc-vbmkl [759.898411ms]
Mar 23 19:36:39.893: INFO: Created: latency-svc-gqccc
Mar 23 19:36:39.922: INFO: Got endpoints: latency-svc-8d9h9 [749.758335ms]
Mar 23 19:36:39.987: INFO: Created: latency-svc-sz2wf
Mar 23 19:36:39.988: INFO: Got endpoints: latency-svc-m86st [770.106087ms]
Mar 23 19:36:40.006: INFO: Created: latency-svc-d2c4v
Mar 23 19:36:40.015: INFO: Got endpoints: latency-svc-rtjv7 [749.439736ms]
Mar 23 19:36:40.042: INFO: Created: latency-svc-lxt2v
Mar 23 19:36:40.063: INFO: Got endpoints: latency-svc-hg2f9 [749.807735ms]
Mar 23 19:36:40.078: INFO: Created: latency-svc-9r22h
Mar 23 19:36:40.119: INFO: Got endpoints: latency-svc-xzphx [750.196334ms]
Mar 23 19:36:40.135: INFO: Created: latency-svc-sn2w5
Mar 23 19:36:40.164: INFO: Got endpoints: latency-svc-zvw75 [747.69554ms]
Mar 23 19:36:40.178: INFO: Created: latency-svc-t2khm
Mar 23 19:36:40.211: INFO: Got endpoints: latency-svc-4jfcg [748.675838ms]
Mar 23 19:36:40.227: INFO: Created: latency-svc-mc4bq
Mar 23 19:36:40.265: INFO: Got endpoints: latency-svc-86nzj [737.425865ms]
Mar 23 19:36:40.290: INFO: Created: latency-svc-z84kz
Mar 23 19:36:40.313: INFO: Got endpoints: latency-svc-x6tzm [749.438536ms]
Mar 23 19:36:40.326: INFO: Created: latency-svc-7s9kl
Mar 23 19:36:40.365: INFO: Got endpoints: latency-svc-rz5n5 [745.088847ms]
Mar 23 19:36:40.379: INFO: Created: latency-svc-68lrp
Mar 23 19:36:40.415: INFO: Got endpoints: latency-svc-dplvj [746.615642ms]
Mar 23 19:36:40.437: INFO: Created: latency-svc-ln968
Mar 23 19:36:40.465: INFO: Got endpoints: latency-svc-z4tqw [753.157927ms]
Mar 23 19:36:40.480: INFO: Created: latency-svc-2zxk5
Mar 23 19:36:40.514: INFO: Got endpoints: latency-svc-z4kbp [748.532538ms]
Mar 23 19:36:40.528: INFO: Created: latency-svc-m7z5z
Mar 23 19:36:40.567: INFO: Got endpoints: latency-svc-lhvmx [753.586826ms]
Mar 23 19:36:40.582: INFO: Created: latency-svc-pd76n
Mar 23 19:36:40.612: INFO: Got endpoints: latency-svc-gqccc [739.53766ms]
Mar 23 19:36:40.623: INFO: Created: latency-svc-ll7vf
Mar 23 19:36:40.667: INFO: Got endpoints: latency-svc-sz2wf [745.304446ms]
Mar 23 19:36:40.682: INFO: Created: latency-svc-bxqcz
Mar 23 19:36:40.712: INFO: Got endpoints: latency-svc-d2c4v [724.864994ms]
Mar 23 19:36:40.728: INFO: Created: latency-svc-g8vzt
Mar 23 19:36:40.763: INFO: Got endpoints: latency-svc-lxt2v [747.75984ms]
Mar 23 19:36:40.776: INFO: Created: latency-svc-4pgbg
Mar 23 19:36:40.811: INFO: Got endpoints: latency-svc-9r22h [748.03274ms]
Mar 23 19:36:40.824: INFO: Created: latency-svc-h66kh
Mar 23 19:36:40.867: INFO: Got endpoints: latency-svc-sn2w5 [747.64144ms]
Mar 23 19:36:40.904: INFO: Created: latency-svc-7p9gd
Mar 23 19:36:40.916: INFO: Got endpoints: latency-svc-t2khm [751.140932ms]
Mar 23 19:36:40.933: INFO: Created: latency-svc-jvq7p
Mar 23 19:36:40.980: INFO: Got endpoints: latency-svc-mc4bq [768.438091ms]
Mar 23 19:36:41.002: INFO: Created: latency-svc-fvlws
Mar 23 19:36:41.017: INFO: Got endpoints: latency-svc-z84kz [751.65793ms]
Mar 23 19:36:41.034: INFO: Created: latency-svc-5fv98
Mar 23 19:36:41.063: INFO: Got endpoints: latency-svc-7s9kl [749.239737ms]
Mar 23 19:36:41.082: INFO: Created: latency-svc-k2c97
Mar 23 19:36:41.113: INFO: Got endpoints: latency-svc-68lrp [747.98284ms]
Mar 23 19:36:41.132: INFO: Created: latency-svc-q4gw4
Mar 23 19:36:41.165: INFO: Got endpoints: latency-svc-ln968 [749.486035ms]
Mar 23 19:36:41.177: INFO: Created: latency-svc-kfzsf
Mar 23 19:36:41.215: INFO: Got endpoints: latency-svc-2zxk5 [749.582036ms]
Mar 23 19:36:41.232: INFO: Created: latency-svc-kn24n
Mar 23 19:36:41.271: INFO: Got endpoints: latency-svc-m7z5z [756.908918ms]
Mar 23 19:36:41.304: INFO: Created: latency-svc-5xpvs
Mar 23 19:36:41.332: INFO: Got endpoints: latency-svc-pd76n [765.276698ms]
Mar 23 19:36:41.349: INFO: Created: latency-svc-np6ks
Mar 23 19:36:41.371: INFO: Got endpoints: latency-svc-ll7vf [759.021513ms]
Mar 23 19:36:41.382: INFO: Created: latency-svc-7tnrn
Mar 23 19:36:41.412: INFO: Got endpoints: latency-svc-bxqcz [744.640547ms]
Mar 23 19:36:41.427: INFO: Created: latency-svc-l4v5d
Mar 23 19:36:41.463: INFO: Got endpoints: latency-svc-g8vzt [749.985635ms]
Mar 23 19:36:41.475: INFO: Created: latency-svc-prlp7
Mar 23 19:36:41.512: INFO: Got endpoints: latency-svc-4pgbg [749.505836ms]
Mar 23 19:36:41.524: INFO: Created: latency-svc-xgbjt
Mar 23 19:36:41.563: INFO: Got endpoints: latency-svc-h66kh [751.664331ms]
Mar 23 19:36:41.578: INFO: Created: latency-svc-glh9m
Mar 23 19:36:41.621: INFO: Got endpoints: latency-svc-7p9gd [753.440127ms]
Mar 23 19:36:41.632: INFO: Created: latency-svc-gwdlz
Mar 23 19:36:41.663: INFO: Got endpoints: latency-svc-jvq7p [747.350041ms]
Mar 23 19:36:41.677: INFO: Created: latency-svc-sj6lm
Mar 23 19:36:41.712: INFO: Got endpoints: latency-svc-fvlws [731.527778ms]
Mar 23 19:36:41.726: INFO: Created: latency-svc-74wpk
Mar 23 19:36:41.762: INFO: Got endpoints: latency-svc-5fv98 [744.819747ms]
Mar 23 19:36:41.793: INFO: Created: latency-svc-6t4vr
Mar 23 19:36:41.815: INFO: Got endpoints: latency-svc-k2c97 [751.87323ms]
Mar 23 19:36:41.828: INFO: Created: latency-svc-99mff
Mar 23 19:36:41.864: INFO: Got endpoints: latency-svc-q4gw4 [750.859833ms]
Mar 23 19:36:41.887: INFO: Created: latency-svc-d8vn9
Mar 23 19:36:41.918: INFO: Got endpoints: latency-svc-kfzsf [753.201828ms]
Mar 23 19:36:41.961: INFO: Created: latency-svc-6xxfq
Mar 23 19:36:41.982: INFO: Got endpoints: latency-svc-kn24n [766.635595ms]
Mar 23 19:36:42.003: INFO: Created: latency-svc-qh5rb
Mar 23 19:36:42.021: INFO: Got endpoints: latency-svc-5xpvs [750.115034ms]
Mar 23 19:36:42.057: INFO: Created: latency-svc-9jlnj
Mar 23 19:36:42.065: INFO: Got endpoints: latency-svc-np6ks [732.856075ms]
Mar 23 19:36:42.103: INFO: Created: latency-svc-842qs
Mar 23 19:36:42.117: INFO: Got endpoints: latency-svc-7tnrn [746.350543ms]
Mar 23 19:36:42.131: INFO: Created: latency-svc-xmst2
Mar 23 19:36:42.164: INFO: Got endpoints: latency-svc-l4v5d [751.97703ms]
Mar 23 19:36:42.178: INFO: Created: latency-svc-w7s4g
Mar 23 19:36:42.214: INFO: Got endpoints: latency-svc-prlp7 [751.73463ms]
Mar 23 19:36:42.234: INFO: Created: latency-svc-wdfdz
Mar 23 19:36:42.264: INFO: Got endpoints: latency-svc-xgbjt [752.01533ms]
Mar 23 19:36:42.287: INFO: Created: latency-svc-xpzwl
Mar 23 19:36:42.315: INFO: Got endpoints: latency-svc-glh9m [752.08853ms]
Mar 23 19:36:42.332: INFO: Created: latency-svc-rsjqk
Mar 23 19:36:42.369: INFO: Got endpoints: latency-svc-gwdlz [747.72634ms]
Mar 23 19:36:42.381: INFO: Created: latency-svc-25bxc
Mar 23 19:36:42.413: INFO: Got endpoints: latency-svc-sj6lm [749.667136ms]
Mar 23 19:36:42.425: INFO: Created: latency-svc-ppq6b
Mar 23 19:36:42.466: INFO: Got endpoints: latency-svc-74wpk [753.724925ms]
Mar 23 19:36:42.481: INFO: Created: latency-svc-jx28f
Mar 23 19:36:42.514: INFO: Got endpoints: latency-svc-6t4vr [752.576629ms]
Mar 23 19:36:42.553: INFO: Created: latency-svc-xqf4m
Mar 23 19:36:42.564: INFO: Got endpoints: latency-svc-99mff [748.431639ms]
Mar 23 19:36:42.592: INFO: Created: latency-svc-h9ccp
Mar 23 19:36:42.619: INFO: Got endpoints: latency-svc-d8vn9 [755.243622ms]
Mar 23 19:36:42.656: INFO: Created: latency-svc-f5zmk
Mar 23 19:36:42.670: INFO: Got endpoints: latency-svc-6xxfq [752.29813ms]
Mar 23 19:36:42.686: INFO: Created: latency-svc-gb729
Mar 23 19:36:42.720: INFO: Got endpoints: latency-svc-qh5rb [737.609964ms]
Mar 23 19:36:42.739: INFO: Created: latency-svc-4mjnp
Mar 23 19:36:42.774: INFO: Got endpoints: latency-svc-9jlnj [753.184627ms]
Mar 23 19:36:42.792: INFO: Created: latency-svc-gntcp
Mar 23 19:36:42.813: INFO: Got endpoints: latency-svc-842qs [748.383539ms]
Mar 23 19:36:42.829: INFO: Created: latency-svc-6xdpp
Mar 23 19:36:42.870: INFO: Got endpoints: latency-svc-xmst2 [752.416629ms]
Mar 23 19:36:42.912: INFO: Created: latency-svc-sbgm9
Mar 23 19:36:42.924: INFO: Got endpoints: latency-svc-w7s4g [760.080805ms]
Mar 23 19:36:42.947: INFO: Created: latency-svc-gs4p6
Mar 23 19:36:42.979: INFO: Got endpoints: latency-svc-wdfdz [764.772377ms]
Mar 23 19:36:43.002: INFO: Created: latency-svc-vkxt8
Mar 23 19:36:43.023: INFO: Got endpoints: latency-svc-xpzwl [758.11458ms]
Mar 23 19:36:43.049: INFO: Created: latency-svc-7xxjp
Mar 23 19:36:43.101: INFO: Got endpoints: latency-svc-rsjqk [785.471093ms]
Mar 23 19:36:43.115: INFO: Got endpoints: latency-svc-25bxc [745.558183ms]
Mar 23 19:36:43.121: INFO: Created: latency-svc-7xrlz
Mar 23 19:36:43.130: INFO: Created: latency-svc-58jtr
Mar 23 19:36:43.162: INFO: Got endpoints: latency-svc-ppq6b [747.898965ms]
Mar 23 19:36:43.173: INFO: Created: latency-svc-dgwls
Mar 23 19:36:43.213: INFO: Got endpoints: latency-svc-jx28f [747.87565ms]
Mar 23 19:36:43.227: INFO: Created: latency-svc-k9h7s
Mar 23 19:36:43.261: INFO: Got endpoints: latency-svc-xqf4m [747.048138ms]
Mar 23 19:36:43.272: INFO: Created: latency-svc-nzljz
Mar 23 19:36:43.312: INFO: Got endpoints: latency-svc-h9ccp [747.899721ms]
Mar 23 19:36:43.323: INFO: Created: latency-svc-cm2vg
Mar 23 19:36:43.362: INFO: Got endpoints: latency-svc-f5zmk [742.363419ms]
Mar 23 19:36:43.379: INFO: Created: latency-svc-t4659
Mar 23 19:36:43.412: INFO: Got endpoints: latency-svc-gb729 [741.429306ms]
Mar 23 19:36:43.424: INFO: Created: latency-svc-ngjdx
Mar 23 19:36:43.463: INFO: Got endpoints: latency-svc-4mjnp [743.162788ms]
Mar 23 19:36:43.477: INFO: Created: latency-svc-s946d
Mar 23 19:36:43.520: INFO: Got endpoints: latency-svc-gntcp [745.233766ms]
Mar 23 19:36:43.534: INFO: Created: latency-svc-67fhr
Mar 23 19:36:43.564: INFO: Got endpoints: latency-svc-6xdpp [749.912842ms]
Mar 23 19:36:43.582: INFO: Created: latency-svc-hmxqp
Mar 23 19:36:43.614: INFO: Got endpoints: latency-svc-sbgm9 [744.009442ms]
Mar 23 19:36:43.633: INFO: Created: latency-svc-bbb69
Mar 23 19:36:43.672: INFO: Got endpoints: latency-svc-gs4p6 [746.771325ms]
Mar 23 19:36:43.685: INFO: Created: latency-svc-4kb77
Mar 23 19:36:43.713: INFO: Got endpoints: latency-svc-vkxt8 [732.593062ms]
Mar 23 19:36:43.762: INFO: Got endpoints: latency-svc-7xxjp [739.778743ms]
Mar 23 19:36:43.816: INFO: Got endpoints: latency-svc-7xrlz [714.66551ms]
Mar 23 19:36:43.865: INFO: Got endpoints: latency-svc-58jtr [750.225515ms]
Mar 23 19:36:43.922: INFO: Got endpoints: latency-svc-dgwls [760.806888ms]
Mar 23 19:36:43.982: INFO: Got endpoints: latency-svc-k9h7s [768.827767ms]
Mar 23 19:36:44.027: INFO: Got endpoints: latency-svc-nzljz [765.087377ms]
Mar 23 19:36:44.077: INFO: Got endpoints: latency-svc-cm2vg [765.536675ms]
Mar 23 19:36:44.115: INFO: Got endpoints: latency-svc-t4659 [753.040808ms]
Mar 23 19:36:44.168: INFO: Got endpoints: latency-svc-ngjdx [755.8847ms]
Mar 23 19:36:44.213: INFO: Got endpoints: latency-svc-s946d [750.270816ms]
Mar 23 19:36:44.264: INFO: Got endpoints: latency-svc-67fhr [744.101532ms]
Mar 23 19:36:44.313: INFO: Got endpoints: latency-svc-hmxqp [748.959419ms]
Mar 23 19:36:44.361: INFO: Got endpoints: latency-svc-bbb69 [747.232224ms]
Mar 23 19:36:44.411: INFO: Got endpoints: latency-svc-4kb77 [738.768746ms]
Mar 23 19:36:44.412: INFO: Latencies: [102.016303ms 135.788339ms 136.778734ms 141.160413ms 155.634543ms 157.136136ms 157.363934ms 171.022268ms 183.393808ms 227.260294ms 233.025166ms 233.429264ms 241.388625ms 245.018707ms 248.233292ms 252.234073ms 254.75626ms 270.056386ms 272.062476ms 288.089798ms 291.83608ms 321.915434ms 324.154522ms 336.836661ms 353.222481ms 361.378341ms 363.591331ms 363.73453ms 377.845561ms 385.370624ms 388.49521ms 388.815308ms 398.071963ms 401.906244ms 403.340837ms 404.290932ms 409.842306ms 410.142504ms 414.029485ms 419.225359ms 421.425049ms 424.971532ms 426.237426ms 429.43631ms 434.397686ms 438.284767ms 441.70555ms 441.88255ms 447.340201ms 454.219289ms 456.294779ms 459.054465ms 460.192562ms 460.844957ms 461.043856ms 461.962152ms 463.844343ms 469.337016ms 471.598105ms 473.158397ms 473.395496ms 488.385523ms 488.726027ms 508.283226ms 511.797142ms 524.450547ms 530.461018ms 547.467488ms 564.369119ms 584.873224ms 634.930848ms 668.092064ms 699.346239ms 714.66551ms 724.864994ms 728.832584ms 731.527778ms 732.593062ms 732.856075ms 733.215575ms 733.652086ms 734.80137ms 737.425865ms 737.609964ms 738.768746ms 739.13906ms 739.53766ms 739.778743ms 740.242758ms 740.655831ms 741.429306ms 741.977554ms 742.363419ms 743.162788ms 743.273751ms 743.338651ms 744.009442ms 744.023049ms 744.101532ms 744.640547ms 744.819747ms 744.909146ms 745.028646ms 745.088847ms 745.233766ms 745.304446ms 745.558183ms 746.350543ms 746.615642ms 746.771325ms 747.048138ms 747.232224ms 747.321541ms 747.350041ms 747.64144ms 747.69554ms 747.72634ms 747.75984ms 747.87565ms 747.898965ms 747.899721ms 747.98284ms 748.03274ms 748.236439ms 748.383539ms 748.431639ms 748.532538ms 748.619038ms 748.675838ms 748.959419ms 749.239737ms 749.438536ms 749.439736ms 749.486035ms 749.505836ms 749.582036ms 749.667136ms 749.758335ms 749.807735ms 749.912842ms 749.985635ms 750.115034ms 750.196334ms 750.225515ms 750.270816ms 750.327334ms 750.489033ms 750.524708ms 750.859833ms 750.963932ms 751.099532ms 751.140932ms 751.190532ms 751.299745ms 751.65793ms 751.664331ms 751.73463ms 751.87323ms 751.97703ms 751.99753ms 752.01533ms 752.08853ms 752.29813ms 752.416629ms 752.530628ms 752.576629ms 753.040808ms 753.122228ms 753.157927ms 753.184627ms 753.201828ms 753.440127ms 753.586826ms 753.724925ms 753.738926ms 754.001725ms 755.243622ms 755.265922ms 755.869821ms 755.8847ms 755.91982ms 756.17175ms 756.908918ms 758.11458ms 758.681315ms 759.021513ms 759.898411ms 760.080805ms 760.806888ms 764.772377ms 765.087377ms 765.276698ms 765.536675ms 766.635595ms 767.594293ms 768.438091ms 768.827767ms 770.106087ms 778.249668ms 785.471093ms]
Mar 23 19:36:44.412: INFO: 50 %ile: 744.819747ms
Mar 23 19:36:44.413: INFO: 90 %ile: 755.91982ms
Mar 23 19:36:44.413: INFO: 99 %ile: 778.249668ms
Mar 23 19:36:44.413: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Mar 23 19:36:44.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-856" for this suite. 03/23/23 19:36:44.424
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":196,"skipped":3548,"failed":0}
------------------------------
• [SLOW TEST] [10.821 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:33.612
    Mar 23 19:36:33.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svc-latency 03/23/23 19:36:33.613
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:33.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:33.643
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Mar 23 19:36:33.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-856 03/23/23 19:36:33.651
    I0323 19:36:33.657545      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-856, replica count: 1
    I0323 19:36:34.709003      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 19:36:35.709375      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:36:35.831: INFO: Created: latency-svc-cfwdg
    Mar 23 19:36:35.859: INFO: Got endpoints: latency-svc-cfwdg [49.701358ms]
    Mar 23 19:36:35.908: INFO: Created: latency-svc-7lsd5
    Mar 23 19:36:35.920: INFO: Created: latency-svc-57txd
    Mar 23 19:36:35.972: INFO: Created: latency-svc-qhmqz
    Mar 23 19:36:35.990: INFO: Created: latency-svc-zlm7s
    Mar 23 19:36:35.997: INFO: Got endpoints: latency-svc-7lsd5 [136.778734ms]
    Mar 23 19:36:35.998: INFO: Got endpoints: latency-svc-57txd [135.788339ms]
    Mar 23 19:36:36.003: INFO: Got endpoints: latency-svc-qhmqz [141.160413ms]
    Mar 23 19:36:36.019: INFO: Got endpoints: latency-svc-zlm7s [157.136136ms]
    Mar 23 19:36:36.028: INFO: Created: latency-svc-4f76q
    Mar 23 19:36:36.045: INFO: Got endpoints: latency-svc-4f76q [183.393808ms]
    Mar 23 19:36:36.081: INFO: Created: latency-svc-r4xrm
    Mar 23 19:36:36.093: INFO: Created: latency-svc-qfkwk
    Mar 23 19:36:36.103: INFO: Got endpoints: latency-svc-r4xrm [241.388625ms]
    Mar 23 19:36:36.107: INFO: Got endpoints: latency-svc-qfkwk [245.018707ms]
    Mar 23 19:36:36.116: INFO: Created: latency-svc-vflxj
    Mar 23 19:36:36.132: INFO: Got endpoints: latency-svc-vflxj [270.056386ms]
    Mar 23 19:36:36.136: INFO: Created: latency-svc-2zmcr
    Mar 23 19:36:36.182: INFO: Got endpoints: latency-svc-2zmcr [321.915434ms]
    Mar 23 19:36:36.187: INFO: Created: latency-svc-k8bpz
    Mar 23 19:36:36.199: INFO: Got endpoints: latency-svc-k8bpz [336.836661ms]
    Mar 23 19:36:36.210: INFO: Created: latency-svc-2vzlq
    Mar 23 19:36:36.226: INFO: Got endpoints: latency-svc-2vzlq [363.73453ms]
    Mar 23 19:36:36.237: INFO: Created: latency-svc-5n9gv
    Mar 23 19:36:36.247: INFO: Got endpoints: latency-svc-5n9gv [385.370624ms]
    Mar 23 19:36:36.252: INFO: Created: latency-svc-rfssl
    Mar 23 19:36:36.272: INFO: Got endpoints: latency-svc-rfssl [409.842306ms]
    Mar 23 19:36:36.291: INFO: Created: latency-svc-s852h
    Mar 23 19:36:36.298: INFO: Created: latency-svc-qjbcb
    Mar 23 19:36:36.322: INFO: Created: latency-svc-vbfqt
    Mar 23 19:36:36.323: INFO: Got endpoints: latency-svc-s852h [460.844957ms]
    Mar 23 19:36:36.323: INFO: Got endpoints: latency-svc-qjbcb [461.043856ms]
    Mar 23 19:36:36.351: INFO: Got endpoints: latency-svc-vbfqt [353.222481ms]
    Mar 23 19:36:36.352: INFO: Created: latency-svc-l6hp6
    Mar 23 19:36:36.386: INFO: Got endpoints: latency-svc-l6hp6 [388.49521ms]
    Mar 23 19:36:36.401: INFO: Created: latency-svc-np5z7
    Mar 23 19:36:36.429: INFO: Got endpoints: latency-svc-np5z7 [426.237426ms]
    Mar 23 19:36:36.431: INFO: Created: latency-svc-dtdf4
    Mar 23 19:36:36.491: INFO: Got endpoints: latency-svc-dtdf4 [471.598105ms]
    Mar 23 19:36:36.501: INFO: Created: latency-svc-b62d6
    Mar 23 19:36:36.533: INFO: Got endpoints: latency-svc-b62d6 [488.385523ms]
    Mar 23 19:36:36.547: INFO: Created: latency-svc-tffpw
    Mar 23 19:36:36.558: INFO: Got endpoints: latency-svc-tffpw [454.219289ms]
    Mar 23 19:36:36.563: INFO: Created: latency-svc-lb7tm
    Mar 23 19:36:36.577: INFO: Got endpoints: latency-svc-lb7tm [469.337016ms]
    Mar 23 19:36:36.590: INFO: Created: latency-svc-5gdlj
    Mar 23 19:36:36.594: INFO: Got endpoints: latency-svc-5gdlj [461.962152ms]
    Mar 23 19:36:36.601: INFO: Created: latency-svc-m6kwp
    Mar 23 19:36:36.609: INFO: Created: latency-svc-sl8q2
    Mar 23 19:36:36.641: INFO: Got endpoints: latency-svc-sl8q2 [441.88255ms]
    Mar 23 19:36:36.641: INFO: Got endpoints: latency-svc-m6kwp [459.054465ms]
    Mar 23 19:36:36.644: INFO: Created: latency-svc-wnppl
    Mar 23 19:36:36.664: INFO: Got endpoints: latency-svc-wnppl [438.284767ms]
    Mar 23 19:36:36.669: INFO: Created: latency-svc-t5grt
    Mar 23 19:36:36.682: INFO: Got endpoints: latency-svc-t5grt [434.397686ms]
    Mar 23 19:36:36.683: INFO: Created: latency-svc-bpxm9
    Mar 23 19:36:36.701: INFO: Got endpoints: latency-svc-bpxm9 [429.43631ms]
    Mar 23 19:36:36.708: INFO: Created: latency-svc-6vfjb
    Mar 23 19:36:36.726: INFO: Got endpoints: latency-svc-6vfjb [403.340837ms]
    Mar 23 19:36:36.727: INFO: Created: latency-svc-bkhpz
    Mar 23 19:36:36.733: INFO: Got endpoints: latency-svc-bkhpz [410.142504ms]
    Mar 23 19:36:36.741: INFO: Created: latency-svc-8ww4t
    Mar 23 19:36:36.755: INFO: Got endpoints: latency-svc-8ww4t [404.290932ms]
    Mar 23 19:36:36.766: INFO: Created: latency-svc-kgqz9
    Mar 23 19:36:36.775: INFO: Got endpoints: latency-svc-kgqz9 [388.815308ms]
    Mar 23 19:36:36.785: INFO: Created: latency-svc-c2kp4
    Mar 23 19:36:36.793: INFO: Got endpoints: latency-svc-c2kp4 [363.591331ms]
    Mar 23 19:36:36.802: INFO: Created: latency-svc-hxzjx
    Mar 23 19:36:36.815: INFO: Got endpoints: latency-svc-hxzjx [324.154522ms]
    Mar 23 19:36:36.967: INFO: Created: latency-svc-l6wjp
    Mar 23 19:36:36.997: INFO: Got endpoints: latency-svc-l6wjp [463.844343ms]
    Mar 23 19:36:37.029: INFO: Created: latency-svc-rss5q
    Mar 23 19:36:37.029: INFO: Created: latency-svc-nfxql
    Mar 23 19:36:37.030: INFO: Created: latency-svc-5k2pw
    Mar 23 19:36:37.030: INFO: Created: latency-svc-q5t9f
    Mar 23 19:36:37.030: INFO: Created: latency-svc-w7x2p
    Mar 23 19:36:37.030: INFO: Created: latency-svc-ng2gr
    Mar 23 19:36:37.030: INFO: Created: latency-svc-79826
    Mar 23 19:36:37.031: INFO: Created: latency-svc-9g9r9
    Mar 23 19:36:37.032: INFO: Created: latency-svc-hlxpl
    Mar 23 19:36:37.032: INFO: Created: latency-svc-f296j
    Mar 23 19:36:37.033: INFO: Created: latency-svc-lgt97
    Mar 23 19:36:37.033: INFO: Created: latency-svc-qz4wf
    Mar 23 19:36:37.034: INFO: Created: latency-svc-jzr9x
    Mar 23 19:36:37.034: INFO: Created: latency-svc-dnlcb
    Mar 23 19:36:37.065: INFO: Got endpoints: latency-svc-lgt97 [272.062476ms]
    Mar 23 19:36:37.088: INFO: Got endpoints: latency-svc-f296j [530.461018ms]
    Mar 23 19:36:37.101: INFO: Got endpoints: latency-svc-jzr9x [524.450547ms]
    Mar 23 19:36:37.102: INFO: Got endpoints: latency-svc-hlxpl [508.283226ms]
    Mar 23 19:36:37.103: INFO: Got endpoints: latency-svc-rss5q [288.089798ms]
    Mar 23 19:36:37.114: INFO: Got endpoints: latency-svc-9g9r9 [473.395496ms]
    Mar 23 19:36:37.114: INFO: Got endpoints: latency-svc-79826 [473.158397ms]
    Mar 23 19:36:37.115: INFO: Created: latency-svc-glhmv
    Mar 23 19:36:37.120: INFO: Got endpoints: latency-svc-ng2gr [456.294779ms]
    Mar 23 19:36:37.123: INFO: Got endpoints: latency-svc-w7x2p [441.70555ms]
    Mar 23 19:36:37.127: INFO: Got endpoints: latency-svc-q5t9f [424.971532ms]
    Mar 23 19:36:37.129: INFO: Created: latency-svc-c29nq
    Mar 23 19:36:37.140: INFO: Got endpoints: latency-svc-5k2pw [414.029485ms]
    Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-dnlcb [419.225359ms]
    Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-glhmv [155.634543ms]
    Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-nfxql [377.845561ms]
    Mar 23 19:36:37.153: INFO: Got endpoints: latency-svc-qz4wf [398.071963ms]
    Mar 23 19:36:37.167: INFO: Got endpoints: latency-svc-c29nq [102.016303ms]
    Mar 23 19:36:37.302: INFO: Created: latency-svc-8psfq
    Mar 23 19:36:37.303: INFO: Created: latency-svc-k2h8m
    Mar 23 19:36:37.305: INFO: Created: latency-svc-swwfx
    Mar 23 19:36:37.315: INFO: Got endpoints: latency-svc-swwfx [227.260294ms]
    Mar 23 19:36:37.324: INFO: Got endpoints: latency-svc-k2h8m [171.022268ms]
    Mar 23 19:36:37.325: INFO: Got endpoints: latency-svc-8psfq [157.363934ms]
    Mar 23 19:36:37.325: INFO: Created: latency-svc-xcxfs
    Mar 23 19:36:37.333: INFO: Created: latency-svc-c795r
    Mar 23 19:36:37.333: INFO: Created: latency-svc-qqcj4
    Mar 23 19:36:37.333: INFO: Created: latency-svc-2w5z9
    Mar 23 19:36:37.333: INFO: Created: latency-svc-bpwkj
    Mar 23 19:36:37.334: INFO: Created: latency-svc-h25zq
    Mar 23 19:36:37.334: INFO: Created: latency-svc-lwshs
    Mar 23 19:36:37.334: INFO: Created: latency-svc-z2fgl
    Mar 23 19:36:37.334: INFO: Created: latency-svc-x5w7q
    Mar 23 19:36:37.334: INFO: Got endpoints: latency-svc-x5w7q [233.025166ms]
    Mar 23 19:36:37.334: INFO: Created: latency-svc-qxpk5
    Mar 23 19:36:37.334: INFO: Created: latency-svc-9tnf5
    Mar 23 19:36:37.335: INFO: Created: latency-svc-hrgnw
    Mar 23 19:36:37.348: INFO: Got endpoints: latency-svc-xcxfs [233.429264ms]
    Mar 23 19:36:37.351: INFO: Got endpoints: latency-svc-z2fgl [248.233292ms]
    Mar 23 19:36:37.362: INFO: Created: latency-svc-l4kz4
    Mar 23 19:36:37.367: INFO: Got endpoints: latency-svc-h25zq [252.234073ms]
    Mar 23 19:36:37.378: INFO: Got endpoints: latency-svc-9tnf5 [254.75626ms]
    Mar 23 19:36:37.385: INFO: Created: latency-svc-jhptg
    Mar 23 19:36:37.401: INFO: Created: latency-svc-tsp6q
    Mar 23 19:36:37.418: INFO: Created: latency-svc-p59nn
    Mar 23 19:36:37.418: INFO: Got endpoints: latency-svc-bpwkj [291.83608ms]
    Mar 23 19:36:37.438: INFO: Created: latency-svc-sss4d
    Mar 23 19:36:37.452: INFO: Created: latency-svc-k2snh
    Mar 23 19:36:37.465: INFO: Got endpoints: latency-svc-lwshs [361.378341ms]
    Mar 23 19:36:37.474: INFO: Created: latency-svc-zcxpp
    Mar 23 19:36:37.488: INFO: Created: latency-svc-58g74
    Mar 23 19:36:37.498: INFO: Created: latency-svc-hpwxd
    Mar 23 19:36:37.522: INFO: Got endpoints: latency-svc-hrgnw [401.906244ms]
    Mar 23 19:36:37.536: INFO: Created: latency-svc-xfwbh
    Mar 23 19:36:37.549: INFO: Created: latency-svc-qzb5w
    Mar 23 19:36:37.562: INFO: Got endpoints: latency-svc-2w5z9 [421.425049ms]
    Mar 23 19:36:37.585: INFO: Created: latency-svc-njpfj
    Mar 23 19:36:37.613: INFO: Got endpoints: latency-svc-c795r [460.192562ms]
    Mar 23 19:36:37.629: INFO: Created: latency-svc-cv67d
    Mar 23 19:36:37.665: INFO: Got endpoints: latency-svc-qxpk5 [511.797142ms]
    Mar 23 19:36:37.682: INFO: Created: latency-svc-mjmbj
    Mar 23 19:36:37.718: INFO: Got endpoints: latency-svc-qqcj4 [564.369119ms]
    Mar 23 19:36:37.734: INFO: Created: latency-svc-4zv7w
    Mar 23 19:36:37.763: INFO: Got endpoints: latency-svc-l4kz4 [447.340201ms]
    Mar 23 19:36:37.781: INFO: Created: latency-svc-g7m7j
    Mar 23 19:36:37.813: INFO: Got endpoints: latency-svc-jhptg [488.726027ms]
    Mar 23 19:36:37.827: INFO: Created: latency-svc-dkrpk
    Mar 23 19:36:37.872: INFO: Got endpoints: latency-svc-tsp6q [547.467488ms]
    Mar 23 19:36:37.898: INFO: Created: latency-svc-lknz5
    Mar 23 19:36:37.919: INFO: Got endpoints: latency-svc-p59nn [584.873224ms]
    Mar 23 19:36:37.951: INFO: Created: latency-svc-nqhk4
    Mar 23 19:36:37.986: INFO: Got endpoints: latency-svc-sss4d [634.930848ms]
    Mar 23 19:36:38.008: INFO: Created: latency-svc-74wkq
    Mar 23 19:36:38.016: INFO: Got endpoints: latency-svc-k2snh [668.092064ms]
    Mar 23 19:36:38.037: INFO: Created: latency-svc-7qrcc
    Mar 23 19:36:38.067: INFO: Got endpoints: latency-svc-zcxpp [699.346239ms]
    Mar 23 19:36:38.107: INFO: Created: latency-svc-72lsd
    Mar 23 19:36:38.112: INFO: Got endpoints: latency-svc-58g74 [733.652086ms]
    Mar 23 19:36:38.137: INFO: Created: latency-svc-cn8cw
    Mar 23 19:36:38.170: INFO: Got endpoints: latency-svc-hpwxd [751.299745ms]
    Mar 23 19:36:38.182: INFO: Created: latency-svc-24gnj
    Mar 23 19:36:38.221: INFO: Got endpoints: latency-svc-xfwbh [756.17175ms]
    Mar 23 19:36:38.231: INFO: Created: latency-svc-gwp2h
    Mar 23 19:36:38.263: INFO: Got endpoints: latency-svc-qzb5w [740.655831ms]
    Mar 23 19:36:38.276: INFO: Created: latency-svc-plxkg
    Mar 23 19:36:38.313: INFO: Got endpoints: latency-svc-njpfj [750.524708ms]
    Mar 23 19:36:38.331: INFO: Created: latency-svc-sfmtt
    Mar 23 19:36:38.364: INFO: Got endpoints: latency-svc-cv67d [751.099532ms]
    Mar 23 19:36:38.377: INFO: Created: latency-svc-7mn9c
    Mar 23 19:36:38.433: INFO: Got endpoints: latency-svc-mjmbj [767.594293ms]
    Mar 23 19:36:38.452: INFO: Created: latency-svc-pjwfg
    Mar 23 19:36:38.462: INFO: Got endpoints: latency-svc-4zv7w [744.023049ms]
    Mar 23 19:36:38.477: INFO: Created: latency-svc-ds5fb
    Mar 23 19:36:38.511: INFO: Got endpoints: latency-svc-g7m7j [748.236439ms]
    Mar 23 19:36:38.527: INFO: Created: latency-svc-vszrv
    Mar 23 19:36:38.570: INFO: Got endpoints: latency-svc-dkrpk [755.91982ms]
    Mar 23 19:36:38.593: INFO: Created: latency-svc-qdwvr
    Mar 23 19:36:38.616: INFO: Got endpoints: latency-svc-lknz5 [741.977554ms]
    Mar 23 19:36:38.631: INFO: Created: latency-svc-fmh2l
    Mar 23 19:36:38.666: INFO: Got endpoints: latency-svc-nqhk4 [745.028646ms]
    Mar 23 19:36:38.686: INFO: Created: latency-svc-fkd6m
    Mar 23 19:36:38.719: INFO: Got endpoints: latency-svc-74wkq [733.215575ms]
    Mar 23 19:36:38.737: INFO: Created: latency-svc-q8r5f
    Mar 23 19:36:38.769: INFO: Got endpoints: latency-svc-7qrcc [752.530628ms]
    Mar 23 19:36:38.789: INFO: Created: latency-svc-mgffx
    Mar 23 19:36:38.812: INFO: Got endpoints: latency-svc-72lsd [744.909146ms]
    Mar 23 19:36:38.837: INFO: Created: latency-svc-74wxq
    Mar 23 19:36:38.891: INFO: Got endpoints: latency-svc-cn8cw [778.249668ms]
    Mar 23 19:36:38.924: INFO: Got endpoints: latency-svc-24gnj [754.001725ms]
    Mar 23 19:36:38.964: INFO: Created: latency-svc-qnz6m
    Mar 23 19:36:38.977: INFO: Got endpoints: latency-svc-gwp2h [755.869821ms]
    Mar 23 19:36:38.982: INFO: Created: latency-svc-bdkws
    Mar 23 19:36:38.998: INFO: Created: latency-svc-p7nlx
    Mar 23 19:36:39.014: INFO: Got endpoints: latency-svc-plxkg [750.489033ms]
    Mar 23 19:36:39.036: INFO: Created: latency-svc-6qg7x
    Mar 23 19:36:39.064: INFO: Got endpoints: latency-svc-sfmtt [750.963932ms]
    Mar 23 19:36:39.079: INFO: Created: latency-svc-rcshn
    Mar 23 19:36:39.112: INFO: Got endpoints: latency-svc-7mn9c [747.321541ms]
    Mar 23 19:36:39.132: INFO: Created: latency-svc-vbmkl
    Mar 23 19:36:39.172: INFO: Got endpoints: latency-svc-pjwfg [739.13906ms]
    Mar 23 19:36:39.183: INFO: Created: latency-svc-8d9h9
    Mar 23 19:36:39.217: INFO: Got endpoints: latency-svc-ds5fb [755.265922ms]
    Mar 23 19:36:39.235: INFO: Created: latency-svc-m86st
    Mar 23 19:36:39.265: INFO: Got endpoints: latency-svc-vszrv [753.738926ms]
    Mar 23 19:36:39.286: INFO: Created: latency-svc-rtjv7
    Mar 23 19:36:39.313: INFO: Got endpoints: latency-svc-qdwvr [743.338651ms]
    Mar 23 19:36:39.330: INFO: Created: latency-svc-hg2f9
    Mar 23 19:36:39.369: INFO: Got endpoints: latency-svc-fmh2l [753.122228ms]
    Mar 23 19:36:39.382: INFO: Created: latency-svc-xzphx
    Mar 23 19:36:39.417: INFO: Got endpoints: latency-svc-fkd6m [750.327334ms]
    Mar 23 19:36:39.427: INFO: Created: latency-svc-zvw75
    Mar 23 19:36:39.462: INFO: Got endpoints: latency-svc-q8r5f [743.273751ms]
    Mar 23 19:36:39.483: INFO: Created: latency-svc-4jfcg
    Mar 23 19:36:39.528: INFO: Got endpoints: latency-svc-mgffx [758.681315ms]
    Mar 23 19:36:39.558: INFO: Created: latency-svc-86nzj
    Mar 23 19:36:39.564: INFO: Got endpoints: latency-svc-74wxq [751.99753ms]
    Mar 23 19:36:39.584: INFO: Created: latency-svc-x6tzm
    Mar 23 19:36:39.619: INFO: Got endpoints: latency-svc-qnz6m [728.832584ms]
    Mar 23 19:36:39.638: INFO: Created: latency-svc-rz5n5
    Mar 23 19:36:39.668: INFO: Got endpoints: latency-svc-bdkws [740.242758ms]
    Mar 23 19:36:39.679: INFO: Created: latency-svc-dplvj
    Mar 23 19:36:39.712: INFO: Got endpoints: latency-svc-p7nlx [734.80137ms]
    Mar 23 19:36:39.724: INFO: Created: latency-svc-z4tqw
    Mar 23 19:36:39.765: INFO: Got endpoints: latency-svc-6qg7x [751.190532ms]
    Mar 23 19:36:39.788: INFO: Created: latency-svc-z4kbp
    Mar 23 19:36:39.813: INFO: Got endpoints: latency-svc-rcshn [748.619038ms]
    Mar 23 19:36:39.826: INFO: Created: latency-svc-lhvmx
    Mar 23 19:36:39.872: INFO: Got endpoints: latency-svc-vbmkl [759.898411ms]
    Mar 23 19:36:39.893: INFO: Created: latency-svc-gqccc
    Mar 23 19:36:39.922: INFO: Got endpoints: latency-svc-8d9h9 [749.758335ms]
    Mar 23 19:36:39.987: INFO: Created: latency-svc-sz2wf
    Mar 23 19:36:39.988: INFO: Got endpoints: latency-svc-m86st [770.106087ms]
    Mar 23 19:36:40.006: INFO: Created: latency-svc-d2c4v
    Mar 23 19:36:40.015: INFO: Got endpoints: latency-svc-rtjv7 [749.439736ms]
    Mar 23 19:36:40.042: INFO: Created: latency-svc-lxt2v
    Mar 23 19:36:40.063: INFO: Got endpoints: latency-svc-hg2f9 [749.807735ms]
    Mar 23 19:36:40.078: INFO: Created: latency-svc-9r22h
    Mar 23 19:36:40.119: INFO: Got endpoints: latency-svc-xzphx [750.196334ms]
    Mar 23 19:36:40.135: INFO: Created: latency-svc-sn2w5
    Mar 23 19:36:40.164: INFO: Got endpoints: latency-svc-zvw75 [747.69554ms]
    Mar 23 19:36:40.178: INFO: Created: latency-svc-t2khm
    Mar 23 19:36:40.211: INFO: Got endpoints: latency-svc-4jfcg [748.675838ms]
    Mar 23 19:36:40.227: INFO: Created: latency-svc-mc4bq
    Mar 23 19:36:40.265: INFO: Got endpoints: latency-svc-86nzj [737.425865ms]
    Mar 23 19:36:40.290: INFO: Created: latency-svc-z84kz
    Mar 23 19:36:40.313: INFO: Got endpoints: latency-svc-x6tzm [749.438536ms]
    Mar 23 19:36:40.326: INFO: Created: latency-svc-7s9kl
    Mar 23 19:36:40.365: INFO: Got endpoints: latency-svc-rz5n5 [745.088847ms]
    Mar 23 19:36:40.379: INFO: Created: latency-svc-68lrp
    Mar 23 19:36:40.415: INFO: Got endpoints: latency-svc-dplvj [746.615642ms]
    Mar 23 19:36:40.437: INFO: Created: latency-svc-ln968
    Mar 23 19:36:40.465: INFO: Got endpoints: latency-svc-z4tqw [753.157927ms]
    Mar 23 19:36:40.480: INFO: Created: latency-svc-2zxk5
    Mar 23 19:36:40.514: INFO: Got endpoints: latency-svc-z4kbp [748.532538ms]
    Mar 23 19:36:40.528: INFO: Created: latency-svc-m7z5z
    Mar 23 19:36:40.567: INFO: Got endpoints: latency-svc-lhvmx [753.586826ms]
    Mar 23 19:36:40.582: INFO: Created: latency-svc-pd76n
    Mar 23 19:36:40.612: INFO: Got endpoints: latency-svc-gqccc [739.53766ms]
    Mar 23 19:36:40.623: INFO: Created: latency-svc-ll7vf
    Mar 23 19:36:40.667: INFO: Got endpoints: latency-svc-sz2wf [745.304446ms]
    Mar 23 19:36:40.682: INFO: Created: latency-svc-bxqcz
    Mar 23 19:36:40.712: INFO: Got endpoints: latency-svc-d2c4v [724.864994ms]
    Mar 23 19:36:40.728: INFO: Created: latency-svc-g8vzt
    Mar 23 19:36:40.763: INFO: Got endpoints: latency-svc-lxt2v [747.75984ms]
    Mar 23 19:36:40.776: INFO: Created: latency-svc-4pgbg
    Mar 23 19:36:40.811: INFO: Got endpoints: latency-svc-9r22h [748.03274ms]
    Mar 23 19:36:40.824: INFO: Created: latency-svc-h66kh
    Mar 23 19:36:40.867: INFO: Got endpoints: latency-svc-sn2w5 [747.64144ms]
    Mar 23 19:36:40.904: INFO: Created: latency-svc-7p9gd
    Mar 23 19:36:40.916: INFO: Got endpoints: latency-svc-t2khm [751.140932ms]
    Mar 23 19:36:40.933: INFO: Created: latency-svc-jvq7p
    Mar 23 19:36:40.980: INFO: Got endpoints: latency-svc-mc4bq [768.438091ms]
    Mar 23 19:36:41.002: INFO: Created: latency-svc-fvlws
    Mar 23 19:36:41.017: INFO: Got endpoints: latency-svc-z84kz [751.65793ms]
    Mar 23 19:36:41.034: INFO: Created: latency-svc-5fv98
    Mar 23 19:36:41.063: INFO: Got endpoints: latency-svc-7s9kl [749.239737ms]
    Mar 23 19:36:41.082: INFO: Created: latency-svc-k2c97
    Mar 23 19:36:41.113: INFO: Got endpoints: latency-svc-68lrp [747.98284ms]
    Mar 23 19:36:41.132: INFO: Created: latency-svc-q4gw4
    Mar 23 19:36:41.165: INFO: Got endpoints: latency-svc-ln968 [749.486035ms]
    Mar 23 19:36:41.177: INFO: Created: latency-svc-kfzsf
    Mar 23 19:36:41.215: INFO: Got endpoints: latency-svc-2zxk5 [749.582036ms]
    Mar 23 19:36:41.232: INFO: Created: latency-svc-kn24n
    Mar 23 19:36:41.271: INFO: Got endpoints: latency-svc-m7z5z [756.908918ms]
    Mar 23 19:36:41.304: INFO: Created: latency-svc-5xpvs
    Mar 23 19:36:41.332: INFO: Got endpoints: latency-svc-pd76n [765.276698ms]
    Mar 23 19:36:41.349: INFO: Created: latency-svc-np6ks
    Mar 23 19:36:41.371: INFO: Got endpoints: latency-svc-ll7vf [759.021513ms]
    Mar 23 19:36:41.382: INFO: Created: latency-svc-7tnrn
    Mar 23 19:36:41.412: INFO: Got endpoints: latency-svc-bxqcz [744.640547ms]
    Mar 23 19:36:41.427: INFO: Created: latency-svc-l4v5d
    Mar 23 19:36:41.463: INFO: Got endpoints: latency-svc-g8vzt [749.985635ms]
    Mar 23 19:36:41.475: INFO: Created: latency-svc-prlp7
    Mar 23 19:36:41.512: INFO: Got endpoints: latency-svc-4pgbg [749.505836ms]
    Mar 23 19:36:41.524: INFO: Created: latency-svc-xgbjt
    Mar 23 19:36:41.563: INFO: Got endpoints: latency-svc-h66kh [751.664331ms]
    Mar 23 19:36:41.578: INFO: Created: latency-svc-glh9m
    Mar 23 19:36:41.621: INFO: Got endpoints: latency-svc-7p9gd [753.440127ms]
    Mar 23 19:36:41.632: INFO: Created: latency-svc-gwdlz
    Mar 23 19:36:41.663: INFO: Got endpoints: latency-svc-jvq7p [747.350041ms]
    Mar 23 19:36:41.677: INFO: Created: latency-svc-sj6lm
    Mar 23 19:36:41.712: INFO: Got endpoints: latency-svc-fvlws [731.527778ms]
    Mar 23 19:36:41.726: INFO: Created: latency-svc-74wpk
    Mar 23 19:36:41.762: INFO: Got endpoints: latency-svc-5fv98 [744.819747ms]
    Mar 23 19:36:41.793: INFO: Created: latency-svc-6t4vr
    Mar 23 19:36:41.815: INFO: Got endpoints: latency-svc-k2c97 [751.87323ms]
    Mar 23 19:36:41.828: INFO: Created: latency-svc-99mff
    Mar 23 19:36:41.864: INFO: Got endpoints: latency-svc-q4gw4 [750.859833ms]
    Mar 23 19:36:41.887: INFO: Created: latency-svc-d8vn9
    Mar 23 19:36:41.918: INFO: Got endpoints: latency-svc-kfzsf [753.201828ms]
    Mar 23 19:36:41.961: INFO: Created: latency-svc-6xxfq
    Mar 23 19:36:41.982: INFO: Got endpoints: latency-svc-kn24n [766.635595ms]
    Mar 23 19:36:42.003: INFO: Created: latency-svc-qh5rb
    Mar 23 19:36:42.021: INFO: Got endpoints: latency-svc-5xpvs [750.115034ms]
    Mar 23 19:36:42.057: INFO: Created: latency-svc-9jlnj
    Mar 23 19:36:42.065: INFO: Got endpoints: latency-svc-np6ks [732.856075ms]
    Mar 23 19:36:42.103: INFO: Created: latency-svc-842qs
    Mar 23 19:36:42.117: INFO: Got endpoints: latency-svc-7tnrn [746.350543ms]
    Mar 23 19:36:42.131: INFO: Created: latency-svc-xmst2
    Mar 23 19:36:42.164: INFO: Got endpoints: latency-svc-l4v5d [751.97703ms]
    Mar 23 19:36:42.178: INFO: Created: latency-svc-w7s4g
    Mar 23 19:36:42.214: INFO: Got endpoints: latency-svc-prlp7 [751.73463ms]
    Mar 23 19:36:42.234: INFO: Created: latency-svc-wdfdz
    Mar 23 19:36:42.264: INFO: Got endpoints: latency-svc-xgbjt [752.01533ms]
    Mar 23 19:36:42.287: INFO: Created: latency-svc-xpzwl
    Mar 23 19:36:42.315: INFO: Got endpoints: latency-svc-glh9m [752.08853ms]
    Mar 23 19:36:42.332: INFO: Created: latency-svc-rsjqk
    Mar 23 19:36:42.369: INFO: Got endpoints: latency-svc-gwdlz [747.72634ms]
    Mar 23 19:36:42.381: INFO: Created: latency-svc-25bxc
    Mar 23 19:36:42.413: INFO: Got endpoints: latency-svc-sj6lm [749.667136ms]
    Mar 23 19:36:42.425: INFO: Created: latency-svc-ppq6b
    Mar 23 19:36:42.466: INFO: Got endpoints: latency-svc-74wpk [753.724925ms]
    Mar 23 19:36:42.481: INFO: Created: latency-svc-jx28f
    Mar 23 19:36:42.514: INFO: Got endpoints: latency-svc-6t4vr [752.576629ms]
    Mar 23 19:36:42.553: INFO: Created: latency-svc-xqf4m
    Mar 23 19:36:42.564: INFO: Got endpoints: latency-svc-99mff [748.431639ms]
    Mar 23 19:36:42.592: INFO: Created: latency-svc-h9ccp
    Mar 23 19:36:42.619: INFO: Got endpoints: latency-svc-d8vn9 [755.243622ms]
    Mar 23 19:36:42.656: INFO: Created: latency-svc-f5zmk
    Mar 23 19:36:42.670: INFO: Got endpoints: latency-svc-6xxfq [752.29813ms]
    Mar 23 19:36:42.686: INFO: Created: latency-svc-gb729
    Mar 23 19:36:42.720: INFO: Got endpoints: latency-svc-qh5rb [737.609964ms]
    Mar 23 19:36:42.739: INFO: Created: latency-svc-4mjnp
    Mar 23 19:36:42.774: INFO: Got endpoints: latency-svc-9jlnj [753.184627ms]
    Mar 23 19:36:42.792: INFO: Created: latency-svc-gntcp
    Mar 23 19:36:42.813: INFO: Got endpoints: latency-svc-842qs [748.383539ms]
    Mar 23 19:36:42.829: INFO: Created: latency-svc-6xdpp
    Mar 23 19:36:42.870: INFO: Got endpoints: latency-svc-xmst2 [752.416629ms]
    Mar 23 19:36:42.912: INFO: Created: latency-svc-sbgm9
    Mar 23 19:36:42.924: INFO: Got endpoints: latency-svc-w7s4g [760.080805ms]
    Mar 23 19:36:42.947: INFO: Created: latency-svc-gs4p6
    Mar 23 19:36:42.979: INFO: Got endpoints: latency-svc-wdfdz [764.772377ms]
    Mar 23 19:36:43.002: INFO: Created: latency-svc-vkxt8
    Mar 23 19:36:43.023: INFO: Got endpoints: latency-svc-xpzwl [758.11458ms]
    Mar 23 19:36:43.049: INFO: Created: latency-svc-7xxjp
    Mar 23 19:36:43.101: INFO: Got endpoints: latency-svc-rsjqk [785.471093ms]
    Mar 23 19:36:43.115: INFO: Got endpoints: latency-svc-25bxc [745.558183ms]
    Mar 23 19:36:43.121: INFO: Created: latency-svc-7xrlz
    Mar 23 19:36:43.130: INFO: Created: latency-svc-58jtr
    Mar 23 19:36:43.162: INFO: Got endpoints: latency-svc-ppq6b [747.898965ms]
    Mar 23 19:36:43.173: INFO: Created: latency-svc-dgwls
    Mar 23 19:36:43.213: INFO: Got endpoints: latency-svc-jx28f [747.87565ms]
    Mar 23 19:36:43.227: INFO: Created: latency-svc-k9h7s
    Mar 23 19:36:43.261: INFO: Got endpoints: latency-svc-xqf4m [747.048138ms]
    Mar 23 19:36:43.272: INFO: Created: latency-svc-nzljz
    Mar 23 19:36:43.312: INFO: Got endpoints: latency-svc-h9ccp [747.899721ms]
    Mar 23 19:36:43.323: INFO: Created: latency-svc-cm2vg
    Mar 23 19:36:43.362: INFO: Got endpoints: latency-svc-f5zmk [742.363419ms]
    Mar 23 19:36:43.379: INFO: Created: latency-svc-t4659
    Mar 23 19:36:43.412: INFO: Got endpoints: latency-svc-gb729 [741.429306ms]
    Mar 23 19:36:43.424: INFO: Created: latency-svc-ngjdx
    Mar 23 19:36:43.463: INFO: Got endpoints: latency-svc-4mjnp [743.162788ms]
    Mar 23 19:36:43.477: INFO: Created: latency-svc-s946d
    Mar 23 19:36:43.520: INFO: Got endpoints: latency-svc-gntcp [745.233766ms]
    Mar 23 19:36:43.534: INFO: Created: latency-svc-67fhr
    Mar 23 19:36:43.564: INFO: Got endpoints: latency-svc-6xdpp [749.912842ms]
    Mar 23 19:36:43.582: INFO: Created: latency-svc-hmxqp
    Mar 23 19:36:43.614: INFO: Got endpoints: latency-svc-sbgm9 [744.009442ms]
    Mar 23 19:36:43.633: INFO: Created: latency-svc-bbb69
    Mar 23 19:36:43.672: INFO: Got endpoints: latency-svc-gs4p6 [746.771325ms]
    Mar 23 19:36:43.685: INFO: Created: latency-svc-4kb77
    Mar 23 19:36:43.713: INFO: Got endpoints: latency-svc-vkxt8 [732.593062ms]
    Mar 23 19:36:43.762: INFO: Got endpoints: latency-svc-7xxjp [739.778743ms]
    Mar 23 19:36:43.816: INFO: Got endpoints: latency-svc-7xrlz [714.66551ms]
    Mar 23 19:36:43.865: INFO: Got endpoints: latency-svc-58jtr [750.225515ms]
    Mar 23 19:36:43.922: INFO: Got endpoints: latency-svc-dgwls [760.806888ms]
    Mar 23 19:36:43.982: INFO: Got endpoints: latency-svc-k9h7s [768.827767ms]
    Mar 23 19:36:44.027: INFO: Got endpoints: latency-svc-nzljz [765.087377ms]
    Mar 23 19:36:44.077: INFO: Got endpoints: latency-svc-cm2vg [765.536675ms]
    Mar 23 19:36:44.115: INFO: Got endpoints: latency-svc-t4659 [753.040808ms]
    Mar 23 19:36:44.168: INFO: Got endpoints: latency-svc-ngjdx [755.8847ms]
    Mar 23 19:36:44.213: INFO: Got endpoints: latency-svc-s946d [750.270816ms]
    Mar 23 19:36:44.264: INFO: Got endpoints: latency-svc-67fhr [744.101532ms]
    Mar 23 19:36:44.313: INFO: Got endpoints: latency-svc-hmxqp [748.959419ms]
    Mar 23 19:36:44.361: INFO: Got endpoints: latency-svc-bbb69 [747.232224ms]
    Mar 23 19:36:44.411: INFO: Got endpoints: latency-svc-4kb77 [738.768746ms]
    Mar 23 19:36:44.412: INFO: Latencies: [102.016303ms 135.788339ms 136.778734ms 141.160413ms 155.634543ms 157.136136ms 157.363934ms 171.022268ms 183.393808ms 227.260294ms 233.025166ms 233.429264ms 241.388625ms 245.018707ms 248.233292ms 252.234073ms 254.75626ms 270.056386ms 272.062476ms 288.089798ms 291.83608ms 321.915434ms 324.154522ms 336.836661ms 353.222481ms 361.378341ms 363.591331ms 363.73453ms 377.845561ms 385.370624ms 388.49521ms 388.815308ms 398.071963ms 401.906244ms 403.340837ms 404.290932ms 409.842306ms 410.142504ms 414.029485ms 419.225359ms 421.425049ms 424.971532ms 426.237426ms 429.43631ms 434.397686ms 438.284767ms 441.70555ms 441.88255ms 447.340201ms 454.219289ms 456.294779ms 459.054465ms 460.192562ms 460.844957ms 461.043856ms 461.962152ms 463.844343ms 469.337016ms 471.598105ms 473.158397ms 473.395496ms 488.385523ms 488.726027ms 508.283226ms 511.797142ms 524.450547ms 530.461018ms 547.467488ms 564.369119ms 584.873224ms 634.930848ms 668.092064ms 699.346239ms 714.66551ms 724.864994ms 728.832584ms 731.527778ms 732.593062ms 732.856075ms 733.215575ms 733.652086ms 734.80137ms 737.425865ms 737.609964ms 738.768746ms 739.13906ms 739.53766ms 739.778743ms 740.242758ms 740.655831ms 741.429306ms 741.977554ms 742.363419ms 743.162788ms 743.273751ms 743.338651ms 744.009442ms 744.023049ms 744.101532ms 744.640547ms 744.819747ms 744.909146ms 745.028646ms 745.088847ms 745.233766ms 745.304446ms 745.558183ms 746.350543ms 746.615642ms 746.771325ms 747.048138ms 747.232224ms 747.321541ms 747.350041ms 747.64144ms 747.69554ms 747.72634ms 747.75984ms 747.87565ms 747.898965ms 747.899721ms 747.98284ms 748.03274ms 748.236439ms 748.383539ms 748.431639ms 748.532538ms 748.619038ms 748.675838ms 748.959419ms 749.239737ms 749.438536ms 749.439736ms 749.486035ms 749.505836ms 749.582036ms 749.667136ms 749.758335ms 749.807735ms 749.912842ms 749.985635ms 750.115034ms 750.196334ms 750.225515ms 750.270816ms 750.327334ms 750.489033ms 750.524708ms 750.859833ms 750.963932ms 751.099532ms 751.140932ms 751.190532ms 751.299745ms 751.65793ms 751.664331ms 751.73463ms 751.87323ms 751.97703ms 751.99753ms 752.01533ms 752.08853ms 752.29813ms 752.416629ms 752.530628ms 752.576629ms 753.040808ms 753.122228ms 753.157927ms 753.184627ms 753.201828ms 753.440127ms 753.586826ms 753.724925ms 753.738926ms 754.001725ms 755.243622ms 755.265922ms 755.869821ms 755.8847ms 755.91982ms 756.17175ms 756.908918ms 758.11458ms 758.681315ms 759.021513ms 759.898411ms 760.080805ms 760.806888ms 764.772377ms 765.087377ms 765.276698ms 765.536675ms 766.635595ms 767.594293ms 768.438091ms 768.827767ms 770.106087ms 778.249668ms 785.471093ms]
    Mar 23 19:36:44.412: INFO: 50 %ile: 744.819747ms
    Mar 23 19:36:44.413: INFO: 90 %ile: 755.91982ms
    Mar 23 19:36:44.413: INFO: 99 %ile: 778.249668ms
    Mar 23 19:36:44.413: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Mar 23 19:36:44.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-856" for this suite. 03/23/23 19:36:44.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:36:44.443
Mar 23 19:36:44.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:36:44.444
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:44.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:44.465
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 23 19:36:44.489: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:37:44.592: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 03/23/23 19:37:44.596
Mar 23 19:37:44.628: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 23 19:37:44.641: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 23 19:37:44.704: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 23 19:37:44.716: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 23 19:37:44.788: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 23 19:37:44.825: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/23/23 19:37:44.825
Mar 23 19:37:44.826: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:37:44.845: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 19.147152ms
Mar 23 19:37:46.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025225731s
Mar 23 19:37:48.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025389139s
Mar 23 19:37:50.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023795452s
Mar 23 19:37:52.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024160161s
Mar 23 19:37:54.853: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027644061s
Mar 23 19:37:56.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.024590374s
Mar 23 19:37:56.850: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 23 19:37:56.851: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:37:56.855: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.646089ms
Mar 23 19:37:58.861: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01008118s
Mar 23 19:37:58.861: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:37:58.861: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:37:58.864: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.709793ms
Mar 23 19:38:00.877: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015833465s
Mar 23 19:38:02.867: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006315893s
Mar 23 19:38:04.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007482793s
Mar 23 19:38:06.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008356094s
Mar 23 19:38:08.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006838401s
Mar 23 19:38:10.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006488705s
Mar 23 19:38:12.875: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01354909s
Mar 23 19:38:14.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007896807s
Mar 23 19:38:16.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006431914s
Mar 23 19:38:18.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006462017s
Mar 23 19:38:20.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006862219s
Mar 23 19:38:22.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008805917s
Mar 23 19:38:24.873: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011682713s
Mar 23 19:38:26.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006851628s
Mar 23 19:38:28.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006688231s
Mar 23 19:38:30.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008906129s
Mar 23 19:38:32.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007386636s
Mar 23 19:38:34.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007562838s
Mar 23 19:38:36.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00675564s
Mar 23 19:38:38.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00682574s
Mar 23 19:38:40.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008446437s
Mar 23 19:38:42.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008033538s
Mar 23 19:38:44.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 46.007517334s
Mar 23 19:38:44.869: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:38:44.869: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:38:44.871: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.583493ms
Mar 23 19:38:44.871: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:38:44.871: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:38:44.874: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.769393ms
Mar 23 19:38:44.874: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:38:44.874: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
Mar 23 19:38:44.877: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.410394ms
Mar 23 19:38:44.877: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 03/23/23 19:38:44.877
Mar 23 19:38:44.887: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Mar 23 19:38:44.893: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002185ms
Mar 23 19:38:46.897: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00972857s
Mar 23 19:38:48.897: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010269864s
Mar 23 19:38:48.898: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:38:48.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-667" for this suite. 03/23/23 19:38:48.94
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":197,"skipped":3557,"failed":0}
------------------------------
• [SLOW TEST] [124.545 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:36:44.443
    Mar 23 19:36:44.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:36:44.444
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:36:44.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:36:44.465
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 23 19:36:44.489: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 19:37:44.592: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 03/23/23 19:37:44.596
    Mar 23 19:37:44.628: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 23 19:37:44.641: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 23 19:37:44.704: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 23 19:37:44.716: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 23 19:37:44.788: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 23 19:37:44.825: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/23/23 19:37:44.825
    Mar 23 19:37:44.826: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:37:44.845: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 19.147152ms
    Mar 23 19:37:46.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025225731s
    Mar 23 19:37:48.851: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025389139s
    Mar 23 19:37:50.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.023795452s
    Mar 23 19:37:52.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.024160161s
    Mar 23 19:37:54.853: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027644061s
    Mar 23 19:37:56.850: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.024590374s
    Mar 23 19:37:56.850: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 23 19:37:56.851: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:37:56.855: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.646089ms
    Mar 23 19:37:58.861: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.01008118s
    Mar 23 19:37:58.861: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:37:58.861: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:37:58.864: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.709793ms
    Mar 23 19:38:00.877: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015833465s
    Mar 23 19:38:02.867: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006315893s
    Mar 23 19:38:04.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007482793s
    Mar 23 19:38:06.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008356094s
    Mar 23 19:38:08.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006838401s
    Mar 23 19:38:10.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006488705s
    Mar 23 19:38:12.875: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01354909s
    Mar 23 19:38:14.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007896807s
    Mar 23 19:38:16.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006431914s
    Mar 23 19:38:18.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006462017s
    Mar 23 19:38:20.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006862219s
    Mar 23 19:38:22.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008805917s
    Mar 23 19:38:24.873: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011682713s
    Mar 23 19:38:26.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006851628s
    Mar 23 19:38:28.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006688231s
    Mar 23 19:38:30.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008906129s
    Mar 23 19:38:32.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007386636s
    Mar 23 19:38:34.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007562838s
    Mar 23 19:38:36.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00675564s
    Mar 23 19:38:38.868: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00682574s
    Mar 23 19:38:40.870: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008446437s
    Mar 23 19:38:42.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008033538s
    Mar 23 19:38:44.869: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 46.007517334s
    Mar 23 19:38:44.869: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:38:44.869: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:38:44.871: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.583493ms
    Mar 23 19:38:44.871: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:38:44.871: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:38:44.874: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.769393ms
    Mar 23 19:38:44.874: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:38:44.874: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-667" to be "running"
    Mar 23 19:38:44.877: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.410394ms
    Mar 23 19:38:44.877: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 03/23/23 19:38:44.877
    Mar 23 19:38:44.887: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Mar 23 19:38:44.893: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.002185ms
    Mar 23 19:38:46.897: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00972857s
    Mar 23 19:38:48.897: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010269864s
    Mar 23 19:38:48.898: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:38:48.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-667" for this suite. 03/23/23 19:38:48.94
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:38:49.001
Mar 23 19:38:49.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:38:49.005
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:49.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:49.031
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Mar 23 19:38:49.081: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7058 to be scheduled
Mar 23 19:38:49.090: INFO: 1 pods are not scheduled: [runtimeclass-7058/test-runtimeclass-runtimeclass-7058-preconfigured-handler-5vddx(09a6a3f8-777f-4bc2-8c5f-5a3365884d56)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 23 19:38:51.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7058" for this suite. 03/23/23 19:38:51.106
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":198,"skipped":3567,"failed":0}
------------------------------
• [2.110 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:38:49.001
    Mar 23 19:38:49.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:38:49.005
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:49.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:49.031
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Mar 23 19:38:49.081: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7058 to be scheduled
    Mar 23 19:38:49.090: INFO: 1 pods are not scheduled: [runtimeclass-7058/test-runtimeclass-runtimeclass-7058-preconfigured-handler-5vddx(09a6a3f8-777f-4bc2-8c5f-5a3365884d56)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 23 19:38:51.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7058" for this suite. 03/23/23 19:38:51.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:38:51.118
Mar 23 19:38:51.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:38:51.119
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:51.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:51.147
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 03/23/23 19:38:51.151
Mar 23 19:38:51.161: INFO: Waiting up to 5m0s for pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70" in namespace "downward-api-8574" to be "running and ready"
Mar 23 19:38:51.175: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 13.685865ms
Mar 23 19:38:51.175: INFO: The phase of Pod labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:38:53.180: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70": Phase="Running", Reason="", readiness=true. Elapsed: 2.019180141s
Mar 23 19:38:53.180: INFO: The phase of Pod labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70 is Running (Ready = true)
Mar 23 19:38:53.180: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70" satisfied condition "running and ready"
Mar 23 19:38:53.736: INFO: Successfully updated pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 19:38:57.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8574" for this suite. 03/23/23 19:38:57.807
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":199,"skipped":3580,"failed":0}
------------------------------
• [SLOW TEST] [6.700 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:38:51.118
    Mar 23 19:38:51.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:38:51.119
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:51.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:51.147
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 03/23/23 19:38:51.151
    Mar 23 19:38:51.161: INFO: Waiting up to 5m0s for pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70" in namespace "downward-api-8574" to be "running and ready"
    Mar 23 19:38:51.175: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70": Phase="Pending", Reason="", readiness=false. Elapsed: 13.685865ms
    Mar 23 19:38:51.175: INFO: The phase of Pod labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:38:53.180: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70": Phase="Running", Reason="", readiness=true. Elapsed: 2.019180141s
    Mar 23 19:38:53.180: INFO: The phase of Pod labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70 is Running (Ready = true)
    Mar 23 19:38:53.180: INFO: Pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70" satisfied condition "running and ready"
    Mar 23 19:38:53.736: INFO: Successfully updated pod "labelsupdatea9c11f34-709d-44c1-a9bd-13eb4260ff70"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 19:38:57.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8574" for this suite. 03/23/23 19:38:57.807
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:38:57.832
Mar 23 19:38:57.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:38:57.842
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:57.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:57.897
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:38:57.904
Mar 23 19:38:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Mar 23 19:38:58.033: INFO: stderr: ""
Mar 23 19:38:58.033: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 03/23/23 19:38:58.033
Mar 23 19:38:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Mar 23 19:38:59.219: INFO: stderr: ""
Mar 23 19:38:59.219: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:38:59.219
Mar 23 19:38:59.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 delete pods e2e-test-httpd-pod'
Mar 23 19:39:05.338: INFO: stderr: ""
Mar 23 19:39:05.338: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:39:05.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5595" for this suite. 03/23/23 19:39:05.345
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":200,"skipped":3580,"failed":0}
------------------------------
• [SLOW TEST] [7.520 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:38:57.832
    Mar 23 19:38:57.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:38:57.842
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:38:57.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:38:57.897
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:38:57.904
    Mar 23 19:38:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Mar 23 19:38:58.033: INFO: stderr: ""
    Mar 23 19:38:58.033: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 03/23/23 19:38:58.033
    Mar 23 19:38:58.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Mar 23 19:38:59.219: INFO: stderr: ""
    Mar 23 19:38:59.219: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 19:38:59.219
    Mar 23 19:38:59.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5595 delete pods e2e-test-httpd-pod'
    Mar 23 19:39:05.338: INFO: stderr: ""
    Mar 23 19:39:05.338: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:39:05.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5595" for this suite. 03/23/23 19:39:05.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:39:05.353
Mar 23 19:39:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:39:05.354
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:05.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:05.409
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:39:05.46
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:39:05.963
STEP: Deploying the webhook pod 03/23/23 19:39:05.97
STEP: Wait for the deployment to be ready 03/23/23 19:39:05.984
Mar 23 19:39:05.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:39:08.006
STEP: Verifying the service has paired with the endpoint 03/23/23 19:39:08.038
Mar 23 19:39:09.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 03/23/23 19:39:09.043
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/23/23 19:39:09.044
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/23/23 19:39:09.044
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/23/23 19:39:09.044
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/23/23 19:39:09.046
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/23/23 19:39:09.046
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/23/23 19:39:09.047
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:39:09.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8001" for this suite. 03/23/23 19:39:09.053
STEP: Destroying namespace "webhook-8001-markers" for this suite. 03/23/23 19:39:09.061
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":201,"skipped":3596,"failed":0}
------------------------------
• [3.881 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:39:05.353
    Mar 23 19:39:05.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:39:05.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:05.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:05.409
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:39:05.46
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:39:05.963
    STEP: Deploying the webhook pod 03/23/23 19:39:05.97
    STEP: Wait for the deployment to be ready 03/23/23 19:39:05.984
    Mar 23 19:39:05.996: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:39:08.006
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:39:08.038
    Mar 23 19:39:09.038: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 03/23/23 19:39:09.043
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 03/23/23 19:39:09.044
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 03/23/23 19:39:09.044
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 03/23/23 19:39:09.044
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 03/23/23 19:39:09.046
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 03/23/23 19:39:09.046
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 03/23/23 19:39:09.047
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:39:09.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8001" for this suite. 03/23/23 19:39:09.053
    STEP: Destroying namespace "webhook-8001-markers" for this suite. 03/23/23 19:39:09.061
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:39:09.235
Mar 23 19:39:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:39:09.237
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:09.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:09.298
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:39:09.313
Mar 23 19:39:09.329: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7314" to be "running and ready"
Mar 23 19:39:09.341: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.89317ms
Mar 23 19:39:09.341: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:39:11.345: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016582153s
Mar 23 19:39:11.345: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 23 19:39:11.345: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 03/23/23 19:39:11.349
Mar 23 19:39:11.356: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7314" to be "running and ready"
Mar 23 19:39:11.359: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.182592ms
Mar 23 19:39:11.359: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:39:13.363: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006978777s
Mar 23 19:39:13.363: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:39:15.365: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008410875s
Mar 23 19:39:15.365: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Mar 23 19:39:15.365: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/23/23 19:39:15.368
Mar 23 19:39:15.376: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 19:39:15.379: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 19:39:17.380: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 19:39:17.384: INFO: Pod pod-with-prestop-exec-hook still exists
Mar 23 19:39:19.379: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Mar 23 19:39:19.383: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 03/23/23 19:39:19.383
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 23 19:39:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7314" for this suite. 03/23/23 19:39:19.425
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":202,"skipped":3608,"failed":0}
------------------------------
• [SLOW TEST] [10.199 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:39:09.235
    Mar 23 19:39:09.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 19:39:09.237
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:09.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:09.298
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/23/23 19:39:09.313
    Mar 23 19:39:09.329: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-7314" to be "running and ready"
    Mar 23 19:39:09.341: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 11.89317ms
    Mar 23 19:39:09.341: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:39:11.345: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.016582153s
    Mar 23 19:39:11.345: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 23 19:39:11.345: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 03/23/23 19:39:11.349
    Mar 23 19:39:11.356: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-7314" to be "running and ready"
    Mar 23 19:39:11.359: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.182592ms
    Mar 23 19:39:11.359: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:39:13.363: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006978777s
    Mar 23 19:39:13.363: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:39:15.365: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.008410875s
    Mar 23 19:39:15.365: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Mar 23 19:39:15.365: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/23/23 19:39:15.368
    Mar 23 19:39:15.376: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 23 19:39:15.379: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 23 19:39:17.380: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 23 19:39:17.384: INFO: Pod pod-with-prestop-exec-hook still exists
    Mar 23 19:39:19.379: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Mar 23 19:39:19.383: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 03/23/23 19:39:19.383
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 23 19:39:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-7314" for this suite. 03/23/23 19:39:19.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:39:19.438
Mar 23 19:39:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:39:19.439
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:19.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:19.47
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 23 19:39:19.491: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:40:19.610: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 03/23/23 19:40:19.615
Mar 23 19:40:19.641: INFO: Created pod: pod0-0-sched-preemption-low-priority
Mar 23 19:40:19.660: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Mar 23 19:40:19.685: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Mar 23 19:40:19.694: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Mar 23 19:40:19.729: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Mar 23 19:40:19.745: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 03/23/23 19:40:19.746
Mar 23 19:40:19.746: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:19.770: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.754347ms
Mar 23 19:40:21.774: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.028342314s
Mar 23 19:40:21.774: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Mar 23 19:40:21.774: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:21.777: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148893ms
Mar 23 19:40:23.781: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006989862s
Mar 23 19:40:23.781: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:40:23.781: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:23.784: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.725694ms
Mar 23 19:40:23.784: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:40:23.784: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:23.786: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.457694ms
Mar 23 19:40:23.787: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:40:23.787: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:23.789: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.795694ms
Mar 23 19:40:23.789: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Mar 23 19:40:23.789: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:23.793: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.361292ms
Mar 23 19:40:23.793: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/23/23 19:40:23.793
Mar 23 19:40:23.799: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2177" to be "running"
Mar 23 19:40:23.803: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.561389ms
Mar 23 19:40:25.808: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008949557s
Mar 23 19:40:27.809: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009971616s
Mar 23 19:40:29.808: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009665252s
Mar 23 19:40:29.809: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:40:29.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2177" for this suite. 03/23/23 19:40:29.834
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":203,"skipped":3613,"failed":0}
------------------------------
• [SLOW TEST] [70.499 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:39:19.438
    Mar 23 19:39:19.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:39:19.439
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:39:19.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:39:19.47
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 23 19:39:19.491: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 19:40:19.610: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 03/23/23 19:40:19.615
    Mar 23 19:40:19.641: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Mar 23 19:40:19.660: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Mar 23 19:40:19.685: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Mar 23 19:40:19.694: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Mar 23 19:40:19.729: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Mar 23 19:40:19.745: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 03/23/23 19:40:19.746
    Mar 23 19:40:19.746: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:19.770: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.754347ms
    Mar 23 19:40:21.774: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.028342314s
    Mar 23 19:40:21.774: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Mar 23 19:40:21.774: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:21.777: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148893ms
    Mar 23 19:40:23.781: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006989862s
    Mar 23 19:40:23.781: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:40:23.781: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:23.784: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.725694ms
    Mar 23 19:40:23.784: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:40:23.784: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:23.786: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.457694ms
    Mar 23 19:40:23.787: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:40:23.787: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:23.789: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.795694ms
    Mar 23 19:40:23.789: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Mar 23 19:40:23.789: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:23.793: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.361292ms
    Mar 23 19:40:23.793: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 03/23/23 19:40:23.793
    Mar 23 19:40:23.799: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-2177" to be "running"
    Mar 23 19:40:23.803: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.561389ms
    Mar 23 19:40:25.808: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008949557s
    Mar 23 19:40:27.809: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009971616s
    Mar 23 19:40:29.808: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009665252s
    Mar 23 19:40:29.809: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:40:29.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2177" for this suite. 03/23/23 19:40:29.834
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:29.941
Mar 23 19:40:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replication-controller 03/23/23 19:40:29.944
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:29.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:29.978
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913 03/23/23 19:40:29.981
Mar 23 19:40:30.002: INFO: Pod name my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Found 0 pods out of 1
Mar 23 19:40:35.012: INFO: Pod name my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Found 1 pods out of 1
Mar 23 19:40:35.012: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913" are running
Mar 23 19:40:35.013: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" in namespace "replication-controller-6870" to be "running"
Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8": Phase="Running", Reason="", readiness=true. Elapsed: 12.557469ms
Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" satisfied condition "running"
Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:30 +0000 UTC Reason: Message:}])
Mar 23 19:40:35.025: INFO: Trying to dial the pod
Mar 23 19:40:40.038: INFO: Controller my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Got expected result from replica 1 [my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8]: "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 23 19:40:40.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6870" for this suite. 03/23/23 19:40:40.044
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":204,"skipped":3616,"failed":0}
------------------------------
• [SLOW TEST] [10.113 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:29.941
    Mar 23 19:40:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replication-controller 03/23/23 19:40:29.944
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:29.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:29.978
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913 03/23/23 19:40:29.981
    Mar 23 19:40:30.002: INFO: Pod name my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Found 0 pods out of 1
    Mar 23 19:40:35.012: INFO: Pod name my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Found 1 pods out of 1
    Mar 23 19:40:35.012: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913" are running
    Mar 23 19:40:35.013: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" in namespace "replication-controller-6870" to be "running"
    Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8": Phase="Running", Reason="", readiness=true. Elapsed: 12.557469ms
    Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" satisfied condition "running"
    Mar 23 19:40:35.025: INFO: Pod "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:30 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:31 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:31 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 19:40:30 +0000 UTC Reason: Message:}])
    Mar 23 19:40:35.025: INFO: Trying to dial the pod
    Mar 23 19:40:40.038: INFO: Controller my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913: Got expected result from replica 1 [my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8]: "my-hostname-basic-a3a76e74-08fc-480b-8b85-9ca2be6f6913-fw7q8", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 23 19:40:40.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6870" for this suite. 03/23/23 19:40:40.044
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:40.058
Mar 23 19:40:40.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 19:40:40.061
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:40.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:40.086
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Mar 23 19:40:40.093: INFO: Creating deployment "test-recreate-deployment"
Mar 23 19:40:40.099: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Mar 23 19:40:40.132: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Mar 23 19:40:42.138: INFO: Waiting deployment "test-recreate-deployment" to complete
Mar 23 19:40:42.141: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Mar 23 19:40:42.155: INFO: Updating deployment test-recreate-deployment
Mar 23 19:40:42.155: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 19:40:42.280: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-3477  ee6d1430-68f0-4e33-9c3c-3e953787e167 29009 2 2023-03-23 19:40:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-23 19:40:42 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-23 19:40:42 +0000 UTC,LastTransitionTime:2023-03-23 19:40:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Mar 23 19:40:42.283: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3477  e56f3257-7e11-4635-823e-432d681325e6 29005 1 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ee6d1430-68f0-4e33-9c3c-3e953787e167 0xc005133b30 0xc005133b31}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6d1430-68f0-4e33-9c3c-3e953787e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:40:42.283: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Mar 23 19:40:42.284: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3477  cb0de5a0-bca9-4279-b692-21ffe59ca858 28996 2 2023-03-23 19:40:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ee6d1430-68f0-4e33-9c3c-3e953787e167 0xc005133a17 0xc005133a18}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6d1430-68f0-4e33-9c3c-3e953787e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:40:42.290: INFO: Pod "test-recreate-deployment-9d58999df-mjg6p" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-mjg6p test-recreate-deployment-9d58999df- deployment-3477  10515402-9383-4c6c-8364-9999b8803ec6 29008 0 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df e56f3257-7e11-4635-823e-432d681325e6 0xc00605dd30 0xc00605dd31}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56f3257-7e11-4635-823e-432d681325e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4zr7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zr7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 19:40:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3477" for this suite. 03/23/23 19:40:42.294
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":205,"skipped":3618,"failed":0}
------------------------------
• [2.242 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:40.058
    Mar 23 19:40:40.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 19:40:40.061
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:40.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:40.086
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Mar 23 19:40:40.093: INFO: Creating deployment "test-recreate-deployment"
    Mar 23 19:40:40.099: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Mar 23 19:40:40.132: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Mar 23 19:40:42.138: INFO: Waiting deployment "test-recreate-deployment" to complete
    Mar 23 19:40:42.141: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Mar 23 19:40:42.155: INFO: Updating deployment test-recreate-deployment
    Mar 23 19:40:42.155: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 19:40:42.280: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3477  ee6d1430-68f0-4e33-9c3c-3e953787e167 29009 2 2023-03-23 19:40:40 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-03-23 19:40:42 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-03-23 19:40:42 +0000 UTC,LastTransitionTime:2023-03-23 19:40:40 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Mar 23 19:40:42.283: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-3477  e56f3257-7e11-4635-823e-432d681325e6 29005 1 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment ee6d1430-68f0-4e33-9c3c-3e953787e167 0xc005133b30 0xc005133b31}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6d1430-68f0-4e33-9c3c-3e953787e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133bc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:40:42.283: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Mar 23 19:40:42.284: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-3477  cb0de5a0-bca9-4279-b692-21ffe59ca858 28996 2 2023-03-23 19:40:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment ee6d1430-68f0-4e33-9c3c-3e953787e167 0xc005133a17 0xc005133a18}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee6d1430-68f0-4e33-9c3c-3e953787e167\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005133ac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:40:42.290: INFO: Pod "test-recreate-deployment-9d58999df-mjg6p" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-mjg6p test-recreate-deployment-9d58999df- deployment-3477  10515402-9383-4c6c-8364-9999b8803ec6 29008 0 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df e56f3257-7e11-4635-823e-432d681325e6 0xc00605dd30 0xc00605dd31}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e56f3257-7e11-4635-823e-432d681325e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4zr7x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4zr7x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:,StartTime:2023-03-23 19:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 19:40:42.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3477" for this suite. 03/23/23 19:40:42.294
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:42.304
Mar 23 19:40:42.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 19:40:42.306
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:42.323
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:42.329
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Mar 23 19:40:42.332: INFO: Creating simple deployment test-new-deployment
Mar 23 19:40:42.349: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 03/23/23 19:40:44.362
STEP: updating a scale subresource 03/23/23 19:40:44.365
STEP: verifying the deployment Spec.Replicas was modified 03/23/23 19:40:44.37
STEP: Patch a scale subresource 03/23/23 19:40:44.376
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 19:40:44.426: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-7303  4b6b2040-9dd7-423c-aed9-1f677796eaa8 29044 3 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007ef1c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 19:40:43 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-23 19:40:43 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 19:40:44.431: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7303  045eaa22-ee1e-4ccf-8378-81de5e3f58a4 29052 2 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4b6b2040-9dd7-423c-aed9-1f677796eaa8 0xc00525e027 0xc00525e028}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b6b2040-9dd7-423c-aed9-1f677796eaa8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00525e0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 19:40:44.440: INFO: Pod "test-new-deployment-845c8977d9-jl5gk" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-jl5gk test-new-deployment-845c8977d9- deployment-7303  67ad09c6-9cd3-444e-94d6-07df2bda6bb4 29033 0 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 045eaa22-ee1e-4ccf-8378-81de5e3f58a4 0xc00525e497 0xc00525e498}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"045eaa22-ee1e-4ccf-8378-81de5e3f58a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77ptg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77ptg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.62,StartTime:2023-03-23 19:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:40:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f7640687bba28d92451c690a9cb371d55c0aa94280a2f2c3552dd29ccf5a0eb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Mar 23 19:40:44.440: INFO: Pod "test-new-deployment-845c8977d9-s5mw4" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-s5mw4 test-new-deployment-845c8977d9- deployment-7303  9f568faa-45ef-452e-b1ce-fefe1030eb19 29051 0 2023-03-23 19:40:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 045eaa22-ee1e-4ccf-8378-81de5e3f58a4 0xc00525e670 0xc00525e671}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"045eaa22-ee1e-4ccf-8378-81de5e3f58a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d729p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d729p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:40:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 19:40:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7303" for this suite. 03/23/23 19:40:44.458
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":206,"skipped":3622,"failed":0}
------------------------------
• [2.213 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:42.304
    Mar 23 19:40:42.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 19:40:42.306
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:42.323
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:42.329
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Mar 23 19:40:42.332: INFO: Creating simple deployment test-new-deployment
    Mar 23 19:40:42.349: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 03/23/23 19:40:44.362
    STEP: updating a scale subresource 03/23/23 19:40:44.365
    STEP: verifying the deployment Spec.Replicas was modified 03/23/23 19:40:44.37
    STEP: Patch a scale subresource 03/23/23 19:40:44.376
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 19:40:44.426: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-7303  4b6b2040-9dd7-423c-aed9-1f677796eaa8 29044 3 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc007ef1c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 19:40:43 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-03-23 19:40:43 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 23 19:40:44.431: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-7303  045eaa22-ee1e-4ccf-8378-81de5e3f58a4 29052 2 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4b6b2040-9dd7-423c-aed9-1f677796eaa8 0xc00525e027 0xc00525e028}] [] [{kube-controller-manager Update apps/v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b6b2040-9dd7-423c-aed9-1f677796eaa8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00525e0b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 19:40:44.440: INFO: Pod "test-new-deployment-845c8977d9-jl5gk" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-jl5gk test-new-deployment-845c8977d9- deployment-7303  67ad09c6-9cd3-444e-94d6-07df2bda6bb4 29033 0 2023-03-23 19:40:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 045eaa22-ee1e-4ccf-8378-81de5e3f58a4 0xc00525e497 0xc00525e498}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"045eaa22-ee1e-4ccf-8378-81de5e3f58a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77ptg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77ptg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:42 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.62,StartTime:2023-03-23 19:40:42 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 19:40:43 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://f7640687bba28d92451c690a9cb371d55c0aa94280a2f2c3552dd29ccf5a0eb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Mar 23 19:40:44.440: INFO: Pod "test-new-deployment-845c8977d9-s5mw4" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-s5mw4 test-new-deployment-845c8977d9- deployment-7303  9f568faa-45ef-452e-b1ce-fefe1030eb19 29051 0 2023-03-23 19:40:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 045eaa22-ee1e-4ccf-8378-81de5e3f58a4 0xc00525e670 0xc00525e671}] [] [{kube-controller-manager Update v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"045eaa22-ee1e-4ccf-8378-81de5e3f58a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 19:40:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d729p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d729p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 19:40:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.4,PodIP:,StartTime:2023-03-23 19:40:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 19:40:44.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7303" for this suite. 03/23/23 19:40:44.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:44.523
Mar 23 19:40:44.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:40:44.524
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:44.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:44.557
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 03/23/23 19:40:44.573
STEP: watching for Pod to be ready 03/23/23 19:40:44.581
Mar 23 19:40:44.586: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions []
Mar 23 19:40:44.587: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
Mar 23 19:40:44.609: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
Mar 23 19:40:45.988: INFO: Found Pod pod-test in namespace pods-7932 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 03/23/23 19:40:45.992
STEP: getting the Pod and ensuring that it's patched 03/23/23 19:40:46.009
STEP: replacing the Pod's status Ready condition to False 03/23/23 19:40:46.012
STEP: check the Pod again to ensure its Ready conditions are False 03/23/23 19:40:46.024
STEP: deleting the Pod via a Collection with a LabelSelector 03/23/23 19:40:46.024
STEP: watching for the Pod to be deleted 03/23/23 19:40:46.034
Mar 23 19:40:46.036: INFO: observed event type MODIFIED
Mar 23 19:40:47.651: INFO: observed event type MODIFIED
Mar 23 19:40:51.013: INFO: observed event type MODIFIED
Mar 23 19:40:51.022: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:40:51.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7932" for this suite. 03/23/23 19:40:51.036
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":207,"skipped":3641,"failed":0}
------------------------------
• [SLOW TEST] [6.521 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:44.523
    Mar 23 19:40:44.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:40:44.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:44.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:44.557
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 03/23/23 19:40:44.573
    STEP: watching for Pod to be ready 03/23/23 19:40:44.581
    Mar 23 19:40:44.586: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Mar 23 19:40:44.587: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
    Mar 23 19:40:44.609: INFO: observed Pod pod-test in namespace pods-7932 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
    Mar 23 19:40:45.988: INFO: Found Pod pod-test in namespace pods-7932 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 19:40:44 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 03/23/23 19:40:45.992
    STEP: getting the Pod and ensuring that it's patched 03/23/23 19:40:46.009
    STEP: replacing the Pod's status Ready condition to False 03/23/23 19:40:46.012
    STEP: check the Pod again to ensure its Ready conditions are False 03/23/23 19:40:46.024
    STEP: deleting the Pod via a Collection with a LabelSelector 03/23/23 19:40:46.024
    STEP: watching for the Pod to be deleted 03/23/23 19:40:46.034
    Mar 23 19:40:46.036: INFO: observed event type MODIFIED
    Mar 23 19:40:47.651: INFO: observed event type MODIFIED
    Mar 23 19:40:51.013: INFO: observed event type MODIFIED
    Mar 23 19:40:51.022: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:40:51.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7932" for this suite. 03/23/23 19:40:51.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:51.051
Mar 23 19:40:51.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replication-controller 03/23/23 19:40:51.053
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:51.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:51.086
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 03/23/23 19:40:51.089
STEP: When the matched label of one of its pods change 03/23/23 19:40:51.099
Mar 23 19:40:51.108: INFO: Pod name pod-release: Found 0 pods out of 1
Mar 23 19:40:56.114: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 03/23/23 19:40:56.129
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Mar 23 19:40:57.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6744" for this suite. 03/23/23 19:40:57.15
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":208,"skipped":3666,"failed":0}
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:51.051
    Mar 23 19:40:51.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replication-controller 03/23/23 19:40:51.053
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:51.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:51.086
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 03/23/23 19:40:51.089
    STEP: When the matched label of one of its pods change 03/23/23 19:40:51.099
    Mar 23 19:40:51.108: INFO: Pod name pod-release: Found 0 pods out of 1
    Mar 23 19:40:56.114: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/23/23 19:40:56.129
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Mar 23 19:40:57.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6744" for this suite. 03/23/23 19:40:57.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:40:57.168
Mar 23 19:40:57.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename cronjob 03/23/23 19:40:57.169
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:57.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:57.191
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 03/23/23 19:40:57.194
STEP: Ensuring more than one job is running at a time 03/23/23 19:40:57.215
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/23/23 19:42:01.22
STEP: Removing cronjob 03/23/23 19:42:01.223
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 23 19:42:01.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8173" for this suite. 03/23/23 19:42:01.235
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":209,"skipped":3681,"failed":0}
------------------------------
• [SLOW TEST] [64.089 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:40:57.168
    Mar 23 19:40:57.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename cronjob 03/23/23 19:40:57.169
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:40:57.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:40:57.191
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 03/23/23 19:40:57.194
    STEP: Ensuring more than one job is running at a time 03/23/23 19:40:57.215
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 03/23/23 19:42:01.22
    STEP: Removing cronjob 03/23/23 19:42:01.223
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 23 19:42:01.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8173" for this suite. 03/23/23 19:42:01.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:01.259
Mar 23 19:42:01.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context-test 03/23/23 19:42:01.26
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:01.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:01.314
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Mar 23 19:42:01.359: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec" in namespace "security-context-test-7570" to be "Succeeded or Failed"
Mar 23 19:42:01.440: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Pending", Reason="", readiness=false. Elapsed: 80.802714ms
Mar 23 19:42:03.444: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085624579s
Mar 23 19:42:05.447: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Running", Reason="", readiness=true. Elapsed: 4.087928233s
Mar 23 19:42:07.784: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Running", Reason="", readiness=false. Elapsed: 6.425067793s
Mar 23 19:42:09.446: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.087351853s
Mar 23 19:42:09.446: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 19:42:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7570" for this suite. 03/23/23 19:42:09.502
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":210,"skipped":3702,"failed":0}
------------------------------
• [SLOW TEST] [8.256 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:01.259
    Mar 23 19:42:01.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context-test 03/23/23 19:42:01.26
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:01.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:01.314
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Mar 23 19:42:01.359: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec" in namespace "security-context-test-7570" to be "Succeeded or Failed"
    Mar 23 19:42:01.440: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Pending", Reason="", readiness=false. Elapsed: 80.802714ms
    Mar 23 19:42:03.444: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085624579s
    Mar 23 19:42:05.447: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Running", Reason="", readiness=true. Elapsed: 4.087928233s
    Mar 23 19:42:07.784: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Running", Reason="", readiness=false. Elapsed: 6.425067793s
    Mar 23 19:42:09.446: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.087351853s
    Mar 23 19:42:09.446: INFO: Pod "alpine-nnp-false-2b424adf-8520-40ae-bb0d-e833b33955ec" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 19:42:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7570" for this suite. 03/23/23 19:42:09.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:09.522
Mar 23 19:42:09.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:42:09.524
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:09.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:09.548
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-d696722b-3412-4636-8cef-3e0d07a1e000 03/23/23 19:42:09.551
STEP: Creating a pod to test consume secrets 03/23/23 19:42:09.684
Mar 23 19:42:09.708: INFO: Waiting up to 5m0s for pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5" in namespace "secrets-8401" to be "Succeeded or Failed"
Mar 23 19:42:09.722: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.088067ms
Mar 23 19:42:11.728: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019920872s
Mar 23 19:42:13.726: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Running", Reason="", readiness=true. Elapsed: 4.018528407s
Mar 23 19:42:15.727: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019494822s
STEP: Saw pod success 03/23/23 19:42:15.727
Mar 23 19:42:15.727: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5" satisfied condition "Succeeded or Failed"
Mar 23 19:42:15.730: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:42:15.769
Mar 23 19:42:15.783: INFO: Waiting for pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 to disappear
Mar 23 19:42:15.785: INFO: Pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:42:15.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8401" for this suite. 03/23/23 19:42:15.79
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":211,"skipped":3710,"failed":0}
------------------------------
• [SLOW TEST] [6.272 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:09.522
    Mar 23 19:42:09.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:42:09.524
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:09.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:09.548
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-d696722b-3412-4636-8cef-3e0d07a1e000 03/23/23 19:42:09.551
    STEP: Creating a pod to test consume secrets 03/23/23 19:42:09.684
    Mar 23 19:42:09.708: INFO: Waiting up to 5m0s for pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5" in namespace "secrets-8401" to be "Succeeded or Failed"
    Mar 23 19:42:09.722: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.088067ms
    Mar 23 19:42:11.728: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019920872s
    Mar 23 19:42:13.726: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Running", Reason="", readiness=true. Elapsed: 4.018528407s
    Mar 23 19:42:15.727: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019494822s
    STEP: Saw pod success 03/23/23 19:42:15.727
    Mar 23 19:42:15.727: INFO: Pod "pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5" satisfied condition "Succeeded or Failed"
    Mar 23 19:42:15.730: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:42:15.769
    Mar 23 19:42:15.783: INFO: Waiting for pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 to disappear
    Mar 23 19:42:15.785: INFO: Pod pod-secrets-60531a77-a606-4101-ae8f-e03d0a8477e5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:42:15.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8401" for this suite. 03/23/23 19:42:15.79
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:15.796
Mar 23 19:42:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:42:15.798
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:15.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:15.815
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-0815fac8-189a-473a-8622-4adcde6fcf88 03/23/23 19:42:15.826
STEP: Creating the pod 03/23/23 19:42:15.833
Mar 23 19:42:15.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49" in namespace "configmap-8824" to be "running and ready"
Mar 23 19:42:15.849: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.342779ms
Mar 23 19:42:15.849: INFO: The phase of Pod pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:42:17.855: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.015204782s
Mar 23 19:42:17.855: INFO: The phase of Pod pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49 is Running (Ready = true)
Mar 23 19:42:17.855: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-0815fac8-189a-473a-8622-4adcde6fcf88 03/23/23 19:42:17.867
STEP: waiting to observe update in volume 03/23/23 19:42:17.871
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:42:19.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8824" for this suite. 03/23/23 19:42:19.889
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":212,"skipped":3731,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:15.796
    Mar 23 19:42:15.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:42:15.798
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:15.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:15.815
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-0815fac8-189a-473a-8622-4adcde6fcf88 03/23/23 19:42:15.826
    STEP: Creating the pod 03/23/23 19:42:15.833
    Mar 23 19:42:15.840: INFO: Waiting up to 5m0s for pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49" in namespace "configmap-8824" to be "running and ready"
    Mar 23 19:42:15.849: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49": Phase="Pending", Reason="", readiness=false. Elapsed: 9.342779ms
    Mar 23 19:42:15.849: INFO: The phase of Pod pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:42:17.855: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.015204782s
    Mar 23 19:42:17.855: INFO: The phase of Pod pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49 is Running (Ready = true)
    Mar 23 19:42:17.855: INFO: Pod "pod-configmaps-fa5c07ce-cf0b-415f-a802-38301ef09f49" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-0815fac8-189a-473a-8622-4adcde6fcf88 03/23/23 19:42:17.867
    STEP: waiting to observe update in volume 03/23/23 19:42:17.871
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:42:19.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8824" for this suite. 03/23/23 19:42:19.889
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:19.899
Mar 23 19:42:19.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:42:19.901
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:19.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:19.924
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 03/23/23 19:42:19.929
STEP: Creating a ResourceQuota 03/23/23 19:42:24.932
STEP: Ensuring resource quota status is calculated 03/23/23 19:42:24.942
STEP: Creating a Service 03/23/23 19:42:26.947
STEP: Creating a NodePort Service 03/23/23 19:42:26.977
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/23/23 19:42:27.033
STEP: Ensuring resource quota status captures service creation 03/23/23 19:42:27.101
STEP: Deleting Services 03/23/23 19:42:29.106
STEP: Ensuring resource quota status released usage 03/23/23 19:42:29.224
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:42:31.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3876" for this suite. 03/23/23 19:42:31.242
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":213,"skipped":3732,"failed":0}
------------------------------
• [SLOW TEST] [11.362 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:19.899
    Mar 23 19:42:19.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:42:19.901
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:19.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:19.924
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 03/23/23 19:42:19.929
    STEP: Creating a ResourceQuota 03/23/23 19:42:24.932
    STEP: Ensuring resource quota status is calculated 03/23/23 19:42:24.942
    STEP: Creating a Service 03/23/23 19:42:26.947
    STEP: Creating a NodePort Service 03/23/23 19:42:26.977
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 03/23/23 19:42:27.033
    STEP: Ensuring resource quota status captures service creation 03/23/23 19:42:27.101
    STEP: Deleting Services 03/23/23 19:42:29.106
    STEP: Ensuring resource quota status released usage 03/23/23 19:42:29.224
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:42:31.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3876" for this suite. 03/23/23 19:42:31.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:31.27
Mar 23 19:42:31.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:42:31.272
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:31.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:31.301
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 03/23/23 19:42:31.319
Mar 23 19:42:31.319: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Mar 23 19:42:31.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:31.746: INFO: stderr: ""
Mar 23 19:42:31.746: INFO: stdout: "service/agnhost-replica created\n"
Mar 23 19:42:31.746: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Mar 23 19:42:31.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:32.174: INFO: stderr: ""
Mar 23 19:42:32.174: INFO: stdout: "service/agnhost-primary created\n"
Mar 23 19:42:32.175: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Mar 23 19:42:32.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:32.479: INFO: stderr: ""
Mar 23 19:42:32.480: INFO: stdout: "service/frontend created\n"
Mar 23 19:42:32.480: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Mar 23 19:42:32.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:32.864: INFO: stderr: ""
Mar 23 19:42:32.864: INFO: stdout: "deployment.apps/frontend created\n"
Mar 23 19:42:32.864: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 23 19:42:32.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:33.196: INFO: stderr: ""
Mar 23 19:42:33.196: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Mar 23 19:42:33.196: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Mar 23 19:42:33.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
Mar 23 19:42:34.620: INFO: stderr: ""
Mar 23 19:42:34.620: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 03/23/23 19:42:34.62
Mar 23 19:42:34.620: INFO: Waiting for all frontend pods to be Running.
Mar 23 19:42:39.672: INFO: Waiting for frontend to serve content.
Mar 23 19:42:39.684: INFO: Trying to add a new entry to the guestbook.
Mar 23 19:42:39.694: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 03/23/23 19:42:39.701
Mar 23 19:42:39.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:39.865: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:39.865: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 03/23/23 19:42:39.865
Mar 23 19:42:39.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:40.010: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:40.010: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/23/23 19:42:40.01
Mar 23 19:42:40.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:40.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:40.133: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/23/23 19:42:40.133
Mar 23 19:42:40.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:40.233: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:40.233: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 03/23/23 19:42:40.233
Mar 23 19:42:40.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:40.431: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:40.431: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 03/23/23 19:42:40.431
Mar 23 19:42:40.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
Mar 23 19:42:40.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 19:42:40.572: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:42:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1612" for this suite. 03/23/23 19:42:40.579
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":214,"skipped":3737,"failed":0}
------------------------------
• [SLOW TEST] [9.318 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:31.27
    Mar 23 19:42:31.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:42:31.272
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:31.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:31.301
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 03/23/23 19:42:31.319
    Mar 23 19:42:31.319: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Mar 23 19:42:31.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:31.746: INFO: stderr: ""
    Mar 23 19:42:31.746: INFO: stdout: "service/agnhost-replica created\n"
    Mar 23 19:42:31.746: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Mar 23 19:42:31.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:32.174: INFO: stderr: ""
    Mar 23 19:42:32.174: INFO: stdout: "service/agnhost-primary created\n"
    Mar 23 19:42:32.175: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Mar 23 19:42:32.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:32.479: INFO: stderr: ""
    Mar 23 19:42:32.480: INFO: stdout: "service/frontend created\n"
    Mar 23 19:42:32.480: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Mar 23 19:42:32.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:32.864: INFO: stderr: ""
    Mar 23 19:42:32.864: INFO: stdout: "deployment.apps/frontend created\n"
    Mar 23 19:42:32.864: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 23 19:42:32.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:33.196: INFO: stderr: ""
    Mar 23 19:42:33.196: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Mar 23 19:42:33.196: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Mar 23 19:42:33.196: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 create -f -'
    Mar 23 19:42:34.620: INFO: stderr: ""
    Mar 23 19:42:34.620: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 03/23/23 19:42:34.62
    Mar 23 19:42:34.620: INFO: Waiting for all frontend pods to be Running.
    Mar 23 19:42:39.672: INFO: Waiting for frontend to serve content.
    Mar 23 19:42:39.684: INFO: Trying to add a new entry to the guestbook.
    Mar 23 19:42:39.694: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 03/23/23 19:42:39.701
    Mar 23 19:42:39.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:39.865: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:39.865: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 03/23/23 19:42:39.865
    Mar 23 19:42:39.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:40.010: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:40.010: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/23/23 19:42:40.01
    Mar 23 19:42:40.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:40.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:40.133: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/23/23 19:42:40.133
    Mar 23 19:42:40.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:40.233: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:40.233: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 03/23/23 19:42:40.233
    Mar 23 19:42:40.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:40.431: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:40.431: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 03/23/23 19:42:40.431
    Mar 23 19:42:40.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-1612 delete --grace-period=0 --force -f -'
    Mar 23 19:42:40.572: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 19:42:40.572: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:42:40.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1612" for this suite. 03/23/23 19:42:40.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:40.588
Mar 23 19:42:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename certificates 03/23/23 19:42:40.591
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:40.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:40.64
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 03/23/23 19:42:41.757
STEP: getting /apis/certificates.k8s.io 03/23/23 19:42:41.762
STEP: getting /apis/certificates.k8s.io/v1 03/23/23 19:42:41.763
STEP: creating 03/23/23 19:42:41.764
STEP: getting 03/23/23 19:42:41.783
STEP: listing 03/23/23 19:42:41.786
STEP: watching 03/23/23 19:42:41.789
Mar 23 19:42:41.789: INFO: starting watch
STEP: patching 03/23/23 19:42:41.79
STEP: updating 03/23/23 19:42:41.797
Mar 23 19:42:41.811: INFO: waiting for watch events with expected annotations
Mar 23 19:42:41.812: INFO: saw patched and updated annotations
STEP: getting /approval 03/23/23 19:42:41.813
STEP: patching /approval 03/23/23 19:42:41.818
STEP: updating /approval 03/23/23 19:42:41.826
STEP: getting /status 03/23/23 19:42:41.835
STEP: patching /status 03/23/23 19:42:41.838
STEP: updating /status 03/23/23 19:42:41.849
STEP: deleting 03/23/23 19:42:41.863
STEP: deleting a collection 03/23/23 19:42:41.874
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:42:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-5007" for this suite. 03/23/23 19:42:41.891
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":215,"skipped":3753,"failed":0}
------------------------------
• [1.308 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:40.588
    Mar 23 19:42:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename certificates 03/23/23 19:42:40.591
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:40.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:40.64
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 03/23/23 19:42:41.757
    STEP: getting /apis/certificates.k8s.io 03/23/23 19:42:41.762
    STEP: getting /apis/certificates.k8s.io/v1 03/23/23 19:42:41.763
    STEP: creating 03/23/23 19:42:41.764
    STEP: getting 03/23/23 19:42:41.783
    STEP: listing 03/23/23 19:42:41.786
    STEP: watching 03/23/23 19:42:41.789
    Mar 23 19:42:41.789: INFO: starting watch
    STEP: patching 03/23/23 19:42:41.79
    STEP: updating 03/23/23 19:42:41.797
    Mar 23 19:42:41.811: INFO: waiting for watch events with expected annotations
    Mar 23 19:42:41.812: INFO: saw patched and updated annotations
    STEP: getting /approval 03/23/23 19:42:41.813
    STEP: patching /approval 03/23/23 19:42:41.818
    STEP: updating /approval 03/23/23 19:42:41.826
    STEP: getting /status 03/23/23 19:42:41.835
    STEP: patching /status 03/23/23 19:42:41.838
    STEP: updating /status 03/23/23 19:42:41.849
    STEP: deleting 03/23/23 19:42:41.863
    STEP: deleting a collection 03/23/23 19:42:41.874
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:42:41.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-5007" for this suite. 03/23/23 19:42:41.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:42:41.898
Mar 23 19:42:41.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:42:41.9
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:41.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:41.931
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-3908 03/23/23 19:42:41.934
W0323 19:42:41.952113      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Mar 23 19:42:41.952: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3908" to be "running and ready"
Mar 23 19:42:41.966: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150071ms
Mar 23 19:42:41.966: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:42:43.977: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025176826s
Mar 23 19:42:43.977: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:42:45.976: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 4.023977826s
Mar 23 19:42:45.976: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Mar 23 19:42:45.976: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Mar 23 19:42:45.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Mar 23 19:42:46.229: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Mar 23 19:42:46.229: INFO: stdout: "iptables"
Mar 23 19:42:46.229: INFO: proxyMode: iptables
Mar 23 19:42:46.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Mar 23 19:42:46.251: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-3908 03/23/23 19:42:46.251
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3908 03/23/23 19:42:46.274
I0323 19:42:46.310314      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3908, replica count: 3
I0323 19:42:49.361551      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:42:52.361736      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:42:52.369: INFO: Creating new exec pod
Mar 23 19:42:52.379: INFO: Waiting up to 5m0s for pod "execpod-affinityt7x4g" in namespace "services-3908" to be "running"
Mar 23 19:42:52.382: INFO: Pod "execpod-affinityt7x4g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851191ms
Mar 23 19:42:54.387: INFO: Pod "execpod-affinityt7x4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.007912861s
Mar 23 19:42:54.387: INFO: Pod "execpod-affinityt7x4g" satisfied condition "running"
Mar 23 19:42:55.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Mar 23 19:42:55.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Mar 23 19:42:55.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:42:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.175.35 80'
Mar 23 19:42:55.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.175.35 80\nConnection to 10.0.175.35 80 port [tcp/http] succeeded!\n"
Mar 23 19:42:55.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:42:55.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.175.35:80/ ; done'
Mar 23 19:42:56.173: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
Mar 23 19:42:56.173: INFO: stdout: "\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt"
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
Mar 23 19:42:56.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.175.35:80/'
Mar 23 19:42:56.402: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
Mar 23 19:42:56.402: INFO: stdout: "affinity-clusterip-timeout-fhvpt"
Mar 23 19:43:16.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.175.35:80/'
Mar 23 19:43:16.625: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
Mar 23 19:43:16.625: INFO: stdout: "affinity-clusterip-timeout-2ct8j"
Mar 23 19:43:16.625: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3908, will wait for the garbage collector to delete the pods 03/23/23 19:43:16.654
Mar 23 19:43:16.715: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.344285ms
Mar 23 19:43:16.816: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.68297ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:43:24.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3908" for this suite. 03/23/23 19:43:24.634
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":216,"skipped":3758,"failed":0}
------------------------------
• [SLOW TEST] [42.747 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:42:41.898
    Mar 23 19:42:41.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:42:41.9
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:42:41.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:42:41.931
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-3908 03/23/23 19:42:41.934
    W0323 19:42:41.952113      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Mar 23 19:42:41.952: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3908" to be "running and ready"
    Mar 23 19:42:41.966: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150071ms
    Mar 23 19:42:41.966: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:42:43.977: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025176826s
    Mar 23 19:42:43.977: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:42:45.976: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 4.023977826s
    Mar 23 19:42:45.976: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Mar 23 19:42:45.976: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Mar 23 19:42:45.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Mar 23 19:42:46.229: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Mar 23 19:42:46.229: INFO: stdout: "iptables"
    Mar 23 19:42:46.229: INFO: proxyMode: iptables
    Mar 23 19:42:46.247: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Mar 23 19:42:46.251: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-3908 03/23/23 19:42:46.251
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-3908 03/23/23 19:42:46.274
    I0323 19:42:46.310314      19 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3908, replica count: 3
    I0323 19:42:49.361551      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 19:42:52.361736      19 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:42:52.369: INFO: Creating new exec pod
    Mar 23 19:42:52.379: INFO: Waiting up to 5m0s for pod "execpod-affinityt7x4g" in namespace "services-3908" to be "running"
    Mar 23 19:42:52.382: INFO: Pod "execpod-affinityt7x4g": Phase="Pending", Reason="", readiness=false. Elapsed: 3.851191ms
    Mar 23 19:42:54.387: INFO: Pod "execpod-affinityt7x4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.007912861s
    Mar 23 19:42:54.387: INFO: Pod "execpod-affinityt7x4g" satisfied condition "running"
    Mar 23 19:42:55.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Mar 23 19:42:55.620: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Mar 23 19:42:55.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:42:55.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.175.35 80'
    Mar 23 19:42:55.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.175.35 80\nConnection to 10.0.175.35 80 port [tcp/http] succeeded!\n"
    Mar 23 19:42:55.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:42:55.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.175.35:80/ ; done'
    Mar 23 19:42:56.173: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
    Mar 23 19:42:56.173: INFO: stdout: "\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt\naffinity-clusterip-timeout-fhvpt"
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Received response from host: affinity-clusterip-timeout-fhvpt
    Mar 23 19:42:56.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.175.35:80/'
    Mar 23 19:42:56.402: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
    Mar 23 19:42:56.402: INFO: stdout: "affinity-clusterip-timeout-fhvpt"
    Mar 23 19:43:16.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3908 exec execpod-affinityt7x4g -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.175.35:80/'
    Mar 23 19:43:16.625: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.175.35:80/\n"
    Mar 23 19:43:16.625: INFO: stdout: "affinity-clusterip-timeout-2ct8j"
    Mar 23 19:43:16.625: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3908, will wait for the garbage collector to delete the pods 03/23/23 19:43:16.654
    Mar 23 19:43:16.715: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 6.344285ms
    Mar 23 19:43:16.816: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.68297ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:43:24.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3908" for this suite. 03/23/23 19:43:24.634
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:43:24.656
Mar 23 19:43:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename subpath 03/23/23 19:43:24.657
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:43:24.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:43:24.685
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/23/23 19:43:24.688
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-g7hp 03/23/23 19:43:24.706
STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:43:24.706
Mar 23 19:43:24.720: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g7hp" in namespace "subpath-5783" to be "Succeeded or Failed"
Mar 23 19:43:24.726: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751686ms
Mar 23 19:43:26.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010155489s
Mar 23 19:43:28.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010172402s
Mar 23 19:43:30.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 6.009865716s
Mar 23 19:43:32.732: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 8.011450524s
Mar 23 19:43:34.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010217537s
Mar 23 19:43:36.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011074946s
Mar 23 19:43:38.733: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 14.012638553s
Mar 23 19:43:40.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 16.009337771s
Mar 23 19:43:42.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 18.009837081s
Mar 23 19:43:44.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 20.010224091s
Mar 23 19:43:46.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=false. Elapsed: 22.0108182s
Mar 23 19:43:48.736: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015374003s
STEP: Saw pod success 03/23/23 19:43:48.736
Mar 23 19:43:48.736: INFO: Pod "pod-subpath-test-downwardapi-g7hp" satisfied condition "Succeeded or Failed"
Mar 23 19:43:48.743: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-subpath-test-downwardapi-g7hp container test-container-subpath-downwardapi-g7hp: <nil>
STEP: delete the pod 03/23/23 19:43:48.754
Mar 23 19:43:48.776: INFO: Waiting for pod pod-subpath-test-downwardapi-g7hp to disappear
Mar 23 19:43:48.779: INFO: Pod pod-subpath-test-downwardapi-g7hp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-g7hp 03/23/23 19:43:48.779
Mar 23 19:43:48.780: INFO: Deleting pod "pod-subpath-test-downwardapi-g7hp" in namespace "subpath-5783"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 23 19:43:48.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5783" for this suite. 03/23/23 19:43:48.788
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":217,"skipped":3815,"failed":0}
------------------------------
• [SLOW TEST] [24.139 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:43:24.656
    Mar 23 19:43:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename subpath 03/23/23 19:43:24.657
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:43:24.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:43:24.685
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/23/23 19:43:24.688
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-g7hp 03/23/23 19:43:24.706
    STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:43:24.706
    Mar 23 19:43:24.720: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-g7hp" in namespace "subpath-5783" to be "Succeeded or Failed"
    Mar 23 19:43:24.726: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Pending", Reason="", readiness=false. Elapsed: 5.751686ms
    Mar 23 19:43:26.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 2.010155489s
    Mar 23 19:43:28.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 4.010172402s
    Mar 23 19:43:30.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 6.009865716s
    Mar 23 19:43:32.732: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 8.011450524s
    Mar 23 19:43:34.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 10.010217537s
    Mar 23 19:43:36.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 12.011074946s
    Mar 23 19:43:38.733: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 14.012638553s
    Mar 23 19:43:40.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 16.009337771s
    Mar 23 19:43:42.730: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 18.009837081s
    Mar 23 19:43:44.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=true. Elapsed: 20.010224091s
    Mar 23 19:43:46.731: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Running", Reason="", readiness=false. Elapsed: 22.0108182s
    Mar 23 19:43:48.736: INFO: Pod "pod-subpath-test-downwardapi-g7hp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015374003s
    STEP: Saw pod success 03/23/23 19:43:48.736
    Mar 23 19:43:48.736: INFO: Pod "pod-subpath-test-downwardapi-g7hp" satisfied condition "Succeeded or Failed"
    Mar 23 19:43:48.743: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-subpath-test-downwardapi-g7hp container test-container-subpath-downwardapi-g7hp: <nil>
    STEP: delete the pod 03/23/23 19:43:48.754
    Mar 23 19:43:48.776: INFO: Waiting for pod pod-subpath-test-downwardapi-g7hp to disappear
    Mar 23 19:43:48.779: INFO: Pod pod-subpath-test-downwardapi-g7hp no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-g7hp 03/23/23 19:43:48.779
    Mar 23 19:43:48.780: INFO: Deleting pod "pod-subpath-test-downwardapi-g7hp" in namespace "subpath-5783"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 23 19:43:48.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5783" for this suite. 03/23/23 19:43:48.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:43:48.811
Mar 23 19:43:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 19:43:48.813
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:43:48.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:43:48.833
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Mar 23 19:43:48.871: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:43:48.88
Mar 23 19:43:48.885: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:48.886: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:48.886: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:48.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:43:48.890: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:49.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:43:49.906: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:50.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 19:43:50.899: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
Mar 23 19:43:51.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:51.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:51.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:51.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 19:43:51.899: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:52.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 19:43:52.901: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 03/23/23 19:43:52.919
STEP: Check that daemon pods images are updated. 03/23/23 19:43:52.936
Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-v46j9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:52.957: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:52.957: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:52.958: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:53.966: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:53.966: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:54.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:54.963: INFO: Pod daemon-set-58qfv is not available
Mar 23 19:43:54.963: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:55.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:55.964: INFO: Pod daemon-set-58qfv is not available
Mar 23 19:43:55.964: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:55.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:55.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:55.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:56.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:57.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:58.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:59.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:43:59.963: INFO: Pod daemon-set-8bvcx is not available
Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:00.973: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Mar 23 19:44:00.973: INFO: Pod daemon-set-8bvcx is not available
Mar 23 19:44:00.978: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:00.979: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:00.979: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.963: INFO: Pod daemon-set-7mfsf is not available
Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 03/23/23 19:44:03.969
Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:03.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 19:44:03.979: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 19:44:04.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 19:44:04.992: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 19:44:05.007
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7792, will wait for the garbage collector to delete the pods 03/23/23 19:44:05.007
Mar 23 19:44:05.073: INFO: Deleting DaemonSet.extensions daemon-set took: 12.833771ms
Mar 23 19:44:05.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.381562ms
Mar 23 19:44:09.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 19:44:09.682: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 19:44:09.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30581"},"items":null}

Mar 23 19:44:09.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30581"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:44:09.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7792" for this suite. 03/23/23 19:44:09.705
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":218,"skipped":3823,"failed":0}
------------------------------
• [SLOW TEST] [20.903 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:43:48.811
    Mar 23 19:43:48.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 19:43:48.813
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:43:48.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:43:48.833
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Mar 23 19:43:48.871: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 19:43:48.88
    Mar 23 19:43:48.885: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:48.886: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:48.886: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:48.890: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:43:48.890: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:49.901: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:49.906: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:43:49.906: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:50.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:50.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 19:43:50.899: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
    Mar 23 19:43:51.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:51.895: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:51.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:51.899: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 19:43:51.899: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
    Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:52.896: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:52.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 19:43:52.901: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 03/23/23 19:43:52.919
    STEP: Check that daemon pods images are updated. 03/23/23 19:43:52.936
    Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:52.943: INFO: Wrong image for pod: daemon-set-v46j9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:52.957: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:52.957: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:52.958: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:53.966: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:53.966: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:53.972: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:54.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:54.963: INFO: Pod daemon-set-58qfv is not available
    Mar 23 19:43:54.963: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:54.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:55.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:55.964: INFO: Pod daemon-set-58qfv is not available
    Mar 23 19:43:55.964: INFO: Wrong image for pod: daemon-set-nss9n. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:55.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:55.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:55.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:56.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:56.973: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:57.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:57.967: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:58.964: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:58.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:59.963: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:43:59.963: INFO: Pod daemon-set-8bvcx is not available
    Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:43:59.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:00.973: INFO: Wrong image for pod: daemon-set-496z6. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Mar 23 19:44:00.973: INFO: Pod daemon-set-8bvcx is not available
    Mar 23 19:44:00.978: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:00.979: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:00.979: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:01.981: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:02.968: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.963: INFO: Pod daemon-set-7mfsf is not available
    Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.969: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 03/23/23 19:44:03.969
    Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.975: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:03.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 19:44:03.979: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:04.986: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 19:44:04.992: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 19:44:04.992: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 19:44:05.007
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7792, will wait for the garbage collector to delete the pods 03/23/23 19:44:05.007
    Mar 23 19:44:05.073: INFO: Deleting DaemonSet.extensions daemon-set took: 12.833771ms
    Mar 23 19:44:05.177: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.381562ms
    Mar 23 19:44:09.682: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 19:44:09.682: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 19:44:09.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30581"},"items":null}

    Mar 23 19:44:09.687: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30581"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:44:09.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7792" for this suite. 03/23/23 19:44:09.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:09.716
Mar 23 19:44:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:44:09.723
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:09.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:09.752
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 03/23/23 19:44:09.758
STEP: Creating a ResourceQuota 03/23/23 19:44:14.762
STEP: Ensuring resource quota status is calculated 03/23/23 19:44:14.771
STEP: Creating a ReplicationController 03/23/23 19:44:16.775
STEP: Ensuring resource quota status captures replication controller creation 03/23/23 19:44:16.791
STEP: Deleting a ReplicationController 03/23/23 19:44:18.796
STEP: Ensuring resource quota status released usage 03/23/23 19:44:18.801
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:44:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4795" for this suite. 03/23/23 19:44:20.811
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":219,"skipped":3853,"failed":0}
------------------------------
• [SLOW TEST] [11.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:09.716
    Mar 23 19:44:09.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:44:09.723
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:09.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:09.752
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 03/23/23 19:44:09.758
    STEP: Creating a ResourceQuota 03/23/23 19:44:14.762
    STEP: Ensuring resource quota status is calculated 03/23/23 19:44:14.771
    STEP: Creating a ReplicationController 03/23/23 19:44:16.775
    STEP: Ensuring resource quota status captures replication controller creation 03/23/23 19:44:16.791
    STEP: Deleting a ReplicationController 03/23/23 19:44:18.796
    STEP: Ensuring resource quota status released usage 03/23/23 19:44:18.801
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:44:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4795" for this suite. 03/23/23 19:44:20.811
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:20.827
Mar 23 19:44:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:44:20.828
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:20.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:20.851
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-6493 03/23/23 19:44:20.855
STEP: creating service affinity-nodeport-transition in namespace services-6493 03/23/23 19:44:20.856
STEP: creating replication controller affinity-nodeport-transition in namespace services-6493 03/23/23 19:44:20.886
I0323 19:44:20.912912      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6493, replica count: 3
I0323 19:44:23.967398      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:44:26.967784      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:44:26.978: INFO: Creating new exec pod
Mar 23 19:44:26.989: INFO: Waiting up to 5m0s for pod "execpod-affinityhpk4w" in namespace "services-6493" to be "running"
Mar 23 19:44:27.001: INFO: Pod "execpod-affinityhpk4w": Phase="Pending", Reason="", readiness=false. Elapsed: 11.174574ms
Mar 23 19:44:29.006: INFO: Pod "execpod-affinityhpk4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.016319853s
Mar 23 19:44:29.006: INFO: Pod "execpod-affinityhpk4w" satisfied condition "running"
Mar 23 19:44:30.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Mar 23 19:44:30.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Mar 23 19:44:30.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:44:30.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.22.177 80'
Mar 23 19:44:30.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.22.177 80\nConnection to 10.0.22.177 80 port [tcp/http] succeeded!\n"
Mar 23 19:44:30.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:44:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 30538'
Mar 23 19:44:30.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 30538\nConnection to 10.240.0.4 30538 port [tcp/*] succeeded!\n"
Mar 23 19:44:30.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:44:30.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 30538'
Mar 23 19:44:30.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 30538\nConnection to 10.240.0.56 30538 port [tcp/*] succeeded!\n"
Mar 23 19:44:30.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:44:30.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:30538/ ; done'
Mar 23 19:44:31.418: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n"
Mar 23 19:44:31.418: INFO: stdout: "\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt"
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:30538/ ; done'
Mar 23 19:44:31.816: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n"
Mar 23 19:44:31.816: INFO: stdout: "\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt"
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
Mar 23 19:44:31.816: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6493, will wait for the garbage collector to delete the pods 03/23/23 19:44:31.83
Mar 23 19:44:31.890: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.935786ms
Mar 23 19:44:31.991: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.075767ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:44:40.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6493" for this suite. 03/23/23 19:44:40.847
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":220,"skipped":3856,"failed":0}
------------------------------
• [SLOW TEST] [20.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:20.827
    Mar 23 19:44:20.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:44:20.828
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:20.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:20.851
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-6493 03/23/23 19:44:20.855
    STEP: creating service affinity-nodeport-transition in namespace services-6493 03/23/23 19:44:20.856
    STEP: creating replication controller affinity-nodeport-transition in namespace services-6493 03/23/23 19:44:20.886
    I0323 19:44:20.912912      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-6493, replica count: 3
    I0323 19:44:23.967398      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 19:44:26.967784      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:44:26.978: INFO: Creating new exec pod
    Mar 23 19:44:26.989: INFO: Waiting up to 5m0s for pod "execpod-affinityhpk4w" in namespace "services-6493" to be "running"
    Mar 23 19:44:27.001: INFO: Pod "execpod-affinityhpk4w": Phase="Pending", Reason="", readiness=false. Elapsed: 11.174574ms
    Mar 23 19:44:29.006: INFO: Pod "execpod-affinityhpk4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.016319853s
    Mar 23 19:44:29.006: INFO: Pod "execpod-affinityhpk4w" satisfied condition "running"
    Mar 23 19:44:30.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Mar 23 19:44:30.252: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Mar 23 19:44:30.252: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:44:30.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.22.177 80'
    Mar 23 19:44:30.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.22.177 80\nConnection to 10.0.22.177 80 port [tcp/http] succeeded!\n"
    Mar 23 19:44:30.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:44:30.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 30538'
    Mar 23 19:44:30.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.4 30538\nConnection to 10.240.0.4 30538 port [tcp/*] succeeded!\n"
    Mar 23 19:44:30.732: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:44:30.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 30538'
    Mar 23 19:44:30.977: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.56 30538\nConnection to 10.240.0.56 30538 port [tcp/*] succeeded!\n"
    Mar 23 19:44:30.977: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:44:30.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:30538/ ; done'
    Mar 23 19:44:31.418: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n"
    Mar 23 19:44:31.418: INFO: stdout: "\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-thftt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-9r2wd\naffinity-nodeport-transition-krxgt"
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-thftt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-9r2wd
    Mar 23 19:44:31.418: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-6493 exec execpod-affinityhpk4w -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:30538/ ; done'
    Mar 23 19:44:31.816: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:30538/\n"
    Mar 23 19:44:31.816: INFO: stdout: "\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt\naffinity-nodeport-transition-krxgt"
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Received response from host: affinity-nodeport-transition-krxgt
    Mar 23 19:44:31.816: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-6493, will wait for the garbage collector to delete the pods 03/23/23 19:44:31.83
    Mar 23 19:44:31.890: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.935786ms
    Mar 23 19:44:31.991: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.075767ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:44:40.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6493" for this suite. 03/23/23 19:44:40.847
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:40.872
Mar 23 19:44:40.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:44:40.873
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:40.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:40.897
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:44:40.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4477" for this suite. 03/23/23 19:44:40.911
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":221,"skipped":3876,"failed":0}
------------------------------
• [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:40.872
    Mar 23 19:44:40.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:44:40.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:40.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:40.897
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:44:40.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4477" for this suite. 03/23/23 19:44:40.911
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:40.924
Mar 23 19:44:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename watch 03/23/23 19:44:40.926
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:40.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:40.965
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 03/23/23 19:44:40.967
STEP: creating a new configmap 03/23/23 19:44:40.969
STEP: modifying the configmap once 03/23/23 19:44:40.975
STEP: closing the watch once it receives two notifications 03/23/23 19:44:40.986
Mar 23 19:44:40.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30852 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:44:40.987: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30854 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 03/23/23 19:44:40.987
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/23/23 19:44:40.998
STEP: deleting the configmap 03/23/23 19:44:41
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/23/23 19:44:41.011
Mar 23 19:44:41.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30856 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:44:41.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30857 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 23 19:44:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7749" for this suite. 03/23/23 19:44:41.029
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":222,"skipped":3883,"failed":0}
------------------------------
• [0.117 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:40.924
    Mar 23 19:44:40.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename watch 03/23/23 19:44:40.926
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:40.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:40.965
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 03/23/23 19:44:40.967
    STEP: creating a new configmap 03/23/23 19:44:40.969
    STEP: modifying the configmap once 03/23/23 19:44:40.975
    STEP: closing the watch once it receives two notifications 03/23/23 19:44:40.986
    Mar 23 19:44:40.986: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30852 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:44:40.987: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30854 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 03/23/23 19:44:40.987
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 03/23/23 19:44:40.998
    STEP: deleting the configmap 03/23/23 19:44:41
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 03/23/23 19:44:41.011
    Mar 23 19:44:41.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30856 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:44:41.012: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7749  b48f7ff4-4b8b-4697-bba9-0820e591ca1a 30857 0 2023-03-23 19:44:40 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-03-23 19:44:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 23 19:44:41.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7749" for this suite. 03/23/23 19:44:41.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:41.043
Mar 23 19:44:41.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:44:41.044
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:41.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:41.072
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 03/23/23 19:44:41.075
Mar 23 19:44:41.098: INFO: Waiting up to 5m0s for pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7" in namespace "emptydir-2249" to be "Succeeded or Failed"
Mar 23 19:44:41.102: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060989ms
Mar 23 19:44:43.107: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008994718s
Mar 23 19:44:45.107: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=true. Elapsed: 4.009612821s
Mar 23 19:44:47.106: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=false. Elapsed: 6.008221887s
Mar 23 19:44:49.112: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013836638s
STEP: Saw pod success 03/23/23 19:44:49.112
Mar 23 19:44:49.112: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7" satisfied condition "Succeeded or Failed"
Mar 23 19:44:49.115: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 container test-container: <nil>
STEP: delete the pod 03/23/23 19:44:49.154
Mar 23 19:44:49.168: INFO: Waiting for pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 to disappear
Mar 23 19:44:49.171: INFO: Pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:44:49.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2249" for this suite. 03/23/23 19:44:49.176
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":223,"skipped":3917,"failed":0}
------------------------------
• [SLOW TEST] [8.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:41.043
    Mar 23 19:44:41.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:44:41.044
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:41.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:41.072
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/23/23 19:44:41.075
    Mar 23 19:44:41.098: INFO: Waiting up to 5m0s for pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7" in namespace "emptydir-2249" to be "Succeeded or Failed"
    Mar 23 19:44:41.102: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060989ms
    Mar 23 19:44:43.107: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008994718s
    Mar 23 19:44:45.107: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=true. Elapsed: 4.009612821s
    Mar 23 19:44:47.106: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Running", Reason="", readiness=false. Elapsed: 6.008221887s
    Mar 23 19:44:49.112: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.013836638s
    STEP: Saw pod success 03/23/23 19:44:49.112
    Mar 23 19:44:49.112: INFO: Pod "pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7" satisfied condition "Succeeded or Failed"
    Mar 23 19:44:49.115: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 container test-container: <nil>
    STEP: delete the pod 03/23/23 19:44:49.154
    Mar 23 19:44:49.168: INFO: Waiting for pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 to disappear
    Mar 23 19:44:49.171: INFO: Pod pod-bc11bb95-3b44-4af4-86df-0f6ea64e19e7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:44:49.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2249" for this suite. 03/23/23 19:44:49.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:49.191
Mar 23 19:44:49.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 19:44:49.192
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:49.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:49.215
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Mar 23 19:44:49.233: INFO: Waiting up to 5m0s for pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f" in namespace "pods-7827" to be "running and ready"
Mar 23 19:44:49.236: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991293ms
Mar 23 19:44:49.236: INFO: The phase of Pod server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:44:51.244: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010643539s
Mar 23 19:44:51.244: INFO: The phase of Pod server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f is Running (Ready = true)
Mar 23 19:44:51.244: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f" satisfied condition "running and ready"
Mar 23 19:44:51.295: INFO: Waiting up to 5m0s for pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd" in namespace "pods-7827" to be "Succeeded or Failed"
Mar 23 19:44:51.304: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635578ms
Mar 23 19:44:53.313: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=true. Elapsed: 2.01832148s
Mar 23 19:44:55.311: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=true. Elapsed: 4.016385188s
Mar 23 19:44:57.312: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=false. Elapsed: 6.017328188s
Mar 23 19:44:59.318: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022847678s
STEP: Saw pod success 03/23/23 19:44:59.318
Mar 23 19:44:59.318: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd" satisfied condition "Succeeded or Failed"
Mar 23 19:44:59.321: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd container env3cont: <nil>
STEP: delete the pod 03/23/23 19:44:59.328
Mar 23 19:44:59.344: INFO: Waiting for pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd to disappear
Mar 23 19:44:59.349: INFO: Pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 19:44:59.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7827" for this suite. 03/23/23 19:44:59.355
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":224,"skipped":3954,"failed":0}
------------------------------
• [SLOW TEST] [10.171 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:49.191
    Mar 23 19:44:49.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 19:44:49.192
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:49.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:49.215
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Mar 23 19:44:49.233: INFO: Waiting up to 5m0s for pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f" in namespace "pods-7827" to be "running and ready"
    Mar 23 19:44:49.236: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991293ms
    Mar 23 19:44:49.236: INFO: The phase of Pod server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:44:51.244: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010643539s
    Mar 23 19:44:51.244: INFO: The phase of Pod server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f is Running (Ready = true)
    Mar 23 19:44:51.244: INFO: Pod "server-envvars-ce9a9341-2de3-4a2a-83e2-36bd89520b2f" satisfied condition "running and ready"
    Mar 23 19:44:51.295: INFO: Waiting up to 5m0s for pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd" in namespace "pods-7827" to be "Succeeded or Failed"
    Mar 23 19:44:51.304: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635578ms
    Mar 23 19:44:53.313: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=true. Elapsed: 2.01832148s
    Mar 23 19:44:55.311: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=true. Elapsed: 4.016385188s
    Mar 23 19:44:57.312: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Running", Reason="", readiness=false. Elapsed: 6.017328188s
    Mar 23 19:44:59.318: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.022847678s
    STEP: Saw pod success 03/23/23 19:44:59.318
    Mar 23 19:44:59.318: INFO: Pod "client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd" satisfied condition "Succeeded or Failed"
    Mar 23 19:44:59.321: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd container env3cont: <nil>
    STEP: delete the pod 03/23/23 19:44:59.328
    Mar 23 19:44:59.344: INFO: Waiting for pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd to disappear
    Mar 23 19:44:59.349: INFO: Pod client-envvars-3e8c0afe-8971-43a4-914d-94bf980b9acd no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 19:44:59.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7827" for this suite. 03/23/23 19:44:59.355
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:59.366
Mar 23 19:44:59.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename endpointslice 03/23/23 19:44:59.369
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:59.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:59.392
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Mar 23 19:44:59.418: INFO: Endpoints addresses: [10.255.255.5 10.255.255.6 10.255.255.7] , ports: [443]
Mar 23 19:44:59.419: INFO: EndpointSlices addresses: [10.255.255.5 10.255.255.6 10.255.255.7] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 23 19:44:59.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1404" for this suite. 03/23/23 19:44:59.429
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":225,"skipped":3979,"failed":0}
------------------------------
• [0.069 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:59.366
    Mar 23 19:44:59.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename endpointslice 03/23/23 19:44:59.369
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:59.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:59.392
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Mar 23 19:44:59.418: INFO: Endpoints addresses: [10.255.255.5 10.255.255.6 10.255.255.7] , ports: [443]
    Mar 23 19:44:59.419: INFO: EndpointSlices addresses: [10.255.255.5 10.255.255.6 10.255.255.7] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 23 19:44:59.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1404" for this suite. 03/23/23 19:44:59.429
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:44:59.435
Mar 23 19:44:59.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:44:59.437
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:59.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:59.46
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:44:59.488
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:45:00.339
STEP: Deploying the webhook pod 03/23/23 19:45:00.347
STEP: Wait for the deployment to be ready 03/23/23 19:45:00.362
Mar 23 19:45:00.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:45:02.396
STEP: Verifying the service has paired with the endpoint 03/23/23 19:45:02.41
Mar 23 19:45:03.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Mar 23 19:45:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2293-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 19:45:03.93
Mar 23 19:45:03.948: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook 03/23/23 19:45:04.063
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:45:06.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4272" for this suite. 03/23/23 19:45:06.779
STEP: Destroying namespace "webhook-4272-markers" for this suite. 03/23/23 19:45:06.784
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":226,"skipped":3982,"failed":0}
------------------------------
• [SLOW TEST] [7.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:44:59.435
    Mar 23 19:44:59.436: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:44:59.437
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:44:59.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:44:59.46
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:44:59.488
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:45:00.339
    STEP: Deploying the webhook pod 03/23/23 19:45:00.347
    STEP: Wait for the deployment to be ready 03/23/23 19:45:00.362
    Mar 23 19:45:00.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:45:02.396
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:45:02.41
    Mar 23 19:45:03.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Mar 23 19:45:03.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2293-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 19:45:03.93
    Mar 23 19:45:03.948: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be mutated by the webhook 03/23/23 19:45:04.063
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:45:06.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4272" for this suite. 03/23/23 19:45:06.779
    STEP: Destroying namespace "webhook-4272-markers" for this suite. 03/23/23 19:45:06.784
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:45:06.898
Mar 23 19:45:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:45:06.899
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:06.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:06.97
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3768 03/23/23 19:45:06.976
STEP: creating service affinity-clusterip in namespace services-3768 03/23/23 19:45:06.976
STEP: creating replication controller affinity-clusterip in namespace services-3768 03/23/23 19:45:07
I0323 19:45:07.020232      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3768, replica count: 3
I0323 19:45:10.071714      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:45:13.071984      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:45:13.079: INFO: Creating new exec pod
Mar 23 19:45:13.088: INFO: Waiting up to 5m0s for pod "execpod-affinity7hrfj" in namespace "services-3768" to be "running"
Mar 23 19:45:13.097: INFO: Pod "execpod-affinity7hrfj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91048ms
Mar 23 19:45:15.104: INFO: Pod "execpod-affinity7hrfj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015948521s
Mar 23 19:45:17.102: INFO: Pod "execpod-affinity7hrfj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013648967s
Mar 23 19:45:17.102: INFO: Pod "execpod-affinity7hrfj" satisfied condition "running"
Mar 23 19:45:18.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Mar 23 19:45:18.346: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Mar 23 19:45:18.346: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:45:18.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.54.8 80'
Mar 23 19:45:18.564: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.0.54.8 80\nConnection to 10.0.54.8 80 port [tcp/http] succeeded!\n"
Mar 23 19:45:18.564: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 19:45:18.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.54.8:80/ ; done'
Mar 23 19:45:18.918: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n"
Mar 23 19:45:18.918: INFO: stdout: "\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm"
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
Mar 23 19:45:18.918: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3768, will wait for the garbage collector to delete the pods 03/23/23 19:45:18.947
Mar 23 19:45:19.008: INFO: Deleting ReplicationController affinity-clusterip took: 6.354385ms
Mar 23 19:45:19.108: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.972958ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:45:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3768" for this suite. 03/23/23 19:45:25.229
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":227,"skipped":4043,"failed":0}
------------------------------
• [SLOW TEST] [18.357 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:45:06.898
    Mar 23 19:45:06.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:45:06.899
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:06.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:06.97
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3768 03/23/23 19:45:06.976
    STEP: creating service affinity-clusterip in namespace services-3768 03/23/23 19:45:06.976
    STEP: creating replication controller affinity-clusterip in namespace services-3768 03/23/23 19:45:07
    I0323 19:45:07.020232      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3768, replica count: 3
    I0323 19:45:10.071714      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 19:45:13.071984      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:45:13.079: INFO: Creating new exec pod
    Mar 23 19:45:13.088: INFO: Waiting up to 5m0s for pod "execpod-affinity7hrfj" in namespace "services-3768" to be "running"
    Mar 23 19:45:13.097: INFO: Pod "execpod-affinity7hrfj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.91048ms
    Mar 23 19:45:15.104: INFO: Pod "execpod-affinity7hrfj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015948521s
    Mar 23 19:45:17.102: INFO: Pod "execpod-affinity7hrfj": Phase="Running", Reason="", readiness=true. Elapsed: 4.013648967s
    Mar 23 19:45:17.102: INFO: Pod "execpod-affinity7hrfj" satisfied condition "running"
    Mar 23 19:45:18.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Mar 23 19:45:18.346: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Mar 23 19:45:18.346: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:45:18.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.54.8 80'
    Mar 23 19:45:18.564: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 10.0.54.8 80\nConnection to 10.0.54.8 80 port [tcp/http] succeeded!\n"
    Mar 23 19:45:18.564: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 19:45:18.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-3768 exec execpod-affinity7hrfj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.54.8:80/ ; done'
    Mar 23 19:45:18.918: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.54.8:80/\n"
    Mar 23 19:45:18.918: INFO: stdout: "\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm\naffinity-clusterip-pcplm"
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Received response from host: affinity-clusterip-pcplm
    Mar 23 19:45:18.918: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3768, will wait for the garbage collector to delete the pods 03/23/23 19:45:18.947
    Mar 23 19:45:19.008: INFO: Deleting ReplicationController affinity-clusterip took: 6.354385ms
    Mar 23 19:45:19.108: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.972958ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:45:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3768" for this suite. 03/23/23 19:45:25.229
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:45:25.266
Mar 23 19:45:25.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:45:25.267
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:25.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:25.343
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  03/23/23 19:45:25.348
Mar 23 19:45:25.366: INFO: Waiting up to 5m0s for pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3" in namespace "svcaccounts-2309" to be "Succeeded or Failed"
Mar 23 19:45:25.377: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474774ms
Mar 23 19:45:27.381: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014412388s
Mar 23 19:45:29.383: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Running", Reason="", readiness=false. Elapsed: 4.016482907s
Mar 23 19:45:31.387: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020604326s
STEP: Saw pod success 03/23/23 19:45:31.387
Mar 23 19:45:31.387: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3" satisfied condition "Succeeded or Failed"
Mar 23 19:45:31.391: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:45:31.419
Mar 23 19:45:31.433: INFO: Waiting for pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 to disappear
Mar 23 19:45:31.435: INFO: Pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 19:45:31.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2309" for this suite. 03/23/23 19:45:31.442
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":228,"skipped":4085,"failed":0}
------------------------------
• [SLOW TEST] [6.186 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:45:25.266
    Mar 23 19:45:25.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:45:25.267
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:25.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:25.343
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  03/23/23 19:45:25.348
    Mar 23 19:45:25.366: INFO: Waiting up to 5m0s for pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3" in namespace "svcaccounts-2309" to be "Succeeded or Failed"
    Mar 23 19:45:25.377: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.474774ms
    Mar 23 19:45:27.381: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Running", Reason="", readiness=true. Elapsed: 2.014412388s
    Mar 23 19:45:29.383: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Running", Reason="", readiness=false. Elapsed: 4.016482907s
    Mar 23 19:45:31.387: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020604326s
    STEP: Saw pod success 03/23/23 19:45:31.387
    Mar 23 19:45:31.387: INFO: Pod "test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3" satisfied condition "Succeeded or Failed"
    Mar 23 19:45:31.391: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:45:31.419
    Mar 23 19:45:31.433: INFO: Waiting for pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 to disappear
    Mar 23 19:45:31.435: INFO: Pod test-pod-9e799e7a-c7b1-4aec-92b2-a2e7b3179ee3 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 19:45:31.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2309" for this suite. 03/23/23 19:45:31.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:45:31.467
Mar 23 19:45:31.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 19:45:31.468
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:31.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:31.491
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-55db50af-c1a5-48af-8f7b-7bc5d481e66f 03/23/23 19:45:31.51
STEP: Creating configMap with name cm-test-opt-upd-78c7ada5-5208-4ee7-bb7d-8903c01b86a7 03/23/23 19:45:31.517
STEP: Creating the pod 03/23/23 19:45:31.528
Mar 23 19:45:31.545: INFO: Waiting up to 5m0s for pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050" in namespace "configmap-9584" to be "running and ready"
Mar 23 19:45:31.555: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Pending", Reason="", readiness=false. Elapsed: 10.501176ms
Mar 23 19:45:31.555: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:45:33.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015605489s
Mar 23 19:45:33.561: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:45:35.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Running", Reason="", readiness=true. Elapsed: 4.016046013s
Mar 23 19:45:35.561: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Running (Ready = true)
Mar 23 19:45:35.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-55db50af-c1a5-48af-8f7b-7bc5d481e66f 03/23/23 19:45:35.58
STEP: Updating configmap cm-test-opt-upd-78c7ada5-5208-4ee7-bb7d-8903c01b86a7 03/23/23 19:45:35.586
STEP: Creating configMap with name cm-test-opt-create-217938df-c052-4372-9f98-f38f779a25ea 03/23/23 19:45:35.59
STEP: waiting to observe update in volume 03/23/23 19:45:35.594
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 19:47:06.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9584" for this suite. 03/23/23 19:47:06.014
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":229,"skipped":4096,"failed":0}
------------------------------
• [SLOW TEST] [94.555 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:45:31.467
    Mar 23 19:45:31.467: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 19:45:31.468
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:45:31.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:45:31.491
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-55db50af-c1a5-48af-8f7b-7bc5d481e66f 03/23/23 19:45:31.51
    STEP: Creating configMap with name cm-test-opt-upd-78c7ada5-5208-4ee7-bb7d-8903c01b86a7 03/23/23 19:45:31.517
    STEP: Creating the pod 03/23/23 19:45:31.528
    Mar 23 19:45:31.545: INFO: Waiting up to 5m0s for pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050" in namespace "configmap-9584" to be "running and ready"
    Mar 23 19:45:31.555: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Pending", Reason="", readiness=false. Elapsed: 10.501176ms
    Mar 23 19:45:31.555: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:45:33.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015605489s
    Mar 23 19:45:33.561: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:45:35.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050": Phase="Running", Reason="", readiness=true. Elapsed: 4.016046013s
    Mar 23 19:45:35.561: INFO: The phase of Pod pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050 is Running (Ready = true)
    Mar 23 19:45:35.561: INFO: Pod "pod-configmaps-476b7b46-8733-4ff9-823a-3bef732a3050" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-55db50af-c1a5-48af-8f7b-7bc5d481e66f 03/23/23 19:45:35.58
    STEP: Updating configmap cm-test-opt-upd-78c7ada5-5208-4ee7-bb7d-8903c01b86a7 03/23/23 19:45:35.586
    STEP: Creating configMap with name cm-test-opt-create-217938df-c052-4372-9f98-f38f779a25ea 03/23/23 19:45:35.59
    STEP: waiting to observe update in volume 03/23/23 19:45:35.594
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 19:47:06.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9584" for this suite. 03/23/23 19:47:06.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:06.034
Mar 23 19:47:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:47:06.035
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:06.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:06.058
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:47:06.107
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:47:06.988
STEP: Deploying the webhook pod 03/23/23 19:47:06.994
STEP: Wait for the deployment to be ready 03/23/23 19:47:07.019
Mar 23 19:47:07.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:47:09.057
STEP: Verifying the service has paired with the endpoint 03/23/23 19:47:09.083
Mar 23 19:47:10.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/23/23 19:47:10.087
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/23/23 19:47:10.103
STEP: Creating a dummy validating-webhook-configuration object 03/23/23 19:47:10.122
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/23/23 19:47:10.133
STEP: Creating a dummy mutating-webhook-configuration object 03/23/23 19:47:10.139
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/23/23 19:47:10.153
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:47:10.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9941" for this suite. 03/23/23 19:47:10.176
STEP: Destroying namespace "webhook-9941-markers" for this suite. 03/23/23 19:47:10.181
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":230,"skipped":4146,"failed":0}
------------------------------
• [4.290 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:06.034
    Mar 23 19:47:06.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:47:06.035
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:06.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:06.058
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:47:06.107
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:47:06.988
    STEP: Deploying the webhook pod 03/23/23 19:47:06.994
    STEP: Wait for the deployment to be ready 03/23/23 19:47:07.019
    Mar 23 19:47:07.045: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:47:09.057
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:47:09.083
    Mar 23 19:47:10.083: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/23/23 19:47:10.087
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 03/23/23 19:47:10.103
    STEP: Creating a dummy validating-webhook-configuration object 03/23/23 19:47:10.122
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 03/23/23 19:47:10.133
    STEP: Creating a dummy mutating-webhook-configuration object 03/23/23 19:47:10.139
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 03/23/23 19:47:10.153
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:47:10.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9941" for this suite. 03/23/23 19:47:10.176
    STEP: Destroying namespace "webhook-9941-markers" for this suite. 03/23/23 19:47:10.181
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:10.329
Mar 23 19:47:10.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:47:10.331
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:10.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:10.354
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-931955ef-8e16-4ea6-837e-b91bd512df8a 03/23/23 19:47:10.365
STEP: Creating a pod to test consume configMaps 03/23/23 19:47:10.375
Mar 23 19:47:10.387: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f" in namespace "projected-2976" to be "Succeeded or Failed"
Mar 23 19:47:10.400: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.285976ms
Mar 23 19:47:12.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01754311s
Mar 23 19:47:14.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Running", Reason="", readiness=false. Elapsed: 4.017522352s
Mar 23 19:47:16.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018306716s
STEP: Saw pod success 03/23/23 19:47:16.405
Mar 23 19:47:16.406: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f" satisfied condition "Succeeded or Failed"
Mar 23 19:47:16.409: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f container projected-configmap-volume-test: <nil>
STEP: delete the pod 03/23/23 19:47:16.456
Mar 23 19:47:16.469: INFO: Waiting for pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f to disappear
Mar 23 19:47:16.472: INFO: Pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 19:47:16.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2976" for this suite. 03/23/23 19:47:16.476
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":231,"skipped":4205,"failed":0}
------------------------------
• [SLOW TEST] [6.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:10.329
    Mar 23 19:47:10.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:47:10.331
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:10.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:10.354
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-931955ef-8e16-4ea6-837e-b91bd512df8a 03/23/23 19:47:10.365
    STEP: Creating a pod to test consume configMaps 03/23/23 19:47:10.375
    Mar 23 19:47:10.387: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f" in namespace "projected-2976" to be "Succeeded or Failed"
    Mar 23 19:47:10.400: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.285976ms
    Mar 23 19:47:12.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01754311s
    Mar 23 19:47:14.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Running", Reason="", readiness=false. Elapsed: 4.017522352s
    Mar 23 19:47:16.405: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018306716s
    STEP: Saw pod success 03/23/23 19:47:16.405
    Mar 23 19:47:16.406: INFO: Pod "pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f" satisfied condition "Succeeded or Failed"
    Mar 23 19:47:16.409: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f container projected-configmap-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:47:16.456
    Mar 23 19:47:16.469: INFO: Waiting for pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f to disappear
    Mar 23 19:47:16.472: INFO: Pod pod-projected-configmaps-25dddc43-e212-491b-9563-35816398717f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 19:47:16.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2976" for this suite. 03/23/23 19:47:16.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:16.496
Mar 23 19:47:16.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename csistoragecapacity 03/23/23 19:47:16.497
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:16.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:16.525
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 03/23/23 19:47:16.528
STEP: getting /apis/storage.k8s.io 03/23/23 19:47:16.534
STEP: getting /apis/storage.k8s.io/v1 03/23/23 19:47:16.536
STEP: creating 03/23/23 19:47:16.54
STEP: watching 03/23/23 19:47:16.577
Mar 23 19:47:16.577: INFO: starting watch
STEP: getting 03/23/23 19:47:16.584
STEP: listing in namespace 03/23/23 19:47:16.589
STEP: listing across namespaces 03/23/23 19:47:16.594
STEP: patching 03/23/23 19:47:16.597
STEP: updating 03/23/23 19:47:16.613
Mar 23 19:47:16.623: INFO: waiting for watch events with expected annotations in namespace
Mar 23 19:47:16.624: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 03/23/23 19:47:16.625
STEP: deleting a collection 03/23/23 19:47:16.644
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Mar 23 19:47:16.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-174" for this suite. 03/23/23 19:47:16.678
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":232,"skipped":4210,"failed":0}
------------------------------
• [0.194 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:16.496
    Mar 23 19:47:16.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename csistoragecapacity 03/23/23 19:47:16.497
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:16.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:16.525
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 03/23/23 19:47:16.528
    STEP: getting /apis/storage.k8s.io 03/23/23 19:47:16.534
    STEP: getting /apis/storage.k8s.io/v1 03/23/23 19:47:16.536
    STEP: creating 03/23/23 19:47:16.54
    STEP: watching 03/23/23 19:47:16.577
    Mar 23 19:47:16.577: INFO: starting watch
    STEP: getting 03/23/23 19:47:16.584
    STEP: listing in namespace 03/23/23 19:47:16.589
    STEP: listing across namespaces 03/23/23 19:47:16.594
    STEP: patching 03/23/23 19:47:16.597
    STEP: updating 03/23/23 19:47:16.613
    Mar 23 19:47:16.623: INFO: waiting for watch events with expected annotations in namespace
    Mar 23 19:47:16.624: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 03/23/23 19:47:16.625
    STEP: deleting a collection 03/23/23 19:47:16.644
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Mar 23 19:47:16.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-174" for this suite. 03/23/23 19:47:16.678
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:16.691
Mar 23 19:47:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:47:16.693
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:16.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:16.718
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:47:16.728
Mar 23 19:47:16.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7" in namespace "downward-api-2806" to be "Succeeded or Failed"
Mar 23 19:47:16.764: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.052656ms
Mar 23 19:47:18.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024455986s
Mar 23 19:47:20.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.024195426s
Mar 23 19:47:22.767: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=false. Elapsed: 6.023821266s
Mar 23 19:47:24.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024369605s
STEP: Saw pod success 03/23/23 19:47:24.768
Mar 23 19:47:24.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7" satisfied condition "Succeeded or Failed"
Mar 23 19:47:24.771: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 container client-container: <nil>
STEP: delete the pod 03/23/23 19:47:24.777
Mar 23 19:47:24.791: INFO: Waiting for pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 to disappear
Mar 23 19:47:24.794: INFO: Pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 19:47:24.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2806" for this suite. 03/23/23 19:47:24.8
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":233,"skipped":4212,"failed":0}
------------------------------
• [SLOW TEST] [8.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:16.691
    Mar 23 19:47:16.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:47:16.693
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:16.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:16.718
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:47:16.728
    Mar 23 19:47:16.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7" in namespace "downward-api-2806" to be "Succeeded or Failed"
    Mar 23 19:47:16.764: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.052656ms
    Mar 23 19:47:18.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.024455986s
    Mar 23 19:47:20.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.024195426s
    Mar 23 19:47:22.767: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Running", Reason="", readiness=false. Elapsed: 6.023821266s
    Mar 23 19:47:24.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024369605s
    STEP: Saw pod success 03/23/23 19:47:24.768
    Mar 23 19:47:24.768: INFO: Pod "downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7" satisfied condition "Succeeded or Failed"
    Mar 23 19:47:24.771: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 container client-container: <nil>
    STEP: delete the pod 03/23/23 19:47:24.777
    Mar 23 19:47:24.791: INFO: Waiting for pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 to disappear
    Mar 23 19:47:24.794: INFO: Pod downwardapi-volume-016fb098-7bdb-41d6-bcfd-f4dc7f8650d7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 19:47:24.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2806" for this suite. 03/23/23 19:47:24.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:24.814
Mar 23 19:47:24.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:47:24.816
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:24.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:24.838
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:47:24.845
Mar 23 19:47:24.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017" in namespace "projected-9703" to be "Succeeded or Failed"
Mar 23 19:47:24.873: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471583ms
Mar 23 19:47:26.877: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Running", Reason="", readiness=true. Elapsed: 2.011178932s
Mar 23 19:47:28.877: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Running", Reason="", readiness=false. Elapsed: 4.010607895s
Mar 23 19:47:30.879: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012191153s
STEP: Saw pod success 03/23/23 19:47:30.879
Mar 23 19:47:30.879: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017" satisfied condition "Succeeded or Failed"
Mar 23 19:47:30.882: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 container client-container: <nil>
STEP: delete the pod 03/23/23 19:47:30.891
Mar 23 19:47:30.905: INFO: Waiting for pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 to disappear
Mar 23 19:47:30.907: INFO: Pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:47:30.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9703" for this suite. 03/23/23 19:47:30.912
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":234,"skipped":4225,"failed":0}
------------------------------
• [SLOW TEST] [6.113 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:24.814
    Mar 23 19:47:24.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:47:24.816
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:24.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:24.838
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:47:24.845
    Mar 23 19:47:24.866: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017" in namespace "projected-9703" to be "Succeeded or Failed"
    Mar 23 19:47:24.873: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Pending", Reason="", readiness=false. Elapsed: 6.471583ms
    Mar 23 19:47:26.877: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Running", Reason="", readiness=true. Elapsed: 2.011178932s
    Mar 23 19:47:28.877: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Running", Reason="", readiness=false. Elapsed: 4.010607895s
    Mar 23 19:47:30.879: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012191153s
    STEP: Saw pod success 03/23/23 19:47:30.879
    Mar 23 19:47:30.879: INFO: Pod "downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017" satisfied condition "Succeeded or Failed"
    Mar 23 19:47:30.882: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 container client-container: <nil>
    STEP: delete the pod 03/23/23 19:47:30.891
    Mar 23 19:47:30.905: INFO: Waiting for pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 to disappear
    Mar 23 19:47:30.907: INFO: Pod downwardapi-volume-dc77e6bd-98c3-4a32-8ce0-245086068017 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:47:30.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9703" for this suite. 03/23/23 19:47:30.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:30.929
Mar 23 19:47:30.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:47:30.93
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:30.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:30.951
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2841-delete-me 03/23/23 19:47:30.965
STEP: Waiting for the RuntimeClass to disappear 03/23/23 19:47:30.996
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 23 19:47:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2841" for this suite. 03/23/23 19:47:31.019
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":235,"skipped":4266,"failed":0}
------------------------------
• [0.101 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:30.929
    Mar 23 19:47:30.929: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename runtimeclass 03/23/23 19:47:30.93
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:30.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:30.951
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2841-delete-me 03/23/23 19:47:30.965
    STEP: Waiting for the RuntimeClass to disappear 03/23/23 19:47:30.996
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 23 19:47:31.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2841" for this suite. 03/23/23 19:47:31.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:31.039
Mar 23 19:47:31.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:47:31.042
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:31.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:31.077
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 03/23/23 19:47:31.081
STEP: Creating a ResourceQuota 03/23/23 19:47:36.089
STEP: Ensuring resource quota status is calculated 03/23/23 19:47:36.098
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:47:38.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7948" for this suite. 03/23/23 19:47:38.108
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":236,"skipped":4288,"failed":0}
------------------------------
• [SLOW TEST] [7.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:31.039
    Mar 23 19:47:31.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:47:31.042
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:31.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:31.077
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 03/23/23 19:47:31.081
    STEP: Creating a ResourceQuota 03/23/23 19:47:36.089
    STEP: Ensuring resource quota status is calculated 03/23/23 19:47:36.098
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:47:38.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7948" for this suite. 03/23/23 19:47:38.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:47:38.125
Mar 23 19:47:38.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:47:38.126
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:38.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:38.146
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/23/23 19:47:38.149
Mar 23 19:47:38.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/23/23 19:47:52.102
Mar 23 19:47:52.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:47:55.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:48:08.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2180" for this suite. 03/23/23 19:48:08.566
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":237,"skipped":4294,"failed":0}
------------------------------
• [SLOW TEST] [30.452 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:47:38.125
    Mar 23 19:47:38.125: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:47:38.126
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:47:38.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:47:38.146
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 03/23/23 19:47:38.149
    Mar 23 19:47:38.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 03/23/23 19:47:52.102
    Mar 23 19:47:52.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:47:55.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:48:08.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2180" for this suite. 03/23/23 19:48:08.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:08.579
Mar 23 19:48:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:48:08.58
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:08.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:08.604
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-429293b6-cdfc-478c-a8c9-4b9309024255 03/23/23 19:48:08.607
STEP: Creating a pod to test consume secrets 03/23/23 19:48:08.614
Mar 23 19:48:08.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47" in namespace "projected-5110" to be "Succeeded or Failed"
Mar 23 19:48:08.641: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791873ms
Mar 23 19:48:10.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.017756256s
Mar 23 19:48:12.648: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Running", Reason="", readiness=false. Elapsed: 4.018793355s
Mar 23 19:48:14.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018134361s
STEP: Saw pod success 03/23/23 19:48:14.647
Mar 23 19:48:14.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47" satisfied condition "Succeeded or Failed"
Mar 23 19:48:14.652: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:48:14.66
Mar 23 19:48:14.686: INFO: Waiting for pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 to disappear
Mar 23 19:48:14.688: INFO: Pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 19:48:14.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5110" for this suite. 03/23/23 19:48:14.693
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":238,"skipped":4308,"failed":0}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:08.579
    Mar 23 19:48:08.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:48:08.58
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:08.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:08.604
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-429293b6-cdfc-478c-a8c9-4b9309024255 03/23/23 19:48:08.607
    STEP: Creating a pod to test consume secrets 03/23/23 19:48:08.614
    Mar 23 19:48:08.629: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47" in namespace "projected-5110" to be "Succeeded or Failed"
    Mar 23 19:48:08.641: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Pending", Reason="", readiness=false. Elapsed: 11.791873ms
    Mar 23 19:48:10.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.017756256s
    Mar 23 19:48:12.648: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Running", Reason="", readiness=false. Elapsed: 4.018793355s
    Mar 23 19:48:14.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018134361s
    STEP: Saw pod success 03/23/23 19:48:14.647
    Mar 23 19:48:14.647: INFO: Pod "pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47" satisfied condition "Succeeded or Failed"
    Mar 23 19:48:14.652: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:48:14.66
    Mar 23 19:48:14.686: INFO: Waiting for pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 to disappear
    Mar 23 19:48:14.688: INFO: Pod pod-projected-secrets-127fff0d-635b-4d4a-9897-26071ffb3a47 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 19:48:14.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5110" for this suite. 03/23/23 19:48:14.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:14.716
Mar 23 19:48:14.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename watch 03/23/23 19:48:14.717
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:14.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:14.75
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 03/23/23 19:48:14.753
STEP: modifying the configmap once 03/23/23 19:48:14.759
STEP: modifying the configmap a second time 03/23/23 19:48:14.768
STEP: deleting the configmap 03/23/23 19:48:14.784
STEP: creating a watch on configmaps from the resource version returned by the first update 03/23/23 19:48:14.792
STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/23/23 19:48:14.795
Mar 23 19:48:14.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9429  6eaf3318-ed02-4825-b648-8eb6d69f0192 32234 0 2023-03-23 19:48:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-23 19:48:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 19:48:14.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9429  6eaf3318-ed02-4825-b648-8eb6d69f0192 32235 0 2023-03-23 19:48:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-23 19:48:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 23 19:48:14.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9429" for this suite. 03/23/23 19:48:14.807
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":239,"skipped":4358,"failed":0}
------------------------------
• [0.104 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:14.716
    Mar 23 19:48:14.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename watch 03/23/23 19:48:14.717
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:14.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:14.75
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 03/23/23 19:48:14.753
    STEP: modifying the configmap once 03/23/23 19:48:14.759
    STEP: modifying the configmap a second time 03/23/23 19:48:14.768
    STEP: deleting the configmap 03/23/23 19:48:14.784
    STEP: creating a watch on configmaps from the resource version returned by the first update 03/23/23 19:48:14.792
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 03/23/23 19:48:14.795
    Mar 23 19:48:14.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9429  6eaf3318-ed02-4825-b648-8eb6d69f0192 32234 0 2023-03-23 19:48:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-23 19:48:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 19:48:14.796: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9429  6eaf3318-ed02-4825-b648-8eb6d69f0192 32235 0 2023-03-23 19:48:14 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-03-23 19:48:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 23 19:48:14.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9429" for this suite. 03/23/23 19:48:14.807
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:14.83
Mar 23 19:48:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/23/23 19:48:14.832
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:14.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:14.86
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 03/23/23 19:48:14.863
STEP: Creating hostNetwork=false pod 03/23/23 19:48:14.863
W0323 19:48:14.897910      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
Mar 23 19:48:14.898: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8716" to be "running and ready"
Mar 23 19:48:14.917: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.166356ms
Mar 23 19:48:14.917: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:48:16.921: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02394425s
Mar 23 19:48:16.922: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:48:18.922: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024871052s
Mar 23 19:48:18.923: INFO: The phase of Pod test-pod is Running (Ready = true)
Mar 23 19:48:18.923: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 03/23/23 19:48:18.929
W0323 19:48:18.938179      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
Mar 23 19:48:18.938: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8716" to be "running and ready"
Mar 23 19:48:18.948: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008677ms
Mar 23 19:48:18.948: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:48:20.959: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020761566s
Mar 23 19:48:20.959: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Mar 23 19:48:20.959: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 03/23/23 19:48:20.962
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/23/23 19:48:20.962
Mar 23 19:48:20.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:20.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:20.963: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:20.963: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 23 19:48:21.073: INFO: Exec stderr: ""
Mar 23 19:48:21.073: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.074: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.074: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 23 19:48:21.178: INFO: Exec stderr: ""
Mar 23 19:48:21.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.179: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.179: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 23 19:48:21.302: INFO: Exec stderr: ""
Mar 23 19:48:21.302: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.303: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.303: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 23 19:48:21.430: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/23/23 19:48:21.431
Mar 23 19:48:21.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.432: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.432: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 23 19:48:21.553: INFO: Exec stderr: ""
Mar 23 19:48:21.553: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.553: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.553: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Mar 23 19:48:21.709: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/23/23 19:48:21.709
Mar 23 19:48:21.710: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.711: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.711: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 23 19:48:21.834: INFO: Exec stderr: ""
Mar 23 19:48:21.834: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.835: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.835: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Mar 23 19:48:21.946: INFO: Exec stderr: ""
Mar 23 19:48:21.947: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:21.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:21.947: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:21.947: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 23 19:48:22.061: INFO: Exec stderr: ""
Mar 23 19:48:22.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:48:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:48:22.062: INFO: ExecWithOptions: Clientset creation
Mar 23 19:48:22.062: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Mar 23 19:48:22.180: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Mar 23 19:48:22.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8716" for this suite. 03/23/23 19:48:22.185
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":240,"skipped":4362,"failed":0}
------------------------------
• [SLOW TEST] [7.359 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:14.83
    Mar 23 19:48:14.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 03/23/23 19:48:14.832
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:14.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:14.86
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 03/23/23 19:48:14.863
    STEP: Creating hostNetwork=false pod 03/23/23 19:48:14.863
    W0323 19:48:14.897910      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPath volumes (volume "host-etc-hosts")
    Mar 23 19:48:14.898: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-8716" to be "running and ready"
    Mar 23 19:48:14.917: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.166356ms
    Mar 23 19:48:14.917: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:48:16.921: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02394425s
    Mar 23 19:48:16.922: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:48:18.922: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.024871052s
    Mar 23 19:48:18.923: INFO: The phase of Pod test-pod is Running (Ready = true)
    Mar 23 19:48:18.923: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 03/23/23 19:48:18.929
    W0323 19:48:18.938179      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true), hostPath volumes (volume "host-etc-hosts")
    Mar 23 19:48:18.938: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-8716" to be "running and ready"
    Mar 23 19:48:18.948: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008677ms
    Mar 23 19:48:18.948: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:48:20.959: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020761566s
    Mar 23 19:48:20.959: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Mar 23 19:48:20.959: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 03/23/23 19:48:20.962
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 03/23/23 19:48:20.962
    Mar 23 19:48:20.962: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:20.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:20.963: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:20.963: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 23 19:48:21.073: INFO: Exec stderr: ""
    Mar 23 19:48:21.073: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.074: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.074: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 23 19:48:21.178: INFO: Exec stderr: ""
    Mar 23 19:48:21.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.179: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.179: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 23 19:48:21.302: INFO: Exec stderr: ""
    Mar 23 19:48:21.302: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.303: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.303: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 23 19:48:21.430: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 03/23/23 19:48:21.431
    Mar 23 19:48:21.431: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.432: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.432: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 23 19:48:21.553: INFO: Exec stderr: ""
    Mar 23 19:48:21.553: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.553: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.553: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Mar 23 19:48:21.709: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 03/23/23 19:48:21.709
    Mar 23 19:48:21.710: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.711: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.711: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 23 19:48:21.834: INFO: Exec stderr: ""
    Mar 23 19:48:21.834: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.835: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.835: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Mar 23 19:48:21.946: INFO: Exec stderr: ""
    Mar 23 19:48:21.947: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:21.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:21.947: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:21.947: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 23 19:48:22.061: INFO: Exec stderr: ""
    Mar 23 19:48:22.061: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8716 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:48:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:48:22.062: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:48:22.062: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-8716/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Mar 23 19:48:22.180: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Mar 23 19:48:22.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-8716" for this suite. 03/23/23 19:48:22.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:22.196
Mar 23 19:48:22.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 19:48:22.198
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:22.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:22.22
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/23/23 19:48:22.224
Mar 23 19:48:22.250: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 23 19:48:27.257: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 19:48:27.257
STEP: getting scale subresource 03/23/23 19:48:27.257
STEP: updating a scale subresource 03/23/23 19:48:27.263
STEP: verifying the replicaset Spec.Replicas was modified 03/23/23 19:48:27.277
STEP: Patch a scale subresource 03/23/23 19:48:27.3
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 19:48:27.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4429" for this suite. 03/23/23 19:48:27.382
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":241,"skipped":4374,"failed":0}
------------------------------
• [SLOW TEST] [5.222 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:22.196
    Mar 23 19:48:22.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 19:48:22.198
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:22.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:22.22
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 03/23/23 19:48:22.224
    Mar 23 19:48:22.250: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 23 19:48:27.257: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 19:48:27.257
    STEP: getting scale subresource 03/23/23 19:48:27.257
    STEP: updating a scale subresource 03/23/23 19:48:27.263
    STEP: verifying the replicaset Spec.Replicas was modified 03/23/23 19:48:27.277
    STEP: Patch a scale subresource 03/23/23 19:48:27.3
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 19:48:27.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4429" for this suite. 03/23/23 19:48:27.382
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:27.418
Mar 23 19:48:27.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-runtime 03/23/23 19:48:27.42
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:27.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:27.467
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 03/23/23 19:48:27.476
STEP: wait for the container to reach Succeeded 03/23/23 19:48:27.513
STEP: get the container status 03/23/23 19:48:35.583
STEP: the container should be terminated 03/23/23 19:48:35.586
STEP: the termination message should be set 03/23/23 19:48:35.586
Mar 23 19:48:35.586: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 03/23/23 19:48:35.587
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 23 19:48:35.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9165" for this suite. 03/23/23 19:48:35.605
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":242,"skipped":4374,"failed":0}
------------------------------
• [SLOW TEST] [8.197 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:27.418
    Mar 23 19:48:27.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-runtime 03/23/23 19:48:27.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:27.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:27.467
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 03/23/23 19:48:27.476
    STEP: wait for the container to reach Succeeded 03/23/23 19:48:27.513
    STEP: get the container status 03/23/23 19:48:35.583
    STEP: the container should be terminated 03/23/23 19:48:35.586
    STEP: the termination message should be set 03/23/23 19:48:35.586
    Mar 23 19:48:35.586: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 03/23/23 19:48:35.587
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 23 19:48:35.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9165" for this suite. 03/23/23 19:48:35.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:35.623
Mar 23 19:48:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 19:48:35.624
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:35.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:35.643
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 03/23/23 19:48:35.646
STEP: Counting existing ResourceQuota 03/23/23 19:48:41.651
STEP: Creating a ResourceQuota 03/23/23 19:48:46.656
STEP: Ensuring resource quota status is calculated 03/23/23 19:48:46.663
STEP: Creating a Secret 03/23/23 19:48:48.668
STEP: Ensuring resource quota status captures secret creation 03/23/23 19:48:48.686
STEP: Deleting a secret 03/23/23 19:48:50.69
STEP: Ensuring resource quota status released usage 03/23/23 19:48:50.698
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 19:48:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9349" for this suite. 03/23/23 19:48:52.707
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":243,"skipped":4398,"failed":0}
------------------------------
• [SLOW TEST] [17.094 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:35.623
    Mar 23 19:48:35.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 19:48:35.624
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:35.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:35.643
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 03/23/23 19:48:35.646
    STEP: Counting existing ResourceQuota 03/23/23 19:48:41.651
    STEP: Creating a ResourceQuota 03/23/23 19:48:46.656
    STEP: Ensuring resource quota status is calculated 03/23/23 19:48:46.663
    STEP: Creating a Secret 03/23/23 19:48:48.668
    STEP: Ensuring resource quota status captures secret creation 03/23/23 19:48:48.686
    STEP: Deleting a secret 03/23/23 19:48:50.69
    STEP: Ensuring resource quota status released usage 03/23/23 19:48:50.698
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 19:48:52.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9349" for this suite. 03/23/23 19:48:52.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:48:52.724
Mar 23 19:48:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:48:52.725
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:52.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:52.749
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 23 19:48:52.777: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:49:52.866: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:49:52.871
Mar 23 19:49:52.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption-path 03/23/23 19:49:52.873
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:49:52.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:49:52.911
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 03/23/23 19:49:52.922
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 19:49:52.922
Mar 23 19:49:52.934: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7184" to be "running"
Mar 23 19:49:52.964: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 29.803532ms
Mar 23 19:49:54.967: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.033454855s
Mar 23 19:49:54.968: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 19:49:54.97
Mar 23 19:49:54.986: INFO: found a healthy node: k8s-linuxpool-16392394-2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Mar 23 19:50:35.081: INFO: pods created so far: [1 1 1]
Mar 23 19:50:35.081: INFO: length of pods created so far: 3
Mar 23 19:50:39.093: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Mar 23 19:50:46.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-7184" for this suite. 03/23/23 19:50:46.104
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:50:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8916" for this suite. 03/23/23 19:50:46.159
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":244,"skipped":4410,"failed":0}
------------------------------
• [SLOW TEST] [113.492 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:48:52.724
    Mar 23 19:48:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:48:52.725
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:48:52.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:48:52.749
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 23 19:48:52.777: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 19:49:52.866: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:49:52.871
    Mar 23 19:49:52.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption-path 03/23/23 19:49:52.873
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:49:52.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:49:52.911
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 03/23/23 19:49:52.922
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 19:49:52.922
    Mar 23 19:49:52.934: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7184" to be "running"
    Mar 23 19:49:52.964: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 29.803532ms
    Mar 23 19:49:54.967: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.033454855s
    Mar 23 19:49:54.968: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 19:49:54.97
    Mar 23 19:49:54.986: INFO: found a healthy node: k8s-linuxpool-16392394-2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Mar 23 19:50:35.081: INFO: pods created so far: [1 1 1]
    Mar 23 19:50:35.081: INFO: length of pods created so far: 3
    Mar 23 19:50:39.093: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Mar 23 19:50:46.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-7184" for this suite. 03/23/23 19:50:46.104
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:50:46.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8916" for this suite. 03/23/23 19:50:46.159
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:50:46.218
Mar 23 19:50:46.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 19:50:46.219
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:46.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:46.241
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 03/23/23 19:50:46.254
STEP: Verify that the required pods have come up. 03/23/23 19:50:46.262
Mar 23 19:50:46.274: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 19:50:46.274
Mar 23 19:50:46.274: INFO: Waiting up to 5m0s for pod "test-rs-r5cc8" in namespace "replicaset-8237" to be "running"
Mar 23 19:50:46.288: INFO: Pod "test-rs-r5cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.293868ms
Mar 23 19:50:48.293: INFO: Pod "test-rs-r5cc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018819863s
Mar 23 19:50:48.293: INFO: Pod "test-rs-r5cc8" satisfied condition "running"
STEP: Getting /status 03/23/23 19:50:48.293
Mar 23 19:50:48.296: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 03/23/23 19:50:48.296
Mar 23 19:50:48.303: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 03/23/23 19:50:48.304
Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: ADDED
Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.307: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.307: INFO: Found replicaset test-rs in namespace replicaset-8237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Mar 23 19:50:48.308: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 03/23/23 19:50:48.308
Mar 23 19:50:48.308: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 23 19:50:48.315: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 03/23/23 19:50:48.316
Mar 23 19:50:48.318: INFO: Observed &ReplicaSet event: ADDED
Mar 23 19:50:48.318: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.319: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.319: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.319: INFO: Observed replicaset test-rs in namespace replicaset-8237 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 23 19:50:48.320: INFO: Observed &ReplicaSet event: MODIFIED
Mar 23 19:50:48.320: INFO: Found replicaset test-rs in namespace replicaset-8237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Mar 23 19:50:48.320: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 19:50:48.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8237" for this suite. 03/23/23 19:50:48.325
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":245,"skipped":4432,"failed":0}
------------------------------
• [2.112 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:50:46.218
    Mar 23 19:50:46.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 19:50:46.219
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:46.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:46.241
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 03/23/23 19:50:46.254
    STEP: Verify that the required pods have come up. 03/23/23 19:50:46.262
    Mar 23 19:50:46.274: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 19:50:46.274
    Mar 23 19:50:46.274: INFO: Waiting up to 5m0s for pod "test-rs-r5cc8" in namespace "replicaset-8237" to be "running"
    Mar 23 19:50:46.288: INFO: Pod "test-rs-r5cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.293868ms
    Mar 23 19:50:48.293: INFO: Pod "test-rs-r5cc8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018819863s
    Mar 23 19:50:48.293: INFO: Pod "test-rs-r5cc8" satisfied condition "running"
    STEP: Getting /status 03/23/23 19:50:48.293
    Mar 23 19:50:48.296: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 03/23/23 19:50:48.296
    Mar 23 19:50:48.303: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 03/23/23 19:50:48.304
    Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: ADDED
    Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.306: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.307: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.307: INFO: Found replicaset test-rs in namespace replicaset-8237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Mar 23 19:50:48.308: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 03/23/23 19:50:48.308
    Mar 23 19:50:48.308: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 23 19:50:48.315: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 03/23/23 19:50:48.316
    Mar 23 19:50:48.318: INFO: Observed &ReplicaSet event: ADDED
    Mar 23 19:50:48.318: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.319: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.319: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.319: INFO: Observed replicaset test-rs in namespace replicaset-8237 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 23 19:50:48.320: INFO: Observed &ReplicaSet event: MODIFIED
    Mar 23 19:50:48.320: INFO: Found replicaset test-rs in namespace replicaset-8237 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Mar 23 19:50:48.320: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 19:50:48.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8237" for this suite. 03/23/23 19:50:48.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:50:48.34
Mar 23 19:50:48.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:50:48.342
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:48.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:48.36
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 03/23/23 19:50:48.368
STEP: waiting for available Endpoint 03/23/23 19:50:48.373
STEP: listing all Endpoints 03/23/23 19:50:48.375
STEP: updating the Endpoint 03/23/23 19:50:48.378
STEP: fetching the Endpoint 03/23/23 19:50:48.398
STEP: patching the Endpoint 03/23/23 19:50:48.401
STEP: fetching the Endpoint 03/23/23 19:50:48.41
STEP: deleting the Endpoint by Collection 03/23/23 19:50:48.413
STEP: waiting for Endpoint deletion 03/23/23 19:50:48.424
STEP: fetching the Endpoint 03/23/23 19:50:48.426
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:50:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7928" for this suite. 03/23/23 19:50:48.434
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":246,"skipped":4454,"failed":0}
------------------------------
• [0.099 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:50:48.34
    Mar 23 19:50:48.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:50:48.342
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:48.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:48.36
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 03/23/23 19:50:48.368
    STEP: waiting for available Endpoint 03/23/23 19:50:48.373
    STEP: listing all Endpoints 03/23/23 19:50:48.375
    STEP: updating the Endpoint 03/23/23 19:50:48.378
    STEP: fetching the Endpoint 03/23/23 19:50:48.398
    STEP: patching the Endpoint 03/23/23 19:50:48.401
    STEP: fetching the Endpoint 03/23/23 19:50:48.41
    STEP: deleting the Endpoint by Collection 03/23/23 19:50:48.413
    STEP: waiting for Endpoint deletion 03/23/23 19:50:48.424
    STEP: fetching the Endpoint 03/23/23 19:50:48.426
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:50:48.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7928" for this suite. 03/23/23 19:50:48.434
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:50:48.441
Mar 23 19:50:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 19:50:48.443
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:48.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:48.463
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 in namespace container-probe-9455 03/23/23 19:50:48.467
Mar 23 19:50:48.479: INFO: Waiting up to 5m0s for pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9" in namespace "container-probe-9455" to be "not pending"
Mar 23 19:50:48.488: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453879ms
Mar 23 19:50:50.494: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.014684572s
Mar 23 19:50:50.494: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9" satisfied condition "not pending"
Mar 23 19:50:50.494: INFO: Started pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 in namespace container-probe-9455
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 19:50:50.494
Mar 23 19:50:50.496: INFO: Initial restart count of pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 is 0
STEP: deleting the pod 03/23/23 19:54:51.226
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 19:54:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9455" for this suite. 03/23/23 19:54:51.253
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":247,"skipped":4464,"failed":0}
------------------------------
• [SLOW TEST] [242.821 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:50:48.441
    Mar 23 19:50:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 19:50:48.443
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:50:48.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:50:48.463
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 in namespace container-probe-9455 03/23/23 19:50:48.467
    Mar 23 19:50:48.479: INFO: Waiting up to 5m0s for pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9" in namespace "container-probe-9455" to be "not pending"
    Mar 23 19:50:48.488: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9": Phase="Pending", Reason="", readiness=false. Elapsed: 9.453879ms
    Mar 23 19:50:50.494: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9": Phase="Running", Reason="", readiness=true. Elapsed: 2.014684572s
    Mar 23 19:50:50.494: INFO: Pod "test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9" satisfied condition "not pending"
    Mar 23 19:50:50.494: INFO: Started pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 in namespace container-probe-9455
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 19:50:50.494
    Mar 23 19:50:50.496: INFO: Initial restart count of pod test-webserver-2c0770c4-6892-41cb-8f96-e6048c3555f9 is 0
    STEP: deleting the pod 03/23/23 19:54:51.226
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 19:54:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9455" for this suite. 03/23/23 19:54:51.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:54:51.265
Mar 23 19:54:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-webhook 03/23/23 19:54:51.266
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:54:51.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:54:51.3
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 03/23/23 19:54:51.306
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/23/23 19:54:51.84
STEP: Deploying the custom resource conversion webhook pod 03/23/23 19:54:51.85
STEP: Wait for the deployment to be ready 03/23/23 19:54:51.864
Mar 23 19:54:51.910: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 19:54:53.923
STEP: Verifying the service has paired with the endpoint 03/23/23 19:54:53.935
Mar 23 19:54:54.935: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Mar 23 19:54:54.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Creating a v1 custom resource 03/23/23 19:54:57.641
STEP: v2 custom resource should be converted 03/23/23 19:54:57.648
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:54:58.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3087" for this suite. 03/23/23 19:54:58.225
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":248,"skipped":4498,"failed":0}
------------------------------
• [SLOW TEST] [7.111 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:54:51.265
    Mar 23 19:54:51.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-webhook 03/23/23 19:54:51.266
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:54:51.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:54:51.3
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 03/23/23 19:54:51.306
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 03/23/23 19:54:51.84
    STEP: Deploying the custom resource conversion webhook pod 03/23/23 19:54:51.85
    STEP: Wait for the deployment to be ready 03/23/23 19:54:51.864
    Mar 23 19:54:51.910: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 19:54:53.923
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:54:53.935
    Mar 23 19:54:54.935: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Mar 23 19:54:54.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Creating a v1 custom resource 03/23/23 19:54:57.641
    STEP: v2 custom resource should be converted 03/23/23 19:54:57.648
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:54:58.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3087" for this suite. 03/23/23 19:54:58.225
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:54:58.378
Mar 23 19:54:58.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename subpath 03/23/23 19:54:58.38
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:54:58.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:54:58.582
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 03/23/23 19:54:58.608
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-cc92 03/23/23 19:54:58.645
STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:54:58.645
Mar 23 19:54:58.677: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cc92" in namespace "subpath-4476" to be "Succeeded or Failed"
Mar 23 19:54:58.685: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364794ms
Mar 23 19:55:00.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 2.012381747s
Mar 23 19:55:02.693: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 4.015420714s
Mar 23 19:55:04.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 6.012240391s
Mar 23 19:55:06.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 8.013172761s
Mar 23 19:55:08.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 10.01196416s
Mar 23 19:55:10.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 12.012346269s
Mar 23 19:55:12.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 14.013120578s
Mar 23 19:55:14.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 16.012050589s
Mar 23 19:55:16.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 18.011405843s
Mar 23 19:55:18.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 20.013185753s
Mar 23 19:55:20.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=false. Elapsed: 22.011791368s
Mar 23 19:55:22.692: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014598376s
STEP: Saw pod success 03/23/23 19:55:22.692
Mar 23 19:55:22.692: INFO: Pod "pod-subpath-test-configmap-cc92" satisfied condition "Succeeded or Failed"
Mar 23 19:55:22.695: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-configmap-cc92 container test-container-subpath-configmap-cc92: <nil>
STEP: delete the pod 03/23/23 19:55:22.731
Mar 23 19:55:22.748: INFO: Waiting for pod pod-subpath-test-configmap-cc92 to disappear
Mar 23 19:55:22.751: INFO: Pod pod-subpath-test-configmap-cc92 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cc92 03/23/23 19:55:22.751
Mar 23 19:55:22.751: INFO: Deleting pod "pod-subpath-test-configmap-cc92" in namespace "subpath-4476"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Mar 23 19:55:22.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4476" for this suite. 03/23/23 19:55:22.76
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":249,"skipped":4519,"failed":0}
------------------------------
• [SLOW TEST] [24.391 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:54:58.378
    Mar 23 19:54:58.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename subpath 03/23/23 19:54:58.38
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:54:58.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:54:58.582
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 03/23/23 19:54:58.608
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-cc92 03/23/23 19:54:58.645
    STEP: Creating a pod to test atomic-volume-subpath 03/23/23 19:54:58.645
    Mar 23 19:54:58.677: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cc92" in namespace "subpath-4476" to be "Succeeded or Failed"
    Mar 23 19:54:58.685: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364794ms
    Mar 23 19:55:00.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 2.012381747s
    Mar 23 19:55:02.693: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 4.015420714s
    Mar 23 19:55:04.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 6.012240391s
    Mar 23 19:55:06.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 8.013172761s
    Mar 23 19:55:08.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 10.01196416s
    Mar 23 19:55:10.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 12.012346269s
    Mar 23 19:55:12.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 14.013120578s
    Mar 23 19:55:14.690: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 16.012050589s
    Mar 23 19:55:16.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 18.011405843s
    Mar 23 19:55:18.691: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=true. Elapsed: 20.013185753s
    Mar 23 19:55:20.689: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Running", Reason="", readiness=false. Elapsed: 22.011791368s
    Mar 23 19:55:22.692: INFO: Pod "pod-subpath-test-configmap-cc92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014598376s
    STEP: Saw pod success 03/23/23 19:55:22.692
    Mar 23 19:55:22.692: INFO: Pod "pod-subpath-test-configmap-cc92" satisfied condition "Succeeded or Failed"
    Mar 23 19:55:22.695: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-subpath-test-configmap-cc92 container test-container-subpath-configmap-cc92: <nil>
    STEP: delete the pod 03/23/23 19:55:22.731
    Mar 23 19:55:22.748: INFO: Waiting for pod pod-subpath-test-configmap-cc92 to disappear
    Mar 23 19:55:22.751: INFO: Pod pod-subpath-test-configmap-cc92 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-cc92 03/23/23 19:55:22.751
    Mar 23 19:55:22.751: INFO: Deleting pod "pod-subpath-test-configmap-cc92" in namespace "subpath-4476"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Mar 23 19:55:22.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4476" for this suite. 03/23/23 19:55:22.76
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:22.774
Mar 23 19:55:22.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:55:22.775
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:22.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:22.799
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 03/23/23 19:55:22.802
Mar 23 19:55:22.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e" in namespace "projected-5288" to be "Succeeded or Failed"
Mar 23 19:55:22.829: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760193ms
Mar 23 19:55:24.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011001526s
Mar 23 19:55:26.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Running", Reason="", readiness=false. Elapsed: 4.010303947s
Mar 23 19:55:28.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010841965s
STEP: Saw pod success 03/23/23 19:55:28.834
Mar 23 19:55:28.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e" satisfied condition "Succeeded or Failed"
Mar 23 19:55:28.837: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e container client-container: <nil>
STEP: delete the pod 03/23/23 19:55:28.874
Mar 23 19:55:28.890: INFO: Waiting for pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e to disappear
Mar 23 19:55:28.893: INFO: Pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 19:55:28.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5288" for this suite. 03/23/23 19:55:28.898
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":250,"skipped":4526,"failed":0}
------------------------------
• [SLOW TEST] [6.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:22.774
    Mar 23 19:55:22.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:55:22.775
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:22.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:22.799
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 03/23/23 19:55:22.802
    Mar 23 19:55:22.823: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e" in namespace "projected-5288" to be "Succeeded or Failed"
    Mar 23 19:55:22.829: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.760193ms
    Mar 23 19:55:24.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011001526s
    Mar 23 19:55:26.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Running", Reason="", readiness=false. Elapsed: 4.010303947s
    Mar 23 19:55:28.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010841965s
    STEP: Saw pod success 03/23/23 19:55:28.834
    Mar 23 19:55:28.834: INFO: Pod "downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e" satisfied condition "Succeeded or Failed"
    Mar 23 19:55:28.837: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e container client-container: <nil>
    STEP: delete the pod 03/23/23 19:55:28.874
    Mar 23 19:55:28.890: INFO: Waiting for pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e to disappear
    Mar 23 19:55:28.893: INFO: Pod downwardapi-volume-e66ceede-e73b-4808-b7a4-bf1fbff4db7e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 19:55:28.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5288" for this suite. 03/23/23 19:55:28.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:28.918
Mar 23 19:55:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 19:55:28.919
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:28.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:28.941
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 03/23/23 19:55:28.947
STEP: Ensuring active pods == parallelism 03/23/23 19:55:28.955
STEP: Orphaning one of the Job's Pods 03/23/23 19:55:32.96
Mar 23 19:55:33.474: INFO: Successfully updated pod "adopt-release-2kqjn"
STEP: Checking that the Job readopts the Pod 03/23/23 19:55:33.474
Mar 23 19:55:33.474: INFO: Waiting up to 15m0s for pod "adopt-release-2kqjn" in namespace "job-7756" to be "adopted"
Mar 23 19:55:33.480: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 6.62929ms
Mar 23 19:55:35.485: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011466756s
Mar 23 19:55:35.485: INFO: Pod "adopt-release-2kqjn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 03/23/23 19:55:35.485
Mar 23 19:55:36.010: INFO: Successfully updated pod "adopt-release-2kqjn"
STEP: Checking that the Job releases the Pod 03/23/23 19:55:36.019
Mar 23 19:55:36.021: INFO: Waiting up to 15m0s for pod "adopt-release-2kqjn" in namespace "job-7756" to be "released"
Mar 23 19:55:36.024: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.949995ms
Mar 23 19:55:38.028: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006597564s
Mar 23 19:55:38.028: INFO: Pod "adopt-release-2kqjn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 19:55:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7756" for this suite. 03/23/23 19:55:38.032
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":251,"skipped":4533,"failed":0}
------------------------------
• [SLOW TEST] [9.121 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:28.918
    Mar 23 19:55:28.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 19:55:28.919
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:28.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:28.941
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 03/23/23 19:55:28.947
    STEP: Ensuring active pods == parallelism 03/23/23 19:55:28.955
    STEP: Orphaning one of the Job's Pods 03/23/23 19:55:32.96
    Mar 23 19:55:33.474: INFO: Successfully updated pod "adopt-release-2kqjn"
    STEP: Checking that the Job readopts the Pod 03/23/23 19:55:33.474
    Mar 23 19:55:33.474: INFO: Waiting up to 15m0s for pod "adopt-release-2kqjn" in namespace "job-7756" to be "adopted"
    Mar 23 19:55:33.480: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 6.62929ms
    Mar 23 19:55:35.485: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.011466756s
    Mar 23 19:55:35.485: INFO: Pod "adopt-release-2kqjn" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 03/23/23 19:55:35.485
    Mar 23 19:55:36.010: INFO: Successfully updated pod "adopt-release-2kqjn"
    STEP: Checking that the Job releases the Pod 03/23/23 19:55:36.019
    Mar 23 19:55:36.021: INFO: Waiting up to 15m0s for pod "adopt-release-2kqjn" in namespace "job-7756" to be "released"
    Mar 23 19:55:36.024: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.949995ms
    Mar 23 19:55:38.028: INFO: Pod "adopt-release-2kqjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006597564s
    Mar 23 19:55:38.028: INFO: Pod "adopt-release-2kqjn" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 19:55:38.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7756" for this suite. 03/23/23 19:55:38.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:38.044
Mar 23 19:55:38.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 19:55:38.045
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:38.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:38.073
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-f95ea706-1a5f-4ceb-995e-69d5f392155b 03/23/23 19:55:38.079
STEP: Creating a pod to test consume configMaps 03/23/23 19:55:38.1
Mar 23 19:55:38.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e" in namespace "projected-7581" to be "Succeeded or Failed"
Mar 23 19:55:38.132: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062087ms
Mar 23 19:55:40.137: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013046153s
Mar 23 19:55:42.136: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012728897s
Mar 23 19:55:44.135: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011975742s
STEP: Saw pod success 03/23/23 19:55:44.135
Mar 23 19:55:44.136: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e" satisfied condition "Succeeded or Failed"
Mar 23 19:55:44.138: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e container agnhost-container: <nil>
STEP: delete the pod 03/23/23 19:55:44.148
Mar 23 19:55:44.163: INFO: Waiting for pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e to disappear
Mar 23 19:55:44.167: INFO: Pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 19:55:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7581" for this suite. 03/23/23 19:55:44.172
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":252,"skipped":4552,"failed":0}
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:38.044
    Mar 23 19:55:38.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 19:55:38.045
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:38.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:38.073
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-f95ea706-1a5f-4ceb-995e-69d5f392155b 03/23/23 19:55:38.079
    STEP: Creating a pod to test consume configMaps 03/23/23 19:55:38.1
    Mar 23 19:55:38.123: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e" in namespace "projected-7581" to be "Succeeded or Failed"
    Mar 23 19:55:38.132: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062087ms
    Mar 23 19:55:40.137: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Running", Reason="", readiness=true. Elapsed: 2.013046153s
    Mar 23 19:55:42.136: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012728897s
    Mar 23 19:55:44.135: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011975742s
    STEP: Saw pod success 03/23/23 19:55:44.135
    Mar 23 19:55:44.136: INFO: Pod "pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e" satisfied condition "Succeeded or Failed"
    Mar 23 19:55:44.138: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 19:55:44.148
    Mar 23 19:55:44.163: INFO: Waiting for pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e to disappear
    Mar 23 19:55:44.167: INFO: Pod pod-projected-configmaps-5adc30a9-bedf-4c05-b9b6-38d7a550199e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 19:55:44.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7581" for this suite. 03/23/23 19:55:44.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:44.189
Mar 23 19:55:44.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:55:44.191
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:44.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:44.227
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Mar 23 19:55:44.263: INFO: Waiting up to 5m0s for pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391" in namespace "kubelet-test-3609" to be "running and ready"
Mar 23 19:55:44.278: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391": Phase="Pending", Reason="", readiness=false. Elapsed: 15.355576ms
Mar 23 19:55:44.278: INFO: The phase of Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:55:46.282: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391": Phase="Running", Reason="", readiness=true. Elapsed: 2.019572612s
Mar 23 19:55:46.282: INFO: The phase of Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 is Running (Ready = true)
Mar 23 19:55:46.282: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 23 19:55:46.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3609" for this suite. 03/23/23 19:55:46.296
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":253,"skipped":4576,"failed":0}
------------------------------
• [2.112 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:44.189
    Mar 23 19:55:44.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:55:44.191
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:44.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:44.227
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Mar 23 19:55:44.263: INFO: Waiting up to 5m0s for pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391" in namespace "kubelet-test-3609" to be "running and ready"
    Mar 23 19:55:44.278: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391": Phase="Pending", Reason="", readiness=false. Elapsed: 15.355576ms
    Mar 23 19:55:44.278: INFO: The phase of Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:55:46.282: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391": Phase="Running", Reason="", readiness=true. Elapsed: 2.019572612s
    Mar 23 19:55:46.282: INFO: The phase of Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 is Running (Ready = true)
    Mar 23 19:55:46.282: INFO: Pod "busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 23 19:55:46.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3609" for this suite. 03/23/23 19:55:46.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:46.312
Mar 23 19:55:46.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename init-container 03/23/23 19:55:46.313
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:46.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:46.332
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 03/23/23 19:55:46.335
Mar 23 19:55:46.335: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 19:55:50.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3871" for this suite. 03/23/23 19:55:50.425
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":254,"skipped":4597,"failed":0}
------------------------------
• [4.119 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:46.312
    Mar 23 19:55:46.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename init-container 03/23/23 19:55:46.313
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:46.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:46.332
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 03/23/23 19:55:46.335
    Mar 23 19:55:46.335: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 19:55:50.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3871" for this suite. 03/23/23 19:55:50.425
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:50.444
Mar 23 19:55:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 19:55:50.445
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:50.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:50.469
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 03/23/23 19:55:50.477
Mar 23 19:55:50.490: INFO: Waiting up to 5m0s for pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf" in namespace "emptydir-3994" to be "Succeeded or Failed"
Mar 23 19:55:50.493: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320394ms
Mar 23 19:55:52.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430932s
Mar 23 19:55:54.503: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Running", Reason="", readiness=false. Elapsed: 4.013094668s
Mar 23 19:55:56.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006944879s
STEP: Saw pod success 03/23/23 19:55:56.497
Mar 23 19:55:56.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf" satisfied condition "Succeeded or Failed"
Mar 23 19:55:56.500: INFO: Trying to get logs from node k8s-linuxpool-16392394-0 pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf container test-container: <nil>
STEP: delete the pod 03/23/23 19:55:56.544
Mar 23 19:55:56.566: INFO: Waiting for pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf to disappear
Mar 23 19:55:56.569: INFO: Pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 19:55:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3994" for this suite. 03/23/23 19:55:56.577
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":255,"skipped":4636,"failed":0}
------------------------------
• [SLOW TEST] [6.143 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:50.444
    Mar 23 19:55:50.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 19:55:50.445
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:50.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:50.469
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/23/23 19:55:50.477
    Mar 23 19:55:50.490: INFO: Waiting up to 5m0s for pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf" in namespace "emptydir-3994" to be "Succeeded or Failed"
    Mar 23 19:55:50.493: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.320394ms
    Mar 23 19:55:52.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Running", Reason="", readiness=true. Elapsed: 2.007430932s
    Mar 23 19:55:54.503: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Running", Reason="", readiness=false. Elapsed: 4.013094668s
    Mar 23 19:55:56.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006944879s
    STEP: Saw pod success 03/23/23 19:55:56.497
    Mar 23 19:55:56.497: INFO: Pod "pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf" satisfied condition "Succeeded or Failed"
    Mar 23 19:55:56.500: INFO: Trying to get logs from node k8s-linuxpool-16392394-0 pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf container test-container: <nil>
    STEP: delete the pod 03/23/23 19:55:56.544
    Mar 23 19:55:56.566: INFO: Waiting for pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf to disappear
    Mar 23 19:55:56.569: INFO: Pod pod-ce9c454d-1f3a-40ff-afcc-4fc3fcd94aaf no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 19:55:56.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3994" for this suite. 03/23/23 19:55:56.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:56.594
Mar 23 19:55:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:55:56.595
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:56.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:56.616
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Mar 23 19:55:56.647: INFO: Waiting up to 5m0s for pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c" in namespace "svcaccounts-7928" to be "running"
Mar 23 19:55:56.662: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.976473ms
Mar 23 19:55:58.667: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020370826s
Mar 23 19:55:58.667: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c" satisfied condition "running"
STEP: reading a file in the container 03/23/23 19:55:58.667
Mar 23 19:55:58.667: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 03/23/23 19:55:58.895
Mar 23 19:55:58.895: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 03/23/23 19:55:59.146
Mar 23 19:55:59.146: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Mar 23 19:55:59.385: INFO: Got root ca configmap in namespace "svcaccounts-7928"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 19:55:59.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7928" for this suite. 03/23/23 19:55:59.392
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":256,"skipped":4704,"failed":0}
------------------------------
• [2.807 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:56.594
    Mar 23 19:55:56.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 19:55:56.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:56.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:56.616
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Mar 23 19:55:56.647: INFO: Waiting up to 5m0s for pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c" in namespace "svcaccounts-7928" to be "running"
    Mar 23 19:55:56.662: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.976473ms
    Mar 23 19:55:58.667: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c": Phase="Running", Reason="", readiness=true. Elapsed: 2.020370826s
    Mar 23 19:55:58.667: INFO: Pod "pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c" satisfied condition "running"
    STEP: reading a file in the container 03/23/23 19:55:58.667
    Mar 23 19:55:58.667: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 03/23/23 19:55:58.895
    Mar 23 19:55:58.895: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 03/23/23 19:55:59.146
    Mar 23 19:55:59.146: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7928 pod-service-account-519ce581-bc43-4283-9d40-7167f4ba827c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Mar 23 19:55:59.385: INFO: Got root ca configmap in namespace "svcaccounts-7928"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 19:55:59.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7928" for this suite. 03/23/23 19:55:59.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:55:59.412
Mar 23 19:55:59.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 19:55:59.413
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:59.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:59.434
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-e227ec47-4dea-4ffb-9a5b-2448e1b80c88 03/23/23 19:55:59.436
STEP: Creating a pod to test consume secrets 03/23/23 19:55:59.451
Mar 23 19:55:59.464: INFO: Waiting up to 5m0s for pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39" in namespace "secrets-2338" to be "Succeeded or Failed"
Mar 23 19:55:59.476: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.873978ms
Mar 23 19:56:01.500: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036363898s
Mar 23 19:56:03.481: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017565194s
STEP: Saw pod success 03/23/23 19:56:03.481
Mar 23 19:56:03.481: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39" satisfied condition "Succeeded or Failed"
Mar 23 19:56:03.485: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 19:56:03.492
Mar 23 19:56:03.534: INFO: Waiting for pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 to disappear
Mar 23 19:56:03.550: INFO: Pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 19:56:03.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2338" for this suite. 03/23/23 19:56:03.557
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":257,"skipped":4739,"failed":0}
------------------------------
• [4.182 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:55:59.412
    Mar 23 19:55:59.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 19:55:59.413
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:55:59.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:55:59.434
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-e227ec47-4dea-4ffb-9a5b-2448e1b80c88 03/23/23 19:55:59.436
    STEP: Creating a pod to test consume secrets 03/23/23 19:55:59.451
    Mar 23 19:55:59.464: INFO: Waiting up to 5m0s for pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39" in namespace "secrets-2338" to be "Succeeded or Failed"
    Mar 23 19:55:59.476: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 12.873978ms
    Mar 23 19:56:01.500: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036363898s
    Mar 23 19:56:03.481: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017565194s
    STEP: Saw pod success 03/23/23 19:56:03.481
    Mar 23 19:56:03.481: INFO: Pod "pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39" satisfied condition "Succeeded or Failed"
    Mar 23 19:56:03.485: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 19:56:03.492
    Mar 23 19:56:03.534: INFO: Waiting for pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 to disappear
    Mar 23 19:56:03.550: INFO: Pod pod-secrets-a77d337b-fa83-4414-ac4f-eaa5b077bb39 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 19:56:03.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2338" for this suite. 03/23/23 19:56:03.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:03.602
Mar 23 19:56:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:56:03.605
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:03.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:03.644
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Mar 23 19:56:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2416" for this suite. 03/23/23 19:56:03.733
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":258,"skipped":4758,"failed":0}
------------------------------
• [0.149 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:03.602
    Mar 23 19:56:03.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubelet-test 03/23/23 19:56:03.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:03.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:03.644
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Mar 23 19:56:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2416" for this suite. 03/23/23 19:56:03.733
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:03.752
Mar 23 19:56:03.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 19:56:03.754
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:03.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:03.784
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-9305 03/23/23 19:56:03.791
STEP: creating replication controller nodeport-test in namespace services-9305 03/23/23 19:56:03.824
I0323 19:56:03.927324      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9305, replica count: 2
I0323 19:56:06.979575      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 19:56:09.979818      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 19:56:09.979: INFO: Creating new exec pod
Mar 23 19:56:09.987: INFO: Waiting up to 5m0s for pod "execpod9w88k" in namespace "services-9305" to be "running"
Mar 23 19:56:09.992: INFO: Pod "execpod9w88k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138791ms
Mar 23 19:56:12.004: INFO: Pod "execpod9w88k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017437941s
Mar 23 19:56:13.996: INFO: Pod "execpod9w88k": Phase="Running", Reason="", readiness=true. Elapsed: 4.009442306s
Mar 23 19:56:13.996: INFO: Pod "execpod9w88k" satisfied condition "running"
Mar 23 19:56:15.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Mar 23 19:56:15.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Mar 23 19:56:15.289: INFO: stdout: "nodeport-test-bg4r6"
Mar 23 19:56:15.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
Mar 23 19:56:15.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
Mar 23 19:56:15.532: INFO: stdout: ""
Mar 23 19:56:16.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
Mar 23 19:56:16.759: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
Mar 23 19:56:16.759: INFO: stdout: ""
Mar 23 19:56:17.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
Mar 23 19:56:17.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
Mar 23 19:56:17.745: INFO: stdout: ""
Mar 23 19:56:18.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
Mar 23 19:56:18.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
Mar 23 19:56:18.772: INFO: stdout: "nodeport-test-cftq9"
Mar 23 19:56:18.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 31667'
Mar 23 19:56:18.997: INFO: stderr: "+ nc -v -t -w 2 10.240.0.56 31667\n+ echo hostName\nConnection to 10.240.0.56 31667 port [tcp/*] succeeded!\n"
Mar 23 19:56:18.997: INFO: stdout: "nodeport-test-cftq9"
Mar 23 19:56:18.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 31667'
Mar 23 19:56:19.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.30 31667\nConnection to 10.240.0.30 31667 port [tcp/*] succeeded!\n"
Mar 23 19:56:19.262: INFO: stdout: "nodeport-test-bg4r6"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 19:56:19.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9305" for this suite. 03/23/23 19:56:19.267
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":259,"skipped":4761,"failed":0}
------------------------------
• [SLOW TEST] [15.525 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:03.752
    Mar 23 19:56:03.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 19:56:03.754
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:03.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:03.784
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-9305 03/23/23 19:56:03.791
    STEP: creating replication controller nodeport-test in namespace services-9305 03/23/23 19:56:03.824
    I0323 19:56:03.927324      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-9305, replica count: 2
    I0323 19:56:06.979575      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 19:56:09.979818      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 19:56:09.979: INFO: Creating new exec pod
    Mar 23 19:56:09.987: INFO: Waiting up to 5m0s for pod "execpod9w88k" in namespace "services-9305" to be "running"
    Mar 23 19:56:09.992: INFO: Pod "execpod9w88k": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138791ms
    Mar 23 19:56:12.004: INFO: Pod "execpod9w88k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017437941s
    Mar 23 19:56:13.996: INFO: Pod "execpod9w88k": Phase="Running", Reason="", readiness=true. Elapsed: 4.009442306s
    Mar 23 19:56:13.996: INFO: Pod "execpod9w88k" satisfied condition "running"
    Mar 23 19:56:15.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Mar 23 19:56:15.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Mar 23 19:56:15.289: INFO: stdout: "nodeport-test-bg4r6"
    Mar 23 19:56:15.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
    Mar 23 19:56:15.532: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
    Mar 23 19:56:15.532: INFO: stdout: ""
    Mar 23 19:56:16.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
    Mar 23 19:56:16.759: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
    Mar 23 19:56:16.759: INFO: stdout: ""
    Mar 23 19:56:17.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
    Mar 23 19:56:17.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
    Mar 23 19:56:17.745: INFO: stdout: ""
    Mar 23 19:56:18.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.64.190 80'
    Mar 23 19:56:18.771: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.64.190 80\nConnection to 10.0.64.190 80 port [tcp/http] succeeded!\n"
    Mar 23 19:56:18.772: INFO: stdout: "nodeport-test-cftq9"
    Mar 23 19:56:18.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.56 31667'
    Mar 23 19:56:18.997: INFO: stderr: "+ nc -v -t -w 2 10.240.0.56 31667\n+ echo hostName\nConnection to 10.240.0.56 31667 port [tcp/*] succeeded!\n"
    Mar 23 19:56:18.997: INFO: stdout: "nodeport-test-cftq9"
    Mar 23 19:56:18.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-9305 exec execpod9w88k -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 31667'
    Mar 23 19:56:19.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.240.0.30 31667\nConnection to 10.240.0.30 31667 port [tcp/*] succeeded!\n"
    Mar 23 19:56:19.262: INFO: stdout: "nodeport-test-bg4r6"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 19:56:19.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9305" for this suite. 03/23/23 19:56:19.267
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:19.277
Mar 23 19:56:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-pred 03/23/23 19:56:19.279
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:19.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:19.297
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 23 19:56:19.300: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 19:56:19.324: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 19:56:19.333: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
Mar 23 19:56:19.347: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 19:56:19.347: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 19:56:19.347: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 19:56:19.347: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 19:56:19.347: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 19:56:19.347: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 19:56:19.347: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:56:19.347: INFO: execpod9w88k from services-9305 started at 2023-03-23 19:56:09 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container agnhost-container ready: true, restart count 0
Mar 23 19:56:19.347: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 19:56:19.347: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container e2e ready: true, restart count 0
Mar 23 19:56:19.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:56:19.347: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 19:56:19.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:56:19.347: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:56:19.347: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
Mar 23 19:56:19.365: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 19:56:19.365: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 19:56:19.365: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 19:56:19.365: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 19:56:19.365: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 19:56:19.365: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 19:56:19.365: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:56:19.365: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container metrics-server ready: true, restart count 0
Mar 23 19:56:19.365: INFO: nodeport-test-bg4r6 from services-9305 started at 2023-03-23 19:56:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container nodeport-test ready: true, restart count 0
Mar 23 19:56:19.365: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 19:56:19.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:56:19.365: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 19:56:19.365: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
Mar 23 19:56:19.378: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.378: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 19:56:19.378: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.378: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 19:56:19.378: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.378: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 19:56:19.379: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container coredns ready: true, restart count 0
Mar 23 19:56:19.379: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 19:56:19.379: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 19:56:19.379: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 19:56:19.379: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 19:56:19.379: INFO: busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 from kubelet-test-3609 started at 2023-03-23 19:55:44 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 ready: true, restart count 0
Mar 23 19:56:19.379: INFO: nodeport-test-cftq9 from services-9305 started at 2023-03-23 19:56:04 +0000 UTC (1 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container nodeport-test ready: true, restart count 0
Mar 23 19:56:19.379: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 19:56:19.379: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 19:56:19.379: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node k8s-linuxpool-16392394-0 03/23/23 19:56:19.415
STEP: verifying the node has the label node k8s-linuxpool-16392394-1 03/23/23 19:56:19.438
STEP: verifying the node has the label node k8s-linuxpool-16392394-2 03/23/23 19:56:19.463
Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-dgzkr requesting resource cpu=50m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-z42jm requesting resource cpu=50m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-zvvq5 requesting resource cpu=50m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.493: INFO: Pod azure-npm-g2ts9 requesting resource cpu=10m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.493: INFO: Pod azure-npm-tkkkr requesting resource cpu=10m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.493: INFO: Pod azure-npm-zmbq7 requesting resource cpu=10m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-6t4kz requesting resource cpu=50m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-8vb4w requesting resource cpu=50m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-n24nn requesting resource cpu=50m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.495: INFO: Pod coredns-7c5496644c-xztfb requesting resource cpu=100m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-4f477 requesting resource cpu=30m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-d68wl requesting resource cpu=30m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-vnzfb requesting resource cpu=30m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.495: INFO: Pod kube-proxy-p754s requesting resource cpu=100m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.495: INFO: Pod kube-proxy-rt7c7 requesting resource cpu=100m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.496: INFO: Pod kube-proxy-zktpk requesting resource cpu=100m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.496: INFO: Pod metrics-server-5c57f79cb6-t8csh requesting resource cpu=100m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.497: INFO: Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.497: INFO: Pod execpod9w88k requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.497: INFO: Pod nodeport-test-bg4r6 requesting resource cpu=0m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.499: INFO: Pod nodeport-test-cftq9 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.499: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.499: INFO: Pod sonobuoy-e2e-job-4ee5050bb09d4ac4 requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.499: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.500: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.500: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw requesting resource cpu=0m on Node k8s-linuxpool-16392394-1
STEP: Starting Pods to consume most of the cluster CPU. 03/23/23 19:56:19.5
Mar 23 19:56:19.500: INFO: Creating a pod which consumes cpu=1232m on Node k8s-linuxpool-16392394-0
Mar 23 19:56:19.524: INFO: Creating a pod which consumes cpu=1162m on Node k8s-linuxpool-16392394-1
Mar 23 19:56:19.545: INFO: Creating a pod which consumes cpu=1162m on Node k8s-linuxpool-16392394-2
Mar 23 19:56:19.594: INFO: Waiting up to 5m0s for pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5" in namespace "sched-pred-8404" to be "running"
Mar 23 19:56:19.606: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.333378ms
Mar 23 19:56:21.610: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016448469s
Mar 23 19:56:21.610: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5" satisfied condition "running"
Mar 23 19:56:21.610: INFO: Waiting up to 5m0s for pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e" in namespace "sched-pred-8404" to be "running"
Mar 23 19:56:21.613: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.891394ms
Mar 23 19:56:23.619: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00898908s
Mar 23 19:56:23.620: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e" satisfied condition "running"
Mar 23 19:56:23.620: INFO: Waiting up to 5m0s for pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2" in namespace "sched-pred-8404" to be "running"
Mar 23 19:56:23.622: INFO: Pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2": Phase="Running", Reason="", readiness=true. Elapsed: 2.619895ms
Mar 23 19:56:23.622: INFO: Pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 03/23/23 19:56:23.623
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e4c958f0d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2 to k8s-linuxpool-16392394-2] 03/23/23 19:56:23.626
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e7a05086e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.626
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e7d545c34], Reason = [Created], Message = [Created container filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e89b16630], Reason = [Started], Message = [Started container filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248e4b5d9489], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e to k8s-linuxpool-16392394-1] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248e970b04d3], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-wr7tz" : failed to sync configmap cache: timed out waiting for the condition] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248ee9c2d3a0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248eede11691], Reason = [Created], Message = [Created container filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248efaa2ec4a], Reason = [Started], Message = [Started container filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e47cb629d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5 to k8s-linuxpool-16392394-0] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e78511330], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e7bc4ade8], Reason = [Created], Message = [Created container filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e87bc5a72], Reason = [Started], Message = [Started container filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5] 03/23/23 19:56:23.627
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.174f248f3bc16106], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 03/23/23 19:56:23.645
STEP: removing the label node off the node k8s-linuxpool-16392394-0 03/23/23 19:56:24.665
STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.701
STEP: removing the label node off the node k8s-linuxpool-16392394-1 03/23/23 19:56:24.706
STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.739
STEP: removing the label node off the node k8s-linuxpool-16392394-2 03/23/23 19:56:24.752
STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.803
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:56:24.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8404" for this suite. 03/23/23 19:56:24.816
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":260,"skipped":4765,"failed":0}
------------------------------
• [SLOW TEST] [5.549 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:19.277
    Mar 23 19:56:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-pred 03/23/23 19:56:19.279
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:19.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:19.297
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 23 19:56:19.300: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 23 19:56:19.324: INFO: Waiting for terminating namespaces to be deleted...
    Mar 23 19:56:19.333: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
    Mar 23 19:56:19.347: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: execpod9w88k from services-9305 started at 2023-03-23 19:56:09 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container agnhost-container ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container e2e ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 19:56:19.347: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 19:56:19.347: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
    Mar 23 19:56:19.365: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: azure-npm-g2ts9 from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: metrics-server-5c57f79cb6-t8csh from kube-system started at 2023-03-23 18:30:25 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: nodeport-test-bg4r6 from services-9305 started at 2023-03-23 19:56:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container nodeport-test ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 19:56:19.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 19:56:19.365: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
    Mar 23 19:56:19.378: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.378: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 19:56:19.378: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.378: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 19:56:19.378: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.378: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container coredns ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 from kubelet-test-3609 started at 2023-03-23 19:55:44 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: nodeport-test-cftq9 from services-9305 started at 2023-03-23 19:56:04 +0000 UTC (1 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container nodeport-test ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 19:56:19.379: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 19:56:19.379: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node k8s-linuxpool-16392394-0 03/23/23 19:56:19.415
    STEP: verifying the node has the label node k8s-linuxpool-16392394-1 03/23/23 19:56:19.438
    STEP: verifying the node has the label node k8s-linuxpool-16392394-2 03/23/23 19:56:19.463
    Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-dgzkr requesting resource cpu=50m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-z42jm requesting resource cpu=50m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.493: INFO: Pod azure-ip-masq-agent-zvvq5 requesting resource cpu=50m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.493: INFO: Pod azure-npm-g2ts9 requesting resource cpu=10m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.493: INFO: Pod azure-npm-tkkkr requesting resource cpu=10m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.493: INFO: Pod azure-npm-zmbq7 requesting resource cpu=10m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-6t4kz requesting resource cpu=50m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-8vb4w requesting resource cpu=50m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.494: INFO: Pod cloud-node-manager-n24nn requesting resource cpu=50m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.495: INFO: Pod coredns-7c5496644c-xztfb requesting resource cpu=100m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-4f477 requesting resource cpu=30m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-d68wl requesting resource cpu=30m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.495: INFO: Pod csi-azuredisk-node-vnzfb requesting resource cpu=30m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.495: INFO: Pod kube-proxy-p754s requesting resource cpu=100m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.495: INFO: Pod kube-proxy-rt7c7 requesting resource cpu=100m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.496: INFO: Pod kube-proxy-zktpk requesting resource cpu=100m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.496: INFO: Pod metrics-server-5c57f79cb6-t8csh requesting resource cpu=100m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.497: INFO: Pod busybox-scheduling-380448ed-06d2-4235-903f-0fe04ca56391 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.497: INFO: Pod execpod9w88k requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.497: INFO: Pod nodeport-test-bg4r6 requesting resource cpu=0m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.499: INFO: Pod nodeport-test-cftq9 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.499: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.499: INFO: Pod sonobuoy-e2e-job-4ee5050bb09d4ac4 requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.499: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz requesting resource cpu=0m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.500: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 requesting resource cpu=0m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.500: INFO: Pod sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw requesting resource cpu=0m on Node k8s-linuxpool-16392394-1
    STEP: Starting Pods to consume most of the cluster CPU. 03/23/23 19:56:19.5
    Mar 23 19:56:19.500: INFO: Creating a pod which consumes cpu=1232m on Node k8s-linuxpool-16392394-0
    Mar 23 19:56:19.524: INFO: Creating a pod which consumes cpu=1162m on Node k8s-linuxpool-16392394-1
    Mar 23 19:56:19.545: INFO: Creating a pod which consumes cpu=1162m on Node k8s-linuxpool-16392394-2
    Mar 23 19:56:19.594: INFO: Waiting up to 5m0s for pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5" in namespace "sched-pred-8404" to be "running"
    Mar 23 19:56:19.606: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.333378ms
    Mar 23 19:56:21.610: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016448469s
    Mar 23 19:56:21.610: INFO: Pod "filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5" satisfied condition "running"
    Mar 23 19:56:21.610: INFO: Waiting up to 5m0s for pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e" in namespace "sched-pred-8404" to be "running"
    Mar 23 19:56:21.613: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.891394ms
    Mar 23 19:56:23.619: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.00898908s
    Mar 23 19:56:23.620: INFO: Pod "filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e" satisfied condition "running"
    Mar 23 19:56:23.620: INFO: Waiting up to 5m0s for pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2" in namespace "sched-pred-8404" to be "running"
    Mar 23 19:56:23.622: INFO: Pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2": Phase="Running", Reason="", readiness=true. Elapsed: 2.619895ms
    Mar 23 19:56:23.622: INFO: Pod "filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 03/23/23 19:56:23.623
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e4c958f0d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2 to k8s-linuxpool-16392394-2] 03/23/23 19:56:23.626
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e7a05086e], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.626
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e7d545c34], Reason = [Created], Message = [Created container filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2.174f248e89b16630], Reason = [Started], Message = [Started container filler-pod-066e8a47-b73c-49e4-b03b-6e5cf34b5df2] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248e4b5d9489], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e to k8s-linuxpool-16392394-1] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248e970b04d3], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-wr7tz" : failed to sync configmap cache: timed out waiting for the condition] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248ee9c2d3a0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248eede11691], Reason = [Created], Message = [Created container filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e.174f248efaa2ec4a], Reason = [Started], Message = [Started container filler-pod-41c89cc5-c2bb-42be-a69d-c374c366cc0e] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e47cb629d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8404/filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5 to k8s-linuxpool-16392394-0] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e78511330], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e7bc4ade8], Reason = [Created], Message = [Created container filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5.174f248e87bc5a72], Reason = [Started], Message = [Started container filler-pod-5b8fc218-ed92-430c-a119-9fb85dd891a5] 03/23/23 19:56:23.627
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.174f248f3bc16106], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/master: true}. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 03/23/23 19:56:23.645
    STEP: removing the label node off the node k8s-linuxpool-16392394-0 03/23/23 19:56:24.665
    STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.701
    STEP: removing the label node off the node k8s-linuxpool-16392394-1 03/23/23 19:56:24.706
    STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.739
    STEP: removing the label node off the node k8s-linuxpool-16392394-2 03/23/23 19:56:24.752
    STEP: verifying the node doesn't have the label node 03/23/23 19:56:24.803
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:56:24.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8404" for this suite. 03/23/23 19:56:24.816
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:24.837
Mar 23 19:56:24.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 19:56:24.838
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:24.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:24.881
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 19:56:24.948
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:56:25.387
STEP: Deploying the webhook pod 03/23/23 19:56:25.394
STEP: Wait for the deployment to be ready 03/23/23 19:56:25.41
Mar 23 19:56:25.428: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Mar 23 19:56:27.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/23/23 19:56:29.446
STEP: Verifying the service has paired with the endpoint 03/23/23 19:56:29.484
Mar 23 19:56:30.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Mar 23 19:56:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2928-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 19:56:31.017
Mar 23 19:56:31.050: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource that should be mutated by the webhook 03/23/23 19:56:31.164
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:56:33.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1656" for this suite. 03/23/23 19:56:33.719
STEP: Destroying namespace "webhook-1656-markers" for this suite. 03/23/23 19:56:33.726
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":261,"skipped":4774,"failed":0}
------------------------------
• [SLOW TEST] [9.027 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:24.837
    Mar 23 19:56:24.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 19:56:24.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:24.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:24.881
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 19:56:24.948
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 19:56:25.387
    STEP: Deploying the webhook pod 03/23/23 19:56:25.394
    STEP: Wait for the deployment to be ready 03/23/23 19:56:25.41
    Mar 23 19:56:25.428: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Mar 23 19:56:27.438: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 19, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/23/23 19:56:29.446
    STEP: Verifying the service has paired with the endpoint 03/23/23 19:56:29.484
    Mar 23 19:56:30.484: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Mar 23 19:56:30.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2928-crds.webhook.example.com via the AdmissionRegistration API 03/23/23 19:56:31.017
    Mar 23 19:56:31.050: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource that should be mutated by the webhook 03/23/23 19:56:31.164
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:56:33.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1656" for this suite. 03/23/23 19:56:33.719
    STEP: Destroying namespace "webhook-1656-markers" for this suite. 03/23/23 19:56:33.726
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:33.868
Mar 23 19:56:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename podtemplate 03/23/23 19:56:33.87
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:33.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:33.959
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Mar 23 19:56:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-2265" for this suite. 03/23/23 19:56:34.071
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":262,"skipped":4786,"failed":0}
------------------------------
• [0.209 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:33.868
    Mar 23 19:56:33.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename podtemplate 03/23/23 19:56:33.87
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:33.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:33.959
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Mar 23 19:56:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-2265" for this suite. 03/23/23 19:56:34.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:34.079
Mar 23 19:56:34.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename hostport 03/23/23 19:56:34.081
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:34.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:34.113
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/23/23 19:56:34.125
W0323 19:56:34.137902      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Mar 23 19:56:34.138: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-787" to be "running and ready"
Mar 23 19:56:34.170: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.481237ms
Mar 23 19:56:34.170: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:56:36.175: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037126524s
Mar 23 19:56:36.175: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:56:38.175: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.037557835s
Mar 23 19:56:38.175: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 23 19:56:38.175: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.240.0.4 on the node which pod1 resides and expect scheduled 03/23/23 19:56:38.175
W0323 19:56:38.187471      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Mar 23 19:56:38.187: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-787" to be "running and ready"
Mar 23 19:56:38.192: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23599ms
Mar 23 19:56:38.192: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:56:40.197: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009385975s
Mar 23 19:56:40.197: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 23 19:56:40.197: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.240.0.4 but use UDP protocol on the node which pod2 resides 03/23/23 19:56:40.197
W0323 19:56:40.202778      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
Mar 23 19:56:40.202: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-787" to be "running and ready"
Mar 23 19:56:40.205: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631895ms
Mar 23 19:56:40.205: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:56:42.210: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007587178s
Mar 23 19:56:42.210: INFO: The phase of Pod pod3 is Running (Ready = true)
Mar 23 19:56:42.210: INFO: Pod "pod3" satisfied condition "running and ready"
W0323 19:56:42.219381      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Mar 23 19:56:42.219: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-787" to be "running and ready"
Mar 23 19:56:42.222: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649495ms
Mar 23 19:56:42.222: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Mar 23 19:56:44.226: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007042143s
Mar 23 19:56:44.226: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Mar 23 19:56:44.226: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/23/23 19:56:44.229
Mar 23 19:56:44.229: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.4 http://127.0.0.1:54323/hostname] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:56:44.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:56:44.230: INFO: ExecWithOptions: Clientset creation
Mar 23 19:56:44.230: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.240.0.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.4, port: 54323 03/23/23 19:56:44.392
Mar 23 19:56:44.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.4:54323/hostname] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:56:44.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:56:44.393: INFO: ExecWithOptions: Clientset creation
Mar 23 19:56:44.393: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.240.0.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.4, port: 54323 UDP 03/23/23 19:56:44.517
Mar 23 19:56:44.518: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.240.0.4 54323] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 19:56:44.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 19:56:44.519: INFO: ExecWithOptions: Clientset creation
Mar 23 19:56:44.519: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.240.0.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Mar 23 19:56:49.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-787" for this suite. 03/23/23 19:56:49.648
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":263,"skipped":4791,"failed":0}
------------------------------
• [SLOW TEST] [15.579 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:34.079
    Mar 23 19:56:34.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename hostport 03/23/23 19:56:34.081
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:34.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:34.113
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 03/23/23 19:56:34.125
    W0323 19:56:34.137902      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Mar 23 19:56:34.138: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-787" to be "running and ready"
    Mar 23 19:56:34.170: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.481237ms
    Mar 23 19:56:34.170: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:56:36.175: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037126524s
    Mar 23 19:56:36.175: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:56:38.175: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.037557835s
    Mar 23 19:56:38.175: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 23 19:56:38.175: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.240.0.4 on the node which pod1 resides and expect scheduled 03/23/23 19:56:38.175
    W0323 19:56:38.187471      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Mar 23 19:56:38.187: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-787" to be "running and ready"
    Mar 23 19:56:38.192: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23599ms
    Mar 23 19:56:38.192: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:56:40.197: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009385975s
    Mar 23 19:56:40.197: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 23 19:56:40.197: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.240.0.4 but use UDP protocol on the node which pod2 resides 03/23/23 19:56:40.197
    W0323 19:56:40.202778      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54323)
    Mar 23 19:56:40.202: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-787" to be "running and ready"
    Mar 23 19:56:40.205: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.631895ms
    Mar 23 19:56:40.205: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:56:42.210: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007587178s
    Mar 23 19:56:42.210: INFO: The phase of Pod pod3 is Running (Ready = true)
    Mar 23 19:56:42.210: INFO: Pod "pod3" satisfied condition "running and ready"
    W0323 19:56:42.219381      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Mar 23 19:56:42.219: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-787" to be "running and ready"
    Mar 23 19:56:42.222: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.649495ms
    Mar 23 19:56:42.222: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 19:56:44.226: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.007042143s
    Mar 23 19:56:44.226: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Mar 23 19:56:44.226: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 03/23/23 19:56:44.229
    Mar 23 19:56:44.229: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.240.0.4 http://127.0.0.1:54323/hostname] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:56:44.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:56:44.230: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:56:44.230: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.240.0.4+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.4, port: 54323 03/23/23 19:56:44.392
    Mar 23 19:56:44.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.240.0.4:54323/hostname] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:56:44.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:56:44.393: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:56:44.393: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.240.0.4%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.240.0.4, port: 54323 UDP 03/23/23 19:56:44.517
    Mar 23 19:56:44.518: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.240.0.4 54323] Namespace:hostport-787 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 19:56:44.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 19:56:44.519: INFO: ExecWithOptions: Clientset creation
    Mar 23 19:56:44.519: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/hostport-787/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.240.0.4+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Mar 23 19:56:49.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-787" for this suite. 03/23/23 19:56:49.648
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:56:49.664
Mar 23 19:56:49.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 19:56:49.666
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:49.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:49.688
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 03/23/23 19:56:49.697
STEP: create the rc2 03/23/23 19:56:49.706
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/23/23 19:56:54.721
STEP: delete the rc simpletest-rc-to-be-deleted 03/23/23 19:56:55.085
STEP: wait for the rc to be deleted 03/23/23 19:56:55.101
STEP: Gathering metrics 03/23/23 19:57:00.115
Mar 23 19:57:00.220: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 19:57:00.224: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.175492ms
Mar 23 19:57:00.224: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 19:57:00.224: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 19:58:00.513: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Mar 23 19:58:00.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-22nn5" in namespace "gc-4840"
Mar 23 19:58:00.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d6bn" in namespace "gc-4840"
Mar 23 19:58:00.578: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vkzh" in namespace "gc-4840"
Mar 23 19:58:00.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lcc6" in namespace "gc-4840"
Mar 23 19:58:00.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x5k9" in namespace "gc-4840"
Mar 23 19:58:00.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hc47" in namespace "gc-4840"
Mar 23 19:58:00.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksmp" in namespace "gc-4840"
Mar 23 19:58:00.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwrj" in namespace "gc-4840"
Mar 23 19:58:00.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tktn" in namespace "gc-4840"
Mar 23 19:58:00.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w9z6" in namespace "gc-4840"
Mar 23 19:58:00.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kjwc" in namespace "gc-4840"
Mar 23 19:58:00.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qnhr" in namespace "gc-4840"
Mar 23 19:58:00.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb92v" in namespace "gc-4840"
Mar 23 19:58:00.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2fzw" in namespace "gc-4840"
Mar 23 19:58:00.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd86p" in namespace "gc-4840"
Mar 23 19:58:01.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhk2d" in namespace "gc-4840"
Mar 23 19:58:01.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsdxw" in namespace "gc-4840"
Mar 23 19:58:01.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsjkz" in namespace "gc-4840"
Mar 23 19:58:01.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjp6q" in namespace "gc-4840"
Mar 23 19:58:01.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwgsl" in namespace "gc-4840"
Mar 23 19:58:01.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwfsz" in namespace "gc-4840"
Mar 23 19:58:01.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz52g" in namespace "gc-4840"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 19:58:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4840" for this suite. 03/23/23 19:58:01.318
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":264,"skipped":4791,"failed":0}
------------------------------
• [SLOW TEST] [71.674 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:56:49.664
    Mar 23 19:56:49.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 19:56:49.666
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:56:49.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:56:49.688
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 03/23/23 19:56:49.697
    STEP: create the rc2 03/23/23 19:56:49.706
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 03/23/23 19:56:54.721
    STEP: delete the rc simpletest-rc-to-be-deleted 03/23/23 19:56:55.085
    STEP: wait for the rc to be deleted 03/23/23 19:56:55.101
    STEP: Gathering metrics 03/23/23 19:57:00.115
    Mar 23 19:57:00.220: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 19:57:00.224: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 4.175492ms
    Mar 23 19:57:00.224: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 19:57:00.224: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 19:58:00.513: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    Mar 23 19:58:00.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-22nn5" in namespace "gc-4840"
    Mar 23 19:58:00.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-2d6bn" in namespace "gc-4840"
    Mar 23 19:58:00.578: INFO: Deleting pod "simpletest-rc-to-be-deleted-2vkzh" in namespace "gc-4840"
    Mar 23 19:58:00.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lcc6" in namespace "gc-4840"
    Mar 23 19:58:00.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-5x5k9" in namespace "gc-4840"
    Mar 23 19:58:00.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hc47" in namespace "gc-4840"
    Mar 23 19:58:00.718: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ksmp" in namespace "gc-4840"
    Mar 23 19:58:00.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mwrj" in namespace "gc-4840"
    Mar 23 19:58:00.797: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tktn" in namespace "gc-4840"
    Mar 23 19:58:00.827: INFO: Deleting pod "simpletest-rc-to-be-deleted-7w9z6" in namespace "gc-4840"
    Mar 23 19:58:00.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kjwc" in namespace "gc-4840"
    Mar 23 19:58:00.874: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qnhr" in namespace "gc-4840"
    Mar 23 19:58:00.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-cb92v" in namespace "gc-4840"
    Mar 23 19:58:00.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2fzw" in namespace "gc-4840"
    Mar 23 19:58:00.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-dd86p" in namespace "gc-4840"
    Mar 23 19:58:01.009: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhk2d" in namespace "gc-4840"
    Mar 23 19:58:01.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsdxw" in namespace "gc-4840"
    Mar 23 19:58:01.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsjkz" in namespace "gc-4840"
    Mar 23 19:58:01.140: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjp6q" in namespace "gc-4840"
    Mar 23 19:58:01.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwgsl" in namespace "gc-4840"
    Mar 23 19:58:01.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwfsz" in namespace "gc-4840"
    Mar 23 19:58:01.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-jz52g" in namespace "gc-4840"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 19:58:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4840" for this suite. 03/23/23 19:58:01.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:58:01.343
Mar 23 19:58:01.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:58:01.345
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:58:01.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:58:01.458
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Mar 23 19:58:01.512: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 19:59:01.650: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:01.667
Mar 23 19:59:01.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-preemption-path 03/23/23 19:59:01.668
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:01.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:01.695
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Mar 23 19:59:01.745: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Mar 23 19:59:01.749: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Mar 23 19:59:01.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-4649" for this suite. 03/23/23 19:59:01.812
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Mar 23 19:59:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5066" for this suite. 03/23/23 19:59:01.868
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":265,"skipped":4797,"failed":0}
------------------------------
• [SLOW TEST] [60.691 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:58:01.343
    Mar 23 19:58:01.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption 03/23/23 19:58:01.345
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:58:01.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:58:01.458
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Mar 23 19:58:01.512: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 19:59:01.650: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:01.667
    Mar 23 19:59:01.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-preemption-path 03/23/23 19:59:01.668
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:01.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:01.695
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Mar 23 19:59:01.745: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Mar 23 19:59:01.749: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Mar 23 19:59:01.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-4649" for this suite. 03/23/23 19:59:01.812
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 19:59:01.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5066" for this suite. 03/23/23 19:59:01.868
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:02.036
Mar 23 19:59:02.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 19:59:02.038
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:02.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:02.125
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 03/23/23 19:59:02.129
Mar 23 19:59:02.129: INFO: namespace kubectl-752
Mar 23 19:59:02.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 create -f -'
Mar 23 19:59:03.272: INFO: stderr: ""
Mar 23 19:59:03.272: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 03/23/23 19:59:03.272
Mar 23 19:59:04.277: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:59:04.277: INFO: Found 0 / 1
Mar 23 19:59:05.279: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:59:05.279: INFO: Found 0 / 1
Mar 23 19:59:06.277: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:59:06.277: INFO: Found 1 / 1
Mar 23 19:59:06.277: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Mar 23 19:59:06.280: INFO: Selector matched 1 pods for map[app:agnhost]
Mar 23 19:59:06.281: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Mar 23 19:59:06.281: INFO: wait on agnhost-primary startup in kubectl-752 
Mar 23 19:59:06.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 logs agnhost-primary-d4f5d agnhost-primary'
Mar 23 19:59:06.427: INFO: stderr: ""
Mar 23 19:59:06.427: INFO: stdout: "Paused\n"
STEP: exposing RC 03/23/23 19:59:06.427
Mar 23 19:59:06.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Mar 23 19:59:06.557: INFO: stderr: ""
Mar 23 19:59:06.557: INFO: stdout: "service/rm2 exposed\n"
Mar 23 19:59:06.569: INFO: Service rm2 in namespace kubectl-752 found.
STEP: exposing service 03/23/23 19:59:08.575
Mar 23 19:59:08.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Mar 23 19:59:08.709: INFO: stderr: ""
Mar 23 19:59:08.709: INFO: stdout: "service/rm3 exposed\n"
Mar 23 19:59:08.731: INFO: Service rm3 in namespace kubectl-752 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 19:59:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-752" for this suite. 03/23/23 19:59:10.745
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":266,"skipped":4815,"failed":0}
------------------------------
• [SLOW TEST] [8.721 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:02.036
    Mar 23 19:59:02.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 19:59:02.038
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:02.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:02.125
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 03/23/23 19:59:02.129
    Mar 23 19:59:02.129: INFO: namespace kubectl-752
    Mar 23 19:59:02.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 create -f -'
    Mar 23 19:59:03.272: INFO: stderr: ""
    Mar 23 19:59:03.272: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 03/23/23 19:59:03.272
    Mar 23 19:59:04.277: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:59:04.277: INFO: Found 0 / 1
    Mar 23 19:59:05.279: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:59:05.279: INFO: Found 0 / 1
    Mar 23 19:59:06.277: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:59:06.277: INFO: Found 1 / 1
    Mar 23 19:59:06.277: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Mar 23 19:59:06.280: INFO: Selector matched 1 pods for map[app:agnhost]
    Mar 23 19:59:06.281: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Mar 23 19:59:06.281: INFO: wait on agnhost-primary startup in kubectl-752 
    Mar 23 19:59:06.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 logs agnhost-primary-d4f5d agnhost-primary'
    Mar 23 19:59:06.427: INFO: stderr: ""
    Mar 23 19:59:06.427: INFO: stdout: "Paused\n"
    STEP: exposing RC 03/23/23 19:59:06.427
    Mar 23 19:59:06.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Mar 23 19:59:06.557: INFO: stderr: ""
    Mar 23 19:59:06.557: INFO: stdout: "service/rm2 exposed\n"
    Mar 23 19:59:06.569: INFO: Service rm2 in namespace kubectl-752 found.
    STEP: exposing service 03/23/23 19:59:08.575
    Mar 23 19:59:08.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-752 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Mar 23 19:59:08.709: INFO: stderr: ""
    Mar 23 19:59:08.709: INFO: stdout: "service/rm3 exposed\n"
    Mar 23 19:59:08.731: INFO: Service rm3 in namespace kubectl-752 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 19:59:10.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-752" for this suite. 03/23/23 19:59:10.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:10.76
Mar 23 19:59:10.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:59:10.761
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:10.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:10.789
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Mar 23 19:59:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 19:59:14.175
Mar 23 19:59:14.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 create -f -'
Mar 23 19:59:15.182: INFO: stderr: ""
Mar 23 19:59:15.182: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 19:59:15.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 delete e2e-test-crd-publish-openapi-5305-crds test-cr'
Mar 23 19:59:15.299: INFO: stderr: ""
Mar 23 19:59:15.299: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Mar 23 19:59:15.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 apply -f -'
Mar 23 19:59:16.387: INFO: stderr: ""
Mar 23 19:59:16.387: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Mar 23 19:59:16.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 delete e2e-test-crd-publish-openapi-5305-crds test-cr'
Mar 23 19:59:16.522: INFO: stderr: ""
Mar 23 19:59:16.523: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 03/23/23 19:59:16.523
Mar 23 19:59:16.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 explain e2e-test-crd-publish-openapi-5305-crds'
Mar 23 19:59:16.811: INFO: stderr: ""
Mar 23 19:59:16.811: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5305-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 19:59:21.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5131" for this suite. 03/23/23 19:59:21.529
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":267,"skipped":4839,"failed":0}
------------------------------
• [SLOW TEST] [10.775 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:10.76
    Mar 23 19:59:10.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 19:59:10.761
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:10.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:10.789
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Mar 23 19:59:10.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 03/23/23 19:59:14.175
    Mar 23 19:59:14.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 create -f -'
    Mar 23 19:59:15.182: INFO: stderr: ""
    Mar 23 19:59:15.182: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 23 19:59:15.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 delete e2e-test-crd-publish-openapi-5305-crds test-cr'
    Mar 23 19:59:15.299: INFO: stderr: ""
    Mar 23 19:59:15.299: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Mar 23 19:59:15.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 apply -f -'
    Mar 23 19:59:16.387: INFO: stderr: ""
    Mar 23 19:59:16.387: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Mar 23 19:59:16.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 --namespace=crd-publish-openapi-5131 delete e2e-test-crd-publish-openapi-5305-crds test-cr'
    Mar 23 19:59:16.522: INFO: stderr: ""
    Mar 23 19:59:16.523: INFO: stdout: "e2e-test-crd-publish-openapi-5305-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 03/23/23 19:59:16.523
    Mar 23 19:59:16.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=crd-publish-openapi-5131 explain e2e-test-crd-publish-openapi-5305-crds'
    Mar 23 19:59:16.811: INFO: stderr: ""
    Mar 23 19:59:16.811: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5305-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 19:59:21.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5131" for this suite. 03/23/23 19:59:21.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:21.544
Mar 23 19:59:21.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 19:59:21.546
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:21.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:21.572
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 03/23/23 19:59:21.575
Mar 23 19:59:21.631: INFO: Waiting up to 5m0s for pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4" in namespace "downward-api-7501" to be "Succeeded or Failed"
Mar 23 19:59:21.659: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.066145ms
Mar 23 19:59:23.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.032618272s
Mar 23 19:59:25.665: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Running", Reason="", readiness=false. Elapsed: 4.033399321s
Mar 23 19:59:27.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032771389s
STEP: Saw pod success 03/23/23 19:59:27.664
Mar 23 19:59:27.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4" satisfied condition "Succeeded or Failed"
Mar 23 19:59:27.667: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 container dapi-container: <nil>
STEP: delete the pod 03/23/23 19:59:27.701
Mar 23 19:59:27.718: INFO: Waiting for pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 to disappear
Mar 23 19:59:27.721: INFO: Pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Mar 23 19:59:27.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7501" for this suite. 03/23/23 19:59:27.725
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":268,"skipped":4860,"failed":0}
------------------------------
• [SLOW TEST] [6.186 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:21.544
    Mar 23 19:59:21.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 19:59:21.546
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:21.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:21.572
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 03/23/23 19:59:21.575
    Mar 23 19:59:21.631: INFO: Waiting up to 5m0s for pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4" in namespace "downward-api-7501" to be "Succeeded or Failed"
    Mar 23 19:59:21.659: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.066145ms
    Mar 23 19:59:23.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.032618272s
    Mar 23 19:59:25.665: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Running", Reason="", readiness=false. Elapsed: 4.033399321s
    Mar 23 19:59:27.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032771389s
    STEP: Saw pod success 03/23/23 19:59:27.664
    Mar 23 19:59:27.664: INFO: Pod "downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4" satisfied condition "Succeeded or Failed"
    Mar 23 19:59:27.667: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 19:59:27.701
    Mar 23 19:59:27.718: INFO: Waiting for pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 to disappear
    Mar 23 19:59:27.721: INFO: Pod downward-api-f61fe8e5-5d1c-4933-a61b-1ac704629bd4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Mar 23 19:59:27.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7501" for this suite. 03/23/23 19:59:27.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:27.738
Mar 23 19:59:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename discovery 03/23/23 19:59:27.74
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:27.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:27.762
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 03/23/23 19:59:27.768
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Mar 23 19:59:28.140: INFO: Checking APIGroup: apiregistration.k8s.io
Mar 23 19:59:28.141: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Mar 23 19:59:28.141: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Mar 23 19:59:28.141: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Mar 23 19:59:28.141: INFO: Checking APIGroup: apps
Mar 23 19:59:28.142: INFO: PreferredVersion.GroupVersion: apps/v1
Mar 23 19:59:28.142: INFO: Versions found [{apps/v1 v1}]
Mar 23 19:59:28.142: INFO: apps/v1 matches apps/v1
Mar 23 19:59:28.142: INFO: Checking APIGroup: events.k8s.io
Mar 23 19:59:28.143: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Mar 23 19:59:28.143: INFO: Versions found [{events.k8s.io/v1 v1}]
Mar 23 19:59:28.143: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Mar 23 19:59:28.143: INFO: Checking APIGroup: authentication.k8s.io
Mar 23 19:59:28.145: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Mar 23 19:59:28.145: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Mar 23 19:59:28.145: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Mar 23 19:59:28.145: INFO: Checking APIGroup: authorization.k8s.io
Mar 23 19:59:28.146: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Mar 23 19:59:28.146: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Mar 23 19:59:28.146: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Mar 23 19:59:28.146: INFO: Checking APIGroup: autoscaling
Mar 23 19:59:28.147: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Mar 23 19:59:28.147: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Mar 23 19:59:28.147: INFO: autoscaling/v2 matches autoscaling/v2
Mar 23 19:59:28.147: INFO: Checking APIGroup: batch
Mar 23 19:59:28.148: INFO: PreferredVersion.GroupVersion: batch/v1
Mar 23 19:59:28.148: INFO: Versions found [{batch/v1 v1}]
Mar 23 19:59:28.148: INFO: batch/v1 matches batch/v1
Mar 23 19:59:28.149: INFO: Checking APIGroup: certificates.k8s.io
Mar 23 19:59:28.150: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Mar 23 19:59:28.150: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Mar 23 19:59:28.150: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Mar 23 19:59:28.150: INFO: Checking APIGroup: networking.k8s.io
Mar 23 19:59:28.151: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Mar 23 19:59:28.151: INFO: Versions found [{networking.k8s.io/v1 v1}]
Mar 23 19:59:28.151: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Mar 23 19:59:28.151: INFO: Checking APIGroup: policy
Mar 23 19:59:28.152: INFO: PreferredVersion.GroupVersion: policy/v1
Mar 23 19:59:28.152: INFO: Versions found [{policy/v1 v1}]
Mar 23 19:59:28.152: INFO: policy/v1 matches policy/v1
Mar 23 19:59:28.152: INFO: Checking APIGroup: rbac.authorization.k8s.io
Mar 23 19:59:28.154: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Mar 23 19:59:28.154: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Mar 23 19:59:28.154: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Mar 23 19:59:28.154: INFO: Checking APIGroup: storage.k8s.io
Mar 23 19:59:28.155: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Mar 23 19:59:28.155: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Mar 23 19:59:28.155: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Mar 23 19:59:28.155: INFO: Checking APIGroup: admissionregistration.k8s.io
Mar 23 19:59:28.156: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Mar 23 19:59:28.156: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Mar 23 19:59:28.156: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Mar 23 19:59:28.156: INFO: Checking APIGroup: apiextensions.k8s.io
Mar 23 19:59:28.158: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Mar 23 19:59:28.158: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Mar 23 19:59:28.158: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Mar 23 19:59:28.158: INFO: Checking APIGroup: scheduling.k8s.io
Mar 23 19:59:28.159: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Mar 23 19:59:28.160: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Mar 23 19:59:28.160: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Mar 23 19:59:28.160: INFO: Checking APIGroup: coordination.k8s.io
Mar 23 19:59:28.161: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Mar 23 19:59:28.161: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Mar 23 19:59:28.161: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Mar 23 19:59:28.161: INFO: Checking APIGroup: node.k8s.io
Mar 23 19:59:28.162: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Mar 23 19:59:28.162: INFO: Versions found [{node.k8s.io/v1 v1}]
Mar 23 19:59:28.162: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Mar 23 19:59:28.162: INFO: Checking APIGroup: discovery.k8s.io
Mar 23 19:59:28.163: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Mar 23 19:59:28.163: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Mar 23 19:59:28.163: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Mar 23 19:59:28.163: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Mar 23 19:59:28.164: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Mar 23 19:59:28.164: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Mar 23 19:59:28.164: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Mar 23 19:59:28.164: INFO: Checking APIGroup: snapshot.storage.k8s.io
Mar 23 19:59:28.166: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Mar 23 19:59:28.166: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Mar 23 19:59:28.166: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Mar 23 19:59:28.166: INFO: Checking APIGroup: metrics.k8s.io
Mar 23 19:59:28.167: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Mar 23 19:59:28.167: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Mar 23 19:59:28.167: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Mar 23 19:59:28.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7378" for this suite. 03/23/23 19:59:28.173
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":269,"skipped":4888,"failed":0}
------------------------------
• [0.440 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:27.738
    Mar 23 19:59:27.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename discovery 03/23/23 19:59:27.74
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:27.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:27.762
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 03/23/23 19:59:27.768
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Mar 23 19:59:28.140: INFO: Checking APIGroup: apiregistration.k8s.io
    Mar 23 19:59:28.141: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Mar 23 19:59:28.141: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Mar 23 19:59:28.141: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Mar 23 19:59:28.141: INFO: Checking APIGroup: apps
    Mar 23 19:59:28.142: INFO: PreferredVersion.GroupVersion: apps/v1
    Mar 23 19:59:28.142: INFO: Versions found [{apps/v1 v1}]
    Mar 23 19:59:28.142: INFO: apps/v1 matches apps/v1
    Mar 23 19:59:28.142: INFO: Checking APIGroup: events.k8s.io
    Mar 23 19:59:28.143: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Mar 23 19:59:28.143: INFO: Versions found [{events.k8s.io/v1 v1}]
    Mar 23 19:59:28.143: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Mar 23 19:59:28.143: INFO: Checking APIGroup: authentication.k8s.io
    Mar 23 19:59:28.145: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Mar 23 19:59:28.145: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Mar 23 19:59:28.145: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Mar 23 19:59:28.145: INFO: Checking APIGroup: authorization.k8s.io
    Mar 23 19:59:28.146: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Mar 23 19:59:28.146: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Mar 23 19:59:28.146: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Mar 23 19:59:28.146: INFO: Checking APIGroup: autoscaling
    Mar 23 19:59:28.147: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Mar 23 19:59:28.147: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Mar 23 19:59:28.147: INFO: autoscaling/v2 matches autoscaling/v2
    Mar 23 19:59:28.147: INFO: Checking APIGroup: batch
    Mar 23 19:59:28.148: INFO: PreferredVersion.GroupVersion: batch/v1
    Mar 23 19:59:28.148: INFO: Versions found [{batch/v1 v1}]
    Mar 23 19:59:28.148: INFO: batch/v1 matches batch/v1
    Mar 23 19:59:28.149: INFO: Checking APIGroup: certificates.k8s.io
    Mar 23 19:59:28.150: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Mar 23 19:59:28.150: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Mar 23 19:59:28.150: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Mar 23 19:59:28.150: INFO: Checking APIGroup: networking.k8s.io
    Mar 23 19:59:28.151: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Mar 23 19:59:28.151: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Mar 23 19:59:28.151: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Mar 23 19:59:28.151: INFO: Checking APIGroup: policy
    Mar 23 19:59:28.152: INFO: PreferredVersion.GroupVersion: policy/v1
    Mar 23 19:59:28.152: INFO: Versions found [{policy/v1 v1}]
    Mar 23 19:59:28.152: INFO: policy/v1 matches policy/v1
    Mar 23 19:59:28.152: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Mar 23 19:59:28.154: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Mar 23 19:59:28.154: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Mar 23 19:59:28.154: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Mar 23 19:59:28.154: INFO: Checking APIGroup: storage.k8s.io
    Mar 23 19:59:28.155: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Mar 23 19:59:28.155: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Mar 23 19:59:28.155: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Mar 23 19:59:28.155: INFO: Checking APIGroup: admissionregistration.k8s.io
    Mar 23 19:59:28.156: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Mar 23 19:59:28.156: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Mar 23 19:59:28.156: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Mar 23 19:59:28.156: INFO: Checking APIGroup: apiextensions.k8s.io
    Mar 23 19:59:28.158: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Mar 23 19:59:28.158: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Mar 23 19:59:28.158: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Mar 23 19:59:28.158: INFO: Checking APIGroup: scheduling.k8s.io
    Mar 23 19:59:28.159: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Mar 23 19:59:28.160: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Mar 23 19:59:28.160: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Mar 23 19:59:28.160: INFO: Checking APIGroup: coordination.k8s.io
    Mar 23 19:59:28.161: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Mar 23 19:59:28.161: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Mar 23 19:59:28.161: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Mar 23 19:59:28.161: INFO: Checking APIGroup: node.k8s.io
    Mar 23 19:59:28.162: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Mar 23 19:59:28.162: INFO: Versions found [{node.k8s.io/v1 v1}]
    Mar 23 19:59:28.162: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Mar 23 19:59:28.162: INFO: Checking APIGroup: discovery.k8s.io
    Mar 23 19:59:28.163: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Mar 23 19:59:28.163: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Mar 23 19:59:28.163: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Mar 23 19:59:28.163: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Mar 23 19:59:28.164: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Mar 23 19:59:28.164: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Mar 23 19:59:28.164: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Mar 23 19:59:28.164: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Mar 23 19:59:28.166: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Mar 23 19:59:28.166: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Mar 23 19:59:28.166: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Mar 23 19:59:28.166: INFO: Checking APIGroup: metrics.k8s.io
    Mar 23 19:59:28.167: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Mar 23 19:59:28.167: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Mar 23 19:59:28.167: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Mar 23 19:59:28.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7378" for this suite. 03/23/23 19:59:28.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 19:59:28.18
Mar 23 19:59:28.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename cronjob 03/23/23 19:59:28.183
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:28.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:28.201
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 03/23/23 19:59:28.208
STEP: Ensuring no jobs are scheduled 03/23/23 19:59:28.215
STEP: Ensuring no job exists by listing jobs explicitly 03/23/23 20:04:28.223
STEP: Removing cronjob 03/23/23 20:04:28.226
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 23 20:04:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7392" for this suite. 03/23/23 20:04:28.237
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":270,"skipped":4923,"failed":0}
------------------------------
• [SLOW TEST] [300.067 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 19:59:28.18
    Mar 23 19:59:28.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename cronjob 03/23/23 19:59:28.183
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 19:59:28.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 19:59:28.201
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 03/23/23 19:59:28.208
    STEP: Ensuring no jobs are scheduled 03/23/23 19:59:28.215
    STEP: Ensuring no job exists by listing jobs explicitly 03/23/23 20:04:28.223
    STEP: Removing cronjob 03/23/23 20:04:28.226
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 23 20:04:28.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7392" for this suite. 03/23/23 20:04:28.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:28.254
Mar 23 20:04:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 20:04:28.255
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:28.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:28.285
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 03/23/23 20:04:28.288
Mar 23 20:04:28.312: INFO: created test-pod-1
Mar 23 20:04:28.334: INFO: created test-pod-2
Mar 23 20:04:28.344: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 03/23/23 20:04:28.344
Mar 23 20:04:28.344: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4915' to be running and ready
Mar 23 20:04:28.379: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 23 20:04:28.379: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 23 20:04:28.379: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 23 20:04:28.379: INFO: 0 / 3 pods in namespace 'pods-4915' are running and ready (0 seconds elapsed)
Mar 23 20:04:28.379: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
Mar 23 20:04:28.379: INFO: POD         NODE                      PHASE    GRACE  CONDITIONS
Mar 23 20:04:28.379: INFO: test-pod-1  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
Mar 23 20:04:28.379: INFO: test-pod-2  k8s-linuxpool-16392394-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
Mar 23 20:04:28.380: INFO: test-pod-3  k8s-linuxpool-16392394-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
Mar 23 20:04:28.380: INFO: 
Mar 23 20:04:30.392: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 23 20:04:30.392: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Mar 23 20:04:30.392: INFO: 1 / 3 pods in namespace 'pods-4915' are running and ready (2 seconds elapsed)
Mar 23 20:04:30.392: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
Mar 23 20:04:30.392: INFO: POD         NODE                      PHASE    GRACE  CONDITIONS
Mar 23 20:04:30.392: INFO: test-pod-1  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
Mar 23 20:04:30.392: INFO: test-pod-3  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
Mar 23 20:04:30.393: INFO: 
Mar 23 20:04:32.390: INFO: 3 / 3 pods in namespace 'pods-4915' are running and ready (4 seconds elapsed)
Mar 23 20:04:32.390: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 03/23/23 20:04:32.415
Mar 23 20:04:32.420: INFO: Pod quantity 3 is different from expected quantity 0
Mar 23 20:04:33.430: INFO: Pod quantity 3 is different from expected quantity 0
Mar 23 20:04:34.424: INFO: Pod quantity 3 is different from expected quantity 0
Mar 23 20:04:35.427: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Mar 23 20:04:36.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4915" for this suite. 03/23/23 20:04:36.427
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":271,"skipped":4963,"failed":0}
------------------------------
• [SLOW TEST] [8.179 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:28.254
    Mar 23 20:04:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 20:04:28.255
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:28.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:28.285
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 03/23/23 20:04:28.288
    Mar 23 20:04:28.312: INFO: created test-pod-1
    Mar 23 20:04:28.334: INFO: created test-pod-2
    Mar 23 20:04:28.344: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 03/23/23 20:04:28.344
    Mar 23 20:04:28.344: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-4915' to be running and ready
    Mar 23 20:04:28.379: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 23 20:04:28.379: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 23 20:04:28.379: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 23 20:04:28.379: INFO: 0 / 3 pods in namespace 'pods-4915' are running and ready (0 seconds elapsed)
    Mar 23 20:04:28.379: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
    Mar 23 20:04:28.379: INFO: POD         NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 20:04:28.379: INFO: test-pod-1  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
    Mar 23 20:04:28.379: INFO: test-pod-2  k8s-linuxpool-16392394-2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
    Mar 23 20:04:28.380: INFO: test-pod-3  k8s-linuxpool-16392394-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
    Mar 23 20:04:28.380: INFO: 
    Mar 23 20:04:30.392: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 23 20:04:30.392: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Mar 23 20:04:30.392: INFO: 1 / 3 pods in namespace 'pods-4915' are running and ready (2 seconds elapsed)
    Mar 23 20:04:30.392: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
    Mar 23 20:04:30.392: INFO: POD         NODE                      PHASE    GRACE  CONDITIONS
    Mar 23 20:04:30.392: INFO: test-pod-1  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
    Mar 23 20:04:30.392: INFO: test-pod-3  k8s-linuxpool-16392394-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-03-23 20:04:28 +0000 UTC  }]
    Mar 23 20:04:30.393: INFO: 
    Mar 23 20:04:32.390: INFO: 3 / 3 pods in namespace 'pods-4915' are running and ready (4 seconds elapsed)
    Mar 23 20:04:32.390: INFO: expected 0 pod replicas in namespace 'pods-4915', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 03/23/23 20:04:32.415
    Mar 23 20:04:32.420: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 23 20:04:33.430: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 23 20:04:34.424: INFO: Pod quantity 3 is different from expected quantity 0
    Mar 23 20:04:35.427: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Mar 23 20:04:36.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4915" for this suite. 03/23/23 20:04:36.427
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:36.437
Mar 23 20:04:36.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:04:36.438
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:36.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:36.463
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 03/23/23 20:04:36.467
Mar 23 20:04:36.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52" in namespace "projected-4121" to be "Succeeded or Failed"
Mar 23 20:04:36.516: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Pending", Reason="", readiness=false. Elapsed: 25.867442ms
Mar 23 20:04:38.521: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Running", Reason="", readiness=true. Elapsed: 2.031061313s
Mar 23 20:04:40.520: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Running", Reason="", readiness=false. Elapsed: 4.030293053s
Mar 23 20:04:42.520: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030079991s
STEP: Saw pod success 03/23/23 20:04:42.52
Mar 23 20:04:42.521: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52" satisfied condition "Succeeded or Failed"
Mar 23 20:04:42.523: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 container client-container: <nil>
STEP: delete the pod 03/23/23 20:04:42.561
Mar 23 20:04:42.580: INFO: Waiting for pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 to disappear
Mar 23 20:04:42.582: INFO: Pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 20:04:42.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4121" for this suite. 03/23/23 20:04:42.589
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":272,"skipped":4963,"failed":0}
------------------------------
• [SLOW TEST] [6.158 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:36.437
    Mar 23 20:04:36.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:04:36.438
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:36.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:36.463
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 03/23/23 20:04:36.467
    Mar 23 20:04:36.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52" in namespace "projected-4121" to be "Succeeded or Failed"
    Mar 23 20:04:36.516: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Pending", Reason="", readiness=false. Elapsed: 25.867442ms
    Mar 23 20:04:38.521: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Running", Reason="", readiness=true. Elapsed: 2.031061313s
    Mar 23 20:04:40.520: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Running", Reason="", readiness=false. Elapsed: 4.030293053s
    Mar 23 20:04:42.520: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030079991s
    STEP: Saw pod success 03/23/23 20:04:42.52
    Mar 23 20:04:42.521: INFO: Pod "downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52" satisfied condition "Succeeded or Failed"
    Mar 23 20:04:42.523: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 container client-container: <nil>
    STEP: delete the pod 03/23/23 20:04:42.561
    Mar 23 20:04:42.580: INFO: Waiting for pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 to disappear
    Mar 23 20:04:42.582: INFO: Pod downwardapi-volume-f6bb6bc1-b7d5-4e32-8ecc-8b49f69bbc52 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 20:04:42.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4121" for this suite. 03/23/23 20:04:42.589
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:42.596
Mar 23 20:04:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 20:04:42.601
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:42.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:42.625
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 03/23/23 20:04:42.628
STEP: Getting a ResourceQuota 03/23/23 20:04:42.637
STEP: Updating a ResourceQuota 03/23/23 20:04:42.647
STEP: Verifying a ResourceQuota was modified 03/23/23 20:04:42.663
STEP: Deleting a ResourceQuota 03/23/23 20:04:42.669
STEP: Verifying the deleted ResourceQuota 03/23/23 20:04:42.68
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 20:04:42.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-987" for this suite. 03/23/23 20:04:42.693
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":273,"skipped":4966,"failed":0}
------------------------------
• [0.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:42.596
    Mar 23 20:04:42.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 20:04:42.601
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:42.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:42.625
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 03/23/23 20:04:42.628
    STEP: Getting a ResourceQuota 03/23/23 20:04:42.637
    STEP: Updating a ResourceQuota 03/23/23 20:04:42.647
    STEP: Verifying a ResourceQuota was modified 03/23/23 20:04:42.663
    STEP: Deleting a ResourceQuota 03/23/23 20:04:42.669
    STEP: Verifying the deleted ResourceQuota 03/23/23 20:04:42.68
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 20:04:42.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-987" for this suite. 03/23/23 20:04:42.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:42.713
Mar 23 20:04:42.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:04:42.715
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:42.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:42.735
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 03/23/23 20:04:42.742
Mar 23 20:04:42.763: INFO: Waiting up to 5m0s for pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d" in namespace "emptydir-152" to be "Succeeded or Failed"
Mar 23 20:04:42.782: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.808262ms
Mar 23 20:04:44.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022610592s
Mar 23 20:04:46.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Running", Reason="", readiness=false. Elapsed: 4.022767405s
Mar 23 20:04:48.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023064585s
STEP: Saw pod success 03/23/23 20:04:48.786
Mar 23 20:04:48.787: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d" satisfied condition "Succeeded or Failed"
Mar 23 20:04:48.789: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d container test-container: <nil>
STEP: delete the pod 03/23/23 20:04:48.795
Mar 23 20:04:48.817: INFO: Waiting for pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d to disappear
Mar 23 20:04:48.819: INFO: Pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:04:48.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-152" for this suite. 03/23/23 20:04:48.824
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":4983,"failed":0}
------------------------------
• [SLOW TEST] [6.120 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:42.713
    Mar 23 20:04:42.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:04:42.715
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:42.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:42.735
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 03/23/23 20:04:42.742
    Mar 23 20:04:42.763: INFO: Waiting up to 5m0s for pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d" in namespace "emptydir-152" to be "Succeeded or Failed"
    Mar 23 20:04:42.782: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.808262ms
    Mar 23 20:04:44.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022610592s
    Mar 23 20:04:46.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Running", Reason="", readiness=false. Elapsed: 4.022767405s
    Mar 23 20:04:48.786: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023064585s
    STEP: Saw pod success 03/23/23 20:04:48.786
    Mar 23 20:04:48.787: INFO: Pod "pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d" satisfied condition "Succeeded or Failed"
    Mar 23 20:04:48.789: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d container test-container: <nil>
    STEP: delete the pod 03/23/23 20:04:48.795
    Mar 23 20:04:48.817: INFO: Waiting for pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d to disappear
    Mar 23 20:04:48.819: INFO: Pod pod-4f44dccc-4ac3-4455-b95e-0cb9eee6618d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:04:48.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-152" for this suite. 03/23/23 20:04:48.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:48.837
Mar 23 20:04:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:04:48.838
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:48.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:48.865
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Mar 23 20:04:48.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:04:49.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8423" for this suite. 03/23/23 20:04:49.479
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":275,"skipped":4993,"failed":0}
------------------------------
• [0.685 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:48.837
    Mar 23 20:04:48.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:04:48.838
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:48.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:48.865
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Mar 23 20:04:48.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:04:49.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8423" for this suite. 03/23/23 20:04:49.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:49.536
Mar 23 20:04:49.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:04:49.537
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:49.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:49.679
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-85cd1fef-a680-4f05-a907-243d5cbe10e8 03/23/23 20:04:49.684
STEP: Creating a pod to test consume configMaps 03/23/23 20:04:49.696
Mar 23 20:04:49.710: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e" in namespace "projected-6532" to be "Succeeded or Failed"
Mar 23 20:04:49.719: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343881ms
Mar 23 20:04:51.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012142754s
Mar 23 20:04:53.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012212649s
Mar 23 20:04:55.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012245368s
STEP: Saw pod success 03/23/23 20:04:55.723
Mar 23 20:04:55.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e" satisfied condition "Succeeded or Failed"
Mar 23 20:04:55.726: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e container agnhost-container: <nil>
STEP: delete the pod 03/23/23 20:04:55.732
Mar 23 20:04:55.745: INFO: Waiting for pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e to disappear
Mar 23 20:04:55.748: INFO: Pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 20:04:55.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6532" for this suite. 03/23/23 20:04:55.754
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":276,"skipped":5073,"failed":0}
------------------------------
• [SLOW TEST] [6.230 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:49.536
    Mar 23 20:04:49.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:04:49.537
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:49.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:49.679
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-85cd1fef-a680-4f05-a907-243d5cbe10e8 03/23/23 20:04:49.684
    STEP: Creating a pod to test consume configMaps 03/23/23 20:04:49.696
    Mar 23 20:04:49.710: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e" in namespace "projected-6532" to be "Succeeded or Failed"
    Mar 23 20:04:49.719: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.343881ms
    Mar 23 20:04:51.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012142754s
    Mar 23 20:04:53.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Running", Reason="", readiness=false. Elapsed: 4.012212649s
    Mar 23 20:04:55.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012245368s
    STEP: Saw pod success 03/23/23 20:04:55.723
    Mar 23 20:04:55.723: INFO: Pod "pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e" satisfied condition "Succeeded or Failed"
    Mar 23 20:04:55.726: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 20:04:55.732
    Mar 23 20:04:55.745: INFO: Waiting for pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e to disappear
    Mar 23 20:04:55.748: INFO: Pod pod-projected-configmaps-7cad2f80-3765-4025-983f-8342645cbe8e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 20:04:55.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6532" for this suite. 03/23/23 20:04:55.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:04:55.771
Mar 23 20:04:55.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename taint-single-pod 03/23/23 20:04:55.776
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:55.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:55.805
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Mar 23 20:04:55.816: INFO: Waiting up to 1m0s for all nodes to be ready
Mar 23 20:05:55.897: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Mar 23 20:05:55.901: INFO: Starting informer...
STEP: Starting pod... 03/23/23 20:05:55.901
Mar 23 20:05:56.116: INFO: Pod is running on k8s-linuxpool-16392394-1. Tainting Node
STEP: Trying to apply a taint on the Node 03/23/23 20:05:56.116
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 20:05:56.134
STEP: Waiting short time to make sure Pod is queued for deletion 03/23/23 20:05:56.143
Mar 23 20:05:56.143: INFO: Pod wasn't evicted. Proceeding
Mar 23 20:05:56.143: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 20:05:56.176
STEP: Waiting some time to make sure that toleration time passed. 03/23/23 20:05:56.188
Mar 23 20:07:11.190: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Mar 23 20:07:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-55" for this suite. 03/23/23 20:07:11.197
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":277,"skipped":5088,"failed":0}
------------------------------
• [SLOW TEST] [135.434 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:04:55.771
    Mar 23 20:04:55.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename taint-single-pod 03/23/23 20:04:55.776
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:04:55.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:04:55.805
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Mar 23 20:04:55.816: INFO: Waiting up to 1m0s for all nodes to be ready
    Mar 23 20:05:55.897: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Mar 23 20:05:55.901: INFO: Starting informer...
    STEP: Starting pod... 03/23/23 20:05:55.901
    Mar 23 20:05:56.116: INFO: Pod is running on k8s-linuxpool-16392394-1. Tainting Node
    STEP: Trying to apply a taint on the Node 03/23/23 20:05:56.116
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 20:05:56.134
    STEP: Waiting short time to make sure Pod is queued for deletion 03/23/23 20:05:56.143
    Mar 23 20:05:56.143: INFO: Pod wasn't evicted. Proceeding
    Mar 23 20:05:56.143: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 03/23/23 20:05:56.176
    STEP: Waiting some time to make sure that toleration time passed. 03/23/23 20:05:56.188
    Mar 23 20:07:11.190: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 20:07:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-55" for this suite. 03/23/23 20:07:11.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:11.21
Mar 23 20:07:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 20:07:11.211
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:11.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:11.275
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 03/23/23 20:07:11.301
Mar 23 20:07:11.301: INFO: Creating simple deployment test-deployment-8q6rc
Mar 23 20:07:11.324: INFO: deployment "test-deployment-8q6rc" doesn't have the required revision set
STEP: Getting /status 03/23/23 20:07:13.337
Mar 23 20:07:13.341: INFO: Deployment test-deployment-8q6rc has Conditions: [{Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 03/23/23 20:07:13.341
Mar 23 20:07:13.354: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 7, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8q6rc-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 03/23/23 20:07:13.354
Mar 23 20:07:13.357: INFO: Observed &Deployment event: ADDED
Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
Mar 23 20:07:13.357: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 23 20:07:13.358: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8q6rc-777898ffcc" is progressing.}
Mar 23 20:07:13.358: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
Mar 23 20:07:13.359: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.359: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 23 20:07:13.359: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
Mar 23 20:07:13.359: INFO: Found Deployment test-deployment-8q6rc in namespace deployment-8328 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 23 20:07:13.359: INFO: Deployment test-deployment-8q6rc has an updated status
STEP: patching the Statefulset Status 03/23/23 20:07:13.359
Mar 23 20:07:13.359: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 23 20:07:13.368: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 03/23/23 20:07:13.368
Mar 23 20:07:13.371: INFO: Observed &Deployment event: ADDED
Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
Mar 23 20:07:13.372: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8q6rc-777898ffcc" is progressing.}
Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 23 20:07:13.374: INFO: Observed &Deployment event: MODIFIED
Mar 23 20:07:13.374: INFO: Found deployment test-deployment-8q6rc in namespace deployment-8328 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Mar 23 20:07:13.374: INFO: Deployment test-deployment-8q6rc has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 20:07:13.378: INFO: Deployment "test-deployment-8q6rc":
&Deployment{ObjectMeta:{test-deployment-8q6rc  deployment-8328  9a77a370-fa5a-44f2-b619-7020efa7a357 38370 1 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-23 20:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-23 20:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000db4268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8q6rc-777898ffcc",LastUpdateTime:2023-03-23 20:07:13 +0000 UTC,LastTransitionTime:2023-03-23 20:07:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 20:07:13.382: INFO: New ReplicaSet "test-deployment-8q6rc-777898ffcc" of Deployment "test-deployment-8q6rc":
&ReplicaSet{ObjectMeta:{test-deployment-8q6rc-777898ffcc  deployment-8328  5a4f1026-7c08-4be4-89b9-bf794027af57 38365 1 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8q6rc 9a77a370-fa5a-44f2-b619-7020efa7a357 0xc0051033f0 0xc0051033f1}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a77a370-fa5a-44f2-b619-7020efa7a357\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:07:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005103498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:07:13.388: INFO: Pod "test-deployment-8q6rc-777898ffcc-zbh55" is available:
&Pod{ObjectMeta:{test-deployment-8q6rc-777898ffcc-zbh55 test-deployment-8q6rc-777898ffcc- deployment-8328  70b362cf-3448-4775-8379-ddda61cfbde4 38364 0 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-8q6rc-777898ffcc 5a4f1026-7c08-4be4-89b9-bf794027af57 0xc000db4750 0xc000db4751}] [] [{kube-controller-manager Update v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a4f1026-7c08-4be4-89b9-bf794027af57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:07:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l55s6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l55s6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.59,StartTime:2023-03-23 20:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7107032dac1a4f0d823aac147cdae5f038d053de8004d6bf56ed063f71ec365b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 20:07:13.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8328" for this suite. 03/23/23 20:07:13.394
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":278,"skipped":5105,"failed":0}
------------------------------
• [2.192 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:11.21
    Mar 23 20:07:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 20:07:11.211
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:11.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:11.275
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 03/23/23 20:07:11.301
    Mar 23 20:07:11.301: INFO: Creating simple deployment test-deployment-8q6rc
    Mar 23 20:07:11.324: INFO: deployment "test-deployment-8q6rc" doesn't have the required revision set
    STEP: Getting /status 03/23/23 20:07:13.337
    Mar 23 20:07:13.341: INFO: Deployment test-deployment-8q6rc has Conditions: [{Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 03/23/23 20:07:13.341
    Mar 23 20:07:13.354: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 7, 12, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 7, 11, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8q6rc-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 03/23/23 20:07:13.354
    Mar 23 20:07:13.357: INFO: Observed &Deployment event: ADDED
    Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
    Mar 23 20:07:13.357: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
    Mar 23 20:07:13.357: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 23 20:07:13.358: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8q6rc-777898ffcc" is progressing.}
    Mar 23 20:07:13.358: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 23 20:07:13.358: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
    Mar 23 20:07:13.359: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.359: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 23 20:07:13.359: INFO: Observed Deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
    Mar 23 20:07:13.359: INFO: Found Deployment test-deployment-8q6rc in namespace deployment-8328 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 23 20:07:13.359: INFO: Deployment test-deployment-8q6rc has an updated status
    STEP: patching the Statefulset Status 03/23/23 20:07:13.359
    Mar 23 20:07:13.359: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 23 20:07:13.368: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 03/23/23 20:07:13.368
    Mar 23 20:07:13.371: INFO: Observed &Deployment event: ADDED
    Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
    Mar 23 20:07:13.372: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8q6rc-777898ffcc"}
    Mar 23 20:07:13.372: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:11 +0000 UTC 2023-03-23 20:07:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8q6rc-777898ffcc" is progressing.}
    Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
    Mar 23 20:07:13.373: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:12 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-03-23 20:07:12 +0000 UTC 2023-03-23 20:07:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8q6rc-777898ffcc" has successfully progressed.}
    Mar 23 20:07:13.373: INFO: Observed deployment test-deployment-8q6rc in namespace deployment-8328 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 23 20:07:13.374: INFO: Observed &Deployment event: MODIFIED
    Mar 23 20:07:13.374: INFO: Found deployment test-deployment-8q6rc in namespace deployment-8328 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Mar 23 20:07:13.374: INFO: Deployment test-deployment-8q6rc has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 20:07:13.378: INFO: Deployment "test-deployment-8q6rc":
    &Deployment{ObjectMeta:{test-deployment-8q6rc  deployment-8328  9a77a370-fa5a-44f2-b619-7020efa7a357 38370 1 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-03-23 20:07:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-03-23 20:07:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000db4268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-8q6rc-777898ffcc",LastUpdateTime:2023-03-23 20:07:13 +0000 UTC,LastTransitionTime:2023-03-23 20:07:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 23 20:07:13.382: INFO: New ReplicaSet "test-deployment-8q6rc-777898ffcc" of Deployment "test-deployment-8q6rc":
    &ReplicaSet{ObjectMeta:{test-deployment-8q6rc-777898ffcc  deployment-8328  5a4f1026-7c08-4be4-89b9-bf794027af57 38365 1 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8q6rc 9a77a370-fa5a-44f2-b619-7020efa7a357 0xc0051033f0 0xc0051033f1}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9a77a370-fa5a-44f2-b619-7020efa7a357\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:07:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005103498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 20:07:13.388: INFO: Pod "test-deployment-8q6rc-777898ffcc-zbh55" is available:
    &Pod{ObjectMeta:{test-deployment-8q6rc-777898ffcc-zbh55 test-deployment-8q6rc-777898ffcc- deployment-8328  70b362cf-3448-4775-8379-ddda61cfbde4 38364 0 2023-03-23 20:07:11 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-8q6rc-777898ffcc 5a4f1026-7c08-4be4-89b9-bf794027af57 0xc000db4750 0xc000db4751}] [] [{kube-controller-manager Update v1 2023-03-23 20:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5a4f1026-7c08-4be4-89b9-bf794027af57\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:07:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l55s6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l55s6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.59,StartTime:2023-03-23 20:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:07:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://7107032dac1a4f0d823aac147cdae5f038d053de8004d6bf56ed063f71ec365b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 20:07:13.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8328" for this suite. 03/23/23 20:07:13.394
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:13.407
Mar 23 20:07:13.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:07:13.41
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:13.43
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:13.435
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-5ede99a8-db41-4464-94f2-bac55d72fb67 03/23/23 20:07:13.439
STEP: Creating a pod to test consume configMaps 03/23/23 20:07:13.443
Mar 23 20:07:13.459: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc" in namespace "projected-7719" to be "Succeeded or Failed"
Mar 23 20:07:13.470: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.959676ms
Mar 23 20:07:15.474: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014698841s
Mar 23 20:07:17.475: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Running", Reason="", readiness=false. Elapsed: 4.015622112s
Mar 23 20:07:19.474: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014682688s
STEP: Saw pod success 03/23/23 20:07:19.474
Mar 23 20:07:19.475: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc" satisfied condition "Succeeded or Failed"
Mar 23 20:07:19.477: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc container agnhost-container: <nil>
STEP: delete the pod 03/23/23 20:07:19.523
Mar 23 20:07:19.566: INFO: Waiting for pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc to disappear
Mar 23 20:07:19.572: INFO: Pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 20:07:19.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7719" for this suite. 03/23/23 20:07:19.581
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":279,"skipped":5107,"failed":0}
------------------------------
• [SLOW TEST] [6.184 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:13.407
    Mar 23 20:07:13.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:07:13.41
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:13.43
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:13.435
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-5ede99a8-db41-4464-94f2-bac55d72fb67 03/23/23 20:07:13.439
    STEP: Creating a pod to test consume configMaps 03/23/23 20:07:13.443
    Mar 23 20:07:13.459: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc" in namespace "projected-7719" to be "Succeeded or Failed"
    Mar 23 20:07:13.470: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.959676ms
    Mar 23 20:07:15.474: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Running", Reason="", readiness=true. Elapsed: 2.014698841s
    Mar 23 20:07:17.475: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Running", Reason="", readiness=false. Elapsed: 4.015622112s
    Mar 23 20:07:19.474: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014682688s
    STEP: Saw pod success 03/23/23 20:07:19.474
    Mar 23 20:07:19.475: INFO: Pod "pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc" satisfied condition "Succeeded or Failed"
    Mar 23 20:07:19.477: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 20:07:19.523
    Mar 23 20:07:19.566: INFO: Waiting for pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc to disappear
    Mar 23 20:07:19.572: INFO: Pod pod-projected-configmaps-71940c65-28a2-4e36-8d98-65d22da35bdc no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 20:07:19.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7719" for this suite. 03/23/23 20:07:19.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:19.594
Mar 23 20:07:19.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename job 03/23/23 20:07:19.595
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:19.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:19.623
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 03/23/23 20:07:19.628
STEP: Ensuring job reaches completions 03/23/23 20:07:19.649
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Mar 23 20:07:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5207" for this suite. 03/23/23 20:07:35.659
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":280,"skipped":5116,"failed":0}
------------------------------
• [SLOW TEST] [16.072 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:19.594
    Mar 23 20:07:19.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename job 03/23/23 20:07:19.595
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:19.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:19.623
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 03/23/23 20:07:19.628
    STEP: Ensuring job reaches completions 03/23/23 20:07:19.649
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Mar 23 20:07:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5207" for this suite. 03/23/23 20:07:35.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:35.668
Mar 23 20:07:35.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 20:07:35.67
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:35.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:35.69
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 20:07:35.718
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:07:36.237
STEP: Deploying the webhook pod 03/23/23 20:07:36.254
STEP: Wait for the deployment to be ready 03/23/23 20:07:36.274
Mar 23 20:07:36.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 20:07:38.313
STEP: Verifying the service has paired with the endpoint 03/23/23 20:07:38.333
Mar 23 20:07:39.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 03/23/23 20:07:39.418
STEP: Creating a configMap that should be mutated 03/23/23 20:07:39.431
STEP: Deleting the collection of validation webhooks 03/23/23 20:07:39.459
STEP: Creating a configMap that should not be mutated 03/23/23 20:07:39.519
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:07:39.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3369" for this suite. 03/23/23 20:07:39.549
STEP: Destroying namespace "webhook-3369-markers" for this suite. 03/23/23 20:07:39.57
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":281,"skipped":5121,"failed":0}
------------------------------
• [4.062 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:35.668
    Mar 23 20:07:35.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 20:07:35.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:35.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:35.69
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 20:07:35.718
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:07:36.237
    STEP: Deploying the webhook pod 03/23/23 20:07:36.254
    STEP: Wait for the deployment to be ready 03/23/23 20:07:36.274
    Mar 23 20:07:36.301: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 20:07:38.313
    STEP: Verifying the service has paired with the endpoint 03/23/23 20:07:38.333
    Mar 23 20:07:39.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 03/23/23 20:07:39.418
    STEP: Creating a configMap that should be mutated 03/23/23 20:07:39.431
    STEP: Deleting the collection of validation webhooks 03/23/23 20:07:39.459
    STEP: Creating a configMap that should not be mutated 03/23/23 20:07:39.519
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:07:39.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3369" for this suite. 03/23/23 20:07:39.549
    STEP: Destroying namespace "webhook-3369-markers" for this suite. 03/23/23 20:07:39.57
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:39.731
Mar 23 20:07:39.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename init-container 03/23/23 20:07:39.733
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:39.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:39.773
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 03/23/23 20:07:39.778
Mar 23 20:07:39.778: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 20:07:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5958" for this suite. 03/23/23 20:07:48.579
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":282,"skipped":5123,"failed":0}
------------------------------
• [SLOW TEST] [8.865 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:39.731
    Mar 23 20:07:39.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename init-container 03/23/23 20:07:39.733
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:39.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:39.773
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 03/23/23 20:07:39.778
    Mar 23 20:07:39.778: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 20:07:48.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5958" for this suite. 03/23/23 20:07:48.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:48.6
Mar 23 20:07:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 20:07:48.605
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:48.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:48.641
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-04bb43b4-18f9-4265-95f0-cecbbd4fc5ee 03/23/23 20:07:48.683
STEP: Creating a pod to test consume secrets 03/23/23 20:07:48.688
Mar 23 20:07:48.700: INFO: Waiting up to 5m0s for pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534" in namespace "secrets-4819" to be "Succeeded or Failed"
Mar 23 20:07:48.714: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Pending", Reason="", readiness=false. Elapsed: 13.41667ms
Mar 23 20:07:50.718: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Running", Reason="", readiness=true. Elapsed: 2.017914848s
Mar 23 20:07:52.718: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Running", Reason="", readiness=false. Elapsed: 4.017701424s
Mar 23 20:07:54.720: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019314296s
STEP: Saw pod success 03/23/23 20:07:54.72
Mar 23 20:07:54.720: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534" satisfied condition "Succeeded or Failed"
Mar 23 20:07:54.724: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 20:07:54.73
Mar 23 20:07:54.750: INFO: Waiting for pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 to disappear
Mar 23 20:07:54.753: INFO: Pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 20:07:54.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4819" for this suite. 03/23/23 20:07:54.76
STEP: Destroying namespace "secret-namespace-1855" for this suite. 03/23/23 20:07:54.765
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":283,"skipped":5129,"failed":0}
------------------------------
• [SLOW TEST] [6.172 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:48.6
    Mar 23 20:07:48.601: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 20:07:48.605
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:48.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:48.641
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-04bb43b4-18f9-4265-95f0-cecbbd4fc5ee 03/23/23 20:07:48.683
    STEP: Creating a pod to test consume secrets 03/23/23 20:07:48.688
    Mar 23 20:07:48.700: INFO: Waiting up to 5m0s for pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534" in namespace "secrets-4819" to be "Succeeded or Failed"
    Mar 23 20:07:48.714: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Pending", Reason="", readiness=false. Elapsed: 13.41667ms
    Mar 23 20:07:50.718: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Running", Reason="", readiness=true. Elapsed: 2.017914848s
    Mar 23 20:07:52.718: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Running", Reason="", readiness=false. Elapsed: 4.017701424s
    Mar 23 20:07:54.720: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019314296s
    STEP: Saw pod success 03/23/23 20:07:54.72
    Mar 23 20:07:54.720: INFO: Pod "pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534" satisfied condition "Succeeded or Failed"
    Mar 23 20:07:54.724: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:07:54.73
    Mar 23 20:07:54.750: INFO: Waiting for pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 to disappear
    Mar 23 20:07:54.753: INFO: Pod pod-secrets-c640af16-dad2-412b-a0d3-6c8506873534 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 20:07:54.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4819" for this suite. 03/23/23 20:07:54.76
    STEP: Destroying namespace "secret-namespace-1855" for this suite. 03/23/23 20:07:54.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:07:54.777
Mar 23 20:07:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 20:07:54.778
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:54.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:54.798
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 03/23/23 20:07:54.801
Mar 23 20:07:54.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 create -f -'
Mar 23 20:07:55.522: INFO: stderr: ""
Mar 23 20:07:55.522: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:07:55.522
Mar 23 20:07:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:07:55.642: INFO: stderr: ""
Mar 23 20:07:55.642: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
Mar 23 20:07:55.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:07:55.766: INFO: stderr: ""
Mar 23 20:07:55.766: INFO: stdout: ""
Mar 23 20:07:55.766: INFO: update-demo-nautilus-9mwcc is created but not running
Mar 23 20:08:00.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:08:00.895: INFO: stderr: ""
Mar 23 20:08:00.895: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
Mar 23 20:08:00.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:08:01.027: INFO: stderr: ""
Mar 23 20:08:01.027: INFO: stdout: ""
Mar 23 20:08:01.027: INFO: update-demo-nautilus-9mwcc is created but not running
Mar 23 20:08:06.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:08:06.124: INFO: stderr: ""
Mar 23 20:08:06.124: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
Mar 23 20:08:06.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:08:06.216: INFO: stderr: ""
Mar 23 20:08:06.217: INFO: stdout: "true"
Mar 23 20:08:06.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:08:06.333: INFO: stderr: ""
Mar 23 20:08:06.333: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:08:06.333: INFO: validating pod update-demo-nautilus-9mwcc
Mar 23 20:08:06.338: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:08:06.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:08:06.338: INFO: update-demo-nautilus-9mwcc is verified up and running
Mar 23 20:08:06.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:08:06.436: INFO: stderr: ""
Mar 23 20:08:06.436: INFO: stdout: ""
Mar 23 20:08:06.436: INFO: update-demo-nautilus-w4zks is created but not running
Mar 23 20:08:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:08:11.540: INFO: stderr: ""
Mar 23 20:08:11.540: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
Mar 23 20:08:11.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:08:11.637: INFO: stderr: ""
Mar 23 20:08:11.637: INFO: stdout: "true"
Mar 23 20:08:11.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:08:11.748: INFO: stderr: ""
Mar 23 20:08:11.748: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:08:11.748: INFO: validating pod update-demo-nautilus-9mwcc
Mar 23 20:08:11.752: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:08:11.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:08:11.752: INFO: update-demo-nautilus-9mwcc is verified up and running
Mar 23 20:08:11.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:08:11.857: INFO: stderr: ""
Mar 23 20:08:11.857: INFO: stdout: "true"
Mar 23 20:08:11.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:08:11.958: INFO: stderr: ""
Mar 23 20:08:11.958: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:08:11.958: INFO: validating pod update-demo-nautilus-w4zks
Mar 23 20:08:11.963: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:08:11.963: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:08:11.963: INFO: update-demo-nautilus-w4zks is verified up and running
STEP: using delete to clean up resources 03/23/23 20:08:11.963
Mar 23 20:08:11.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 delete --grace-period=0 --force -f -'
Mar 23 20:08:12.108: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 20:08:12.108: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 20:08:12.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get rc,svc -l name=update-demo --no-headers'
Mar 23 20:08:12.241: INFO: stderr: "No resources found in kubectl-959 namespace.\n"
Mar 23 20:08:12.241: INFO: stdout: ""
Mar 23 20:08:12.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 20:08:12.376: INFO: stderr: ""
Mar 23 20:08:12.376: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 20:08:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-959" for this suite. 03/23/23 20:08:12.381
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":284,"skipped":5166,"failed":0}
------------------------------
• [SLOW TEST] [17.612 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:07:54.777
    Mar 23 20:07:54.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 20:07:54.778
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:07:54.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:07:54.798
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 03/23/23 20:07:54.801
    Mar 23 20:07:54.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 create -f -'
    Mar 23 20:07:55.522: INFO: stderr: ""
    Mar 23 20:07:55.522: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:07:55.522
    Mar 23 20:07:55.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:07:55.642: INFO: stderr: ""
    Mar 23 20:07:55.642: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
    Mar 23 20:07:55.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:07:55.766: INFO: stderr: ""
    Mar 23 20:07:55.766: INFO: stdout: ""
    Mar 23 20:07:55.766: INFO: update-demo-nautilus-9mwcc is created but not running
    Mar 23 20:08:00.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:08:00.895: INFO: stderr: ""
    Mar 23 20:08:00.895: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
    Mar 23 20:08:00.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:08:01.027: INFO: stderr: ""
    Mar 23 20:08:01.027: INFO: stdout: ""
    Mar 23 20:08:01.027: INFO: update-demo-nautilus-9mwcc is created but not running
    Mar 23 20:08:06.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:08:06.124: INFO: stderr: ""
    Mar 23 20:08:06.124: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
    Mar 23 20:08:06.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:08:06.216: INFO: stderr: ""
    Mar 23 20:08:06.217: INFO: stdout: "true"
    Mar 23 20:08:06.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:08:06.333: INFO: stderr: ""
    Mar 23 20:08:06.333: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:08:06.333: INFO: validating pod update-demo-nautilus-9mwcc
    Mar 23 20:08:06.338: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:08:06.338: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:08:06.338: INFO: update-demo-nautilus-9mwcc is verified up and running
    Mar 23 20:08:06.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:08:06.436: INFO: stderr: ""
    Mar 23 20:08:06.436: INFO: stdout: ""
    Mar 23 20:08:06.436: INFO: update-demo-nautilus-w4zks is created but not running
    Mar 23 20:08:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:08:11.540: INFO: stderr: ""
    Mar 23 20:08:11.540: INFO: stdout: "update-demo-nautilus-9mwcc update-demo-nautilus-w4zks "
    Mar 23 20:08:11.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:08:11.637: INFO: stderr: ""
    Mar 23 20:08:11.637: INFO: stdout: "true"
    Mar 23 20:08:11.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-9mwcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:08:11.748: INFO: stderr: ""
    Mar 23 20:08:11.748: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:08:11.748: INFO: validating pod update-demo-nautilus-9mwcc
    Mar 23 20:08:11.752: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:08:11.752: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:08:11.752: INFO: update-demo-nautilus-9mwcc is verified up and running
    Mar 23 20:08:11.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:08:11.857: INFO: stderr: ""
    Mar 23 20:08:11.857: INFO: stdout: "true"
    Mar 23 20:08:11.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods update-demo-nautilus-w4zks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:08:11.958: INFO: stderr: ""
    Mar 23 20:08:11.958: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:08:11.958: INFO: validating pod update-demo-nautilus-w4zks
    Mar 23 20:08:11.963: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:08:11.963: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:08:11.963: INFO: update-demo-nautilus-w4zks is verified up and running
    STEP: using delete to clean up resources 03/23/23 20:08:11.963
    Mar 23 20:08:11.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 delete --grace-period=0 --force -f -'
    Mar 23 20:08:12.108: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 20:08:12.108: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 23 20:08:12.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get rc,svc -l name=update-demo --no-headers'
    Mar 23 20:08:12.241: INFO: stderr: "No resources found in kubectl-959 namespace.\n"
    Mar 23 20:08:12.241: INFO: stdout: ""
    Mar 23 20:08:12.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-959 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 23 20:08:12.376: INFO: stderr: ""
    Mar 23 20:08:12.376: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 20:08:12.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-959" for this suite. 03/23/23 20:08:12.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:12.389
Mar 23 20:08:12.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename watch 03/23/23 20:08:12.39
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:12.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:12.41
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 03/23/23 20:08:12.413
STEP: creating a watch on configmaps with label B 03/23/23 20:08:12.414
STEP: creating a watch on configmaps with label A or B 03/23/23 20:08:12.416
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.418
Mar 23 20:08:12.433: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38972 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:12.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38972 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.434
Mar 23 20:08:12.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38974 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:12.457: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38974 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/23/23 20:08:12.457
Mar 23 20:08:12.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38976 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:12.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38976 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.465
Mar 23 20:08:12.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38977 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:12.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38977 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/23/23 20:08:12.47
Mar 23 20:08:12.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 38978 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:12.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 38978 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/23/23 20:08:22.475
Mar 23 20:08:22.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 39037 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Mar 23 20:08:22.491: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 39037 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Mar 23 20:08:32.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8149" for this suite. 03/23/23 20:08:32.498
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":285,"skipped":5171,"failed":0}
------------------------------
• [SLOW TEST] [20.119 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:12.389
    Mar 23 20:08:12.389: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename watch 03/23/23 20:08:12.39
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:12.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:12.41
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 03/23/23 20:08:12.413
    STEP: creating a watch on configmaps with label B 03/23/23 20:08:12.414
    STEP: creating a watch on configmaps with label A or B 03/23/23 20:08:12.416
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.418
    Mar 23 20:08:12.433: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38972 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:12.434: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38972 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.434
    Mar 23 20:08:12.456: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38974 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:12.457: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38974 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 03/23/23 20:08:12.457
    Mar 23 20:08:12.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38976 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:12.465: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38976 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 03/23/23 20:08:12.465
    Mar 23 20:08:12.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38977 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:12.470: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8149  535d5be9-cba2-494f-9949-55a10dd45fe0 38977 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 03/23/23 20:08:12.47
    Mar 23 20:08:12.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 38978 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:12.474: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 38978 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 03/23/23 20:08:22.475
    Mar 23 20:08:22.490: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 39037 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Mar 23 20:08:22.491: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8149  24de7d2c-1450-42bd-937d-b492eef7a071 39037 0 2023-03-23 20:08:12 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-03-23 20:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Mar 23 20:08:32.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8149" for this suite. 03/23/23 20:08:32.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:32.518
Mar 23 20:08:32.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 20:08:32.519
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:32.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:32.549
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 20:08:32.584
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:08:33.159
STEP: Deploying the webhook pod 03/23/23 20:08:33.167
STEP: Wait for the deployment to be ready 03/23/23 20:08:33.189
Mar 23 20:08:33.201: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Mar 23 20:08:35.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 03/23/23 20:08:37.224
STEP: Verifying the service has paired with the endpoint 03/23/23 20:08:37.242
Mar 23 20:08:38.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 03/23/23 20:08:38.246
STEP: Creating a custom resource definition that should be denied by the webhook 03/23/23 20:08:38.278
Mar 23 20:08:38.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:08:38.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5526" for this suite. 03/23/23 20:08:38.306
STEP: Destroying namespace "webhook-5526-markers" for this suite. 03/23/23 20:08:38.315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":286,"skipped":5226,"failed":0}
------------------------------
• [SLOW TEST] [5.954 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:32.518
    Mar 23 20:08:32.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 20:08:32.519
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:32.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:32.549
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 20:08:32.584
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:08:33.159
    STEP: Deploying the webhook pod 03/23/23 20:08:33.167
    STEP: Wait for the deployment to be ready 03/23/23 20:08:33.189
    Mar 23 20:08:33.201: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Mar 23 20:08:35.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 8, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 03/23/23 20:08:37.224
    STEP: Verifying the service has paired with the endpoint 03/23/23 20:08:37.242
    Mar 23 20:08:38.242: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 03/23/23 20:08:38.246
    STEP: Creating a custom resource definition that should be denied by the webhook 03/23/23 20:08:38.278
    Mar 23 20:08:38.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:08:38.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5526" for this suite. 03/23/23 20:08:38.306
    STEP: Destroying namespace "webhook-5526-markers" for this suite. 03/23/23 20:08:38.315
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:38.475
Mar 23 20:08:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:08:38.477
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:38.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:38.501
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-2aef0d13-88b5-4569-b21b-25c21c22623a 03/23/23 20:08:38.505
STEP: Creating secret with name secret-projected-all-test-volume-fa3cb178-683e-4b6f-9f5d-27538e45c247 03/23/23 20:08:38.51
STEP: Creating a pod to test Check all projections for projected volume plugin 03/23/23 20:08:38.517
Mar 23 20:08:38.540: INFO: Waiting up to 5m0s for pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b" in namespace "projected-3571" to be "Succeeded or Failed"
Mar 23 20:08:38.556: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.365564ms
Mar 23 20:08:40.562: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021927019s
Mar 23 20:08:42.562: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Running", Reason="", readiness=true. Elapsed: 4.022231586s
Mar 23 20:08:44.563: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Running", Reason="", readiness=false. Elapsed: 6.022651152s
Mar 23 20:08:46.564: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024367016s
STEP: Saw pod success 03/23/23 20:08:46.564
Mar 23 20:08:46.565: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b" satisfied condition "Succeeded or Failed"
Mar 23 20:08:46.568: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b container projected-all-volume-test: <nil>
STEP: delete the pod 03/23/23 20:08:46.576
Mar 23 20:08:46.591: INFO: Waiting for pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b to disappear
Mar 23 20:08:46.594: INFO: Pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Mar 23 20:08:46.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3571" for this suite. 03/23/23 20:08:46.6
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":287,"skipped":5241,"failed":0}
------------------------------
• [SLOW TEST] [8.136 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:38.475
    Mar 23 20:08:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:08:38.477
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:38.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:38.501
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-2aef0d13-88b5-4569-b21b-25c21c22623a 03/23/23 20:08:38.505
    STEP: Creating secret with name secret-projected-all-test-volume-fa3cb178-683e-4b6f-9f5d-27538e45c247 03/23/23 20:08:38.51
    STEP: Creating a pod to test Check all projections for projected volume plugin 03/23/23 20:08:38.517
    Mar 23 20:08:38.540: INFO: Waiting up to 5m0s for pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b" in namespace "projected-3571" to be "Succeeded or Failed"
    Mar 23 20:08:38.556: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.365564ms
    Mar 23 20:08:40.562: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021927019s
    Mar 23 20:08:42.562: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Running", Reason="", readiness=true. Elapsed: 4.022231586s
    Mar 23 20:08:44.563: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Running", Reason="", readiness=false. Elapsed: 6.022651152s
    Mar 23 20:08:46.564: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024367016s
    STEP: Saw pod success 03/23/23 20:08:46.564
    Mar 23 20:08:46.565: INFO: Pod "projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b" satisfied condition "Succeeded or Failed"
    Mar 23 20:08:46.568: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b container projected-all-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:08:46.576
    Mar 23 20:08:46.591: INFO: Waiting for pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b to disappear
    Mar 23 20:08:46.594: INFO: Pod projected-volume-dbc7ae4c-de1d-41f3-931a-827bf4fd607b no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Mar 23 20:08:46.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3571" for this suite. 03/23/23 20:08:46.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:46.624
Mar 23 20:08:46.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 20:08:46.626
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:46.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:46.654
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Mar 23 20:08:46.673: INFO: Got root ca configmap in namespace "svcaccounts-4435"
Mar 23 20:08:46.685: INFO: Deleted root ca configmap in namespace "svcaccounts-4435"
STEP: waiting for a new root ca configmap created 03/23/23 20:08:47.185
Mar 23 20:08:47.189: INFO: Recreated root ca configmap in namespace "svcaccounts-4435"
Mar 23 20:08:47.193: INFO: Updated root ca configmap in namespace "svcaccounts-4435"
STEP: waiting for the root ca configmap reconciled 03/23/23 20:08:47.694
Mar 23 20:08:47.699: INFO: Reconciled root ca configmap in namespace "svcaccounts-4435"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 20:08:47.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4435" for this suite. 03/23/23 20:08:47.704
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":288,"skipped":5253,"failed":0}
------------------------------
• [1.090 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:46.624
    Mar 23 20:08:46.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 20:08:46.626
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:46.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:46.654
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Mar 23 20:08:46.673: INFO: Got root ca configmap in namespace "svcaccounts-4435"
    Mar 23 20:08:46.685: INFO: Deleted root ca configmap in namespace "svcaccounts-4435"
    STEP: waiting for a new root ca configmap created 03/23/23 20:08:47.185
    Mar 23 20:08:47.189: INFO: Recreated root ca configmap in namespace "svcaccounts-4435"
    Mar 23 20:08:47.193: INFO: Updated root ca configmap in namespace "svcaccounts-4435"
    STEP: waiting for the root ca configmap reconciled 03/23/23 20:08:47.694
    Mar 23 20:08:47.699: INFO: Reconciled root ca configmap in namespace "svcaccounts-4435"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 20:08:47.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4435" for this suite. 03/23/23 20:08:47.704
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:47.722
Mar 23 20:08:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 20:08:47.724
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:47.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:47.746
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Mar 23 20:08:47.761: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Mar 23 20:08:52.766: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 20:08:52.766
Mar 23 20:08:52.766: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/23/23 20:08:52.789
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 20:08:56.819: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4021  746ec5d4-f420-45e4-b1c5-e4562fc5e9de 39313 1 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047afd98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 20:08:52 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-03-23 20:08:54 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 20:08:56.823: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4021  109fd139-a241-41dc-9169-da83ee59ef12 39303 1 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 746ec5d4-f420-45e4-b1c5-e4562fc5e9de 0xc004214d37 0xc004214d38}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"746ec5d4-f420-45e4-b1c5-e4562fc5e9de\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004214de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:08:56.826: INFO: Pod "test-cleanup-deployment-69cb9c5497-b6d4f" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-b6d4f test-cleanup-deployment-69cb9c5497- deployment-4021  40754894-8b74-4c97-9869-af72fa0ba87d 39302 0 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 109fd139-a241-41dc-9169-da83ee59ef12 0xc00bac4127 0xc00bac4128}] [] [{kube-controller-manager Update v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"109fd139-a241-41dc-9169-da83ee59ef12\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w24wr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w24wr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.57,StartTime:2023-03-23 20:08:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:08:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1d6466fef81a9f7b7bcfa883f2438a3cdf552510d04a809f22b6480e6cef9983,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 20:08:56.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4021" for this suite. 03/23/23 20:08:56.83
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":289,"skipped":5255,"failed":0}
------------------------------
• [SLOW TEST] [9.119 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:47.722
    Mar 23 20:08:47.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 20:08:47.724
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:47.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:47.746
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Mar 23 20:08:47.761: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Mar 23 20:08:52.766: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 20:08:52.766
    Mar 23 20:08:52.766: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 03/23/23 20:08:52.789
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 20:08:56.819: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4021  746ec5d4-f420-45e4-b1c5-e4562fc5e9de 39313 1 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0047afd98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 20:08:52 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-03-23 20:08:54 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 23 20:08:56.823: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4021  109fd139-a241-41dc-9169-da83ee59ef12 39303 1 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 746ec5d4-f420-45e4-b1c5-e4562fc5e9de 0xc004214d37 0xc004214d38}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"746ec5d4-f420-45e4-b1c5-e4562fc5e9de\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004214de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 20:08:56.826: INFO: Pod "test-cleanup-deployment-69cb9c5497-b6d4f" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-b6d4f test-cleanup-deployment-69cb9c5497- deployment-4021  40754894-8b74-4c97-9869-af72fa0ba87d 39302 0 2023-03-23 20:08:52 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 109fd139-a241-41dc-9169-da83ee59ef12 0xc00bac4127 0xc00bac4128}] [] [{kube-controller-manager Update v1 2023-03-23 20:08:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"109fd139-a241-41dc-9169-da83ee59ef12\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:08:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w24wr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w24wr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:08:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.57,StartTime:2023-03-23 20:08:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:08:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1d6466fef81a9f7b7bcfa883f2438a3cdf552510d04a809f22b6480e6cef9983,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 20:08:56.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4021" for this suite. 03/23/23 20:08:56.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:08:56.849
Mar 23 20:08:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 20:08:56.85
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:56.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:56.872
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Mar 23 20:08:56.906: INFO: Pod name sample-pod: Found 0 pods out of 1
Mar 23 20:09:01.928: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 20:09:01.928
STEP: Scaling up "test-rs" replicaset  03/23/23 20:09:01.928
Mar 23 20:09:01.949: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 03/23/23 20:09:01.949
W0323 20:09:01.966085      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Mar 23 20:09:01.968: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
Mar 23 20:09:02.018: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
Mar 23 20:09:02.075: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
Mar 23 20:09:02.106: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
Mar 23 20:09:03.811: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 2, AvailableReplicas 2
Mar 23 20:09:03.971: INFO: observed Replicaset test-rs in namespace replicaset-3974 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 20:09:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3974" for this suite. 03/23/23 20:09:03.984
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":290,"skipped":5294,"failed":0}
------------------------------
• [SLOW TEST] [7.143 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:08:56.849
    Mar 23 20:08:56.849: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 20:08:56.85
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:08:56.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:08:56.872
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Mar 23 20:08:56.906: INFO: Pod name sample-pod: Found 0 pods out of 1
    Mar 23 20:09:01.928: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 20:09:01.928
    STEP: Scaling up "test-rs" replicaset  03/23/23 20:09:01.928
    Mar 23 20:09:01.949: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 03/23/23 20:09:01.949
    W0323 20:09:01.966085      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Mar 23 20:09:01.968: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
    Mar 23 20:09:02.018: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
    Mar 23 20:09:02.075: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
    Mar 23 20:09:02.106: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 1, AvailableReplicas 1
    Mar 23 20:09:03.811: INFO: observed ReplicaSet test-rs in namespace replicaset-3974 with ReadyReplicas 2, AvailableReplicas 2
    Mar 23 20:09:03.971: INFO: observed Replicaset test-rs in namespace replicaset-3974 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 20:09:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3974" for this suite. 03/23/23 20:09:03.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:04.018
Mar 23 20:09:04.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename ephemeral-containers-test 03/23/23 20:09:04.023
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:04.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:04.076
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 03/23/23 20:09:04.082
Mar 23 20:09:04.105: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5814" to be "running and ready"
Mar 23 20:09:04.113: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803982ms
Mar 23 20:09:04.113: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:09:06.118: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01217625s
Mar 23 20:09:06.118: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Mar 23 20:09:06.118: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 03/23/23 20:09:06.12
Mar 23 20:09:06.139: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5814" to be "container debugger running"
Mar 23 20:09:06.142: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.081994ms
Mar 23 20:09:08.147: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008561259s
Mar 23 20:09:10.146: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007656933s
Mar 23 20:09:10.147: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 03/23/23 20:09:10.147
Mar 23 20:09:10.147: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5814 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:09:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:09:10.148: INFO: ExecWithOptions: Clientset creation
Mar 23 20:09:10.148: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/ephemeral-containers-test-5814/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Mar 23 20:09:10.394: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 20:09:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-5814" for this suite. 03/23/23 20:09:10.406
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":291,"skipped":5338,"failed":0}
------------------------------
• [SLOW TEST] [6.394 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:04.018
    Mar 23 20:09:04.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename ephemeral-containers-test 03/23/23 20:09:04.023
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:04.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:04.076
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 03/23/23 20:09:04.082
    Mar 23 20:09:04.105: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5814" to be "running and ready"
    Mar 23 20:09:04.113: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.803982ms
    Mar 23 20:09:04.113: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:09:06.118: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01217625s
    Mar 23 20:09:06.118: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Mar 23 20:09:06.118: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 03/23/23 20:09:06.12
    Mar 23 20:09:06.139: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5814" to be "container debugger running"
    Mar 23 20:09:06.142: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.081994ms
    Mar 23 20:09:08.147: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008561259s
    Mar 23 20:09:10.146: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007656933s
    Mar 23 20:09:10.147: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 03/23/23 20:09:10.147
    Mar 23 20:09:10.147: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5814 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:09:10.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:09:10.148: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:09:10.148: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/ephemeral-containers-test-5814/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Mar 23 20:09:10.394: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 20:09:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-5814" for this suite. 03/23/23 20:09:10.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:10.414
Mar 23 20:09:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 20:09:10.418
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:10.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:10.448
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 03/23/23 20:09:10.451
Mar 23 20:09:10.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: rename a version 03/23/23 20:09:19.594
STEP: check the new version name is served 03/23/23 20:09:19.627
STEP: check the old version name is removed 03/23/23 20:09:22.525
STEP: check the other version is not changed 03/23/23 20:09:24.338
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:09:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7651" for this suite. 03/23/23 20:09:31.241
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":292,"skipped":5361,"failed":0}
------------------------------
• [SLOW TEST] [20.839 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:10.414
    Mar 23 20:09:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 20:09:10.418
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:10.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:10.448
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 03/23/23 20:09:10.451
    Mar 23 20:09:10.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: rename a version 03/23/23 20:09:19.594
    STEP: check the new version name is served 03/23/23 20:09:19.627
    STEP: check the old version name is removed 03/23/23 20:09:22.525
    STEP: check the other version is not changed 03/23/23 20:09:24.338
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:09:31.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7651" for this suite. 03/23/23 20:09:31.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:31.262
Mar 23 20:09:31.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename proxy 03/23/23 20:09:31.263
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:31.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:31.308
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 03/23/23 20:09:31.359
STEP: creating replication controller proxy-service-7wzmk in namespace proxy-2520 03/23/23 20:09:31.361
I0323 20:09:31.381480      19 runners.go:193] Created replication controller with name: proxy-service-7wzmk, namespace: proxy-2520, replica count: 1
I0323 20:09:32.438167      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 20:09:33.438386      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 20:09:34.438602      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 20:09:35.438949      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 20:09:36.439857      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 20:09:36.445: INFO: setup took 5.133316002s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/23/23 20:09:36.445
Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 17.82416ms)
Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.159762ms)
Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 17.554561ms)
Mar 23 20:09:36.473: INFO: (0) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 27.152539ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 27.624438ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 27.520838ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 27.467538ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 28.159436ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 28.846535ms)
Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 28.074137ms)
Mar 23 20:09:36.475: INFO: (0) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 28.952635ms)
Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 34.986721ms)
Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 35.382221ms)
Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 34.827822ms)
Mar 23 20:09:36.482: INFO: (0) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 36.261219ms)
Mar 23 20:09:36.482: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 36.676018ms)
Mar 23 20:09:36.499: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.771565ms)
Mar 23 20:09:36.499: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 15.730065ms)
Mar 23 20:09:36.504: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 20.715853ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.451352ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 22.18695ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.871451ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 21.705952ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 21.834151ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.803151ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 22.019151ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 21.98025ms)
Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 22.21015ms)
Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.750147ms)
Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.259145ms)
Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 24.045746ms)
Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.294746ms)
Mar 23 20:09:36.519: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 10.111578ms)
Mar 23 20:09:36.528: INFO: (2) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.284257ms)
Mar 23 20:09:36.530: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.293754ms)
Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 21.894751ms)
Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.28735ms)
Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 22.61885ms)
Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 22.58815ms)
Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.368447ms)
Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 27.496738ms)
Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 27.491738ms)
Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 28.701836ms)
Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 28.318537ms)
Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 28.102037ms)
Mar 23 20:09:36.538: INFO: (2) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 29.264734ms)
Mar 23 20:09:36.539: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 30.428632ms)
Mar 23 20:09:36.542: INFO: (2) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 33.072526ms)
Mar 23 20:09:36.563: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.663456ms)
Mar 23 20:09:36.563: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.554854ms)
Mar 23 20:09:36.564: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.859653ms)
Mar 23 20:09:36.566: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 23.798547ms)
Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 23.777347ms)
Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 23.737447ms)
Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 24.096446ms)
Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.851146ms)
Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 24.077546ms)
Mar 23 20:09:36.568: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 25.191144ms)
Mar 23 20:09:36.568: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 25.347843ms)
Mar 23 20:09:36.574: INFO: (3) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 30.910031ms)
Mar 23 20:09:36.576: INFO: (3) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 33.050826ms)
Mar 23 20:09:36.576: INFO: (3) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 33.012126ms)
Mar 23 20:09:36.577: INFO: (3) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 34.485822ms)
Mar 23 20:09:36.578: INFO: (3) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 34.853222ms)
Mar 23 20:09:36.595: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.486765ms)
Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.106063ms)
Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 16.513163ms)
Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.852762ms)
Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 17.110562ms)
Mar 23 20:09:36.598: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 19.844755ms)
Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.995855ms)
Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.807456ms)
Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.687156ms)
Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.235655ms)
Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.220454ms)
Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.744952ms)
Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.679352ms)
Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.96745ms)
Mar 23 20:09:36.602: INFO: (4) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 22.707849ms)
Mar 23 20:09:36.603: INFO: (4) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.148946ms)
Mar 23 20:09:36.613: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 9.371379ms)
Mar 23 20:09:36.623: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 18.172359ms)
Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.825457ms)
Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.684056ms)
Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.380856ms)
Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.623856ms)
Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 19.550856ms)
Mar 23 20:09:36.625: INFO: (5) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.858356ms)
Mar 23 20:09:36.626: INFO: (5) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.094853ms)
Mar 23 20:09:36.627: INFO: (5) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.948448ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 22.550649ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.751749ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 23.367648ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 24.028246ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 24.148046ms)
Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.618346ms)
Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 16.430963ms)
Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.085964ms)
Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.747662ms)
Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 17.091262ms)
Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 17.150262ms)
Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.42356ms)
Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 17.515561ms)
Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.016859ms)
Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 18.398758ms)
Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.031757ms)
Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.133057ms)
Mar 23 20:09:36.652: INFO: (6) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 22.444049ms)
Mar 23 20:09:36.653: INFO: (6) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.875749ms)
Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.737047ms)
Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.649447ms)
Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.882346ms)
Mar 23 20:09:36.667: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 12.001473ms)
Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.069168ms)
Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 14.793267ms)
Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 14.737767ms)
Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 15.116966ms)
Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.508565ms)
Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.823664ms)
Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 15.779664ms)
Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.118363ms)
Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.260263ms)
Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 16.316963ms)
Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 16.298964ms)
Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 16.846762ms)
Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 20.095354ms)
Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 20.269755ms)
Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.767353ms)
Mar 23 20:09:36.683: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 7.408584ms)
Mar 23 20:09:36.683: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 7.193584ms)
Mar 23 20:09:36.685: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 8.82878ms)
Mar 23 20:09:36.690: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.641869ms)
Mar 23 20:09:36.695: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.220657ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.564356ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.411254ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 19.672556ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.393257ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 20.627554ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 19.668256ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.790755ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.354054ms)
Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 20.518954ms)
Mar 23 20:09:36.697: INFO: (8) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.317652ms)
Mar 23 20:09:36.697: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.058752ms)
Mar 23 20:09:36.706: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 7.948682ms)
Mar 23 20:09:36.706: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 8.57238ms)
Mar 23 20:09:36.707: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 8.91588ms)
Mar 23 20:09:36.710: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 11.761574ms)
Mar 23 20:09:36.710: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 11.731874ms)
Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 12.709371ms)
Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 12.713371ms)
Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 13.12497ms)
Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 13.025671ms)
Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 12.970371ms)
Mar 23 20:09:36.714: INFO: (9) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 16.314763ms)
Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 18.330559ms)
Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.469456ms)
Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.140357ms)
Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.440556ms)
Mar 23 20:09:36.718: INFO: (9) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 19.833155ms)
Mar 23 20:09:36.731: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 12.466072ms)
Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.171771ms)
Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 14.057568ms)
Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.743969ms)
Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 12.992171ms)
Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.612269ms)
Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 13.669969ms)
Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 13.36687ms)
Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 14.011668ms)
Mar 23 20:09:36.735: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.845664ms)
Mar 23 20:09:36.735: INFO: (10) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 16.657763ms)
Mar 23 20:09:36.737: INFO: (10) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 17.94316ms)
Mar 23 20:09:36.740: INFO: (10) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 20.905353ms)
Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.628852ms)
Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.395151ms)
Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.546651ms)
Mar 23 20:09:36.751: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 9.398179ms)
Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 13.189371ms)
Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.64257ms)
Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 13.814469ms)
Mar 23 20:09:36.756: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.450868ms)
Mar 23 20:09:36.756: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 14.224668ms)
Mar 23 20:09:36.759: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.354061ms)
Mar 23 20:09:36.759: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.528061ms)
Mar 23 20:09:36.760: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.92306ms)
Mar 23 20:09:36.760: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 18.411059ms)
Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.876748ms)
Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.329947ms)
Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.668546ms)
Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.612246ms)
Mar 23 20:09:36.766: INFO: (11) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.861446ms)
Mar 23 20:09:36.766: INFO: (11) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.135946ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.14837ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 13.15397ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.653169ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.110168ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.925168ms)
Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 13.974469ms)
Mar 23 20:09:36.781: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 14.455267ms)
Mar 23 20:09:36.782: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 15.550965ms)
Mar 23 20:09:36.783: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.505563ms)
Mar 23 20:09:36.784: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.219662ms)
Mar 23 20:09:36.786: INFO: (12) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.134455ms)
Mar 23 20:09:36.790: INFO: (12) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.087548ms)
Mar 23 20:09:36.790: INFO: (12) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.908446ms)
Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 24.265546ms)
Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.866044ms)
Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.950944ms)
Mar 23 20:09:36.808: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 15.911164ms)
Mar 23 20:09:36.808: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 16.332064ms)
Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.039662ms)
Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.383661ms)
Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.73296ms)
Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 18.12056ms)
Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.78326ms)
Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.240759ms)
Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 18.967257ms)
Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 19.662056ms)
Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.782655ms)
Mar 23 20:09:36.813: INFO: (13) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.499752ms)
Mar 23 20:09:36.814: INFO: (13) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.09465ms)
Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 22.707049ms)
Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.481647ms)
Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.894046ms)
Mar 23 20:09:36.826: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 10.014778ms)
Mar 23 20:09:36.827: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 10.227077ms)
Mar 23 20:09:36.827: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 10.375677ms)
Mar 23 20:09:36.828: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 11.128775ms)
Mar 23 20:09:36.829: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 12.146872ms)
Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 16.848562ms)
Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 15.762265ms)
Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 15.344366ms)
Mar 23 20:09:36.835: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.813659ms)
Mar 23 20:09:36.836: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 18.12116ms)
Mar 23 20:09:36.838: INFO: (14) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.971455ms)
Mar 23 20:09:36.839: INFO: (14) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.172553ms)
Mar 23 20:09:36.839: INFO: (14) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.668351ms)
Mar 23 20:09:36.840: INFO: (14) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 22.557449ms)
Mar 23 20:09:36.840: INFO: (14) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.007248ms)
Mar 23 20:09:36.841: INFO: (14) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.510647ms)
Mar 23 20:09:36.849: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 8.061081ms)
Mar 23 20:09:36.852: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 10.323077ms)
Mar 23 20:09:36.853: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 11.860673ms)
Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 22.688248ms)
Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 23.559547ms)
Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 23.074748ms)
Mar 23 20:09:36.866: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 23.969246ms)
Mar 23 20:09:36.867: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 25.619242ms)
Mar 23 20:09:36.868: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 26.260041ms)
Mar 23 20:09:36.869: INFO: (15) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 27.536638ms)
Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 29.759033ms)
Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 30.574331ms)
Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 30.360931ms)
Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 30.502131ms)
Mar 23 20:09:36.873: INFO: (15) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 31.504429ms)
Mar 23 20:09:36.873: INFO: (15) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 31.30823ms)
Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.044464ms)
Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 16.077064ms)
Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.291063ms)
Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.701963ms)
Mar 23 20:09:36.891: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 16.468063ms)
Mar 23 20:09:36.891: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.419563ms)
Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 18.904558ms)
Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.756456ms)
Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.612756ms)
Mar 23 20:09:36.894: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.137554ms)
Mar 23 20:09:36.894: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.242454ms)
Mar 23 20:09:36.895: INFO: (16) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 21.034753ms)
Mar 23 20:09:36.897: INFO: (16) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.186748ms)
Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.548947ms)
Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.992446ms)
Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 24.032746ms)
Mar 23 20:09:36.908: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 9.742678ms)
Mar 23 20:09:36.912: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.102171ms)
Mar 23 20:09:36.913: INFO: (17) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 14.029369ms)
Mar 23 20:09:36.915: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.185766ms)
Mar 23 20:09:36.915: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 15.233265ms)
Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 16.771962ms)
Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.565463ms)
Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.255961ms)
Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.358457ms)
Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 19.214957ms)
Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.764256ms)
Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 20.016955ms)
Mar 23 20:09:36.922: INFO: (17) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.942449ms)
Mar 23 20:09:36.922: INFO: (17) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.242048ms)
Mar 23 20:09:36.923: INFO: (17) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.207948ms)
Mar 23 20:09:36.923: INFO: (17) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.971446ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 19.595656ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.939755ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 20.059255ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 20.251755ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.413254ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.463054ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.539054ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 21.203153ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 20.621054ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.999453ms)
Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 21.003153ms)
Mar 23 20:09:36.948: INFO: (18) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.056446ms)
Mar 23 20:09:36.948: INFO: (18) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 24.747245ms)
Mar 23 20:09:36.949: INFO: (18) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.733444ms)
Mar 23 20:09:36.949: INFO: (18) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 25.634643ms)
Mar 23 20:09:36.950: INFO: (18) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 26.040741ms)
Mar 23 20:09:36.958: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 7.858682ms)
Mar 23 20:09:36.968: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.151161ms)
Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 18.973457ms)
Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.075357ms)
Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.356956ms)
Mar 23 20:09:36.970: INFO: (19) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.202657ms)
Mar 23 20:09:36.972: INFO: (19) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 20.880153ms)
Mar 23 20:09:36.972: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.79425ms)
Mar 23 20:09:36.974: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 23.732046ms)
Mar 23 20:09:36.974: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 23.951646ms)
Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 24.064746ms)
Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 24.164246ms)
Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 25.284543ms)
Mar 23 20:09:36.977: INFO: (19) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 26.239441ms)
Mar 23 20:09:36.978: INFO: (19) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 27.278139ms)
Mar 23 20:09:36.978: INFO: (19) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 27.460938ms)
STEP: deleting ReplicationController proxy-service-7wzmk in namespace proxy-2520, will wait for the garbage collector to delete the pods 03/23/23 20:09:36.978
Mar 23 20:09:37.046: INFO: Deleting ReplicationController proxy-service-7wzmk took: 14.582068ms
Mar 23 20:09:37.147: INFO: Terminating ReplicationController proxy-service-7wzmk pods took: 101.066972ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 23 20:09:40.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2520" for this suite. 03/23/23 20:09:40.977
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":293,"skipped":5420,"failed":0}
------------------------------
• [SLOW TEST] [9.774 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:31.262
    Mar 23 20:09:31.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename proxy 03/23/23 20:09:31.263
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:31.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:31.308
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 03/23/23 20:09:31.359
    STEP: creating replication controller proxy-service-7wzmk in namespace proxy-2520 03/23/23 20:09:31.361
    I0323 20:09:31.381480      19 runners.go:193] Created replication controller with name: proxy-service-7wzmk, namespace: proxy-2520, replica count: 1
    I0323 20:09:32.438167      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 20:09:33.438386      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 20:09:34.438602      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 20:09:35.438949      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 20:09:36.439857      19 runners.go:193] proxy-service-7wzmk Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 20:09:36.445: INFO: setup took 5.133316002s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 03/23/23 20:09:36.445
    Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 17.82416ms)
    Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.159762ms)
    Mar 23 20:09:36.463: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 17.554561ms)
    Mar 23 20:09:36.473: INFO: (0) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 27.152539ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 27.624438ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 27.520838ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 27.467538ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 28.159436ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 28.846535ms)
    Mar 23 20:09:36.474: INFO: (0) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 28.074137ms)
    Mar 23 20:09:36.475: INFO: (0) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 28.952635ms)
    Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 34.986721ms)
    Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 35.382221ms)
    Mar 23 20:09:36.481: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 34.827822ms)
    Mar 23 20:09:36.482: INFO: (0) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 36.261219ms)
    Mar 23 20:09:36.482: INFO: (0) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 36.676018ms)
    Mar 23 20:09:36.499: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.771565ms)
    Mar 23 20:09:36.499: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 15.730065ms)
    Mar 23 20:09:36.504: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 20.715853ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.451352ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 22.18695ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.871451ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 21.705952ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 21.834151ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.803151ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 22.019151ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 21.98025ms)
    Mar 23 20:09:36.505: INFO: (1) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 22.21015ms)
    Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.750147ms)
    Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.259145ms)
    Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 24.045746ms)
    Mar 23 20:09:36.507: INFO: (1) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.294746ms)
    Mar 23 20:09:36.519: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 10.111578ms)
    Mar 23 20:09:36.528: INFO: (2) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.284257ms)
    Mar 23 20:09:36.530: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.293754ms)
    Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 21.894751ms)
    Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.28735ms)
    Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 22.61885ms)
    Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 22.58815ms)
    Mar 23 20:09:36.532: INFO: (2) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.368447ms)
    Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 27.496738ms)
    Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 27.491738ms)
    Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 28.701836ms)
    Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 28.318537ms)
    Mar 23 20:09:36.537: INFO: (2) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 28.102037ms)
    Mar 23 20:09:36.538: INFO: (2) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 29.264734ms)
    Mar 23 20:09:36.539: INFO: (2) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 30.428632ms)
    Mar 23 20:09:36.542: INFO: (2) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 33.072526ms)
    Mar 23 20:09:36.563: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.663456ms)
    Mar 23 20:09:36.563: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.554854ms)
    Mar 23 20:09:36.564: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.859653ms)
    Mar 23 20:09:36.566: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 23.798547ms)
    Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 23.777347ms)
    Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 23.737447ms)
    Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 24.096446ms)
    Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.851146ms)
    Mar 23 20:09:36.567: INFO: (3) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 24.077546ms)
    Mar 23 20:09:36.568: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 25.191144ms)
    Mar 23 20:09:36.568: INFO: (3) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 25.347843ms)
    Mar 23 20:09:36.574: INFO: (3) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 30.910031ms)
    Mar 23 20:09:36.576: INFO: (3) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 33.050826ms)
    Mar 23 20:09:36.576: INFO: (3) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 33.012126ms)
    Mar 23 20:09:36.577: INFO: (3) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 34.485822ms)
    Mar 23 20:09:36.578: INFO: (3) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 34.853222ms)
    Mar 23 20:09:36.595: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.486765ms)
    Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.106063ms)
    Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 16.513163ms)
    Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.852762ms)
    Mar 23 20:09:36.596: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 17.110562ms)
    Mar 23 20:09:36.598: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 19.844755ms)
    Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.995855ms)
    Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.807456ms)
    Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.687156ms)
    Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.235655ms)
    Mar 23 20:09:36.599: INFO: (4) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.220454ms)
    Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.744952ms)
    Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.679352ms)
    Mar 23 20:09:36.600: INFO: (4) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.96745ms)
    Mar 23 20:09:36.602: INFO: (4) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 22.707849ms)
    Mar 23 20:09:36.603: INFO: (4) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.148946ms)
    Mar 23 20:09:36.613: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 9.371379ms)
    Mar 23 20:09:36.623: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 18.172359ms)
    Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.825457ms)
    Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.684056ms)
    Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.380856ms)
    Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.623856ms)
    Mar 23 20:09:36.624: INFO: (5) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 19.550856ms)
    Mar 23 20:09:36.625: INFO: (5) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.858356ms)
    Mar 23 20:09:36.626: INFO: (5) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.094853ms)
    Mar 23 20:09:36.627: INFO: (5) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.948448ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 22.550649ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.751749ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 23.367648ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 24.028246ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 24.148046ms)
    Mar 23 20:09:36.628: INFO: (5) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.618346ms)
    Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 16.430963ms)
    Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.085964ms)
    Mar 23 20:09:36.646: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.747662ms)
    Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 17.091262ms)
    Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 17.150262ms)
    Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.42356ms)
    Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 17.515561ms)
    Mar 23 20:09:36.647: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.016859ms)
    Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 18.398758ms)
    Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.031757ms)
    Mar 23 20:09:36.648: INFO: (6) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.133057ms)
    Mar 23 20:09:36.652: INFO: (6) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 22.444049ms)
    Mar 23 20:09:36.653: INFO: (6) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.875749ms)
    Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.737047ms)
    Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.649447ms)
    Mar 23 20:09:36.654: INFO: (6) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.882346ms)
    Mar 23 20:09:36.667: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 12.001473ms)
    Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.069168ms)
    Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 14.793267ms)
    Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 14.737767ms)
    Mar 23 20:09:36.669: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 15.116966ms)
    Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.508565ms)
    Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 15.823664ms)
    Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 15.779664ms)
    Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.118363ms)
    Mar 23 20:09:36.670: INFO: (7) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.260263ms)
    Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 16.316963ms)
    Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 16.298964ms)
    Mar 23 20:09:36.671: INFO: (7) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 16.846762ms)
    Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 20.095354ms)
    Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 20.269755ms)
    Mar 23 20:09:36.675: INFO: (7) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.767353ms)
    Mar 23 20:09:36.683: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 7.408584ms)
    Mar 23 20:09:36.683: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 7.193584ms)
    Mar 23 20:09:36.685: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 8.82878ms)
    Mar 23 20:09:36.690: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.641869ms)
    Mar 23 20:09:36.695: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.220657ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.564356ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.411254ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 19.672556ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.393257ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 20.627554ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 19.668256ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.790755ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 20.354054ms)
    Mar 23 20:09:36.696: INFO: (8) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 20.518954ms)
    Mar 23 20:09:36.697: INFO: (8) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.317652ms)
    Mar 23 20:09:36.697: INFO: (8) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.058752ms)
    Mar 23 20:09:36.706: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 7.948682ms)
    Mar 23 20:09:36.706: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 8.57238ms)
    Mar 23 20:09:36.707: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 8.91588ms)
    Mar 23 20:09:36.710: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 11.761574ms)
    Mar 23 20:09:36.710: INFO: (9) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 11.731874ms)
    Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 12.709371ms)
    Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 12.713371ms)
    Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 13.12497ms)
    Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 13.025671ms)
    Mar 23 20:09:36.711: INFO: (9) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 12.970371ms)
    Mar 23 20:09:36.714: INFO: (9) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 16.314763ms)
    Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 18.330559ms)
    Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.469456ms)
    Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 19.140357ms)
    Mar 23 20:09:36.717: INFO: (9) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.440556ms)
    Mar 23 20:09:36.718: INFO: (9) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 19.833155ms)
    Mar 23 20:09:36.731: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 12.466072ms)
    Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.171771ms)
    Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 14.057568ms)
    Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.743969ms)
    Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 12.992171ms)
    Mar 23 20:09:36.732: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.612269ms)
    Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 13.669969ms)
    Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 13.36687ms)
    Mar 23 20:09:36.733: INFO: (10) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 14.011668ms)
    Mar 23 20:09:36.735: INFO: (10) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.845664ms)
    Mar 23 20:09:36.735: INFO: (10) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 16.657763ms)
    Mar 23 20:09:36.737: INFO: (10) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 17.94316ms)
    Mar 23 20:09:36.740: INFO: (10) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 20.905353ms)
    Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.628852ms)
    Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.395151ms)
    Mar 23 20:09:36.741: INFO: (10) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.546651ms)
    Mar 23 20:09:36.751: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 9.398179ms)
    Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 13.189371ms)
    Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.64257ms)
    Mar 23 20:09:36.755: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 13.814469ms)
    Mar 23 20:09:36.756: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.450868ms)
    Mar 23 20:09:36.756: INFO: (11) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 14.224668ms)
    Mar 23 20:09:36.759: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.354061ms)
    Mar 23 20:09:36.759: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.528061ms)
    Mar 23 20:09:36.760: INFO: (11) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.92306ms)
    Mar 23 20:09:36.760: INFO: (11) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 18.411059ms)
    Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.876748ms)
    Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.329947ms)
    Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.668546ms)
    Mar 23 20:09:36.765: INFO: (11) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.612246ms)
    Mar 23 20:09:36.766: INFO: (11) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.861446ms)
    Mar 23 20:09:36.766: INFO: (11) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.135946ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.14837ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 13.15397ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.653169ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 14.110168ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 13.925168ms)
    Mar 23 20:09:36.780: INFO: (12) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 13.974469ms)
    Mar 23 20:09:36.781: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 14.455267ms)
    Mar 23 20:09:36.782: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 15.550965ms)
    Mar 23 20:09:36.783: INFO: (12) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.505563ms)
    Mar 23 20:09:36.784: INFO: (12) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.219662ms)
    Mar 23 20:09:36.786: INFO: (12) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 20.134455ms)
    Mar 23 20:09:36.790: INFO: (12) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.087548ms)
    Mar 23 20:09:36.790: INFO: (12) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 23.908446ms)
    Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 24.265546ms)
    Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.866044ms)
    Mar 23 20:09:36.791: INFO: (12) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.950944ms)
    Mar 23 20:09:36.808: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 15.911164ms)
    Mar 23 20:09:36.808: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 16.332064ms)
    Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 17.039662ms)
    Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.383661ms)
    Mar 23 20:09:36.809: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.73296ms)
    Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 18.12056ms)
    Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 17.78326ms)
    Mar 23 20:09:36.810: INFO: (13) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 18.240759ms)
    Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 18.967257ms)
    Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 19.662056ms)
    Mar 23 20:09:36.811: INFO: (13) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.782655ms)
    Mar 23 20:09:36.813: INFO: (13) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 21.499752ms)
    Mar 23 20:09:36.814: INFO: (13) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 22.09465ms)
    Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 22.707049ms)
    Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.481647ms)
    Mar 23 20:09:36.815: INFO: (13) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.894046ms)
    Mar 23 20:09:36.826: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 10.014778ms)
    Mar 23 20:09:36.827: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 10.227077ms)
    Mar 23 20:09:36.827: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 10.375677ms)
    Mar 23 20:09:36.828: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 11.128775ms)
    Mar 23 20:09:36.829: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 12.146872ms)
    Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 16.848562ms)
    Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 15.762265ms)
    Mar 23 20:09:36.833: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 15.344366ms)
    Mar 23 20:09:36.835: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 17.813659ms)
    Mar 23 20:09:36.836: INFO: (14) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 18.12116ms)
    Mar 23 20:09:36.838: INFO: (14) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 19.971455ms)
    Mar 23 20:09:36.839: INFO: (14) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 21.172553ms)
    Mar 23 20:09:36.839: INFO: (14) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 21.668351ms)
    Mar 23 20:09:36.840: INFO: (14) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 22.557449ms)
    Mar 23 20:09:36.840: INFO: (14) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.007248ms)
    Mar 23 20:09:36.841: INFO: (14) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.510647ms)
    Mar 23 20:09:36.849: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 8.061081ms)
    Mar 23 20:09:36.852: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 10.323077ms)
    Mar 23 20:09:36.853: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 11.860673ms)
    Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 22.688248ms)
    Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 23.559547ms)
    Mar 23 20:09:36.865: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 23.074748ms)
    Mar 23 20:09:36.866: INFO: (15) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 23.969246ms)
    Mar 23 20:09:36.867: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 25.619242ms)
    Mar 23 20:09:36.868: INFO: (15) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 26.260041ms)
    Mar 23 20:09:36.869: INFO: (15) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 27.536638ms)
    Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 29.759033ms)
    Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 30.574331ms)
    Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 30.360931ms)
    Mar 23 20:09:36.872: INFO: (15) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 30.502131ms)
    Mar 23 20:09:36.873: INFO: (15) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 31.504429ms)
    Mar 23 20:09:36.873: INFO: (15) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 31.30823ms)
    Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.044464ms)
    Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 16.077064ms)
    Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.291063ms)
    Mar 23 20:09:36.890: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 16.701963ms)
    Mar 23 20:09:36.891: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 16.468063ms)
    Mar 23 20:09:36.891: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 16.419563ms)
    Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 18.904558ms)
    Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.756456ms)
    Mar 23 20:09:36.893: INFO: (16) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.612756ms)
    Mar 23 20:09:36.894: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.137554ms)
    Mar 23 20:09:36.894: INFO: (16) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.242454ms)
    Mar 23 20:09:36.895: INFO: (16) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 21.034753ms)
    Mar 23 20:09:36.897: INFO: (16) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 23.186748ms)
    Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 23.548947ms)
    Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.992446ms)
    Mar 23 20:09:36.898: INFO: (16) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 24.032746ms)
    Mar 23 20:09:36.908: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 9.742678ms)
    Mar 23 20:09:36.912: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 13.102171ms)
    Mar 23 20:09:36.913: INFO: (17) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 14.029369ms)
    Mar 23 20:09:36.915: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 15.185766ms)
    Mar 23 20:09:36.915: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 15.233265ms)
    Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 16.771962ms)
    Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 16.565463ms)
    Mar 23 20:09:36.916: INFO: (17) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.255961ms)
    Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.358457ms)
    Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 19.214957ms)
    Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.764256ms)
    Mar 23 20:09:36.919: INFO: (17) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 20.016955ms)
    Mar 23 20:09:36.922: INFO: (17) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 22.942449ms)
    Mar 23 20:09:36.922: INFO: (17) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 23.242048ms)
    Mar 23 20:09:36.923: INFO: (17) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 23.207948ms)
    Mar 23 20:09:36.923: INFO: (17) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 23.971446ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 19.595656ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 19.939755ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 20.059255ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 20.251755ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.413254ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 20.463054ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 20.539054ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 21.203153ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 20.621054ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 20.999453ms)
    Mar 23 20:09:36.944: INFO: (18) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 21.003153ms)
    Mar 23 20:09:36.948: INFO: (18) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 24.056446ms)
    Mar 23 20:09:36.948: INFO: (18) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 24.747245ms)
    Mar 23 20:09:36.949: INFO: (18) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 24.733444ms)
    Mar 23 20:09:36.949: INFO: (18) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 25.634643ms)
    Mar 23 20:09:36.950: INFO: (18) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 26.040741ms)
    Mar 23 20:09:36.958: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">... (200; 7.858682ms)
    Mar 23 20:09:36.968: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4/proxy/rewriteme">test</a> (200; 17.151161ms)
    Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 18.973457ms)
    Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.075357ms)
    Mar 23 20:09:36.969: INFO: (19) /api/v1/namespaces/proxy-2520/pods/http:proxy-service-7wzmk-5lhw4:160/proxy/: foo (200; 19.356956ms)
    Mar 23 20:09:36.970: INFO: (19) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname2/proxy/: bar (200; 19.202657ms)
    Mar 23 20:09:36.972: INFO: (19) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname2/proxy/: tls qux (200; 20.880153ms)
    Mar 23 20:09:36.972: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:462/proxy/: tls qux (200; 21.79425ms)
    Mar 23 20:09:36.974: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:460/proxy/: tls baz (200; 23.732046ms)
    Mar 23 20:09:36.974: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:1080/proxy/rewriteme">test<... (200; 23.951646ms)
    Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/pods/proxy-service-7wzmk-5lhw4:162/proxy/: bar (200; 24.064746ms)
    Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname2/proxy/: bar (200; 24.164246ms)
    Mar 23 20:09:36.975: INFO: (19) /api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/: <a href="/api/v1/namespaces/proxy-2520/pods/https:proxy-service-7wzmk-5lhw4:443/proxy/tlsrewritem... (200; 25.284543ms)
    Mar 23 20:09:36.977: INFO: (19) /api/v1/namespaces/proxy-2520/services/proxy-service-7wzmk:portname1/proxy/: foo (200; 26.239441ms)
    Mar 23 20:09:36.978: INFO: (19) /api/v1/namespaces/proxy-2520/services/https:proxy-service-7wzmk:tlsportname1/proxy/: tls baz (200; 27.278139ms)
    Mar 23 20:09:36.978: INFO: (19) /api/v1/namespaces/proxy-2520/services/http:proxy-service-7wzmk:portname1/proxy/: foo (200; 27.460938ms)
    STEP: deleting ReplicationController proxy-service-7wzmk in namespace proxy-2520, will wait for the garbage collector to delete the pods 03/23/23 20:09:36.978
    Mar 23 20:09:37.046: INFO: Deleting ReplicationController proxy-service-7wzmk took: 14.582068ms
    Mar 23 20:09:37.147: INFO: Terminating ReplicationController proxy-service-7wzmk pods took: 101.066972ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 23 20:09:40.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-2520" for this suite. 03/23/23 20:09:40.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:41.037
Mar 23 20:09:41.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename endpointslice 03/23/23 20:09:41.038
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:41.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:41.117
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Mar 23 20:09:43.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9991" for this suite. 03/23/23 20:09:43.23
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":294,"skipped":5425,"failed":0}
------------------------------
• [2.219 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:41.037
    Mar 23 20:09:41.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename endpointslice 03/23/23 20:09:41.038
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:41.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:41.117
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Mar 23 20:09:43.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9991" for this suite. 03/23/23 20:09:43.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:43.264
Mar 23 20:09:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 20:09:43.265
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:43.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:43.288
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-41220eb3-0b69-4817-b1be-c269f0f6a826 03/23/23 20:09:43.291
STEP: Creating a pod to test consume secrets 03/23/23 20:09:43.299
Mar 23 20:09:43.308: INFO: Waiting up to 5m0s for pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7" in namespace "secrets-7949" to be "Succeeded or Failed"
Mar 23 20:09:43.312: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.425992ms
Mar 23 20:09:45.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008592796s
Mar 23 20:09:47.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=true. Elapsed: 4.008835211s
Mar 23 20:09:49.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=false. Elapsed: 6.008158328s
Mar 23 20:09:51.318: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010040005s
STEP: Saw pod success 03/23/23 20:09:51.319
Mar 23 20:09:51.319: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7" satisfied condition "Succeeded or Failed"
Mar 23 20:09:51.324: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 20:09:51.359
Mar 23 20:09:51.378: INFO: Waiting for pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 to disappear
Mar 23 20:09:51.383: INFO: Pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 20:09:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7949" for this suite. 03/23/23 20:09:51.389
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":295,"skipped":5451,"failed":0}
------------------------------
• [SLOW TEST] [8.141 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:43.264
    Mar 23 20:09:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 20:09:43.265
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:43.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:43.288
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-41220eb3-0b69-4817-b1be-c269f0f6a826 03/23/23 20:09:43.291
    STEP: Creating a pod to test consume secrets 03/23/23 20:09:43.299
    Mar 23 20:09:43.308: INFO: Waiting up to 5m0s for pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7" in namespace "secrets-7949" to be "Succeeded or Failed"
    Mar 23 20:09:43.312: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.425992ms
    Mar 23 20:09:45.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008592796s
    Mar 23 20:09:47.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=true. Elapsed: 4.008835211s
    Mar 23 20:09:49.317: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Running", Reason="", readiness=false. Elapsed: 6.008158328s
    Mar 23 20:09:51.318: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.010040005s
    STEP: Saw pod success 03/23/23 20:09:51.319
    Mar 23 20:09:51.319: INFO: Pod "pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7" satisfied condition "Succeeded or Failed"
    Mar 23 20:09:51.324: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:09:51.359
    Mar 23 20:09:51.378: INFO: Waiting for pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 to disappear
    Mar 23 20:09:51.383: INFO: Pod pod-secrets-efe2b30c-df22-4ea1-ab91-6d96815023f7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 20:09:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7949" for this suite. 03/23/23 20:09:51.389
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:09:51.417
Mar 23 20:09:51.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 20:09:51.42
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:51.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:51.448
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Mar 23 20:09:51.473: INFO: Waiting up to 5m0s for pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781" in namespace "container-probe-2123" to be "running and ready"
Mar 23 20:09:51.478: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607586ms
Mar 23 20:09:51.478: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:09:53.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 2.010296812s
Mar 23 20:09:53.483: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:09:55.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 4.010932747s
Mar 23 20:09:55.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:09:57.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 6.013460977s
Mar 23 20:09:57.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:09:59.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 8.010960719s
Mar 23 20:09:59.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:01.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 10.01273685s
Mar 23 20:10:01.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:03.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 12.010805491s
Mar 23 20:10:03.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:05.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 14.010820026s
Mar 23 20:10:05.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:07.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 16.010714296s
Mar 23 20:10:07.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:09.485: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 18.011737776s
Mar 23 20:10:09.485: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:11.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 20.012816056s
Mar 23 20:10:11.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
Mar 23 20:10:13.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=true. Elapsed: 22.010335048s
Mar 23 20:10:13.483: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = true)
Mar 23 20:10:13.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781" satisfied condition "running and ready"
Mar 23 20:10:13.487: INFO: Container started at 2023-03-23 20:09:52 +0000 UTC, pod became ready at 2023-03-23 20:10:11 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 20:10:13.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2123" for this suite. 03/23/23 20:10:13.492
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":296,"skipped":5454,"failed":0}
------------------------------
• [SLOW TEST] [22.091 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:09:51.417
    Mar 23 20:09:51.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 20:09:51.42
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:09:51.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:09:51.448
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Mar 23 20:09:51.473: INFO: Waiting up to 5m0s for pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781" in namespace "container-probe-2123" to be "running and ready"
    Mar 23 20:09:51.478: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Pending", Reason="", readiness=false. Elapsed: 5.607586ms
    Mar 23 20:09:51.478: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:09:53.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 2.010296812s
    Mar 23 20:09:53.483: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:09:55.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 4.010932747s
    Mar 23 20:09:55.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:09:57.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 6.013460977s
    Mar 23 20:09:57.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:09:59.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 8.010960719s
    Mar 23 20:09:59.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:01.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 10.01273685s
    Mar 23 20:10:01.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:03.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 12.010805491s
    Mar 23 20:10:03.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:05.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 14.010820026s
    Mar 23 20:10:05.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:07.484: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 16.010714296s
    Mar 23 20:10:07.484: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:09.485: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 18.011737776s
    Mar 23 20:10:09.485: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:11.486: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=false. Elapsed: 20.012816056s
    Mar 23 20:10:11.486: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = false)
    Mar 23 20:10:13.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781": Phase="Running", Reason="", readiness=true. Elapsed: 22.010335048s
    Mar 23 20:10:13.483: INFO: The phase of Pod test-webserver-91db583b-b72e-4107-8611-46702c44f781 is Running (Ready = true)
    Mar 23 20:10:13.483: INFO: Pod "test-webserver-91db583b-b72e-4107-8611-46702c44f781" satisfied condition "running and ready"
    Mar 23 20:10:13.487: INFO: Container started at 2023-03-23 20:09:52 +0000 UTC, pod became ready at 2023-03-23 20:10:11 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 20:10:13.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2123" for this suite. 03/23/23 20:10:13.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:10:13.511
Mar 23 20:10:13.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:10:13.514
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:13.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:13.537
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 03/23/23 20:10:13.54
Mar 23 20:10:13.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173" in namespace "projected-126" to be "Succeeded or Failed"
Mar 23 20:10:13.559: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Pending", Reason="", readiness=false. Elapsed: 4.19269ms
Mar 23 20:10:15.564: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=true. Elapsed: 2.008895303s
Mar 23 20:10:17.565: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=true. Elapsed: 4.010021423s
Mar 23 20:10:19.565: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=false. Elapsed: 6.010524045s
Mar 23 20:10:21.567: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011794265s
STEP: Saw pod success 03/23/23 20:10:21.567
Mar 23 20:10:21.567: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173" satisfied condition "Succeeded or Failed"
Mar 23 20:10:21.570: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 container client-container: <nil>
STEP: delete the pod 03/23/23 20:10:21.577
Mar 23 20:10:21.601: INFO: Waiting for pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 to disappear
Mar 23 20:10:21.605: INFO: Pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 20:10:21.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-126" for this suite. 03/23/23 20:10:21.62
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":297,"skipped":5466,"failed":0}
------------------------------
• [SLOW TEST] [8.124 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:10:13.511
    Mar 23 20:10:13.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:10:13.514
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:13.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:13.537
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 03/23/23 20:10:13.54
    Mar 23 20:10:13.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173" in namespace "projected-126" to be "Succeeded or Failed"
    Mar 23 20:10:13.559: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Pending", Reason="", readiness=false. Elapsed: 4.19269ms
    Mar 23 20:10:15.564: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=true. Elapsed: 2.008895303s
    Mar 23 20:10:17.565: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=true. Elapsed: 4.010021423s
    Mar 23 20:10:19.565: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Running", Reason="", readiness=false. Elapsed: 6.010524045s
    Mar 23 20:10:21.567: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.011794265s
    STEP: Saw pod success 03/23/23 20:10:21.567
    Mar 23 20:10:21.567: INFO: Pod "downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173" satisfied condition "Succeeded or Failed"
    Mar 23 20:10:21.570: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 container client-container: <nil>
    STEP: delete the pod 03/23/23 20:10:21.577
    Mar 23 20:10:21.601: INFO: Waiting for pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 to disappear
    Mar 23 20:10:21.605: INFO: Pod downwardapi-volume-b57c3b71-f1de-459a-aa3a-eaae1f3f5173 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 20:10:21.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-126" for this suite. 03/23/23 20:10:21.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:10:21.643
Mar 23 20:10:21.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 20:10:21.644
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:21.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:21.678
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-4681 03/23/23 20:10:21.682
STEP: creating service affinity-clusterip-transition in namespace services-4681 03/23/23 20:10:21.682
STEP: creating replication controller affinity-clusterip-transition in namespace services-4681 03/23/23 20:10:21.733
I0323 20:10:21.750709      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4681, replica count: 3
I0323 20:10:24.811756      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 20:10:24.819: INFO: Creating new exec pod
Mar 23 20:10:24.830: INFO: Waiting up to 5m0s for pod "execpod-affinity9qhsk" in namespace "services-4681" to be "running"
Mar 23 20:10:24.838: INFO: Pod "execpod-affinity9qhsk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.334986ms
Mar 23 20:10:26.846: INFO: Pod "execpod-affinity9qhsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.015767909s
Mar 23 20:10:26.846: INFO: Pod "execpod-affinity9qhsk" satisfied condition "running"
Mar 23 20:10:27.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Mar 23 20:10:28.078: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Mar 23 20:10:28.078: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:10:28.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.84.223 80'
Mar 23 20:10:28.319: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.84.223 80\nConnection to 10.0.84.223 80 port [tcp/http] succeeded!\n"
Mar 23 20:10:28.319: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:10:28.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.84.223:80/ ; done'
Mar 23 20:10:28.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n"
Mar 23 20:10:28.780: INFO: stdout: "\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb"
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:28.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.84.223:80/ ; done'
Mar 23 20:10:29.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n"
Mar 23 20:10:29.202: INFO: stdout: "\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb"
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.208: INFO: Received response from host: affinity-clusterip-transition-f2cjb
Mar 23 20:10:29.208: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4681, will wait for the garbage collector to delete the pods 03/23/23 20:10:29.232
Mar 23 20:10:29.306: INFO: Deleting ReplicationController affinity-clusterip-transition took: 19.559962ms
Mar 23 20:10:29.406: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.152507ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 20:10:37.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4681" for this suite. 03/23/23 20:10:37.351
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":298,"skipped":5521,"failed":0}
------------------------------
• [SLOW TEST] [15.715 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:10:21.643
    Mar 23 20:10:21.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 20:10:21.644
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:21.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:21.678
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-4681 03/23/23 20:10:21.682
    STEP: creating service affinity-clusterip-transition in namespace services-4681 03/23/23 20:10:21.682
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4681 03/23/23 20:10:21.733
    I0323 20:10:21.750709      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4681, replica count: 3
    I0323 20:10:24.811756      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 20:10:24.819: INFO: Creating new exec pod
    Mar 23 20:10:24.830: INFO: Waiting up to 5m0s for pod "execpod-affinity9qhsk" in namespace "services-4681" to be "running"
    Mar 23 20:10:24.838: INFO: Pod "execpod-affinity9qhsk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.334986ms
    Mar 23 20:10:26.846: INFO: Pod "execpod-affinity9qhsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.015767909s
    Mar 23 20:10:26.846: INFO: Pod "execpod-affinity9qhsk" satisfied condition "running"
    Mar 23 20:10:27.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Mar 23 20:10:28.078: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Mar 23 20:10:28.078: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:10:28.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.84.223 80'
    Mar 23 20:10:28.319: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.84.223 80\nConnection to 10.0.84.223 80 port [tcp/http] succeeded!\n"
    Mar 23 20:10:28.319: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:10:28.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.84.223:80/ ; done'
    Mar 23 20:10:28.780: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n"
    Mar 23 20:10:28.780: INFO: stdout: "\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-cwz72\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-qh2d5\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb"
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-cwz72
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-qh2d5
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:28.781: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:28.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-4681 exec execpod-affinity9qhsk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.84.223:80/ ; done'
    Mar 23 20:10:29.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.84.223:80/\n"
    Mar 23 20:10:29.202: INFO: stdout: "\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb\naffinity-clusterip-transition-f2cjb"
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.202: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.208: INFO: Received response from host: affinity-clusterip-transition-f2cjb
    Mar 23 20:10:29.208: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4681, will wait for the garbage collector to delete the pods 03/23/23 20:10:29.232
    Mar 23 20:10:29.306: INFO: Deleting ReplicationController affinity-clusterip-transition took: 19.559962ms
    Mar 23 20:10:29.406: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.152507ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 20:10:37.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4681" for this suite. 03/23/23 20:10:37.351
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:10:37.36
Mar 23 20:10:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 20:10:37.366
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:37.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:37.389
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a in namespace container-probe-5111 03/23/23 20:10:37.391
Mar 23 20:10:37.405: INFO: Waiting up to 5m0s for pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a" in namespace "container-probe-5111" to be "not pending"
Mar 23 20:10:37.409: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913392ms
Mar 23 20:10:39.413: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a": Phase="Running", Reason="", readiness=true. Elapsed: 2.0085055s
Mar 23 20:10:39.414: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a" satisfied condition "not pending"
Mar 23 20:10:39.414: INFO: Started pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a in namespace container-probe-5111
STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 20:10:39.414
Mar 23 20:10:39.419: INFO: Initial restart count of pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a is 0
Mar 23 20:11:29.601: INFO: Restart count of pod container-probe-5111/busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a is now 1 (50.181199765s elapsed)
STEP: deleting the pod 03/23/23 20:11:29.601
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 20:11:29.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5111" for this suite. 03/23/23 20:11:29.661
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":299,"skipped":5531,"failed":0}
------------------------------
• [SLOW TEST] [52.307 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:10:37.36
    Mar 23 20:10:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 20:10:37.366
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:10:37.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:10:37.389
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a in namespace container-probe-5111 03/23/23 20:10:37.391
    Mar 23 20:10:37.405: INFO: Waiting up to 5m0s for pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a" in namespace "container-probe-5111" to be "not pending"
    Mar 23 20:10:37.409: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913392ms
    Mar 23 20:10:39.413: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a": Phase="Running", Reason="", readiness=true. Elapsed: 2.0085055s
    Mar 23 20:10:39.414: INFO: Pod "busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a" satisfied condition "not pending"
    Mar 23 20:10:39.414: INFO: Started pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a in namespace container-probe-5111
    STEP: checking the pod's current state and verifying that restartCount is present 03/23/23 20:10:39.414
    Mar 23 20:10:39.419: INFO: Initial restart count of pod busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a is 0
    Mar 23 20:11:29.601: INFO: Restart count of pod container-probe-5111/busybox-5a8efc28-d33f-4edc-8775-0bc3101f724a is now 1 (50.181199765s elapsed)
    STEP: deleting the pod 03/23/23 20:11:29.601
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 20:11:29.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5111" for this suite. 03/23/23 20:11:29.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:11:29.67
Mar 23 20:11:29.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sched-pred 03/23/23 20:11:29.67
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:11:29.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:11:29.696
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Mar 23 20:11:29.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Mar 23 20:11:29.719: INFO: Waiting for terminating namespaces to be deleted...
Mar 23 20:11:29.723: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
Mar 23 20:11:29.737: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.737: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 20:11:29.737: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.737: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 20:11:29.737: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.737: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 20:11:29.737: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 20:11:29.737: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 20:11:29.737: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 20:11:29.737: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 20:11:29.737: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.738: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:11:29.738: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.738: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Mar 23 20:11:29.738: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 20:11:29.738: INFO: 	Container e2e ready: true, restart count 0
Mar 23 20:11:29.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 20:11:29.738: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 20:11:29.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 20:11:29.738: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 20:11:29.738: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
Mar 23 20:11:29.753: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.754: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 20:11:29.754: INFO: azure-npm-bwmlt from kube-system started at 2023-03-23 20:05:58 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.754: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 20:11:29.754: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.755: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 20:11:29.755: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
Mar 23 20:11:29.755: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 20:11:29.755: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 20:11:29.755: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 20:11:29.756: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.756: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:11:29.756: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 20:11:29.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 20:11:29.756: INFO: 	Container systemd-logs ready: true, restart count 0
Mar 23 20:11:29.757: INFO: 
Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
Mar 23 20:11:29.768: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
Mar 23 20:11:29.768: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container azure-npm ready: true, restart count 0
Mar 23 20:11:29.768: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container cloud-node-manager ready: true, restart count 0
Mar 23 20:11:29.768: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container coredns ready: true, restart count 0
Mar 23 20:11:29.768: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container azuredisk ready: true, restart count 0
Mar 23 20:11:29.768: INFO: 	Container liveness-probe ready: true, restart count 0
Mar 23 20:11:29.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
Mar 23 20:11:29.768: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container kube-proxy ready: true, restart count 0
Mar 23 20:11:29.768: INFO: metrics-server-5c57f79cb6-6b8sx from kube-system started at 2023-03-23 20:05:56 +0000 UTC (1 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container metrics-server ready: true, restart count 0
Mar 23 20:11:29.768: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
Mar 23 20:11:29.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Mar 23 20:11:29.768: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 20:11:29.768
Mar 23 20:11:29.778: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9168" to be "running"
Mar 23 20:11:29.782: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016792ms
Mar 23 20:11:31.788: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010061323s
Mar 23 20:11:31.788: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 20:11:31.792
STEP: Trying to apply a random label on the found node. 03/23/23 20:11:31.842
STEP: verifying the node has the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c 95 03/23/23 20:11:31.858
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/23/23 20:11:31.865
W0323 20:11:31.873585      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Mar 23 20:11:31.873: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9168" to be "not pending"
Mar 23 20:11:31.879: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489788ms
Mar 23 20:11:33.884: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599044s
Mar 23 20:11:33.884: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.240.0.56 on the node which pod4 resides and expect not scheduled 03/23/23 20:11:33.884
W0323 20:11:33.890598      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
Mar 23 20:11:33.890: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9168" to be "not pending"
Mar 23 20:11:33.894: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777993ms
Mar 23 20:11:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806568s
Mar 23 20:11:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008441953s
Mar 23 20:11:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008379438s
Mar 23 20:11:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009272482s
Mar 23 20:11:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008950422s
Mar 23 20:11:45.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008611712s
Mar 23 20:11:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.0089415s
Mar 23 20:11:49.903: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012344059s
Mar 23 20:11:51.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007821606s
Mar 23 20:11:53.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008220842s
Mar 23 20:11:55.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009857233s
Mar 23 20:11:57.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009058026s
Mar 23 20:11:59.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009354506s
Mar 23 20:12:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009354586s
Mar 23 20:12:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008871867s
Mar 23 20:12:05.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009847881s
Mar 23 20:12:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008767876s
Mar 23 20:12:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007860571s
Mar 23 20:12:11.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009538759s
Mar 23 20:12:13.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008831041s
Mar 23 20:12:15.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008232864s
Mar 23 20:12:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008244086s
Mar 23 20:12:19.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008157407s
Mar 23 20:12:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010211769s
Mar 23 20:12:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008435982s
Mar 23 20:12:25.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011222684s
Mar 23 20:12:27.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008129999s
Mar 23 20:12:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007567807s
Mar 23 20:12:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007957801s
Mar 23 20:12:33.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009332893s
Mar 23 20:12:35.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00812409s
Mar 23 20:12:37.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007803083s
Mar 23 20:12:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008670958s
Mar 23 20:12:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009203534s
Mar 23 20:12:43.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009283711s
Mar 23 20:12:45.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008435877s
Mar 23 20:12:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008688495s
Mar 23 20:12:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007455617s
Mar 23 20:12:51.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009811002s
Mar 23 20:12:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008405723s
Mar 23 20:12:55.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010060688s
Mar 23 20:12:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009650557s
Mar 23 20:12:59.897: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006975032s
Mar 23 20:13:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009949395s
Mar 23 20:13:03.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008277574s
Mar 23 20:13:05.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008022449s
Mar 23 20:13:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008346223s
Mar 23 20:13:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0076838s
Mar 23 20:13:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008882366s
Mar 23 20:13:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008260837s
Mar 23 20:13:15.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008656505s
Mar 23 20:13:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007958376s
Mar 23 20:13:19.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008509448s
Mar 23 20:13:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009302021s
Mar 23 20:13:23.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01124329s
Mar 23 20:13:25.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009601268s
Mar 23 20:13:27.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009294942s
Mar 23 20:13:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008011318s
Mar 23 20:13:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007840092s
Mar 23 20:13:33.906: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015542249s
Mar 23 20:13:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00846754s
Mar 23 20:13:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008405717s
Mar 23 20:13:39.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007830394s
Mar 23 20:13:41.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010635864s
Mar 23 20:13:43.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009756623s
Mar 23 20:13:45.903: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013087572s
Mar 23 20:13:47.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00816314s
Mar 23 20:13:49.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009078794s
Mar 23 20:13:51.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008284746s
Mar 23 20:13:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008332596s
Mar 23 20:13:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007814047s
Mar 23 20:13:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.010148792s
Mar 23 20:13:59.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007897352s
Mar 23 20:14:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009708303s
Mar 23 20:14:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00860846s
Mar 23 20:14:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009241913s
Mar 23 20:14:07.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007885587s
Mar 23 20:14:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007346253s
Mar 23 20:14:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009083414s
Mar 23 20:14:13.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008321281s
Mar 23 20:14:15.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009177318s
Mar 23 20:14:17.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.00931246s
Mar 23 20:14:19.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007535406s
Mar 23 20:14:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009856243s
Mar 23 20:14:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008639456s
Mar 23 20:14:25.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008673169s
Mar 23 20:14:27.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008801982s
Mar 23 20:14:29.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008883895s
Mar 23 20:14:31.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009852037s
Mar 23 20:14:33.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011041879s
Mar 23 20:14:35.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008219031s
Mar 23 20:14:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008988574s
Mar 23 20:14:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008656328s
Mar 23 20:14:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008728481s
Mar 23 20:14:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008523635s
Mar 23 20:14:45.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010743183s
Mar 23 20:14:47.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007718241s
Mar 23 20:14:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.00814689s
Mar 23 20:14:51.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009988836s
Mar 23 20:14:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00843689s
Mar 23 20:14:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007475648s
Mar 23 20:14:57.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008530502s
Mar 23 20:14:59.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008419359s
Mar 23 20:15:01.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008050117s
Mar 23 20:15:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009030197s
Mar 23 20:15:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009242981s
Mar 23 20:15:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008718866s
Mar 23 20:15:09.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00916855s
Mar 23 20:15:11.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007476992s
Mar 23 20:15:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007783334s
Mar 23 20:15:15.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009505873s
Mar 23 20:15:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00759032s
Mar 23 20:15:19.904: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.014062457s
Mar 23 20:15:21.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00892461s
Mar 23 20:15:23.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00978265s
Mar 23 20:15:25.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.010058291s
Mar 23 20:15:27.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008131395s
Mar 23 20:15:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007650191s
Mar 23 20:15:31.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009932481s
Mar 23 20:15:33.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007813481s
Mar 23 20:15:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008406693s
Mar 23 20:15:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009003606s
Mar 23 20:15:39.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011430916s
Mar 23 20:15:41.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009567035s
Mar 23 20:15:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008384565s
Mar 23 20:15:45.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009831691s
Mar 23 20:15:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008846022s
Mar 23 20:15:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008137252s
Mar 23 20:15:51.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008927892s
Mar 23 20:15:53.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008207736s
Mar 23 20:15:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00799128s
Mar 23 20:15:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010090518s
Mar 23 20:15:59.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008852869s
Mar 23 20:16:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009764317s
Mar 23 20:16:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008921769s
Mar 23 20:16:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009102818s
Mar 23 20:16:07.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007684774s
Mar 23 20:16:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007905927s
Mar 23 20:16:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00832818s
Mar 23 20:16:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008258933s
Mar 23 20:16:15.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.0104151s
Mar 23 20:16:17.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009255277s
Mar 23 20:16:19.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009274551s
Mar 23 20:16:21.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010439622s
Mar 23 20:16:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008387832s
Mar 23 20:16:25.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008932541s
Mar 23 20:16:27.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008544952s
Mar 23 20:16:29.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010404158s
Mar 23 20:16:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008166656s
Mar 23 20:16:33.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008070546s
Mar 23 20:16:33.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011567639s
STEP: removing the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c off the node k8s-linuxpool-16392394-1 03/23/23 20:16:33.902
STEP: verifying the node doesn't have the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c 03/23/23 20:16:33.922
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Mar 23 20:16:33.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9168" for this suite. 03/23/23 20:16:33.934
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":300,"skipped":5554,"failed":0}
------------------------------
• [SLOW TEST] [304.273 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:11:29.67
    Mar 23 20:11:29.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sched-pred 03/23/23 20:11:29.67
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:11:29.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:11:29.696
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Mar 23 20:11:29.698: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Mar 23 20:11:29.719: INFO: Waiting for terminating namespaces to be deleted...
    Mar 23 20:11:29.723: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-0 before test
    Mar 23 20:11:29.737: INFO: azure-ip-masq-agent-zvvq5 from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.737: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: azure-npm-zmbq7 from kube-system started at 2023-03-23 18:30:38 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.737: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: cloud-node-manager-n24nn from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.737: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: csi-azuredisk-node-vnzfb from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 20:11:29.737: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 20:11:29.737: INFO: kube-proxy-zktpk from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.738: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: sonobuoy from sonobuoy started at 2023-03-23 18:33:29 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.738: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: sonobuoy-e2e-job-4ee5050bb09d4ac4 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 20:11:29.738: INFO: 	Container e2e ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-dqdsz from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 20:11:29.738: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 20:11:29.738: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-1 before test
    Mar 23 20:11:29.753: INFO: azure-ip-masq-agent-z42jm from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.754: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 20:11:29.754: INFO: azure-npm-bwmlt from kube-system started at 2023-03-23 20:05:58 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.754: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 20:11:29.754: INFO: cloud-node-manager-6t4kz from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.755: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 20:11:29.755: INFO: csi-azuredisk-node-d68wl from kube-system started at 2023-03-23 18:30:07 +0000 UTC (3 container statuses recorded)
    Mar 23 20:11:29.755: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 20:11:29.755: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 20:11:29.755: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 20:11:29.756: INFO: kube-proxy-rt7c7 from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.756: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 20:11:29.756: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-r49bw from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 20:11:29.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 20:11:29.756: INFO: 	Container systemd-logs ready: true, restart count 0
    Mar 23 20:11:29.757: INFO: 
    Logging pods the apiserver thinks is on node k8s-linuxpool-16392394-2 before test
    Mar 23 20:11:29.768: INFO: azure-ip-masq-agent-dgzkr from kube-system started at 2023-03-23 18:30:04 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container azure-ip-masq-agent ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: azure-npm-tkkkr from kube-system started at 2023-03-23 18:57:09 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container azure-npm ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: cloud-node-manager-8vb4w from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container cloud-node-manager ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: coredns-7c5496644c-xztfb from kube-system started at 2023-03-23 18:30:03 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container coredns ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: csi-azuredisk-node-4f477 from kube-system started at 2023-03-23 18:30:08 +0000 UTC (3 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container azuredisk ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: 	Container liveness-probe ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: kube-proxy-p754s from kube-system started at 2023-03-23 18:30:05 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container kube-proxy ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: metrics-server-5c57f79cb6-6b8sx from kube-system started at 2023-03-23 20:05:56 +0000 UTC (1 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container metrics-server ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: sonobuoy-systemd-logs-daemon-set-4ac85979dc754c7f-nn492 from sonobuoy started at 2023-03-23 18:33:34 +0000 UTC (2 container statuses recorded)
    Mar 23 20:11:29.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Mar 23 20:11:29.768: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 03/23/23 20:11:29.768
    Mar 23 20:11:29.778: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-9168" to be "running"
    Mar 23 20:11:29.782: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016792ms
    Mar 23 20:11:31.788: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.010061323s
    Mar 23 20:11:31.788: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 03/23/23 20:11:31.792
    STEP: Trying to apply a random label on the found node. 03/23/23 20:11:31.842
    STEP: verifying the node has the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c 95 03/23/23 20:11:31.858
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 03/23/23 20:11:31.865
    W0323 20:11:31.873585      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Mar 23 20:11:31.873: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-9168" to be "not pending"
    Mar 23 20:11:31.879: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.489788ms
    Mar 23 20:11:33.884: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.010599044s
    Mar 23 20:11:33.884: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.240.0.56 on the node which pod4 resides and expect not scheduled 03/23/23 20:11:33.884
    W0323 20:11:33.890598      19 warnings.go:70] would violate PodSecurity "baseline:latest": hostPort (container "agnhost" uses hostPort 54322)
    Mar 23 20:11:33.890: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-9168" to be "not pending"
    Mar 23 20:11:33.894: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.777993ms
    Mar 23 20:11:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008806568s
    Mar 23 20:11:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008441953s
    Mar 23 20:11:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008379438s
    Mar 23 20:11:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009272482s
    Mar 23 20:11:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008950422s
    Mar 23 20:11:45.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008611712s
    Mar 23 20:11:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.0089415s
    Mar 23 20:11:49.903: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.012344059s
    Mar 23 20:11:51.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.007821606s
    Mar 23 20:11:53.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008220842s
    Mar 23 20:11:55.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009857233s
    Mar 23 20:11:57.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009058026s
    Mar 23 20:11:59.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.009354506s
    Mar 23 20:12:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009354586s
    Mar 23 20:12:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008871867s
    Mar 23 20:12:05.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.009847881s
    Mar 23 20:12:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008767876s
    Mar 23 20:12:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007860571s
    Mar 23 20:12:11.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.009538759s
    Mar 23 20:12:13.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008831041s
    Mar 23 20:12:15.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008232864s
    Mar 23 20:12:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008244086s
    Mar 23 20:12:19.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.008157407s
    Mar 23 20:12:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010211769s
    Mar 23 20:12:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008435982s
    Mar 23 20:12:25.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.011222684s
    Mar 23 20:12:27.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008129999s
    Mar 23 20:12:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007567807s
    Mar 23 20:12:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007957801s
    Mar 23 20:12:33.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.009332893s
    Mar 23 20:12:35.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00812409s
    Mar 23 20:12:37.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007803083s
    Mar 23 20:12:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008670958s
    Mar 23 20:12:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009203534s
    Mar 23 20:12:43.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.009283711s
    Mar 23 20:12:45.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008435877s
    Mar 23 20:12:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008688495s
    Mar 23 20:12:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007455617s
    Mar 23 20:12:51.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009811002s
    Mar 23 20:12:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008405723s
    Mar 23 20:12:55.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010060688s
    Mar 23 20:12:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009650557s
    Mar 23 20:12:59.897: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006975032s
    Mar 23 20:13:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009949395s
    Mar 23 20:13:03.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008277574s
    Mar 23 20:13:05.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008022449s
    Mar 23 20:13:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008346223s
    Mar 23 20:13:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.0076838s
    Mar 23 20:13:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008882366s
    Mar 23 20:13:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008260837s
    Mar 23 20:13:15.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008656505s
    Mar 23 20:13:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007958376s
    Mar 23 20:13:19.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008509448s
    Mar 23 20:13:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009302021s
    Mar 23 20:13:23.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.01124329s
    Mar 23 20:13:25.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009601268s
    Mar 23 20:13:27.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009294942s
    Mar 23 20:13:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.008011318s
    Mar 23 20:13:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007840092s
    Mar 23 20:13:33.906: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.015542249s
    Mar 23 20:13:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00846754s
    Mar 23 20:13:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.008405717s
    Mar 23 20:13:39.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.007830394s
    Mar 23 20:13:41.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.010635864s
    Mar 23 20:13:43.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.009756623s
    Mar 23 20:13:45.903: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.013087572s
    Mar 23 20:13:47.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.00816314s
    Mar 23 20:13:49.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.009078794s
    Mar 23 20:13:51.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008284746s
    Mar 23 20:13:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008332596s
    Mar 23 20:13:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007814047s
    Mar 23 20:13:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.010148792s
    Mar 23 20:13:59.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007897352s
    Mar 23 20:14:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009708303s
    Mar 23 20:14:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.00860846s
    Mar 23 20:14:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009241913s
    Mar 23 20:14:07.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007885587s
    Mar 23 20:14:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.007346253s
    Mar 23 20:14:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009083414s
    Mar 23 20:14:13.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008321281s
    Mar 23 20:14:15.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009177318s
    Mar 23 20:14:17.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.00931246s
    Mar 23 20:14:19.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007535406s
    Mar 23 20:14:21.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.009856243s
    Mar 23 20:14:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008639456s
    Mar 23 20:14:25.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008673169s
    Mar 23 20:14:27.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008801982s
    Mar 23 20:14:29.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.008883895s
    Mar 23 20:14:31.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009852037s
    Mar 23 20:14:33.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011041879s
    Mar 23 20:14:35.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008219031s
    Mar 23 20:14:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008988574s
    Mar 23 20:14:39.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008656328s
    Mar 23 20:14:41.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.008728481s
    Mar 23 20:14:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008523635s
    Mar 23 20:14:45.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010743183s
    Mar 23 20:14:47.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007718241s
    Mar 23 20:14:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.00814689s
    Mar 23 20:14:51.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009988836s
    Mar 23 20:14:53.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.00843689s
    Mar 23 20:14:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007475648s
    Mar 23 20:14:57.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.008530502s
    Mar 23 20:14:59.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008419359s
    Mar 23 20:15:01.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.008050117s
    Mar 23 20:15:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.009030197s
    Mar 23 20:15:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009242981s
    Mar 23 20:15:07.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008718866s
    Mar 23 20:15:09.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.00916855s
    Mar 23 20:15:11.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.007476992s
    Mar 23 20:15:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007783334s
    Mar 23 20:15:15.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009505873s
    Mar 23 20:15:17.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.00759032s
    Mar 23 20:15:19.904: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.014062457s
    Mar 23 20:15:21.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.00892461s
    Mar 23 20:15:23.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.00978265s
    Mar 23 20:15:25.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.010058291s
    Mar 23 20:15:27.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008131395s
    Mar 23 20:15:29.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007650191s
    Mar 23 20:15:31.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009932481s
    Mar 23 20:15:33.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007813481s
    Mar 23 20:15:35.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008406693s
    Mar 23 20:15:37.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009003606s
    Mar 23 20:15:39.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011430916s
    Mar 23 20:15:41.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009567035s
    Mar 23 20:15:43.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008384565s
    Mar 23 20:15:45.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009831691s
    Mar 23 20:15:47.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.008846022s
    Mar 23 20:15:49.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008137252s
    Mar 23 20:15:51.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008927892s
    Mar 23 20:15:53.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.008207736s
    Mar 23 20:15:55.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00799128s
    Mar 23 20:15:57.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010090518s
    Mar 23 20:15:59.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008852869s
    Mar 23 20:16:01.900: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009764317s
    Mar 23 20:16:03.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008921769s
    Mar 23 20:16:05.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009102818s
    Mar 23 20:16:07.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007684774s
    Mar 23 20:16:09.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007905927s
    Mar 23 20:16:11.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00832818s
    Mar 23 20:16:13.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008258933s
    Mar 23 20:16:15.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.0104151s
    Mar 23 20:16:17.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.009255277s
    Mar 23 20:16:19.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.009274551s
    Mar 23 20:16:21.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010439622s
    Mar 23 20:16:23.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008387832s
    Mar 23 20:16:25.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008932541s
    Mar 23 20:16:27.899: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008544952s
    Mar 23 20:16:29.901: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.010404158s
    Mar 23 20:16:31.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008166656s
    Mar 23 20:16:33.898: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008070546s
    Mar 23 20:16:33.902: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011567639s
    STEP: removing the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c off the node k8s-linuxpool-16392394-1 03/23/23 20:16:33.902
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-074f1f03-7460-4554-9f14-e2713ab7d66c 03/23/23 20:16:33.922
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 20:16:33.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-9168" for this suite. 03/23/23 20:16:33.934
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:16:33.947
Mar 23 20:16:33.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context 03/23/23 20:16:33.95
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:33.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:33.993
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/23/23 20:16:33.995
Mar 23 20:16:34.017: INFO: Waiting up to 5m0s for pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7" in namespace "security-context-4815" to be "Succeeded or Failed"
Mar 23 20:16:34.025: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346884ms
Mar 23 20:16:36.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01377726s
Mar 23 20:16:38.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012958352s
Mar 23 20:16:40.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013893258s
STEP: Saw pod success 03/23/23 20:16:40.032
Mar 23 20:16:40.032: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7" satisfied condition "Succeeded or Failed"
Mar 23 20:16:40.035: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 container test-container: <nil>
STEP: delete the pod 03/23/23 20:16:40.082
Mar 23 20:16:40.108: INFO: Waiting for pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 to disappear
Mar 23 20:16:40.111: INFO: Pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 20:16:40.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4815" for this suite. 03/23/23 20:16:40.117
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":301,"skipped":5608,"failed":0}
------------------------------
• [SLOW TEST] [6.178 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:16:33.947
    Mar 23 20:16:33.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context 03/23/23 20:16:33.95
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:33.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:33.993
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/23/23 20:16:33.995
    Mar 23 20:16:34.017: INFO: Waiting up to 5m0s for pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7" in namespace "security-context-4815" to be "Succeeded or Failed"
    Mar 23 20:16:34.025: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.346884ms
    Mar 23 20:16:36.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.01377726s
    Mar 23 20:16:38.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Running", Reason="", readiness=true. Elapsed: 4.012958352s
    Mar 23 20:16:40.031: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013893258s
    STEP: Saw pod success 03/23/23 20:16:40.032
    Mar 23 20:16:40.032: INFO: Pod "security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7" satisfied condition "Succeeded or Failed"
    Mar 23 20:16:40.035: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:16:40.082
    Mar 23 20:16:40.108: INFO: Waiting for pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 to disappear
    Mar 23 20:16:40.111: INFO: Pod security-context-7bb46fb5-c3cc-448f-8247-87d72fe589d7 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 20:16:40.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4815" for this suite. 03/23/23 20:16:40.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:16:40.136
Mar 23 20:16:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename proxy 03/23/23 20:16:40.138
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:40.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:40.164
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Mar 23 20:16:40.167: INFO: Creating pod...
Mar 23 20:16:40.178: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7054" to be "running"
Mar 23 20:16:40.189: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.970276ms
Mar 23 20:16:42.196: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018266269s
Mar 23 20:16:44.195: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.017178881s
Mar 23 20:16:44.195: INFO: Pod "agnhost" satisfied condition "running"
Mar 23 20:16:44.195: INFO: Creating service...
Mar 23 20:16:44.221: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/DELETE
Mar 23 20:16:44.238: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 23 20:16:44.238: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/GET
Mar 23 20:16:44.255: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 23 20:16:44.255: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/HEAD
Mar 23 20:16:44.269: INFO: http.Client request:HEAD | StatusCode:200
Mar 23 20:16:44.269: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/OPTIONS
Mar 23 20:16:44.276: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 23 20:16:44.277: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/PATCH
Mar 23 20:16:44.309: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 23 20:16:44.315: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/POST
Mar 23 20:16:44.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 23 20:16:44.341: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/PUT
Mar 23 20:16:44.356: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Mar 23 20:16:44.356: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/DELETE
Mar 23 20:16:44.374: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Mar 23 20:16:44.374: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/GET
Mar 23 20:16:44.387: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Mar 23 20:16:44.387: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/HEAD
Mar 23 20:16:44.400: INFO: http.Client request:HEAD | StatusCode:200
Mar 23 20:16:44.400: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/OPTIONS
Mar 23 20:16:44.411: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Mar 23 20:16:44.411: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/PATCH
Mar 23 20:16:44.424: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Mar 23 20:16:44.424: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/POST
Mar 23 20:16:44.446: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Mar 23 20:16:44.446: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/PUT
Mar 23 20:16:44.455: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Mar 23 20:16:44.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7054" for this suite. 03/23/23 20:16:44.463
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":302,"skipped":5651,"failed":0}
------------------------------
• [4.341 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:16:40.136
    Mar 23 20:16:40.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename proxy 03/23/23 20:16:40.138
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:40.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:40.164
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Mar 23 20:16:40.167: INFO: Creating pod...
    Mar 23 20:16:40.178: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7054" to be "running"
    Mar 23 20:16:40.189: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.970276ms
    Mar 23 20:16:42.196: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018266269s
    Mar 23 20:16:44.195: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.017178881s
    Mar 23 20:16:44.195: INFO: Pod "agnhost" satisfied condition "running"
    Mar 23 20:16:44.195: INFO: Creating service...
    Mar 23 20:16:44.221: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/DELETE
    Mar 23 20:16:44.238: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 23 20:16:44.238: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/GET
    Mar 23 20:16:44.255: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 23 20:16:44.255: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/HEAD
    Mar 23 20:16:44.269: INFO: http.Client request:HEAD | StatusCode:200
    Mar 23 20:16:44.269: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/OPTIONS
    Mar 23 20:16:44.276: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 23 20:16:44.277: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/PATCH
    Mar 23 20:16:44.309: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 23 20:16:44.315: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/POST
    Mar 23 20:16:44.341: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 23 20:16:44.341: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/pods/agnhost/proxy/some/path/with/PUT
    Mar 23 20:16:44.356: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Mar 23 20:16:44.356: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/DELETE
    Mar 23 20:16:44.374: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Mar 23 20:16:44.374: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/GET
    Mar 23 20:16:44.387: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Mar 23 20:16:44.387: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/HEAD
    Mar 23 20:16:44.400: INFO: http.Client request:HEAD | StatusCode:200
    Mar 23 20:16:44.400: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/OPTIONS
    Mar 23 20:16:44.411: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Mar 23 20:16:44.411: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/PATCH
    Mar 23 20:16:44.424: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Mar 23 20:16:44.424: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/POST
    Mar 23 20:16:44.446: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Mar 23 20:16:44.446: INFO: Starting http.Client for https://10.0.0.1:443/api/v1/namespaces/proxy-7054/services/test-service/proxy/some/path/with/PUT
    Mar 23 20:16:44.455: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Mar 23 20:16:44.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7054" for this suite. 03/23/23 20:16:44.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:16:44.486
Mar 23 20:16:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:16:44.487
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:44.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:44.514
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-1f20cf49-a48a-49e3-ad5b-2cc0c2437082 03/23/23 20:16:44.517
STEP: Creating a pod to test consume secrets 03/23/23 20:16:44.525
Mar 23 20:16:44.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3" in namespace "projected-8431" to be "Succeeded or Failed"
Mar 23 20:16:44.580: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.619776ms
Mar 23 20:16:46.604: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035234252s
Mar 23 20:16:48.587: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Running", Reason="", readiness=true. Elapsed: 4.018083697s
Mar 23 20:16:50.603: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Running", Reason="", readiness=false. Elapsed: 6.03379557s
Mar 23 20:16:52.588: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01852461s
STEP: Saw pod success 03/23/23 20:16:52.588
Mar 23 20:16:52.588: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3" satisfied condition "Succeeded or Failed"
Mar 23 20:16:52.592: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 container projected-secret-volume-test: <nil>
STEP: delete the pod 03/23/23 20:16:52.6
Mar 23 20:16:52.615: INFO: Waiting for pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 to disappear
Mar 23 20:16:52.620: INFO: Pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 20:16:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8431" for this suite. 03/23/23 20:16:52.626
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":303,"skipped":5743,"failed":0}
------------------------------
• [SLOW TEST] [8.150 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:16:44.486
    Mar 23 20:16:44.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:16:44.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:44.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:44.514
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-1f20cf49-a48a-49e3-ad5b-2cc0c2437082 03/23/23 20:16:44.517
    STEP: Creating a pod to test consume secrets 03/23/23 20:16:44.525
    Mar 23 20:16:44.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3" in namespace "projected-8431" to be "Succeeded or Failed"
    Mar 23 20:16:44.580: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.619776ms
    Mar 23 20:16:46.604: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035234252s
    Mar 23 20:16:48.587: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Running", Reason="", readiness=true. Elapsed: 4.018083697s
    Mar 23 20:16:50.603: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Running", Reason="", readiness=false. Elapsed: 6.03379557s
    Mar 23 20:16:52.588: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.01852461s
    STEP: Saw pod success 03/23/23 20:16:52.588
    Mar 23 20:16:52.588: INFO: Pod "pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3" satisfied condition "Succeeded or Failed"
    Mar 23 20:16:52.592: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:16:52.6
    Mar 23 20:16:52.615: INFO: Waiting for pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 to disappear
    Mar 23 20:16:52.620: INFO: Pod pod-projected-secrets-a7a10b66-0014-43f9-8425-71415526cab3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 20:16:52.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8431" for this suite. 03/23/23 20:16:52.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:16:52.643
Mar 23 20:16:52.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename sysctl 03/23/23 20:16:52.645
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:52.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:52.667
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/23/23 20:16:52.669
STEP: Watching for error events or started pod 03/23/23 20:16:52.682
STEP: Waiting for pod completion 03/23/23 20:16:54.686
Mar 23 20:16:54.686: INFO: Waiting up to 3m0s for pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9" in namespace "sysctl-5123" to be "completed"
Mar 23 20:16:54.690: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Running", Reason="", readiness=true. Elapsed: 3.286893ms
Mar 23 20:16:56.693: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Running", Reason="", readiness=false. Elapsed: 2.006915981s
Mar 23 20:16:58.695: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008586973s
Mar 23 20:16:58.695: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9" satisfied condition "completed"
STEP: Checking that the pod succeeded 03/23/23 20:16:58.698
STEP: Getting logs from the pod 03/23/23 20:16:58.698
STEP: Checking that the sysctl is actually updated 03/23/23 20:16:58.706
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Mar 23 20:16:58.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5123" for this suite. 03/23/23 20:16:58.711
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":304,"skipped":5763,"failed":0}
------------------------------
• [SLOW TEST] [6.081 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:16:52.643
    Mar 23 20:16:52.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename sysctl 03/23/23 20:16:52.645
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:52.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:52.667
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 03/23/23 20:16:52.669
    STEP: Watching for error events or started pod 03/23/23 20:16:52.682
    STEP: Waiting for pod completion 03/23/23 20:16:54.686
    Mar 23 20:16:54.686: INFO: Waiting up to 3m0s for pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9" in namespace "sysctl-5123" to be "completed"
    Mar 23 20:16:54.690: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Running", Reason="", readiness=true. Elapsed: 3.286893ms
    Mar 23 20:16:56.693: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Running", Reason="", readiness=false. Elapsed: 2.006915981s
    Mar 23 20:16:58.695: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008586973s
    Mar 23 20:16:58.695: INFO: Pod "sysctl-1244110f-6e51-421e-b152-0aa8283594e9" satisfied condition "completed"
    STEP: Checking that the pod succeeded 03/23/23 20:16:58.698
    STEP: Getting logs from the pod 03/23/23 20:16:58.698
    STEP: Checking that the sysctl is actually updated 03/23/23 20:16:58.706
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Mar 23 20:16:58.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5123" for this suite. 03/23/23 20:16:58.711
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:16:58.725
Mar 23 20:16:58.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename gc 03/23/23 20:16:58.726
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:58.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:58.754
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 03/23/23 20:16:58.758
STEP: delete the rc 03/23/23 20:17:03.773
STEP: wait for all pods to be garbage collected 03/23/23 20:17:03.782
STEP: Gathering metrics 03/23/23 20:17:08.789
Mar 23 20:17:08.857: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
Mar 23 20:17:08.861: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.947191ms
Mar 23 20:17:08.861: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
Mar 23 20:17:08.861: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
Mar 23 20:18:09.084: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Mar 23 20:18:09.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1315" for this suite. 03/23/23 20:18:09.091
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":305,"skipped":5763,"failed":0}
------------------------------
• [SLOW TEST] [70.384 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:16:58.725
    Mar 23 20:16:58.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename gc 03/23/23 20:16:58.726
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:16:58.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:16:58.754
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 03/23/23 20:16:58.758
    STEP: delete the rc 03/23/23 20:17:03.773
    STEP: wait for all pods to be garbage collected 03/23/23 20:17:03.782
    STEP: Gathering metrics 03/23/23 20:17:08.789
    Mar 23 20:17:08.857: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8s-master-16392394-2" in namespace "kube-system" to be "running and ready"
    Mar 23 20:17:08.861: INFO: Pod "kube-controller-manager-k8s-master-16392394-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.947191ms
    Mar 23 20:17:08.861: INFO: The phase of Pod kube-controller-manager-k8s-master-16392394-2 is Running (Ready = true)
    Mar 23 20:17:08.861: INFO: Pod "kube-controller-manager-k8s-master-16392394-2" satisfied condition "running and ready"
    Mar 23 20:18:09.084: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Mar 23 20:18:09.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1315" for this suite. 03/23/23 20:18:09.091
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:09.109
Mar 23 20:18:09.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename dns 03/23/23 20:18:09.11
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:09.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:09.133
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5322.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5322.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 03/23/23 20:18:09.137
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5322.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5322.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 03/23/23 20:18:09.137
STEP: creating a pod to probe /etc/hosts 03/23/23 20:18:09.138
STEP: submitting the pod to kubernetes 03/23/23 20:18:09.138
Mar 23 20:18:09.150: INFO: Waiting up to 15m0s for pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4" in namespace "dns-5322" to be "running"
Mar 23 20:18:09.183: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4": Phase="Pending", Reason="", readiness=false. Elapsed: 33.485325ms
Mar 23 20:18:11.190: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.039694825s
Mar 23 20:18:11.190: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4" satisfied condition "running"
STEP: retrieving the pod 03/23/23 20:18:11.19
STEP: looking for the results for each expected name from probers 03/23/23 20:18:11.193
Mar 23 20:18:11.212: INFO: DNS probes using dns-5322/dns-test-90863bea-e791-44e9-849a-1002bd2175a4 succeeded

STEP: deleting the pod 03/23/23 20:18:11.212
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Mar 23 20:18:11.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5322" for this suite. 03/23/23 20:18:11.241
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":306,"skipped":5767,"failed":0}
------------------------------
• [2.140 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:09.109
    Mar 23 20:18:09.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename dns 03/23/23 20:18:09.11
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:09.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:09.133
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5322.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5322.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     03/23/23 20:18:09.137
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5322.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5322.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     03/23/23 20:18:09.137
    STEP: creating a pod to probe /etc/hosts 03/23/23 20:18:09.138
    STEP: submitting the pod to kubernetes 03/23/23 20:18:09.138
    Mar 23 20:18:09.150: INFO: Waiting up to 15m0s for pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4" in namespace "dns-5322" to be "running"
    Mar 23 20:18:09.183: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4": Phase="Pending", Reason="", readiness=false. Elapsed: 33.485325ms
    Mar 23 20:18:11.190: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4": Phase="Running", Reason="", readiness=true. Elapsed: 2.039694825s
    Mar 23 20:18:11.190: INFO: Pod "dns-test-90863bea-e791-44e9-849a-1002bd2175a4" satisfied condition "running"
    STEP: retrieving the pod 03/23/23 20:18:11.19
    STEP: looking for the results for each expected name from probers 03/23/23 20:18:11.193
    Mar 23 20:18:11.212: INFO: DNS probes using dns-5322/dns-test-90863bea-e791-44e9-849a-1002bd2175a4 succeeded

    STEP: deleting the pod 03/23/23 20:18:11.212
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Mar 23 20:18:11.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5322" for this suite. 03/23/23 20:18:11.241
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:11.25
Mar 23 20:18:11.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 20:18:11.251
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:11.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:11.286
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 03/23/23 20:18:11.333
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 20:18:11.34
Mar 23 20:18:11.349: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:11.351: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:11.351: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:11.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:18:11.362: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:12.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:18:12.375: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:13.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:13.374: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:14.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:14.379: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:15.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:15.375: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 20:18:16.376: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/23/23 20:18:16.379
Mar 23 20:18:16.423: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.433: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.433: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:16.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:16.441: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:17.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:17.455: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:18.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:18.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:18.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:18.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:18.452: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:19.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:19.454: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:20.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:20.453: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:21.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:18:21.452: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:18:22.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 20:18:22.451: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 03/23/23 20:18:22.451
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:18:22.458
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7224, will wait for the garbage collector to delete the pods 03/23/23 20:18:22.458
Mar 23 20:18:22.519: INFO: Deleting DaemonSet.extensions daemon-set took: 7.124985ms
Mar 23 20:18:22.620: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022876ms
Mar 23 20:18:33.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:18:33.026: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 20:18:33.029: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42003"},"items":null}

Mar 23 20:18:33.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42003"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 20:18:33.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7224" for this suite. 03/23/23 20:18:33.058
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":307,"skipped":5777,"failed":0}
------------------------------
• [SLOW TEST] [21.817 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:11.25
    Mar 23 20:18:11.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 20:18:11.251
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:11.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:11.286
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 03/23/23 20:18:11.333
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 20:18:11.34
    Mar 23 20:18:11.349: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:11.351: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:11.351: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:11.362: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:18:11.362: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:12.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:12.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:18:12.375: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:13.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:13.374: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:13.374: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
    Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:14.371: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:14.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:14.379: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
    Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:15.369: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:15.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:15.375: INFO: Node k8s-linuxpool-16392394-2 is running 0 daemon pod, expected 1
    Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.370: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 20:18:16.376: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 03/23/23 20:18:16.379
    Mar 23 20:18:16.423: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.433: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.433: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:16.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:16.441: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:17.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:17.455: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:17.455: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:18.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:18.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:18.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:18.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:18.452: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:19.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:19.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:19.454: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:20.449: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:20.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:20.453: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:21.448: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:21.452: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:18:21.452: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:22.447: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:18:22.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 20:18:22.451: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 03/23/23 20:18:22.451
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:18:22.458
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7224, will wait for the garbage collector to delete the pods 03/23/23 20:18:22.458
    Mar 23 20:18:22.519: INFO: Deleting DaemonSet.extensions daemon-set took: 7.124985ms
    Mar 23 20:18:22.620: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022876ms
    Mar 23 20:18:33.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:18:33.026: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 20:18:33.029: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42003"},"items":null}

    Mar 23 20:18:33.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42003"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 20:18:33.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7224" for this suite. 03/23/23 20:18:33.058
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:33.069
Mar 23 20:18:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename containers 03/23/23 20:18:33.07
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:33.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:33.095
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Mar 23 20:18:33.118: INFO: Waiting up to 5m0s for pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8" in namespace "containers-3969" to be "running"
Mar 23 20:18:33.144: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.346141ms
Mar 23 20:18:35.149: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.030793599s
Mar 23 20:18:35.149: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Mar 23 20:18:35.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3969" for this suite. 03/23/23 20:18:35.187
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":308,"skipped":5777,"failed":0}
------------------------------
• [2.126 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:33.069
    Mar 23 20:18:33.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename containers 03/23/23 20:18:33.07
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:33.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:33.095
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Mar 23 20:18:33.118: INFO: Waiting up to 5m0s for pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8" in namespace "containers-3969" to be "running"
    Mar 23 20:18:33.144: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.346141ms
    Mar 23 20:18:35.149: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8": Phase="Running", Reason="", readiness=true. Elapsed: 2.030793599s
    Mar 23 20:18:35.149: INFO: Pod "client-containers-bb9f1b32-05a2-4905-8be7-99522811f7d8" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Mar 23 20:18:35.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3969" for this suite. 03/23/23 20:18:35.187
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:35.215
Mar 23 20:18:35.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 20:18:35.217
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:35.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:35.244
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Mar 23 20:18:35.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5204 version'
Mar 23 20:18:35.343: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Mar 23 20:18:35.343: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-03-01T00:14:37Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 20:18:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5204" for this suite. 03/23/23 20:18:35.35
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":309,"skipped":5833,"failed":0}
------------------------------
• [0.144 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:35.215
    Mar 23 20:18:35.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 20:18:35.217
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:35.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:35.244
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Mar 23 20:18:35.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5204 version'
    Mar 23 20:18:35.343: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Mar 23 20:18:35.343: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-02-22T14:05:25Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.7\", GitCommit:\"723bcdb232300aaf5e147ff19b4df7ec8a20278d\", GitTreeState:\"clean\", BuildDate:\"2023-03-01T00:14:37Z\", GoVersion:\"go1.19.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 20:18:35.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5204" for this suite. 03/23/23 20:18:35.35
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:35.359
Mar 23 20:18:35.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 20:18:35.363
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:35.403
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:35.412
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-f98101ce-b0b8-4f5e-943e-257e63029d37 03/23/23 20:18:35.415
STEP: Creating a pod to test consume secrets 03/23/23 20:18:35.432
Mar 23 20:18:35.448: INFO: Waiting up to 5m0s for pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34" in namespace "secrets-4608" to be "Succeeded or Failed"
Mar 23 20:18:35.453: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015189ms
Mar 23 20:18:37.460: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012809739s
Mar 23 20:18:39.461: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013043323s
Mar 23 20:18:41.458: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010595226s
STEP: Saw pod success 03/23/23 20:18:41.458
Mar 23 20:18:41.458: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34" satisfied condition "Succeeded or Failed"
Mar 23 20:18:41.463: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 20:18:41.47
Mar 23 20:18:41.486: INFO: Waiting for pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 to disappear
Mar 23 20:18:41.489: INFO: Pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 20:18:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4608" for this suite. 03/23/23 20:18:41.495
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":310,"skipped":5836,"failed":0}
------------------------------
• [SLOW TEST] [6.144 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:35.359
    Mar 23 20:18:35.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 20:18:35.363
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:35.403
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:35.412
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-f98101ce-b0b8-4f5e-943e-257e63029d37 03/23/23 20:18:35.415
    STEP: Creating a pod to test consume secrets 03/23/23 20:18:35.432
    Mar 23 20:18:35.448: INFO: Waiting up to 5m0s for pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34" in namespace "secrets-4608" to be "Succeeded or Failed"
    Mar 23 20:18:35.453: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 5.015189ms
    Mar 23 20:18:37.460: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012809739s
    Mar 23 20:18:39.461: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013043323s
    Mar 23 20:18:41.458: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010595226s
    STEP: Saw pod success 03/23/23 20:18:41.458
    Mar 23 20:18:41.458: INFO: Pod "pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34" satisfied condition "Succeeded or Failed"
    Mar 23 20:18:41.463: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:18:41.47
    Mar 23 20:18:41.486: INFO: Waiting for pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 to disappear
    Mar 23 20:18:41.489: INFO: Pod pod-secrets-3fa240f4-d256-481b-9c56-b04c52109e34 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 20:18:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4608" for this suite. 03/23/23 20:18:41.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:41.505
Mar 23 20:18:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 20:18:41.506
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:41.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:41.54
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 03/23/23 20:18:41.543
Mar 23 20:18:41.568: INFO: Waiting up to 5m0s for pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849" in namespace "downward-api-8222" to be "running and ready"
Mar 23 20:18:41.597: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849": Phase="Pending", Reason="", readiness=false. Elapsed: 28.102538ms
Mar 23 20:18:41.597: INFO: The phase of Pod annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:18:43.603: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849": Phase="Running", Reason="", readiness=true. Elapsed: 2.034014322s
Mar 23 20:18:43.603: INFO: The phase of Pod annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849 is Running (Ready = true)
Mar 23 20:18:43.603: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849" satisfied condition "running and ready"
Mar 23 20:18:44.133: INFO: Successfully updated pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 20:18:48.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8222" for this suite. 03/23/23 20:18:48.17
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":311,"skipped":5853,"failed":0}
------------------------------
• [SLOW TEST] [6.670 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:41.505
    Mar 23 20:18:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 20:18:41.506
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:41.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:41.54
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 03/23/23 20:18:41.543
    Mar 23 20:18:41.568: INFO: Waiting up to 5m0s for pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849" in namespace "downward-api-8222" to be "running and ready"
    Mar 23 20:18:41.597: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849": Phase="Pending", Reason="", readiness=false. Elapsed: 28.102538ms
    Mar 23 20:18:41.597: INFO: The phase of Pod annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:18:43.603: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849": Phase="Running", Reason="", readiness=true. Elapsed: 2.034014322s
    Mar 23 20:18:43.603: INFO: The phase of Pod annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849 is Running (Ready = true)
    Mar 23 20:18:43.603: INFO: Pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849" satisfied condition "running and ready"
    Mar 23 20:18:44.133: INFO: Successfully updated pod "annotationupdate69673507-4ae8-4d3f-ac12-d5d4e52b6849"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 20:18:48.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8222" for this suite. 03/23/23 20:18:48.17
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:48.178
Mar 23 20:18:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:18:48.179
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:48.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:48.207
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 03/23/23 20:18:48.21
Mar 23 20:18:48.228: INFO: Waiting up to 5m0s for pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb" in namespace "emptydir-5146" to be "Succeeded or Failed"
Mar 23 20:18:48.236: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101182ms
Mar 23 20:18:50.241: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012526658s
Mar 23 20:18:52.250: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022085022s
STEP: Saw pod success 03/23/23 20:18:52.25
Mar 23 20:18:52.251: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb" satisfied condition "Succeeded or Failed"
Mar 23 20:18:52.255: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb container test-container: <nil>
STEP: delete the pod 03/23/23 20:18:52.263
Mar 23 20:18:52.278: INFO: Waiting for pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb to disappear
Mar 23 20:18:52.282: INFO: Pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:18:52.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5146" for this suite. 03/23/23 20:18:52.289
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":312,"skipped":5854,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:48.178
    Mar 23 20:18:48.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:18:48.179
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:48.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:48.207
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 03/23/23 20:18:48.21
    Mar 23 20:18:48.228: INFO: Waiting up to 5m0s for pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb" in namespace "emptydir-5146" to be "Succeeded or Failed"
    Mar 23 20:18:48.236: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101182ms
    Mar 23 20:18:50.241: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012526658s
    Mar 23 20:18:52.250: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022085022s
    STEP: Saw pod success 03/23/23 20:18:52.25
    Mar 23 20:18:52.251: INFO: Pod "pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb" satisfied condition "Succeeded or Failed"
    Mar 23 20:18:52.255: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb container test-container: <nil>
    STEP: delete the pod 03/23/23 20:18:52.263
    Mar 23 20:18:52.278: INFO: Waiting for pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb to disappear
    Mar 23 20:18:52.282: INFO: Pod pod-15444441-0fa8-46c8-b6c4-6ca303be6ccb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:18:52.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5146" for this suite. 03/23/23 20:18:52.289
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:52.308
Mar 23 20:18:52.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 20:18:52.309
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:52.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:52.348
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 03/23/23 20:18:52.353
Mar 23 20:18:52.371: INFO: Waiting up to 5m0s for pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3" in namespace "var-expansion-5220" to be "Succeeded or Failed"
Mar 23 20:18:52.379: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.136884ms
Mar 23 20:18:54.385: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013336555s
Mar 23 20:18:56.383: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01170493s
STEP: Saw pod success 03/23/23 20:18:56.383
Mar 23 20:18:56.383: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3" satisfied condition "Succeeded or Failed"
Mar 23 20:18:56.387: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 container dapi-container: <nil>
STEP: delete the pod 03/23/23 20:18:56.394
Mar 23 20:18:56.417: INFO: Waiting for pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 to disappear
Mar 23 20:18:56.422: INFO: Pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 20:18:56.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5220" for this suite. 03/23/23 20:18:56.428
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":313,"skipped":5855,"failed":0}
------------------------------
• [4.133 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:52.308
    Mar 23 20:18:52.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 20:18:52.309
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:52.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:52.348
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 03/23/23 20:18:52.353
    Mar 23 20:18:52.371: INFO: Waiting up to 5m0s for pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3" in namespace "var-expansion-5220" to be "Succeeded or Failed"
    Mar 23 20:18:52.379: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.136884ms
    Mar 23 20:18:54.385: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013336555s
    Mar 23 20:18:56.383: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01170493s
    STEP: Saw pod success 03/23/23 20:18:56.383
    Mar 23 20:18:56.383: INFO: Pod "var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3" satisfied condition "Succeeded or Failed"
    Mar 23 20:18:56.387: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 container dapi-container: <nil>
    STEP: delete the pod 03/23/23 20:18:56.394
    Mar 23 20:18:56.417: INFO: Waiting for pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 to disappear
    Mar 23 20:18:56.422: INFO: Pod var-expansion-cffdc67d-9cf6-4df1-9ff3-d30a353ed8f3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 20:18:56.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5220" for this suite. 03/23/23 20:18:56.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:18:56.445
Mar 23 20:18:56.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 20:18:56.446
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:56.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:56.488
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 03/23/23 20:18:56.491
Mar 23 20:18:56.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 create -f -'
Mar 23 20:18:57.165: INFO: stderr: ""
Mar 23 20:18:57.165: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:18:57.165
Mar 23 20:18:57.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:18:57.290: INFO: stderr: ""
Mar 23 20:18:57.290: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
Mar 23 20:18:57.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:18:57.418: INFO: stderr: ""
Mar 23 20:18:57.418: INFO: stdout: ""
Mar 23 20:18:57.418: INFO: update-demo-nautilus-72hdm is created but not running
Mar 23 20:19:02.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:19:02.547: INFO: stderr: ""
Mar 23 20:19:02.547: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
Mar 23 20:19:02.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:02.671: INFO: stderr: ""
Mar 23 20:19:02.671: INFO: stdout: "true"
Mar 23 20:19:02.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:02.788: INFO: stderr: ""
Mar 23 20:19:02.788: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:02.788: INFO: validating pod update-demo-nautilus-72hdm
Mar 23 20:19:02.796: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:02.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:02.796: INFO: update-demo-nautilus-72hdm is verified up and running
Mar 23 20:19:02.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:02.910: INFO: stderr: ""
Mar 23 20:19:02.910: INFO: stdout: "true"
Mar 23 20:19:02.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:03.033: INFO: stderr: ""
Mar 23 20:19:03.033: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:03.033: INFO: validating pod update-demo-nautilus-7mkbh
Mar 23 20:19:03.039: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:03.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:03.039: INFO: update-demo-nautilus-7mkbh is verified up and running
STEP: scaling down the replication controller 03/23/23 20:19:03.039
Mar 23 20:19:03.041: INFO: scanned /root for discovery docs: <nil>
Mar 23 20:19:03.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Mar 23 20:19:04.204: INFO: stderr: ""
Mar 23 20:19:04.204: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:19:04.204
Mar 23 20:19:04.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:19:04.312: INFO: stderr: ""
Mar 23 20:19:04.312: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
STEP: Replicas for name=update-demo: expected=1 actual=2 03/23/23 20:19:04.312
Mar 23 20:19:09.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:19:09.414: INFO: stderr: ""
Mar 23 20:19:09.414: INFO: stdout: "update-demo-nautilus-7mkbh "
Mar 23 20:19:09.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:09.516: INFO: stderr: ""
Mar 23 20:19:09.516: INFO: stdout: "true"
Mar 23 20:19:09.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:09.638: INFO: stderr: ""
Mar 23 20:19:09.638: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:09.638: INFO: validating pod update-demo-nautilus-7mkbh
Mar 23 20:19:09.671: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:09.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:09.671: INFO: update-demo-nautilus-7mkbh is verified up and running
STEP: scaling up the replication controller 03/23/23 20:19:09.671
Mar 23 20:19:09.673: INFO: scanned /root for discovery docs: <nil>
Mar 23 20:19:09.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Mar 23 20:19:10.800: INFO: stderr: ""
Mar 23 20:19:10.800: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:19:10.8
Mar 23 20:19:10.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:19:10.898: INFO: stderr: ""
Mar 23 20:19:10.898: INFO: stdout: "update-demo-nautilus-7mkbh update-demo-nautilus-8bbfm "
Mar 23 20:19:10.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:10.998: INFO: stderr: ""
Mar 23 20:19:10.998: INFO: stdout: "true"
Mar 23 20:19:10.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:11.106: INFO: stderr: ""
Mar 23 20:19:11.106: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:11.106: INFO: validating pod update-demo-nautilus-7mkbh
Mar 23 20:19:11.112: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:11.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:11.112: INFO: update-demo-nautilus-7mkbh is verified up and running
Mar 23 20:19:11.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:11.211: INFO: stderr: ""
Mar 23 20:19:11.211: INFO: stdout: ""
Mar 23 20:19:11.211: INFO: update-demo-nautilus-8bbfm is created but not running
Mar 23 20:19:16.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Mar 23 20:19:16.308: INFO: stderr: ""
Mar 23 20:19:16.308: INFO: stdout: "update-demo-nautilus-7mkbh update-demo-nautilus-8bbfm "
Mar 23 20:19:16.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:16.414: INFO: stderr: ""
Mar 23 20:19:16.414: INFO: stdout: "true"
Mar 23 20:19:16.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:16.518: INFO: stderr: ""
Mar 23 20:19:16.518: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:16.518: INFO: validating pod update-demo-nautilus-7mkbh
Mar 23 20:19:16.523: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:16.523: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:16.523: INFO: update-demo-nautilus-7mkbh is verified up and running
Mar 23 20:19:16.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Mar 23 20:19:16.628: INFO: stderr: ""
Mar 23 20:19:16.628: INFO: stdout: "true"
Mar 23 20:19:16.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Mar 23 20:19:16.717: INFO: stderr: ""
Mar 23 20:19:16.717: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Mar 23 20:19:16.717: INFO: validating pod update-demo-nautilus-8bbfm
Mar 23 20:19:16.723: INFO: got data: {
  "image": "nautilus.jpg"
}

Mar 23 20:19:16.723: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Mar 23 20:19:16.723: INFO: update-demo-nautilus-8bbfm is verified up and running
STEP: using delete to clean up resources 03/23/23 20:19:16.723
Mar 23 20:19:16.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 delete --grace-period=0 --force -f -'
Mar 23 20:19:16.825: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Mar 23 20:19:16.825: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Mar 23 20:19:16.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get rc,svc -l name=update-demo --no-headers'
Mar 23 20:19:16.972: INFO: stderr: "No resources found in kubectl-8646 namespace.\n"
Mar 23 20:19:16.972: INFO: stdout: ""
Mar 23 20:19:16.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Mar 23 20:19:17.085: INFO: stderr: ""
Mar 23 20:19:17.085: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 20:19:17.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8646" for this suite. 03/23/23 20:19:17.091
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":314,"skipped":5870,"failed":0}
------------------------------
• [SLOW TEST] [20.656 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:18:56.445
    Mar 23 20:18:56.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 20:18:56.446
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:18:56.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:18:56.488
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 03/23/23 20:18:56.491
    Mar 23 20:18:56.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 create -f -'
    Mar 23 20:18:57.165: INFO: stderr: ""
    Mar 23 20:18:57.165: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:18:57.165
    Mar 23 20:18:57.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:18:57.290: INFO: stderr: ""
    Mar 23 20:18:57.290: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
    Mar 23 20:18:57.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:18:57.418: INFO: stderr: ""
    Mar 23 20:18:57.418: INFO: stdout: ""
    Mar 23 20:18:57.418: INFO: update-demo-nautilus-72hdm is created but not running
    Mar 23 20:19:02.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:19:02.547: INFO: stderr: ""
    Mar 23 20:19:02.547: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
    Mar 23 20:19:02.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:02.671: INFO: stderr: ""
    Mar 23 20:19:02.671: INFO: stdout: "true"
    Mar 23 20:19:02.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-72hdm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:02.788: INFO: stderr: ""
    Mar 23 20:19:02.788: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:02.788: INFO: validating pod update-demo-nautilus-72hdm
    Mar 23 20:19:02.796: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:02.796: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:02.796: INFO: update-demo-nautilus-72hdm is verified up and running
    Mar 23 20:19:02.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:02.910: INFO: stderr: ""
    Mar 23 20:19:02.910: INFO: stdout: "true"
    Mar 23 20:19:02.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:03.033: INFO: stderr: ""
    Mar 23 20:19:03.033: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:03.033: INFO: validating pod update-demo-nautilus-7mkbh
    Mar 23 20:19:03.039: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:03.039: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:03.039: INFO: update-demo-nautilus-7mkbh is verified up and running
    STEP: scaling down the replication controller 03/23/23 20:19:03.039
    Mar 23 20:19:03.041: INFO: scanned /root for discovery docs: <nil>
    Mar 23 20:19:03.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Mar 23 20:19:04.204: INFO: stderr: ""
    Mar 23 20:19:04.204: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:19:04.204
    Mar 23 20:19:04.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:19:04.312: INFO: stderr: ""
    Mar 23 20:19:04.312: INFO: stdout: "update-demo-nautilus-72hdm update-demo-nautilus-7mkbh "
    STEP: Replicas for name=update-demo: expected=1 actual=2 03/23/23 20:19:04.312
    Mar 23 20:19:09.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:19:09.414: INFO: stderr: ""
    Mar 23 20:19:09.414: INFO: stdout: "update-demo-nautilus-7mkbh "
    Mar 23 20:19:09.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:09.516: INFO: stderr: ""
    Mar 23 20:19:09.516: INFO: stdout: "true"
    Mar 23 20:19:09.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:09.638: INFO: stderr: ""
    Mar 23 20:19:09.638: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:09.638: INFO: validating pod update-demo-nautilus-7mkbh
    Mar 23 20:19:09.671: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:09.671: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:09.671: INFO: update-demo-nautilus-7mkbh is verified up and running
    STEP: scaling up the replication controller 03/23/23 20:19:09.671
    Mar 23 20:19:09.673: INFO: scanned /root for discovery docs: <nil>
    Mar 23 20:19:09.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Mar 23 20:19:10.800: INFO: stderr: ""
    Mar 23 20:19:10.800: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 03/23/23 20:19:10.8
    Mar 23 20:19:10.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:19:10.898: INFO: stderr: ""
    Mar 23 20:19:10.898: INFO: stdout: "update-demo-nautilus-7mkbh update-demo-nautilus-8bbfm "
    Mar 23 20:19:10.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:10.998: INFO: stderr: ""
    Mar 23 20:19:10.998: INFO: stdout: "true"
    Mar 23 20:19:10.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:11.106: INFO: stderr: ""
    Mar 23 20:19:11.106: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:11.106: INFO: validating pod update-demo-nautilus-7mkbh
    Mar 23 20:19:11.112: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:11.112: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:11.112: INFO: update-demo-nautilus-7mkbh is verified up and running
    Mar 23 20:19:11.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:11.211: INFO: stderr: ""
    Mar 23 20:19:11.211: INFO: stdout: ""
    Mar 23 20:19:11.211: INFO: update-demo-nautilus-8bbfm is created but not running
    Mar 23 20:19:16.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Mar 23 20:19:16.308: INFO: stderr: ""
    Mar 23 20:19:16.308: INFO: stdout: "update-demo-nautilus-7mkbh update-demo-nautilus-8bbfm "
    Mar 23 20:19:16.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:16.414: INFO: stderr: ""
    Mar 23 20:19:16.414: INFO: stdout: "true"
    Mar 23 20:19:16.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-7mkbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:16.518: INFO: stderr: ""
    Mar 23 20:19:16.518: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:16.518: INFO: validating pod update-demo-nautilus-7mkbh
    Mar 23 20:19:16.523: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:16.523: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:16.523: INFO: update-demo-nautilus-7mkbh is verified up and running
    Mar 23 20:19:16.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Mar 23 20:19:16.628: INFO: stderr: ""
    Mar 23 20:19:16.628: INFO: stdout: "true"
    Mar 23 20:19:16.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods update-demo-nautilus-8bbfm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Mar 23 20:19:16.717: INFO: stderr: ""
    Mar 23 20:19:16.717: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Mar 23 20:19:16.717: INFO: validating pod update-demo-nautilus-8bbfm
    Mar 23 20:19:16.723: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Mar 23 20:19:16.723: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Mar 23 20:19:16.723: INFO: update-demo-nautilus-8bbfm is verified up and running
    STEP: using delete to clean up resources 03/23/23 20:19:16.723
    Mar 23 20:19:16.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 delete --grace-period=0 --force -f -'
    Mar 23 20:19:16.825: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Mar 23 20:19:16.825: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Mar 23 20:19:16.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get rc,svc -l name=update-demo --no-headers'
    Mar 23 20:19:16.972: INFO: stderr: "No resources found in kubectl-8646 namespace.\n"
    Mar 23 20:19:16.972: INFO: stdout: ""
    Mar 23 20:19:16.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-8646 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Mar 23 20:19:17.085: INFO: stderr: ""
    Mar 23 20:19:17.085: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 20:19:17.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8646" for this suite. 03/23/23 20:19:17.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:17.103
Mar 23 20:19:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename events 03/23/23 20:19:17.106
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:17.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:17.2
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 03/23/23 20:19:17.203
Mar 23 20:19:17.215: INFO: created test-event-1
Mar 23 20:19:17.221: INFO: created test-event-2
Mar 23 20:19:17.226: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 03/23/23 20:19:17.226
STEP: delete collection of events 03/23/23 20:19:17.231
Mar 23 20:19:17.231: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 03/23/23 20:19:17.269
Mar 23 20:19:17.269: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 23 20:19:17.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8437" for this suite. 03/23/23 20:19:17.289
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":315,"skipped":5887,"failed":0}
------------------------------
• [0.193 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:17.103
    Mar 23 20:19:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename events 03/23/23 20:19:17.106
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:17.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:17.2
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 03/23/23 20:19:17.203
    Mar 23 20:19:17.215: INFO: created test-event-1
    Mar 23 20:19:17.221: INFO: created test-event-2
    Mar 23 20:19:17.226: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 03/23/23 20:19:17.226
    STEP: delete collection of events 03/23/23 20:19:17.231
    Mar 23 20:19:17.231: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 03/23/23 20:19:17.269
    Mar 23 20:19:17.269: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 23 20:19:17.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8437" for this suite. 03/23/23 20:19:17.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:17.299
Mar 23 20:19:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 20:19:17.3
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:17.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:17.337
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-9bdf9cdb-b194-4e76-b7d8-02194a53ff80 03/23/23 20:19:17.34
STEP: Creating a pod to test consume secrets 03/23/23 20:19:17.346
Mar 23 20:19:17.356: INFO: Waiting up to 5m0s for pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a" in namespace "secrets-3875" to be "Succeeded or Failed"
Mar 23 20:19:17.374: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.27596ms
Mar 23 20:19:19.380: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023957023s
Mar 23 20:19:21.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023145998s
Mar 23 20:19:23.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022676572s
STEP: Saw pod success 03/23/23 20:19:23.379
Mar 23 20:19:23.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a" satisfied condition "Succeeded or Failed"
Mar 23 20:19:23.382: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a container secret-volume-test: <nil>
STEP: delete the pod 03/23/23 20:19:23.389
Mar 23 20:19:23.411: INFO: Waiting for pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a to disappear
Mar 23 20:19:23.414: INFO: Pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Mar 23 20:19:23.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3875" for this suite. 03/23/23 20:19:23.419
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":316,"skipped":5896,"failed":0}
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:17.299
    Mar 23 20:19:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 20:19:17.3
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:17.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:17.337
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-9bdf9cdb-b194-4e76-b7d8-02194a53ff80 03/23/23 20:19:17.34
    STEP: Creating a pod to test consume secrets 03/23/23 20:19:17.346
    Mar 23 20:19:17.356: INFO: Waiting up to 5m0s for pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a" in namespace "secrets-3875" to be "Succeeded or Failed"
    Mar 23 20:19:17.374: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.27596ms
    Mar 23 20:19:19.380: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023957023s
    Mar 23 20:19:21.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023145998s
    Mar 23 20:19:23.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022676572s
    STEP: Saw pod success 03/23/23 20:19:23.379
    Mar 23 20:19:23.379: INFO: Pod "pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a" satisfied condition "Succeeded or Failed"
    Mar 23 20:19:23.382: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a container secret-volume-test: <nil>
    STEP: delete the pod 03/23/23 20:19:23.389
    Mar 23 20:19:23.411: INFO: Waiting for pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a to disappear
    Mar 23 20:19:23.414: INFO: Pod pod-secrets-99b6355c-2fbf-4b6b-96db-6bb60784526a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 20:19:23.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3875" for this suite. 03/23/23 20:19:23.419
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:23.431
Mar 23 20:19:23.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:23.433
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:23.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:23.462
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 03/23/23 20:19:23.465
Mar 23 20:19:23.477: INFO: Waiting up to 5m0s for pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8" in namespace "emptydir-168" to be "Succeeded or Failed"
Mar 23 20:19:23.500: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.97315ms
Mar 23 20:19:25.511: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034484897s
Mar 23 20:19:27.504: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027515485s
STEP: Saw pod success 03/23/23 20:19:27.504
Mar 23 20:19:27.504: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8" satisfied condition "Succeeded or Failed"
Mar 23 20:19:27.508: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 container test-container: <nil>
STEP: delete the pod 03/23/23 20:19:27.515
Mar 23 20:19:27.534: INFO: Waiting for pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 to disappear
Mar 23 20:19:27.537: INFO: Pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:19:27.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-168" for this suite. 03/23/23 20:19:27.541
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5899,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:23.431
    Mar 23 20:19:23.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:23.433
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:23.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:23.462
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/23/23 20:19:23.465
    Mar 23 20:19:23.477: INFO: Waiting up to 5m0s for pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8" in namespace "emptydir-168" to be "Succeeded or Failed"
    Mar 23 20:19:23.500: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.97315ms
    Mar 23 20:19:25.511: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034484897s
    Mar 23 20:19:27.504: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027515485s
    STEP: Saw pod success 03/23/23 20:19:27.504
    Mar 23 20:19:27.504: INFO: Pod "pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8" satisfied condition "Succeeded or Failed"
    Mar 23 20:19:27.508: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:19:27.515
    Mar 23 20:19:27.534: INFO: Waiting for pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 to disappear
    Mar 23 20:19:27.537: INFO: Pod pod-9f5459e1-d421-4f11-8db9-8afd4d8deaa8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:19:27.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-168" for this suite. 03/23/23 20:19:27.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:27.564
Mar 23 20:19:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:19:27.565
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:27.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:27.584
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-3c2706e1-b456-4b4c-87b2-61f902124f1a 03/23/23 20:19:27.592
STEP: Creating the pod 03/23/23 20:19:27.599
Mar 23 20:19:27.629: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86" in namespace "projected-9395" to be "running and ready"
Mar 23 20:19:27.640: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86": Phase="Pending", Reason="", readiness=false. Elapsed: 11.030676ms
Mar 23 20:19:27.640: INFO: The phase of Pod pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:19:29.648: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86": Phase="Running", Reason="", readiness=true. Elapsed: 2.019140129s
Mar 23 20:19:29.648: INFO: The phase of Pod pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86 is Running (Ready = true)
Mar 23 20:19:29.648: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-3c2706e1-b456-4b4c-87b2-61f902124f1a 03/23/23 20:19:29.67
STEP: waiting to observe update in volume 03/23/23 20:19:29.676
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Mar 23 20:19:31.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9395" for this suite. 03/23/23 20:19:31.698
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":318,"skipped":5918,"failed":0}
------------------------------
• [4.144 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:27.564
    Mar 23 20:19:27.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:19:27.565
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:27.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:27.584
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-3c2706e1-b456-4b4c-87b2-61f902124f1a 03/23/23 20:19:27.592
    STEP: Creating the pod 03/23/23 20:19:27.599
    Mar 23 20:19:27.629: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86" in namespace "projected-9395" to be "running and ready"
    Mar 23 20:19:27.640: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86": Phase="Pending", Reason="", readiness=false. Elapsed: 11.030676ms
    Mar 23 20:19:27.640: INFO: The phase of Pod pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:19:29.648: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86": Phase="Running", Reason="", readiness=true. Elapsed: 2.019140129s
    Mar 23 20:19:29.648: INFO: The phase of Pod pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86 is Running (Ready = true)
    Mar 23 20:19:29.648: INFO: Pod "pod-projected-configmaps-ee605193-cee1-4338-ab96-e50a9ac53f86" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-3c2706e1-b456-4b4c-87b2-61f902124f1a 03/23/23 20:19:29.67
    STEP: waiting to observe update in volume 03/23/23 20:19:29.676
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Mar 23 20:19:31.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9395" for this suite. 03/23/23 20:19:31.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:31.717
Mar 23 20:19:31.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 20:19:31.718
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:31.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:31.755
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-246 03/23/23 20:19:31.757
STEP: creating service affinity-nodeport in namespace services-246 03/23/23 20:19:31.758
STEP: creating replication controller affinity-nodeport in namespace services-246 03/23/23 20:19:31.805
I0323 20:19:31.819384      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-246, replica count: 3
I0323 20:19:34.872973      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Mar 23 20:19:34.885: INFO: Creating new exec pod
Mar 23 20:19:34.899: INFO: Waiting up to 5m0s for pod "execpod-affinityzmq6b" in namespace "services-246" to be "running"
Mar 23 20:19:34.910: INFO: Pod "execpod-affinityzmq6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.565476ms
Mar 23 20:19:36.918: INFO: Pod "execpod-affinityzmq6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.019205427s
Mar 23 20:19:36.918: INFO: Pod "execpod-affinityzmq6b" satisfied condition "running"
Mar 23 20:19:37.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Mar 23 20:19:38.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Mar 23 20:19:38.165: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:19:38.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.159.163 80'
Mar 23 20:19:38.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.159.163 80\nConnection to 10.0.159.163 80 port [tcp/http] succeeded!\n"
Mar 23 20:19:38.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:19:38.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 31129'
Mar 23 20:19:38.621: INFO: stderr: "+ nc -v -t -w 2 10.240.0.4 31129\n+ echo hostName\nConnection to 10.240.0.4 31129 port [tcp/*] succeeded!\n"
Mar 23 20:19:38.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:19:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 31129'
Mar 23 20:19:38.840: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.0.30 31129\nConnection to 10.240.0.30 31129 port [tcp/*] succeeded!\n"
Mar 23 20:19:38.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:19:38.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:31129/ ; done'
Mar 23 20:19:39.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n"
Mar 23 20:19:39.202: INFO: stdout: "\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g"
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
Mar 23 20:19:39.203: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-246, will wait for the garbage collector to delete the pods 03/23/23 20:19:39.215
Mar 23 20:19:39.281: INFO: Deleting ReplicationController affinity-nodeport took: 11.509374ms
Mar 23 20:19:39.382: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.659575ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 20:19:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-246" for this suite. 03/23/23 20:19:43.461
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":319,"skipped":5941,"failed":0}
------------------------------
• [SLOW TEST] [11.765 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:31.717
    Mar 23 20:19:31.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 20:19:31.718
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:31.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:31.755
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-246 03/23/23 20:19:31.757
    STEP: creating service affinity-nodeport in namespace services-246 03/23/23 20:19:31.758
    STEP: creating replication controller affinity-nodeport in namespace services-246 03/23/23 20:19:31.805
    I0323 20:19:31.819384      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-246, replica count: 3
    I0323 20:19:34.872973      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Mar 23 20:19:34.885: INFO: Creating new exec pod
    Mar 23 20:19:34.899: INFO: Waiting up to 5m0s for pod "execpod-affinityzmq6b" in namespace "services-246" to be "running"
    Mar 23 20:19:34.910: INFO: Pod "execpod-affinityzmq6b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.565476ms
    Mar 23 20:19:36.918: INFO: Pod "execpod-affinityzmq6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.019205427s
    Mar 23 20:19:36.918: INFO: Pod "execpod-affinityzmq6b" satisfied condition "running"
    Mar 23 20:19:37.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Mar 23 20:19:38.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Mar 23 20:19:38.165: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:19:38.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.159.163 80'
    Mar 23 20:19:38.382: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.159.163 80\nConnection to 10.0.159.163 80 port [tcp/http] succeeded!\n"
    Mar 23 20:19:38.382: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:19:38.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.4 31129'
    Mar 23 20:19:38.621: INFO: stderr: "+ nc -v -t -w 2 10.240.0.4 31129\n+ echo hostName\nConnection to 10.240.0.4 31129 port [tcp/*] succeeded!\n"
    Mar 23 20:19:38.621: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:19:38.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.240.0.30 31129'
    Mar 23 20:19:38.840: INFO: stderr: "+ + echonc -v hostName -t\n -w 2 10.240.0.30 31129\nConnection to 10.240.0.30 31129 port [tcp/*] succeeded!\n"
    Mar 23 20:19:38.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:19:38.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-246 exec execpod-affinityzmq6b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.240.0.30:31129/ ; done'
    Mar 23 20:19:39.202: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.240.0.30:31129/\n"
    Mar 23 20:19:39.202: INFO: stdout: "\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g\naffinity-nodeport-v254g"
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.202: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.203: INFO: Received response from host: affinity-nodeport-v254g
    Mar 23 20:19:39.203: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-246, will wait for the garbage collector to delete the pods 03/23/23 20:19:39.215
    Mar 23 20:19:39.281: INFO: Deleting ReplicationController affinity-nodeport took: 11.509374ms
    Mar 23 20:19:39.382: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.659575ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 20:19:43.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-246" for this suite. 03/23/23 20:19:43.461
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:43.486
Mar 23 20:19:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:43.487
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:43.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:43.515
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 03/23/23 20:19:43.518
Mar 23 20:19:43.533: INFO: Waiting up to 5m0s for pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67" in namespace "emptydir-3333" to be "Succeeded or Failed"
Mar 23 20:19:43.538: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392088ms
Mar 23 20:19:45.555: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02195572s
Mar 23 20:19:47.545: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205601s
STEP: Saw pod success 03/23/23 20:19:47.545
Mar 23 20:19:47.545: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67" satisfied condition "Succeeded or Failed"
Mar 23 20:19:47.549: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 container test-container: <nil>
STEP: delete the pod 03/23/23 20:19:47.556
Mar 23 20:19:47.574: INFO: Waiting for pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 to disappear
Mar 23 20:19:47.578: INFO: Pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:19:47.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3333" for this suite. 03/23/23 20:19:47.584
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":320,"skipped":5943,"failed":0}
------------------------------
• [4.117 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:43.486
    Mar 23 20:19:43.486: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:43.487
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:43.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:43.515
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 03/23/23 20:19:43.518
    Mar 23 20:19:43.533: INFO: Waiting up to 5m0s for pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67" in namespace "emptydir-3333" to be "Succeeded or Failed"
    Mar 23 20:19:43.538: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Pending", Reason="", readiness=false. Elapsed: 5.392088ms
    Mar 23 20:19:45.555: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02195572s
    Mar 23 20:19:47.545: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01205601s
    STEP: Saw pod success 03/23/23 20:19:47.545
    Mar 23 20:19:47.545: INFO: Pod "pod-6bcb25ed-ec7f-427a-8d00-428709afee67" satisfied condition "Succeeded or Failed"
    Mar 23 20:19:47.549: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:19:47.556
    Mar 23 20:19:47.574: INFO: Waiting for pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 to disappear
    Mar 23 20:19:47.578: INFO: Pod pod-6bcb25ed-ec7f-427a-8d00-428709afee67 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:19:47.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3333" for this suite. 03/23/23 20:19:47.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:47.614
Mar 23 20:19:47.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:47.615
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:47.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:47.697
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 03/23/23 20:19:47.701
Mar 23 20:19:47.721: INFO: Waiting up to 5m0s for pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d" in namespace "emptydir-5904" to be "Succeeded or Failed"
Mar 23 20:19:47.726: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89659ms
Mar 23 20:19:49.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011835243s
Mar 23 20:19:51.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Running", Reason="", readiness=false. Elapsed: 4.01230501s
Mar 23 20:19:53.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012006679s
STEP: Saw pod success 03/23/23 20:19:53.733
Mar 23 20:19:53.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d" satisfied condition "Succeeded or Failed"
Mar 23 20:19:53.738: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d container test-container: <nil>
STEP: delete the pod 03/23/23 20:19:53.747
Mar 23 20:19:53.774: INFO: Waiting for pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d to disappear
Mar 23 20:19:53.777: INFO: Pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:19:53.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5904" for this suite. 03/23/23 20:19:53.784
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":321,"skipped":5969,"failed":0}
------------------------------
• [SLOW TEST] [6.176 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:47.614
    Mar 23 20:19:47.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:19:47.615
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:47.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:47.697
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 03/23/23 20:19:47.701
    Mar 23 20:19:47.721: INFO: Waiting up to 5m0s for pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d" in namespace "emptydir-5904" to be "Succeeded or Failed"
    Mar 23 20:19:47.726: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.89659ms
    Mar 23 20:19:49.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Running", Reason="", readiness=true. Elapsed: 2.011835243s
    Mar 23 20:19:51.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Running", Reason="", readiness=false. Elapsed: 4.01230501s
    Mar 23 20:19:53.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012006679s
    STEP: Saw pod success 03/23/23 20:19:53.733
    Mar 23 20:19:53.733: INFO: Pod "pod-41395fa0-e361-48b9-88ee-9b568d5b517d" satisfied condition "Succeeded or Failed"
    Mar 23 20:19:53.738: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d container test-container: <nil>
    STEP: delete the pod 03/23/23 20:19:53.747
    Mar 23 20:19:53.774: INFO: Waiting for pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d to disappear
    Mar 23 20:19:53.777: INFO: Pod pod-41395fa0-e361-48b9-88ee-9b568d5b517d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:19:53.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5904" for this suite. 03/23/23 20:19:53.784
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:53.798
Mar 23 20:19:53.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename events 03/23/23 20:19:53.799
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:53.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:53.841
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 03/23/23 20:19:53.844
STEP: listing all events in all namespaces 03/23/23 20:19:53.853
STEP: patching the test event 03/23/23 20:19:53.858
STEP: fetching the test event 03/23/23 20:19:53.874
STEP: updating the test event 03/23/23 20:19:53.877
STEP: getting the test event 03/23/23 20:19:53.886
STEP: deleting the test event 03/23/23 20:19:53.891
STEP: listing all events in all namespaces 03/23/23 20:19:53.899
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Mar 23 20:19:53.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1494" for this suite. 03/23/23 20:19:53.909
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":322,"skipped":5970,"failed":0}
------------------------------
• [0.118 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:53.798
    Mar 23 20:19:53.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename events 03/23/23 20:19:53.799
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:53.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:53.841
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 03/23/23 20:19:53.844
    STEP: listing all events in all namespaces 03/23/23 20:19:53.853
    STEP: patching the test event 03/23/23 20:19:53.858
    STEP: fetching the test event 03/23/23 20:19:53.874
    STEP: updating the test event 03/23/23 20:19:53.877
    STEP: getting the test event 03/23/23 20:19:53.886
    STEP: deleting the test event 03/23/23 20:19:53.891
    STEP: listing all events in all namespaces 03/23/23 20:19:53.899
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Mar 23 20:19:53.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1494" for this suite. 03/23/23 20:19:53.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:19:53.922
Mar 23 20:19:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename deployment 03/23/23 20:19:53.923
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:53.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:53.951
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Mar 23 20:19:53.964: INFO: Pod name rollover-pod: Found 0 pods out of 1
Mar 23 20:19:58.975: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 03/23/23 20:19:58.975
Mar 23 20:19:58.975: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Mar 23 20:20:00.980: INFO: Creating deployment "test-rollover-deployment"
Mar 23 20:20:00.989: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Mar 23 20:20:02.997: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Mar 23 20:20:03.003: INFO: Ensure that both replica sets have 1 created replica
Mar 23 20:20:03.009: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Mar 23 20:20:03.022: INFO: Updating deployment test-rollover-deployment
Mar 23 20:20:03.022: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Mar 23 20:20:05.032: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Mar 23 20:20:05.039: INFO: Make sure deployment "test-rollover-deployment" is complete
Mar 23 20:20:05.046: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:20:05.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:20:07.054: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:20:07.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:20:09.055: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:20:09.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:20:11.054: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:20:11.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:20:13.053: INFO: all replica sets need to contain the pod-template-hash label
Mar 23 20:20:13.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Mar 23 20:20:15.055: INFO: 
Mar 23 20:20:15.055: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Mar 23 20:20:15.068: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-7858  64896144-e2e8-4331-a6a7-eabfccb721ee 42978 2 2023-03-23 20:20:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000db4d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 20:20:01 +0000 UTC,LastTransitionTime:2023-03-23 20:20:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-23 20:20:14 +0000 UTC,LastTransitionTime:2023-03-23 20:20:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Mar 23 20:20:15.073: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7858  dee82aa1-41f0-4e16-89ae-73409e532a0c 42968 2 2023-03-23 20:20:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc005102367 0xc005102368}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005102418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:20:15.073: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Mar 23 20:20:15.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7858  d419fd85-e948-4702-a101-e7ed9608f3bf 42977 2 2023-03-23 20:19:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc0051020f7 0xc0051020f8}] [] [{e2e.test Update apps/v1 2023-03-23 20:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051021b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:20:15.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7858  7fc425c8-ae64-4fbd-b96b-5fd63fe37b5c 42917 2 2023-03-23 20:20:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc005102237 0xc005102238}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051022f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Mar 23 20:20:15.080: INFO: Pod "test-rollover-deployment-6d45fd857b-9nz52" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-9nz52 test-rollover-deployment-6d45fd857b- deployment-7858  30002cfe-32dc-491f-9425-3d1639e6fade 42933 0 2023-03-23 20:20:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b dee82aa1-41f0-4e16-89ae-73409e532a0c 0xc004cfabf7 0xc004cfabf8}] [] [{kube-controller-manager Update v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dee82aa1-41f0-4e16-89ae-73409e532a0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:20:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5hst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5hst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.63,StartTime:2023-03-23 20:20:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:20:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2b0d67271ef11b3100e926f25ea65eafff931d2d137fce114151f006cbcfce76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Mar 23 20:20:15.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7858" for this suite. 03/23/23 20:20:15.091
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":323,"skipped":5986,"failed":0}
------------------------------
• [SLOW TEST] [21.185 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:19:53.922
    Mar 23 20:19:53.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename deployment 03/23/23 20:19:53.923
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:19:53.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:19:53.951
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Mar 23 20:19:53.964: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Mar 23 20:19:58.975: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 03/23/23 20:19:58.975
    Mar 23 20:19:58.975: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Mar 23 20:20:00.980: INFO: Creating deployment "test-rollover-deployment"
    Mar 23 20:20:00.989: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Mar 23 20:20:02.997: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Mar 23 20:20:03.003: INFO: Ensure that both replica sets have 1 created replica
    Mar 23 20:20:03.009: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Mar 23 20:20:03.022: INFO: Updating deployment test-rollover-deployment
    Mar 23 20:20:03.022: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Mar 23 20:20:05.032: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Mar 23 20:20:05.039: INFO: Make sure deployment "test-rollover-deployment" is complete
    Mar 23 20:20:05.046: INFO: all replica sets need to contain the pod-template-hash label
    Mar 23 20:20:05.046: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 20:20:07.054: INFO: all replica sets need to contain the pod-template-hash label
    Mar 23 20:20:07.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 20:20:09.055: INFO: all replica sets need to contain the pod-template-hash label
    Mar 23 20:20:09.055: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 20:20:11.054: INFO: all replica sets need to contain the pod-template-hash label
    Mar 23 20:20:11.054: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 20:20:13.053: INFO: all replica sets need to contain the pod-template-hash label
    Mar 23 20:20:13.053: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.March, 23, 20, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.March, 23, 20, 20, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Mar 23 20:20:15.055: INFO: 
    Mar 23 20:20:15.055: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Mar 23 20:20:15.068: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-7858  64896144-e2e8-4331-a6a7-eabfccb721ee 42978 2 2023-03-23 20:20:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000db4d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-03-23 20:20:01 +0000 UTC,LastTransitionTime:2023-03-23 20:20:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-03-23 20:20:14 +0000 UTC,LastTransitionTime:2023-03-23 20:20:01 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Mar 23 20:20:15.073: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-7858  dee82aa1-41f0-4e16-89ae-73409e532a0c 42968 2 2023-03-23 20:20:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc005102367 0xc005102368}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005102418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 20:20:15.073: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Mar 23 20:20:15.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-7858  d419fd85-e948-4702-a101-e7ed9608f3bf 42977 2 2023-03-23 20:19:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc0051020f7 0xc0051020f8}] [] [{e2e.test Update apps/v1 2023-03-23 20:19:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0051021b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 20:20:15.073: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-7858  7fc425c8-ae64-4fbd-b96b-5fd63fe37b5c 42917 2 2023-03-23 20:20:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 64896144-e2e8-4331-a6a7-eabfccb721ee 0xc005102237 0xc005102238}] [] [{kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"64896144-e2e8-4331-a6a7-eabfccb721ee\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051022f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Mar 23 20:20:15.080: INFO: Pod "test-rollover-deployment-6d45fd857b-9nz52" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-9nz52 test-rollover-deployment-6d45fd857b- deployment-7858  30002cfe-32dc-491f-9425-3d1639e6fade 42933 0 2023-03-23 20:20:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b dee82aa1-41f0-4e16-89ae-73409e532a0c 0xc004cfabf7 0xc004cfabf8}] [] [{kube-controller-manager Update v1 2023-03-23 20:20:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dee82aa1-41f0-4e16-89ae-73409e532a0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-03-23 20:20:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.240.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c5hst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c5hst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-linuxpool-16392394-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-03-23 20:20:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.240.0.56,PodIP:10.240.0.63,StartTime:2023-03-23 20:20:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-03-23 20:20:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://2b0d67271ef11b3100e926f25ea65eafff931d2d137fce114151f006cbcfce76,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.240.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Mar 23 20:20:15.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7858" for this suite. 03/23/23 20:20:15.091
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:15.116
Mar 23 20:20:15.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename secrets 03/23/23 20:20:15.117
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:15.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:15.165
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 03/23/23 20:20:15.168
STEP: listing secrets in all namespaces to ensure that there are more than zero 03/23/23 20:20:15.181
STEP: patching the secret 03/23/23 20:20:15.192
STEP: deleting the secret using a LabelSelector 03/23/23 20:20:15.227
STEP: listing secrets in all namespaces, searching for label name and value in patch 03/23/23 20:20:15.249
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Mar 23 20:20:15.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4748" for this suite. 03/23/23 20:20:15.272
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":324,"skipped":6012,"failed":0}
------------------------------
• [0.166 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:15.116
    Mar 23 20:20:15.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename secrets 03/23/23 20:20:15.117
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:15.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:15.165
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 03/23/23 20:20:15.168
    STEP: listing secrets in all namespaces to ensure that there are more than zero 03/23/23 20:20:15.181
    STEP: patching the secret 03/23/23 20:20:15.192
    STEP: deleting the secret using a LabelSelector 03/23/23 20:20:15.227
    STEP: listing secrets in all namespaces, searching for label name and value in patch 03/23/23 20:20:15.249
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Mar 23 20:20:15.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4748" for this suite. 03/23/23 20:20:15.272
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:15.285
Mar 23 20:20:15.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 20:20:15.286
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:15.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:15.319
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 03/23/23 20:20:15.322
Mar 23 20:20:15.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819" in namespace "downward-api-4565" to be "Succeeded or Failed"
Mar 23 20:20:15.370: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Pending", Reason="", readiness=false. Elapsed: 27.883638ms
Mar 23 20:20:17.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032834494s
Mar 23 20:20:19.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03277396s
STEP: Saw pod success 03/23/23 20:20:19.375
Mar 23 20:20:19.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819" satisfied condition "Succeeded or Failed"
Mar 23 20:20:19.379: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 container client-container: <nil>
STEP: delete the pod 03/23/23 20:20:19.387
Mar 23 20:20:19.408: INFO: Waiting for pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 to disappear
Mar 23 20:20:19.412: INFO: Pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 20:20:19.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4565" for this suite. 03/23/23 20:20:19.418
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":325,"skipped":6013,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:15.285
    Mar 23 20:20:15.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 20:20:15.286
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:15.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:15.319
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 03/23/23 20:20:15.322
    Mar 23 20:20:15.342: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819" in namespace "downward-api-4565" to be "Succeeded or Failed"
    Mar 23 20:20:15.370: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Pending", Reason="", readiness=false. Elapsed: 27.883638ms
    Mar 23 20:20:17.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032834494s
    Mar 23 20:20:19.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03277396s
    STEP: Saw pod success 03/23/23 20:20:19.375
    Mar 23 20:20:19.375: INFO: Pod "downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819" satisfied condition "Succeeded or Failed"
    Mar 23 20:20:19.379: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 container client-container: <nil>
    STEP: delete the pod 03/23/23 20:20:19.387
    Mar 23 20:20:19.408: INFO: Waiting for pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 to disappear
    Mar 23 20:20:19.412: INFO: Pod downwardapi-volume-b621a97b-426d-48dd-8f2a-2e0485cf6819 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 20:20:19.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4565" for this suite. 03/23/23 20:20:19.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:19.449
Mar 23 20:20:19.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 20:20:19.45
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:19.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:19.475
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 03/23/23 20:20:19.485
Mar 23 20:20:19.508: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6744" to be "running and ready"
Mar 23 20:20:19.513: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90029ms
Mar 23 20:20:19.513: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:20:21.518: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010361444s
Mar 23 20:20:21.518: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Mar 23 20:20:21.518: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 03/23/23 20:20:21.523
Mar 23 20:20:21.538: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6744" to be "running and ready"
Mar 23 20:20:21.557: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.148457ms
Mar 23 20:20:21.557: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:20:23.564: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026733106s
Mar 23 20:20:23.564: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Mar 23 20:20:23.564: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 03/23/23 20:20:23.568
Mar 23 20:20:23.578: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 20:20:23.583: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 20:20:25.584: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 20:20:25.593: INFO: Pod pod-with-prestop-http-hook still exists
Mar 23 20:20:27.583: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Mar 23 20:20:27.589: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 03/23/23 20:20:27.589
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Mar 23 20:20:27.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6744" for this suite. 03/23/23 20:20:27.635
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":326,"skipped":6038,"failed":0}
------------------------------
• [SLOW TEST] [8.198 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:19.449
    Mar 23 20:20:19.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-lifecycle-hook 03/23/23 20:20:19.45
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:19.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:19.475
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 03/23/23 20:20:19.485
    Mar 23 20:20:19.508: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6744" to be "running and ready"
    Mar 23 20:20:19.513: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.90029ms
    Mar 23 20:20:19.513: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:20:21.518: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010361444s
    Mar 23 20:20:21.518: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Mar 23 20:20:21.518: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 03/23/23 20:20:21.523
    Mar 23 20:20:21.538: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6744" to be "running and ready"
    Mar 23 20:20:21.557: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 19.148457ms
    Mar 23 20:20:21.557: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:20:23.564: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.026733106s
    Mar 23 20:20:23.564: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Mar 23 20:20:23.564: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 03/23/23 20:20:23.568
    Mar 23 20:20:23.578: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 23 20:20:23.583: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 23 20:20:25.584: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 23 20:20:25.593: INFO: Pod pod-with-prestop-http-hook still exists
    Mar 23 20:20:27.583: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Mar 23 20:20:27.589: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 03/23/23 20:20:27.589
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Mar 23 20:20:27.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6744" for this suite. 03/23/23 20:20:27.635
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:27.648
Mar 23 20:20:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:20:27.652
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:27.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:27.678
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 03/23/23 20:20:27.681
Mar 23 20:20:27.697: INFO: Waiting up to 5m0s for pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738" in namespace "emptydir-8088" to be "Succeeded or Failed"
Mar 23 20:20:27.709: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Pending", Reason="", readiness=false. Elapsed: 12.379872ms
Mar 23 20:20:29.715: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Running", Reason="", readiness=true. Elapsed: 2.017602025s
Mar 23 20:20:31.717: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Running", Reason="", readiness=false. Elapsed: 4.019868085s
Mar 23 20:20:33.714: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017142155s
STEP: Saw pod success 03/23/23 20:20:33.714
Mar 23 20:20:33.714: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738" satisfied condition "Succeeded or Failed"
Mar 23 20:20:33.719: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 container test-container: <nil>
STEP: delete the pod 03/23/23 20:20:33.727
Mar 23 20:20:33.745: INFO: Waiting for pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 to disappear
Mar 23 20:20:33.748: INFO: Pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:20:33.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8088" for this suite. 03/23/23 20:20:33.757
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":327,"skipped":6039,"failed":0}
------------------------------
• [SLOW TEST] [6.126 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:27.648
    Mar 23 20:20:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:20:27.652
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:27.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:27.678
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 03/23/23 20:20:27.681
    Mar 23 20:20:27.697: INFO: Waiting up to 5m0s for pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738" in namespace "emptydir-8088" to be "Succeeded or Failed"
    Mar 23 20:20:27.709: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Pending", Reason="", readiness=false. Elapsed: 12.379872ms
    Mar 23 20:20:29.715: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Running", Reason="", readiness=true. Elapsed: 2.017602025s
    Mar 23 20:20:31.717: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Running", Reason="", readiness=false. Elapsed: 4.019868085s
    Mar 23 20:20:33.714: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017142155s
    STEP: Saw pod success 03/23/23 20:20:33.714
    Mar 23 20:20:33.714: INFO: Pod "pod-5b5056ee-6acf-4030-b37c-99be6cb67738" satisfied condition "Succeeded or Failed"
    Mar 23 20:20:33.719: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:20:33.727
    Mar 23 20:20:33.745: INFO: Waiting for pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 to disappear
    Mar 23 20:20:33.748: INFO: Pod pod-5b5056ee-6acf-4030-b37c-99be6cb67738 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:20:33.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8088" for this suite. 03/23/23 20:20:33.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:33.786
Mar 23 20:20:33.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 20:20:33.788
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:33.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:33.823
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Mar 23 20:20:33.869: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 03/23/23 20:20:33.876
Mar 23 20:20:33.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:33.888: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 03/23/23 20:20:33.888
Mar 23 20:20:33.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:33.930: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:34.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:34.940: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:35.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 23 20:20:35.938: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 03/23/23 20:20:35.944
Mar 23 20:20:35.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:35.997: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/23/23 20:20:35.997
Mar 23 20:20:36.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:36.032: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:37.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:37.039: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:38.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:38.042: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:39.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:39.039: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:40.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:40.040: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:20:41.038: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Mar 23 20:20:41.038: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:20:41.046
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5091, will wait for the garbage collector to delete the pods 03/23/23 20:20:41.046
Mar 23 20:20:41.107: INFO: Deleting DaemonSet.extensions daemon-set took: 7.196584ms
Mar 23 20:20:41.208: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.596577ms
Mar 23 20:20:43.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:20:43.612: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 20:20:43.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43275"},"items":null}

Mar 23 20:20:43.618: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43275"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 20:20:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5091" for this suite. 03/23/23 20:20:43.676
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":328,"skipped":6066,"failed":0}
------------------------------
• [SLOW TEST] [9.902 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:33.786
    Mar 23 20:20:33.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 20:20:33.788
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:33.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:33.823
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Mar 23 20:20:33.869: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 03/23/23 20:20:33.876
    Mar 23 20:20:33.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:33.888: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 03/23/23 20:20:33.888
    Mar 23 20:20:33.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:33.930: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:34.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:34.940: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:35.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 23 20:20:35.938: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 03/23/23 20:20:35.944
    Mar 23 20:20:35.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:35.997: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 03/23/23 20:20:35.997
    Mar 23 20:20:36.031: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:36.032: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:37.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:37.039: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:38.042: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:38.042: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:39.039: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:39.039: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:40.040: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:40.040: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:20:41.038: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Mar 23 20:20:41.038: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:20:41.046
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5091, will wait for the garbage collector to delete the pods 03/23/23 20:20:41.046
    Mar 23 20:20:41.107: INFO: Deleting DaemonSet.extensions daemon-set took: 7.196584ms
    Mar 23 20:20:41.208: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.596577ms
    Mar 23 20:20:43.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:20:43.612: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 20:20:43.615: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43275"},"items":null}

    Mar 23 20:20:43.618: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43275"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 20:20:43.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5091" for this suite. 03/23/23 20:20:43.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:43.721
Mar 23 20:20:43.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 20:20:43.728
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:43.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:43.754
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/23/23 20:20:43.764
Mar 23 20:20:43.781: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6092" to be "running and ready"
Mar 23 20:20:43.785: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934891ms
Mar 23 20:20:43.785: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:20:45.790: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867842s
Mar 23 20:20:45.790: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Mar 23 20:20:45.790: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 03/23/23 20:20:45.793
STEP: Then the orphan pod is adopted 03/23/23 20:20:45.804
STEP: When the matched label of one of its pods change 03/23/23 20:20:45.813
Mar 23 20:20:45.818: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 03/23/23 20:20:45.832
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 20:20:45.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6092" for this suite. 03/23/23 20:20:45.863
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":329,"skipped":6090,"failed":0}
------------------------------
• [2.160 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:43.721
    Mar 23 20:20:43.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 20:20:43.728
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:43.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:43.754
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 03/23/23 20:20:43.764
    Mar 23 20:20:43.781: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-6092" to be "running and ready"
    Mar 23 20:20:43.785: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.934891ms
    Mar 23 20:20:43.785: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:20:45.790: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867842s
    Mar 23 20:20:45.790: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Mar 23 20:20:45.790: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 03/23/23 20:20:45.793
    STEP: Then the orphan pod is adopted 03/23/23 20:20:45.804
    STEP: When the matched label of one of its pods change 03/23/23 20:20:45.813
    Mar 23 20:20:45.818: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 03/23/23 20:20:45.832
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 20:20:45.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6092" for this suite. 03/23/23 20:20:45.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:45.884
Mar 23 20:20:45.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:20:45.889
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:45.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:45.912
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 03/23/23 20:20:45.915
Mar 23 20:20:45.935: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af" in namespace "emptydir-7908" to be "running"
Mar 23 20:20:45.944: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845478ms
Mar 23 20:20:47.949: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01449343s
Mar 23 20:20:49.950: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Running", Reason="", readiness=false. Elapsed: 4.015166892s
Mar 23 20:20:49.950: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af" satisfied condition "running"
STEP: Reading file content from the nginx-container 03/23/23 20:20:49.95
Mar 23 20:20:49.950: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7908 PodName:pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:20:49.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:20:49.951: INFO: ExecWithOptions: Clientset creation
Mar 23 20:20:49.951: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/emptydir-7908/pods/pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Mar 23 20:20:50.069: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:20:50.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7908" for this suite. 03/23/23 20:20:50.076
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":330,"skipped":6100,"failed":0}
------------------------------
• [4.200 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:45.884
    Mar 23 20:20:45.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:20:45.889
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:45.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:45.912
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 03/23/23 20:20:45.915
    Mar 23 20:20:45.935: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af" in namespace "emptydir-7908" to be "running"
    Mar 23 20:20:45.944: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Pending", Reason="", readiness=false. Elapsed: 9.845478ms
    Mar 23 20:20:47.949: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01449343s
    Mar 23 20:20:49.950: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af": Phase="Running", Reason="", readiness=false. Elapsed: 4.015166892s
    Mar 23 20:20:49.950: INFO: Pod "pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af" satisfied condition "running"
    STEP: Reading file content from the nginx-container 03/23/23 20:20:49.95
    Mar 23 20:20:49.950: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7908 PodName:pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:20:49.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:20:49.951: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:20:49.951: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/emptydir-7908/pods/pod-sharedvolume-c1acfe20-0bc5-4713-9432-39693860a9af/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Mar 23 20:20:50.069: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:20:50.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7908" for this suite. 03/23/23 20:20:50.076
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:20:50.092
Mar 23 20:20:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 20:20:50.093
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:50.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:50.13
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/23/23 20:20:50.133
Mar 23 20:20:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:20:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:21:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4019" for this suite. 03/23/23 20:21:07.112
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":331,"skipped":6100,"failed":0}
------------------------------
• [SLOW TEST] [17.026 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:20:50.092
    Mar 23 20:20:50.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename crd-publish-openapi 03/23/23 20:20:50.093
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:20:50.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:20:50.13
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 03/23/23 20:20:50.133
    Mar 23 20:20:50.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:20:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:21:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4019" for this suite. 03/23/23 20:21:07.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:21:07.13
Mar 23 20:21:07.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename daemonsets 03/23/23 20:21:07.131
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:07.15
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:07.154
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 03/23/23 20:21:07.186
STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 20:21:07.191
Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:07.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:21:07.203: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:08.218: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:08.219: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:08.219: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:08.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:21:08.228: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:09.210: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:09.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:09.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:09.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:09.215: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 20:21:10.215: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 03/23/23 20:21:10.218
Mar 23 20:21:10.247: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.248: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.249: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:10.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:10.254: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:11.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:11.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:11.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:11.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:11.265: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:12.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:12.272: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:13.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:13.266: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:14.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Mar 23 20:21:14.266: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Mar 23 20:21:15.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Mar 23 20:21:15.266: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:21:15.269
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7500, will wait for the garbage collector to delete the pods 03/23/23 20:21:15.27
Mar 23 20:21:15.333: INFO: Deleting DaemonSet.extensions daemon-set took: 10.477977ms
Mar 23 20:21:15.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.680677ms
Mar 23 20:21:17.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Mar 23 20:21:17.741: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Mar 23 20:21:17.744: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43585"},"items":null}

Mar 23 20:21:17.747: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43585"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Mar 23 20:21:17.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7500" for this suite. 03/23/23 20:21:17.781
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":332,"skipped":6125,"failed":0}
------------------------------
• [SLOW TEST] [10.674 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:21:07.13
    Mar 23 20:21:07.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename daemonsets 03/23/23 20:21:07.131
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:07.15
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:07.154
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 03/23/23 20:21:07.186
    STEP: Check that daemon pods launch on every node of the cluster. 03/23/23 20:21:07.191
    Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:07.197: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:07.203: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:21:07.203: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:08.218: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:08.219: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:08.219: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:08.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:21:08.228: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:09.210: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:09.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:09.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:09.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:09.215: INFO: Node k8s-linuxpool-16392394-1 is running 0 daemon pod, expected 1
    Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.211: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 20:21:10.215: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 03/23/23 20:21:10.218
    Mar 23 20:21:10.247: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.248: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.249: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:10.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:10.254: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:11.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:11.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:11.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:11.265: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:11.265: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:12.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:12.272: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:12.272: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:13.262: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:13.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:13.266: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:14.261: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:14.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Mar 23 20:21:14.266: INFO: Node k8s-linuxpool-16392394-0 is running 0 daemon pod, expected 1
    Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-0 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-1 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:15.260: INFO: DaemonSet pods can't tolerate node k8s-master-16392394-2 with taints [{Key:node-role.kubernetes.io/master Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Mar 23 20:21:15.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Mar 23 20:21:15.266: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 03/23/23 20:21:15.269
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7500, will wait for the garbage collector to delete the pods 03/23/23 20:21:15.27
    Mar 23 20:21:15.333: INFO: Deleting DaemonSet.extensions daemon-set took: 10.477977ms
    Mar 23 20:21:15.434: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.680677ms
    Mar 23 20:21:17.741: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Mar 23 20:21:17.741: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Mar 23 20:21:17.744: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43585"},"items":null}

    Mar 23 20:21:17.747: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43585"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Mar 23 20:21:17.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7500" for this suite. 03/23/23 20:21:17.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:21:17.822
Mar 23 20:21:17.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 20:21:17.823
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:17.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:17.852
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Mar 23 20:21:17.865: INFO: Waiting up to 2m0s for pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" in namespace "var-expansion-7142" to be "container 0 failed with reason CreateContainerConfigError"
Mar 23 20:21:17.876: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378577ms
Mar 23 20:21:19.880: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014684562s
Mar 23 20:21:19.880: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 23 20:21:19.880: INFO: Deleting pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" in namespace "var-expansion-7142"
Mar 23 20:21:19.889: INFO: Wait up to 5m0s for pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 20:21:21.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7142" for this suite. 03/23/23 20:21:21.904
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":333,"skipped":6164,"failed":0}
------------------------------
• [4.097 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:21:17.822
    Mar 23 20:21:17.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 20:21:17.823
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:17.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:17.852
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Mar 23 20:21:17.865: INFO: Waiting up to 2m0s for pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" in namespace "var-expansion-7142" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 23 20:21:17.876: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.378577ms
    Mar 23 20:21:19.880: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014684562s
    Mar 23 20:21:19.880: INFO: Pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 23 20:21:19.880: INFO: Deleting pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" in namespace "var-expansion-7142"
    Mar 23 20:21:19.889: INFO: Wait up to 5m0s for pod "var-expansion-8d9f32da-53e1-4a95-84f7-3d1727b2ad9a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 20:21:21.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7142" for this suite. 03/23/23 20:21:21.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:21:21.921
Mar 23 20:21:21.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pod-network-test 03/23/23 20:21:21.924
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:21.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:21.961
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6316 03/23/23 20:21:21.964
STEP: creating a selector 03/23/23 20:21:21.964
STEP: Creating the service pods in kubernetes 03/23/23 20:21:21.964
Mar 23 20:21:21.965: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 23 20:21:22.034: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6316" to be "running and ready"
Mar 23 20:21:22.041: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.775685ms
Mar 23 20:21:22.041: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:21:24.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012175981s
Mar 23 20:21:24.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:26.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012348388s
Mar 23 20:21:26.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:28.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.0119741s
Mar 23 20:21:28.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:30.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011876111s
Mar 23 20:21:30.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:32.048: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014005618s
Mar 23 20:21:32.048: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:34.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011305336s
Mar 23 20:21:34.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:36.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012166005s
Mar 23 20:21:36.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:38.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012261353s
Mar 23 20:21:38.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:40.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012113602s
Mar 23 20:21:40.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:42.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011358952s
Mar 23 20:21:42.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:21:44.048: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013301303s
Mar 23 20:21:44.048: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 23 20:21:44.048: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 23 20:21:44.054: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6316" to be "running and ready"
Mar 23 20:21:44.059: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.85049ms
Mar 23 20:21:44.059: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 23 20:21:44.059: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 23 20:21:44.064: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6316" to be "running and ready"
Mar 23 20:21:44.067: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.472892ms
Mar 23 20:21:44.067: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 23 20:21:44.067: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/23/23 20:21:44.07
W0323 20:21:44.085485      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
Mar 23 20:21:44.085: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6316" to be "running"
Mar 23 20:21:44.089: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.615692ms
Mar 23 20:21:46.095: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765848s
Mar 23 20:21:48.093: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007611723s
Mar 23 20:21:48.093: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 23 20:21:48.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6316" to be "running"
Mar 23 20:21:48.099: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.728394ms
Mar 23 20:21:48.099: INFO: Pod "host-test-container-pod" satisfied condition "running"
Mar 23 20:21:48.102: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 23 20:21:48.102: INFO: Going to poll 10.240.0.31 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 23 20:21:48.105: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:21:48.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:21:48.106: INFO: ExecWithOptions: Clientset creation
Mar 23 20:21:48.106: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.31+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 20:21:49.224: INFO: Found all 1 expected endpoints: [netserver-0]
Mar 23 20:21:49.224: INFO: Going to poll 10.240.0.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 23 20:21:49.230: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:21:49.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:21:49.230: INFO: ExecWithOptions: Clientset creation
Mar 23 20:21:49.231: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 20:21:50.386: INFO: Found all 1 expected endpoints: [netserver-1]
Mar 23 20:21:50.387: INFO: Going to poll 10.240.0.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Mar 23 20:21:50.392: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:21:50.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:21:50.393: INFO: ExecWithOptions: Clientset creation
Mar 23 20:21:50.393: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Mar 23 20:21:51.538: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 23 20:21:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6316" for this suite. 03/23/23 20:21:51.544
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6183,"failed":0}
------------------------------
• [SLOW TEST] [29.629 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:21:21.921
    Mar 23 20:21:21.921: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pod-network-test 03/23/23 20:21:21.924
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:21.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:21.961
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6316 03/23/23 20:21:21.964
    STEP: creating a selector 03/23/23 20:21:21.964
    STEP: Creating the service pods in kubernetes 03/23/23 20:21:21.964
    Mar 23 20:21:21.965: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 23 20:21:22.034: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6316" to be "running and ready"
    Mar 23 20:21:22.041: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.775685ms
    Mar 23 20:21:22.041: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:21:24.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.012175981s
    Mar 23 20:21:24.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:26.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.012348388s
    Mar 23 20:21:26.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:28.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.0119741s
    Mar 23 20:21:28.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:30.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011876111s
    Mar 23 20:21:30.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:32.048: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014005618s
    Mar 23 20:21:32.048: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:34.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011305336s
    Mar 23 20:21:34.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:36.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012166005s
    Mar 23 20:21:36.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:38.047: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012261353s
    Mar 23 20:21:38.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:40.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012113602s
    Mar 23 20:21:40.047: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:42.046: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011358952s
    Mar 23 20:21:42.046: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:21:44.048: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.013301303s
    Mar 23 20:21:44.048: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 23 20:21:44.048: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 23 20:21:44.054: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6316" to be "running and ready"
    Mar 23 20:21:44.059: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.85049ms
    Mar 23 20:21:44.059: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 23 20:21:44.059: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 23 20:21:44.064: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6316" to be "running and ready"
    Mar 23 20:21:44.067: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.472892ms
    Mar 23 20:21:44.067: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 23 20:21:44.067: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/23/23 20:21:44.07
    W0323 20:21:44.085485      19 warnings.go:70] would violate PodSecurity "baseline:latest": host namespaces (hostNetwork=true)
    Mar 23 20:21:44.085: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6316" to be "running"
    Mar 23 20:21:44.089: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.615692ms
    Mar 23 20:21:46.095: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009765848s
    Mar 23 20:21:48.093: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007611723s
    Mar 23 20:21:48.093: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 23 20:21:48.096: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6316" to be "running"
    Mar 23 20:21:48.099: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.728394ms
    Mar 23 20:21:48.099: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Mar 23 20:21:48.102: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 23 20:21:48.102: INFO: Going to poll 10.240.0.31 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 20:21:48.105: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.31 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:21:48.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:21:48.106: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:21:48.106: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.31+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 20:21:49.224: INFO: Found all 1 expected endpoints: [netserver-0]
    Mar 23 20:21:49.224: INFO: Going to poll 10.240.0.59 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 20:21:49.230: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.59 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:21:49.230: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:21:49.230: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:21:49.231: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.59+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 20:21:50.386: INFO: Found all 1 expected endpoints: [netserver-1]
    Mar 23 20:21:50.387: INFO: Going to poll 10.240.0.17 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Mar 23 20:21:50.392: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.240.0.17 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6316 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:21:50.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:21:50.393: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:21:50.393: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6316/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.240.0.17+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Mar 23 20:21:51.538: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 23 20:21:51.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6316" for this suite. 03/23/23 20:21:51.544
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:21:51.569
Mar 23 20:21:51.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context 03/23/23 20:21:51.57
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:51.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:51.601
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/23/23 20:21:51.604
Mar 23 20:21:51.612: INFO: Waiting up to 5m0s for pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7" in namespace "security-context-4082" to be "Succeeded or Failed"
Mar 23 20:21:51.615: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882993ms
Mar 23 20:21:53.620: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008021369s
Mar 23 20:21:55.622: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Running", Reason="", readiness=false. Elapsed: 4.009277754s
Mar 23 20:21:57.619: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006864646s
STEP: Saw pod success 03/23/23 20:21:57.619
Mar 23 20:21:57.619: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7" satisfied condition "Succeeded or Failed"
Mar 23 20:21:57.623: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 container test-container: <nil>
STEP: delete the pod 03/23/23 20:21:57.658
Mar 23 20:21:57.675: INFO: Waiting for pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 to disappear
Mar 23 20:21:57.679: INFO: Pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 20:21:57.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4082" for this suite. 03/23/23 20:21:57.684
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":335,"skipped":6218,"failed":0}
------------------------------
• [SLOW TEST] [6.127 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:21:51.569
    Mar 23 20:21:51.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context 03/23/23 20:21:51.57
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:51.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:51.601
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 03/23/23 20:21:51.604
    Mar 23 20:21:51.612: INFO: Waiting up to 5m0s for pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7" in namespace "security-context-4082" to be "Succeeded or Failed"
    Mar 23 20:21:51.615: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.882993ms
    Mar 23 20:21:53.620: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008021369s
    Mar 23 20:21:55.622: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Running", Reason="", readiness=false. Elapsed: 4.009277754s
    Mar 23 20:21:57.619: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006864646s
    STEP: Saw pod success 03/23/23 20:21:57.619
    Mar 23 20:21:57.619: INFO: Pod "security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7" satisfied condition "Succeeded or Failed"
    Mar 23 20:21:57.623: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:21:57.658
    Mar 23 20:21:57.675: INFO: Waiting for pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 to disappear
    Mar 23 20:21:57.679: INFO: Pod security-context-e9ed4a7f-8951-4d91-a077-8acf03eb55a7 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 20:21:57.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4082" for this suite. 03/23/23 20:21:57.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:21:57.706
Mar 23 20:21:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:21:57.708
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:57.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:57.73
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 03/23/23 20:21:57.733
Mar 23 20:21:57.746: INFO: Waiting up to 5m0s for pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657" in namespace "emptydir-9766" to be "Succeeded or Failed"
Mar 23 20:21:57.751: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Pending", Reason="", readiness=false. Elapsed: 4.67809ms
Mar 23 20:21:59.755: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008648696s
Mar 23 20:22:01.756: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Running", Reason="", readiness=true. Elapsed: 4.00964975s
Mar 23 20:22:03.758: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Running", Reason="", readiness=false. Elapsed: 6.011720702s
Mar 23 20:22:05.759: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012597357s
STEP: Saw pod success 03/23/23 20:22:05.759
Mar 23 20:22:05.759: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657" satisfied condition "Succeeded or Failed"
Mar 23 20:22:05.762: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 container test-container: <nil>
STEP: delete the pod 03/23/23 20:22:05.77
Mar 23 20:22:05.788: INFO: Waiting for pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 to disappear
Mar 23 20:22:05.791: INFO: Pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:22:05.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9766" for this suite. 03/23/23 20:22:05.797
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":336,"skipped":6272,"failed":0}
------------------------------
• [SLOW TEST] [8.101 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:21:57.706
    Mar 23 20:21:57.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:21:57.708
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:21:57.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:21:57.73
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 03/23/23 20:21:57.733
    Mar 23 20:21:57.746: INFO: Waiting up to 5m0s for pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657" in namespace "emptydir-9766" to be "Succeeded or Failed"
    Mar 23 20:21:57.751: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Pending", Reason="", readiness=false. Elapsed: 4.67809ms
    Mar 23 20:21:59.755: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008648696s
    Mar 23 20:22:01.756: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Running", Reason="", readiness=true. Elapsed: 4.00964975s
    Mar 23 20:22:03.758: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Running", Reason="", readiness=false. Elapsed: 6.011720702s
    Mar 23 20:22:05.759: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.012597357s
    STEP: Saw pod success 03/23/23 20:22:05.759
    Mar 23 20:22:05.759: INFO: Pod "pod-3772f780-171d-40c6-8f24-6a7ecbf56657" satisfied condition "Succeeded or Failed"
    Mar 23 20:22:05.762: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:22:05.77
    Mar 23 20:22:05.788: INFO: Waiting for pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 to disappear
    Mar 23 20:22:05.791: INFO: Pod pod-3772f780-171d-40c6-8f24-6a7ecbf56657 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:22:05.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9766" for this suite. 03/23/23 20:22:05.797
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:05.814
Mar 23 20:22:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename runtimeclass 03/23/23 20:22:05.815
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:05.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:05.844
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Mar 23 20:22:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3662" for this suite. 03/23/23 20:22:05.872
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":337,"skipped":6272,"failed":0}
------------------------------
• [0.069 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:05.814
    Mar 23 20:22:05.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename runtimeclass 03/23/23 20:22:05.815
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:05.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:05.844
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Mar 23 20:22:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3662" for this suite. 03/23/23 20:22:05.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:05.894
Mar 23 20:22:05.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename resourcequota 03/23/23 20:22:05.895
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:05.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:05.923
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 03/23/23 20:22:05.927
STEP: Getting a ResourceQuota 03/23/23 20:22:05.935
STEP: Listing all ResourceQuotas with LabelSelector 03/23/23 20:22:05.938
STEP: Patching the ResourceQuota 03/23/23 20:22:05.941
STEP: Deleting a Collection of ResourceQuotas 03/23/23 20:22:05.947
STEP: Verifying the deleted ResourceQuota 03/23/23 20:22:05.965
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Mar 23 20:22:05.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1273" for this suite. 03/23/23 20:22:05.973
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":338,"skipped":6335,"failed":0}
------------------------------
• [0.086 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:05.894
    Mar 23 20:22:05.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename resourcequota 03/23/23 20:22:05.895
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:05.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:05.923
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 03/23/23 20:22:05.927
    STEP: Getting a ResourceQuota 03/23/23 20:22:05.935
    STEP: Listing all ResourceQuotas with LabelSelector 03/23/23 20:22:05.938
    STEP: Patching the ResourceQuota 03/23/23 20:22:05.941
    STEP: Deleting a Collection of ResourceQuotas 03/23/23 20:22:05.947
    STEP: Verifying the deleted ResourceQuota 03/23/23 20:22:05.965
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Mar 23 20:22:05.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1273" for this suite. 03/23/23 20:22:05.973
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:05.981
Mar 23 20:22:05.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 20:22:05.983
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:06.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:06.009
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2121 03/23/23 20:22:06.011
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/23/23 20:22:06.029
STEP: creating service externalsvc in namespace services-2121 03/23/23 20:22:06.03
STEP: creating replication controller externalsvc in namespace services-2121 03/23/23 20:22:06.05
I0323 20:22:06.075692      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
I0323 20:22:09.126898      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0323 20:22:12.127077      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 03/23/23 20:22:12.13
Mar 23 20:22:12.153: INFO: Creating new exec pod
Mar 23 20:22:12.184: INFO: Waiting up to 5m0s for pod "execpodj4cvp" in namespace "services-2121" to be "running"
Mar 23 20:22:12.190: INFO: Pod "execpodj4cvp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993989ms
Mar 23 20:22:14.196: INFO: Pod "execpodj4cvp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01183185s
Mar 23 20:22:14.197: INFO: Pod "execpodj4cvp" satisfied condition "running"
Mar 23 20:22:14.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-2121 exec execpodj4cvp -- /bin/sh -x -c nslookup clusterip-service.services-2121.svc.cluster.local'
Mar 23 20:22:14.480: INFO: stderr: "+ nslookup clusterip-service.services-2121.svc.cluster.local\n"
Mar 23 20:22:14.480: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 10.0.107.136\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 03/23/23 20:22:14.48
Mar 23 20:22:14.548: INFO: Deleting ReplicationController externalsvc took: 12.970973ms
Mar 23 20:22:14.649: INFO: Terminating ReplicationController externalsvc pods took: 100.775992ms
Mar 23 20:22:17.886: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 20:22:17.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2121" for this suite. 03/23/23 20:22:17.911
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":339,"skipped":6336,"failed":0}
------------------------------
• [SLOW TEST] [11.940 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:05.981
    Mar 23 20:22:05.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 20:22:05.983
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:06.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:06.009
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2121 03/23/23 20:22:06.011
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/23/23 20:22:06.029
    STEP: creating service externalsvc in namespace services-2121 03/23/23 20:22:06.03
    STEP: creating replication controller externalsvc in namespace services-2121 03/23/23 20:22:06.05
    I0323 20:22:06.075692      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
    I0323 20:22:09.126898      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0323 20:22:12.127077      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 03/23/23 20:22:12.13
    Mar 23 20:22:12.153: INFO: Creating new exec pod
    Mar 23 20:22:12.184: INFO: Waiting up to 5m0s for pod "execpodj4cvp" in namespace "services-2121" to be "running"
    Mar 23 20:22:12.190: INFO: Pod "execpodj4cvp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.993989ms
    Mar 23 20:22:14.196: INFO: Pod "execpodj4cvp": Phase="Running", Reason="", readiness=true. Elapsed: 2.01183185s
    Mar 23 20:22:14.197: INFO: Pod "execpodj4cvp" satisfied condition "running"
    Mar 23 20:22:14.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-2121 exec execpodj4cvp -- /bin/sh -x -c nslookup clusterip-service.services-2121.svc.cluster.local'
    Mar 23 20:22:14.480: INFO: stderr: "+ nslookup clusterip-service.services-2121.svc.cluster.local\n"
    Mar 23 20:22:14.480: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nclusterip-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 10.0.107.136\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 03/23/23 20:22:14.48
    Mar 23 20:22:14.548: INFO: Deleting ReplicationController externalsvc took: 12.970973ms
    Mar 23 20:22:14.649: INFO: Terminating ReplicationController externalsvc pods took: 100.775992ms
    Mar 23 20:22:17.886: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 20:22:17.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2121" for this suite. 03/23/23 20:22:17.911
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:17.927
Mar 23 20:22:17.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 20:22:17.928
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:17.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:17.951
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3074 03/23/23 20:22:17.954
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-3074 03/23/23 20:22:17.975
Mar 23 20:22:17.990: INFO: Found 0 stateful pods, waiting for 1
Mar 23 20:22:27.997: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 03/23/23 20:22:28.008
STEP: Getting /status 03/23/23 20:22:28.025
Mar 23 20:22:28.039: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 03/23/23 20:22:28.039
Mar 23 20:22:28.065: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 03/23/23 20:22:28.066
Mar 23 20:22:28.068: INFO: Observed &StatefulSet event: ADDED
Mar 23 20:22:28.068: INFO: Found Statefulset ss in namespace statefulset-3074 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 23 20:22:28.068: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 03/23/23 20:22:28.068
Mar 23 20:22:28.068: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Mar 23 20:22:28.078: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 03/23/23 20:22:28.078
Mar 23 20:22:28.080: INFO: Observed &StatefulSet event: ADDED
Mar 23 20:22:28.080: INFO: Observed Statefulset ss in namespace statefulset-3074 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Mar 23 20:22:28.080: INFO: Observed &StatefulSet event: MODIFIED
Mar 23 20:22:28.080: INFO: Found Statefulset ss in namespace statefulset-3074 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 20:22:28.080: INFO: Deleting all statefulset in ns statefulset-3074
Mar 23 20:22:28.083: INFO: Scaling statefulset ss to 0
Mar 23 20:22:38.112: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 20:22:38.115: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 20:22:38.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3074" for this suite. 03/23/23 20:22:38.133
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":340,"skipped":6336,"failed":0}
------------------------------
• [SLOW TEST] [20.223 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:17.927
    Mar 23 20:22:17.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 20:22:17.928
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:17.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:17.951
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3074 03/23/23 20:22:17.954
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-3074 03/23/23 20:22:17.975
    Mar 23 20:22:17.990: INFO: Found 0 stateful pods, waiting for 1
    Mar 23 20:22:27.997: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 03/23/23 20:22:28.008
    STEP: Getting /status 03/23/23 20:22:28.025
    Mar 23 20:22:28.039: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 03/23/23 20:22:28.039
    Mar 23 20:22:28.065: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 03/23/23 20:22:28.066
    Mar 23 20:22:28.068: INFO: Observed &StatefulSet event: ADDED
    Mar 23 20:22:28.068: INFO: Found Statefulset ss in namespace statefulset-3074 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 23 20:22:28.068: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 03/23/23 20:22:28.068
    Mar 23 20:22:28.068: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Mar 23 20:22:28.078: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 03/23/23 20:22:28.078
    Mar 23 20:22:28.080: INFO: Observed &StatefulSet event: ADDED
    Mar 23 20:22:28.080: INFO: Observed Statefulset ss in namespace statefulset-3074 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Mar 23 20:22:28.080: INFO: Observed &StatefulSet event: MODIFIED
    Mar 23 20:22:28.080: INFO: Found Statefulset ss in namespace statefulset-3074 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 20:22:28.080: INFO: Deleting all statefulset in ns statefulset-3074
    Mar 23 20:22:28.083: INFO: Scaling statefulset ss to 0
    Mar 23 20:22:38.112: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 20:22:38.115: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 20:22:38.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3074" for this suite. 03/23/23 20:22:38.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:38.159
Mar 23 20:22:38.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename svcaccounts 03/23/23 20:22:38.161
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:38.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:38.191
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Mar 23 20:22:38.212: INFO: created pod pod-service-account-defaultsa
Mar 23 20:22:38.212: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Mar 23 20:22:38.219: INFO: created pod pod-service-account-mountsa
Mar 23 20:22:38.219: INFO: pod pod-service-account-mountsa service account token volume mount: true
Mar 23 20:22:38.227: INFO: created pod pod-service-account-nomountsa
Mar 23 20:22:38.227: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Mar 23 20:22:38.236: INFO: created pod pod-service-account-defaultsa-mountspec
Mar 23 20:22:38.236: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Mar 23 20:22:38.241: INFO: created pod pod-service-account-mountsa-mountspec
Mar 23 20:22:38.241: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Mar 23 20:22:38.248: INFO: created pod pod-service-account-nomountsa-mountspec
Mar 23 20:22:38.248: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Mar 23 20:22:38.258: INFO: created pod pod-service-account-defaultsa-nomountspec
Mar 23 20:22:38.258: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Mar 23 20:22:38.267: INFO: created pod pod-service-account-mountsa-nomountspec
Mar 23 20:22:38.267: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Mar 23 20:22:38.275: INFO: created pod pod-service-account-nomountsa-nomountspec
Mar 23 20:22:38.275: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Mar 23 20:22:38.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2857" for this suite. 03/23/23 20:22:38.28
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":341,"skipped":6349,"failed":0}
------------------------------
• [0.160 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:38.159
    Mar 23 20:22:38.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename svcaccounts 03/23/23 20:22:38.161
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:38.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:38.191
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Mar 23 20:22:38.212: INFO: created pod pod-service-account-defaultsa
    Mar 23 20:22:38.212: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Mar 23 20:22:38.219: INFO: created pod pod-service-account-mountsa
    Mar 23 20:22:38.219: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Mar 23 20:22:38.227: INFO: created pod pod-service-account-nomountsa
    Mar 23 20:22:38.227: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Mar 23 20:22:38.236: INFO: created pod pod-service-account-defaultsa-mountspec
    Mar 23 20:22:38.236: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Mar 23 20:22:38.241: INFO: created pod pod-service-account-mountsa-mountspec
    Mar 23 20:22:38.241: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Mar 23 20:22:38.248: INFO: created pod pod-service-account-nomountsa-mountspec
    Mar 23 20:22:38.248: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Mar 23 20:22:38.258: INFO: created pod pod-service-account-defaultsa-nomountspec
    Mar 23 20:22:38.258: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Mar 23 20:22:38.267: INFO: created pod pod-service-account-mountsa-nomountspec
    Mar 23 20:22:38.267: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Mar 23 20:22:38.275: INFO: created pod pod-service-account-nomountsa-nomountspec
    Mar 23 20:22:38.275: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Mar 23 20:22:38.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2857" for this suite. 03/23/23 20:22:38.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:38.325
Mar 23 20:22:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 20:22:38.327
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:38.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:38.372
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 03/23/23 20:22:38.376
Mar 23 20:22:38.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30" in namespace "downward-api-6481" to be "Succeeded or Failed"
Mar 23 20:22:38.407: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Pending", Reason="", readiness=false. Elapsed: 7.661183ms
Mar 23 20:22:40.418: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018450163s
Mar 23 20:22:42.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Running", Reason="", readiness=false. Elapsed: 4.012062183s
Mar 23 20:22:44.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012228888s
STEP: Saw pod success 03/23/23 20:22:44.412
Mar 23 20:22:44.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30" satisfied condition "Succeeded or Failed"
Mar 23 20:22:44.415: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 container client-container: <nil>
STEP: delete the pod 03/23/23 20:22:44.448
Mar 23 20:22:44.462: INFO: Waiting for pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 to disappear
Mar 23 20:22:44.465: INFO: Pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 20:22:44.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6481" for this suite. 03/23/23 20:22:44.472
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":342,"skipped":6357,"failed":0}
------------------------------
• [SLOW TEST] [6.153 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:38.325
    Mar 23 20:22:38.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 20:22:38.327
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:38.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:38.372
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 03/23/23 20:22:38.376
    Mar 23 20:22:38.400: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30" in namespace "downward-api-6481" to be "Succeeded or Failed"
    Mar 23 20:22:38.407: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Pending", Reason="", readiness=false. Elapsed: 7.661183ms
    Mar 23 20:22:40.418: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018450163s
    Mar 23 20:22:42.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Running", Reason="", readiness=false. Elapsed: 4.012062183s
    Mar 23 20:22:44.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012228888s
    STEP: Saw pod success 03/23/23 20:22:44.412
    Mar 23 20:22:44.412: INFO: Pod "downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30" satisfied condition "Succeeded or Failed"
    Mar 23 20:22:44.415: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 container client-container: <nil>
    STEP: delete the pod 03/23/23 20:22:44.448
    Mar 23 20:22:44.462: INFO: Waiting for pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 to disappear
    Mar 23 20:22:44.465: INFO: Pod downwardapi-volume-a99d6d8d-52a6-4a92-ad14-f88fb22c5d30 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 20:22:44.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6481" for this suite. 03/23/23 20:22:44.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:44.479
Mar 23 20:22:44.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename webhook 03/23/23 20:22:44.481
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:44.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:44.5
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 03/23/23 20:22:44.517
STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:22:45.475
STEP: Deploying the webhook pod 03/23/23 20:22:45.486
STEP: Wait for the deployment to be ready 03/23/23 20:22:45.505
Mar 23 20:22:45.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 03/23/23 20:22:47.533
STEP: Verifying the service has paired with the endpoint 03/23/23 20:22:47.578
Mar 23 20:22:48.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 03/23/23 20:22:48.586
STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.604
STEP: Updating a validating webhook configuration's rules to not include the create operation 03/23/23 20:22:48.612
STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.623
STEP: Patching a validating webhook configuration's rules to include the create operation 03/23/23 20:22:48.635
STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.644
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:22:48.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3408" for this suite. 03/23/23 20:22:48.659
STEP: Destroying namespace "webhook-3408-markers" for this suite. 03/23/23 20:22:48.665
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":343,"skipped":6362,"failed":0}
------------------------------
• [4.268 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:44.479
    Mar 23 20:22:44.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename webhook 03/23/23 20:22:44.481
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:44.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:44.5
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 03/23/23 20:22:44.517
    STEP: Create role binding to let webhook read extension-apiserver-authentication 03/23/23 20:22:45.475
    STEP: Deploying the webhook pod 03/23/23 20:22:45.486
    STEP: Wait for the deployment to be ready 03/23/23 20:22:45.505
    Mar 23 20:22:45.521: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 03/23/23 20:22:47.533
    STEP: Verifying the service has paired with the endpoint 03/23/23 20:22:47.578
    Mar 23 20:22:48.579: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 03/23/23 20:22:48.586
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.604
    STEP: Updating a validating webhook configuration's rules to not include the create operation 03/23/23 20:22:48.612
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.623
    STEP: Patching a validating webhook configuration's rules to include the create operation 03/23/23 20:22:48.635
    STEP: Creating a configMap that does not comply to the validation webhook rules 03/23/23 20:22:48.644
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:22:48.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3408" for this suite. 03/23/23 20:22:48.659
    STEP: Destroying namespace "webhook-3408-markers" for this suite. 03/23/23 20:22:48.665
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:48.748
Mar 23 20:22:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename configmap 03/23/23 20:22:48.75
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:48.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:48.796
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-9324f619-2e66-4f30-b017-7ad74562de97 03/23/23 20:22:48.798
STEP: Creating a pod to test consume configMaps 03/23/23 20:22:48.813
Mar 23 20:22:48.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372" in namespace "configmap-8407" to be "Succeeded or Failed"
Mar 23 20:22:48.848: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Pending", Reason="", readiness=false. Elapsed: 17.128661ms
Mar 23 20:22:50.855: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0244066s
Mar 23 20:22:52.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Running", Reason="", readiness=false. Elapsed: 4.021334062s
Mar 23 20:22:54.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021586231s
STEP: Saw pod success 03/23/23 20:22:54.852
Mar 23 20:22:54.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372" satisfied condition "Succeeded or Failed"
Mar 23 20:22:54.855: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 container agnhost-container: <nil>
STEP: delete the pod 03/23/23 20:22:54.862
Mar 23 20:22:54.878: INFO: Waiting for pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 to disappear
Mar 23 20:22:54.881: INFO: Pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Mar 23 20:22:54.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8407" for this suite. 03/23/23 20:22:54.887
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":344,"skipped":6373,"failed":0}
------------------------------
• [SLOW TEST] [6.156 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:48.748
    Mar 23 20:22:48.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename configmap 03/23/23 20:22:48.75
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:48.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:48.796
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-9324f619-2e66-4f30-b017-7ad74562de97 03/23/23 20:22:48.798
    STEP: Creating a pod to test consume configMaps 03/23/23 20:22:48.813
    Mar 23 20:22:48.830: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372" in namespace "configmap-8407" to be "Succeeded or Failed"
    Mar 23 20:22:48.848: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Pending", Reason="", readiness=false. Elapsed: 17.128661ms
    Mar 23 20:22:50.855: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0244066s
    Mar 23 20:22:52.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Running", Reason="", readiness=false. Elapsed: 4.021334062s
    Mar 23 20:22:54.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021586231s
    STEP: Saw pod success 03/23/23 20:22:54.852
    Mar 23 20:22:54.852: INFO: Pod "pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372" satisfied condition "Succeeded or Failed"
    Mar 23 20:22:54.855: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 container agnhost-container: <nil>
    STEP: delete the pod 03/23/23 20:22:54.862
    Mar 23 20:22:54.878: INFO: Waiting for pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 to disappear
    Mar 23 20:22:54.881: INFO: Pod pod-configmaps-a6c3dbf4-8d01-4852-81ec-102d054ea372 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Mar 23 20:22:54.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8407" for this suite. 03/23/23 20:22:54.887
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:22:54.905
Mar 23 20:22:54.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-runtime 03/23/23 20:22:54.908
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:54.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:54.936
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 03/23/23 20:22:54.939
STEP: wait for the container to reach Succeeded 03/23/23 20:22:54.95
STEP: get the container status 03/23/23 20:22:59.989
STEP: the container should be terminated 03/23/23 20:22:59.993
STEP: the termination message should be set 03/23/23 20:22:59.994
Mar 23 20:22:59.994: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 03/23/23 20:22:59.994
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Mar 23 20:23:00.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8612" for this suite. 03/23/23 20:23:00.042
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":345,"skipped":6374,"failed":0}
------------------------------
• [SLOW TEST] [5.147 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:22:54.905
    Mar 23 20:22:54.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-runtime 03/23/23 20:22:54.908
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:22:54.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:22:54.936
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 03/23/23 20:22:54.939
    STEP: wait for the container to reach Succeeded 03/23/23 20:22:54.95
    STEP: get the container status 03/23/23 20:22:59.989
    STEP: the container should be terminated 03/23/23 20:22:59.993
    STEP: the termination message should be set 03/23/23 20:22:59.994
    Mar 23 20:22:59.994: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 03/23/23 20:22:59.994
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Mar 23 20:23:00.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8612" for this suite. 03/23/23 20:23:00.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:00.063
Mar 23 20:23:00.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 20:23:00.066
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:00.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:00.1
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-1000 03/23/23 20:23:00.103
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/23/23 20:23:00.144
STEP: creating service externalsvc in namespace services-1000 03/23/23 20:23:00.144
STEP: creating replication controller externalsvc in namespace services-1000 03/23/23 20:23:00.185
I0323 20:23:00.212871      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1000, replica count: 2
I0323 20:23:03.263259      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 03/23/23 20:23:03.434
Mar 23 20:23:03.463: INFO: Creating new exec pod
Mar 23 20:23:03.481: INFO: Waiting up to 5m0s for pod "execpodcjxdj" in namespace "services-1000" to be "running"
Mar 23 20:23:03.492: INFO: Pod "execpodcjxdj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550076ms
Mar 23 20:23:05.497: INFO: Pod "execpodcjxdj": Phase="Running", Reason="", readiness=true. Elapsed: 2.015629208s
Mar 23 20:23:05.497: INFO: Pod "execpodcjxdj" satisfied condition "running"
Mar 23 20:23:05.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-1000 exec execpodcjxdj -- /bin/sh -x -c nslookup nodeport-service.services-1000.svc.cluster.local'
Mar 23 20:23:05.845: INFO: stderr: "+ nslookup nodeport-service.services-1000.svc.cluster.local\n"
Mar 23 20:23:05.845: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-1000.svc.cluster.local\tcanonical name = externalsvc.services-1000.svc.cluster.local.\nName:\texternalsvc.services-1000.svc.cluster.local\nAddress: 10.0.137.55\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1000, will wait for the garbage collector to delete the pods 03/23/23 20:23:05.845
Mar 23 20:23:05.908: INFO: Deleting ReplicationController externalsvc took: 8.584981ms
Mar 23 20:23:06.008: INFO: Terminating ReplicationController externalsvc pods took: 100.247876ms
Mar 23 20:23:13.857: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 20:23:13.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1000" for this suite. 03/23/23 20:23:13.889
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":346,"skipped":6379,"failed":0}
------------------------------
• [SLOW TEST] [13.847 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:00.063
    Mar 23 20:23:00.064: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 20:23:00.066
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:00.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:00.1
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-1000 03/23/23 20:23:00.103
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 03/23/23 20:23:00.144
    STEP: creating service externalsvc in namespace services-1000 03/23/23 20:23:00.144
    STEP: creating replication controller externalsvc in namespace services-1000 03/23/23 20:23:00.185
    I0323 20:23:00.212871      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-1000, replica count: 2
    I0323 20:23:03.263259      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 03/23/23 20:23:03.434
    Mar 23 20:23:03.463: INFO: Creating new exec pod
    Mar 23 20:23:03.481: INFO: Waiting up to 5m0s for pod "execpodcjxdj" in namespace "services-1000" to be "running"
    Mar 23 20:23:03.492: INFO: Pod "execpodcjxdj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.550076ms
    Mar 23 20:23:05.497: INFO: Pod "execpodcjxdj": Phase="Running", Reason="", readiness=true. Elapsed: 2.015629208s
    Mar 23 20:23:05.497: INFO: Pod "execpodcjxdj" satisfied condition "running"
    Mar 23 20:23:05.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-1000 exec execpodcjxdj -- /bin/sh -x -c nslookup nodeport-service.services-1000.svc.cluster.local'
    Mar 23 20:23:05.845: INFO: stderr: "+ nslookup nodeport-service.services-1000.svc.cluster.local\n"
    Mar 23 20:23:05.845: INFO: stdout: "Server:\t\t10.0.0.10\nAddress:\t10.0.0.10#53\n\nnodeport-service.services-1000.svc.cluster.local\tcanonical name = externalsvc.services-1000.svc.cluster.local.\nName:\texternalsvc.services-1000.svc.cluster.local\nAddress: 10.0.137.55\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-1000, will wait for the garbage collector to delete the pods 03/23/23 20:23:05.845
    Mar 23 20:23:05.908: INFO: Deleting ReplicationController externalsvc took: 8.584981ms
    Mar 23 20:23:06.008: INFO: Terminating ReplicationController externalsvc pods took: 100.247876ms
    Mar 23 20:23:13.857: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 20:23:13.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1000" for this suite. 03/23/23 20:23:13.889
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:13.931
Mar 23 20:23:13.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:23:13.935
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:13.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:13.986
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-45fc3f49-da2a-4a11-ac02-0ba64bd3e4f1 03/23/23 20:23:13.993
STEP: Creating secret with name s-test-opt-upd-23ffb7ee-0fa1-4583-b544-70a8ef9e0db5 03/23/23 20:23:14
STEP: Creating the pod 03/23/23 20:23:14.005
Mar 23 20:23:14.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6" in namespace "projected-9115" to be "running and ready"
Mar 23 20:23:14.047: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293391ms
Mar 23 20:23:14.047: INFO: The phase of Pod pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:23:16.052: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009545634s
Mar 23 20:23:16.052: INFO: The phase of Pod pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6 is Running (Ready = true)
Mar 23 20:23:16.052: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-45fc3f49-da2a-4a11-ac02-0ba64bd3e4f1 03/23/23 20:23:16.081
STEP: Updating secret s-test-opt-upd-23ffb7ee-0fa1-4583-b544-70a8ef9e0db5 03/23/23 20:23:16.09
STEP: Creating secret with name s-test-opt-create-b1597b54-ac24-45b4-85ef-ec5fe4363f7e 03/23/23 20:23:16.098
STEP: waiting to observe update in volume 03/23/23 20:23:16.103
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Mar 23 20:23:18.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9115" for this suite. 03/23/23 20:23:18.139
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":347,"skipped":6386,"failed":0}
------------------------------
• [4.215 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:13.931
    Mar 23 20:23:13.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:23:13.935
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:13.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:13.986
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-45fc3f49-da2a-4a11-ac02-0ba64bd3e4f1 03/23/23 20:23:13.993
    STEP: Creating secret with name s-test-opt-upd-23ffb7ee-0fa1-4583-b544-70a8ef9e0db5 03/23/23 20:23:14
    STEP: Creating the pod 03/23/23 20:23:14.005
    Mar 23 20:23:14.043: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6" in namespace "projected-9115" to be "running and ready"
    Mar 23 20:23:14.047: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293391ms
    Mar 23 20:23:14.047: INFO: The phase of Pod pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:23:16.052: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6": Phase="Running", Reason="", readiness=true. Elapsed: 2.009545634s
    Mar 23 20:23:16.052: INFO: The phase of Pod pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6 is Running (Ready = true)
    Mar 23 20:23:16.052: INFO: Pod "pod-projected-secrets-f8fa4bfe-c547-45ba-8239-38b53495dff6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-45fc3f49-da2a-4a11-ac02-0ba64bd3e4f1 03/23/23 20:23:16.081
    STEP: Updating secret s-test-opt-upd-23ffb7ee-0fa1-4583-b544-70a8ef9e0db5 03/23/23 20:23:16.09
    STEP: Creating secret with name s-test-opt-create-b1597b54-ac24-45b4-85ef-ec5fe4363f7e 03/23/23 20:23:16.098
    STEP: waiting to observe update in volume 03/23/23 20:23:16.103
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Mar 23 20:23:18.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9115" for this suite. 03/23/23 20:23:18.139
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:18.153
Mar 23 20:23:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename downward-api 03/23/23 20:23:18.155
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:18.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:18.178
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 03/23/23 20:23:18.181
Mar 23 20:23:18.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db" in namespace "downward-api-3736" to be "Succeeded or Failed"
Mar 23 20:23:18.202: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58649ms
Mar 23 20:23:20.206: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Running", Reason="", readiness=true. Elapsed: 2.008611786s
Mar 23 20:23:22.207: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Running", Reason="", readiness=false. Elapsed: 4.009878595s
Mar 23 20:23:24.208: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010242907s
STEP: Saw pod success 03/23/23 20:23:24.208
Mar 23 20:23:24.208: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db" satisfied condition "Succeeded or Failed"
Mar 23 20:23:24.213: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db container client-container: <nil>
STEP: delete the pod 03/23/23 20:23:24.227
Mar 23 20:23:24.239: INFO: Waiting for pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db to disappear
Mar 23 20:23:24.243: INFO: Pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Mar 23 20:23:24.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3736" for this suite. 03/23/23 20:23:24.248
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":348,"skipped":6388,"failed":0}
------------------------------
• [SLOW TEST] [6.102 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:18.153
    Mar 23 20:23:18.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename downward-api 03/23/23 20:23:18.155
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:18.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:18.178
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 03/23/23 20:23:18.181
    Mar 23 20:23:18.197: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db" in namespace "downward-api-3736" to be "Succeeded or Failed"
    Mar 23 20:23:18.202: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Pending", Reason="", readiness=false. Elapsed: 4.58649ms
    Mar 23 20:23:20.206: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Running", Reason="", readiness=true. Elapsed: 2.008611786s
    Mar 23 20:23:22.207: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Running", Reason="", readiness=false. Elapsed: 4.009878595s
    Mar 23 20:23:24.208: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010242907s
    STEP: Saw pod success 03/23/23 20:23:24.208
    Mar 23 20:23:24.208: INFO: Pod "downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db" satisfied condition "Succeeded or Failed"
    Mar 23 20:23:24.213: INFO: Trying to get logs from node k8s-linuxpool-16392394-2 pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db container client-container: <nil>
    STEP: delete the pod 03/23/23 20:23:24.227
    Mar 23 20:23:24.239: INFO: Waiting for pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db to disappear
    Mar 23 20:23:24.243: INFO: Pod downwardapi-volume-94aee278-e0a2-4606-aff9-fb6b828ff5db no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Mar 23 20:23:24.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3736" for this suite. 03/23/23 20:23:24.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:24.269
Mar 23 20:23:24.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pod-network-test 03/23/23 20:23:24.271
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:24.292
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:24.295
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-6383 03/23/23 20:23:24.298
STEP: creating a selector 03/23/23 20:23:24.298
STEP: Creating the service pods in kubernetes 03/23/23 20:23:24.299
Mar 23 20:23:24.299: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Mar 23 20:23:24.394: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6383" to be "running and ready"
Mar 23 20:23:24.398: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723291ms
Mar 23 20:23:24.398: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:23:26.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009234892s
Mar 23 20:23:26.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:28.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008561295s
Mar 23 20:23:28.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:30.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007580098s
Mar 23 20:23:30.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:32.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010337393s
Mar 23 20:23:32.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:34.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010298794s
Mar 23 20:23:34.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:36.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009081063s
Mar 23 20:23:36.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:38.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010212926s
Mar 23 20:23:38.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:40.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009799291s
Mar 23 20:23:40.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:42.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010040656s
Mar 23 20:23:42.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:44.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008685351s
Mar 23 20:23:44.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Mar 23 20:23:46.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010605936s
Mar 23 20:23:46.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Mar 23 20:23:46.405: INFO: Pod "netserver-0" satisfied condition "running and ready"
Mar 23 20:23:46.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6383" to be "running and ready"
Mar 23 20:23:46.412: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.581892ms
Mar 23 20:23:46.412: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Mar 23 20:23:46.412: INFO: Pod "netserver-1" satisfied condition "running and ready"
Mar 23 20:23:46.414: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6383" to be "running and ready"
Mar 23 20:23:46.418: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.282393ms
Mar 23 20:23:46.418: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Mar 23 20:23:46.418: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 03/23/23 20:23:46.421
Mar 23 20:23:46.427: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6383" to be "running"
Mar 23 20:23:46.431: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074591ms
Mar 23 20:23:48.435: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007897771s
Mar 23 20:23:48.435: INFO: Pod "test-container-pod" satisfied condition "running"
Mar 23 20:23:48.438: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Mar 23 20:23:48.439: INFO: Breadth first check of 10.240.0.33 on host 10.240.0.30...
Mar 23 20:23:48.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.33&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:23:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:23:48.443: INFO: ExecWithOptions: Clientset creation
Mar 23 20:23:48.443: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.33%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 20:23:48.569: INFO: Waiting for responses: map[]
Mar 23 20:23:48.569: INFO: reached 10.240.0.33 after 0/1 tries
Mar 23 20:23:48.569: INFO: Breadth first check of 10.240.0.68 on host 10.240.0.56...
Mar 23 20:23:48.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.68&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:23:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:23:48.575: INFO: ExecWithOptions: Clientset creation
Mar 23 20:23:48.575: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 20:23:48.696: INFO: Waiting for responses: map[]
Mar 23 20:23:48.696: INFO: reached 10.240.0.68 after 0/1 tries
Mar 23 20:23:48.696: INFO: Breadth first check of 10.240.0.28 on host 10.240.0.4...
Mar 23 20:23:48.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.28&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Mar 23 20:23:48.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
Mar 23 20:23:48.701: INFO: ExecWithOptions: Clientset creation
Mar 23 20:23:48.701: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Mar 23 20:23:48.841: INFO: Waiting for responses: map[]
Mar 23 20:23:48.841: INFO: reached 10.240.0.28 after 0/1 tries
Mar 23 20:23:48.841: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Mar 23 20:23:48.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6383" for this suite. 03/23/23 20:23:48.849
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":349,"skipped":6451,"failed":0}
------------------------------
• [SLOW TEST] [24.586 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:24.269
    Mar 23 20:23:24.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pod-network-test 03/23/23 20:23:24.271
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:24.292
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:24.295
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-6383 03/23/23 20:23:24.298
    STEP: creating a selector 03/23/23 20:23:24.298
    STEP: Creating the service pods in kubernetes 03/23/23 20:23:24.299
    Mar 23 20:23:24.299: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Mar 23 20:23:24.394: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6383" to be "running and ready"
    Mar 23 20:23:24.398: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.723291ms
    Mar 23 20:23:24.398: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:23:26.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009234892s
    Mar 23 20:23:26.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:28.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008561295s
    Mar 23 20:23:28.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:30.402: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007580098s
    Mar 23 20:23:30.402: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:32.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010337393s
    Mar 23 20:23:32.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:34.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010298794s
    Mar 23 20:23:34.405: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:36.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009081063s
    Mar 23 20:23:36.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:38.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010212926s
    Mar 23 20:23:38.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:40.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009799291s
    Mar 23 20:23:40.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:42.404: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010040656s
    Mar 23 20:23:42.404: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:44.403: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.008685351s
    Mar 23 20:23:44.403: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Mar 23 20:23:46.405: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010605936s
    Mar 23 20:23:46.405: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Mar 23 20:23:46.405: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Mar 23 20:23:46.408: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6383" to be "running and ready"
    Mar 23 20:23:46.412: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.581892ms
    Mar 23 20:23:46.412: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Mar 23 20:23:46.412: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Mar 23 20:23:46.414: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-6383" to be "running and ready"
    Mar 23 20:23:46.418: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.282393ms
    Mar 23 20:23:46.418: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Mar 23 20:23:46.418: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 03/23/23 20:23:46.421
    Mar 23 20:23:46.427: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6383" to be "running"
    Mar 23 20:23:46.431: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.074591ms
    Mar 23 20:23:48.435: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007897771s
    Mar 23 20:23:48.435: INFO: Pod "test-container-pod" satisfied condition "running"
    Mar 23 20:23:48.438: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Mar 23 20:23:48.439: INFO: Breadth first check of 10.240.0.33 on host 10.240.0.30...
    Mar 23 20:23:48.441: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.33&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:23:48.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:23:48.443: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:23:48.443: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.33%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 20:23:48.569: INFO: Waiting for responses: map[]
    Mar 23 20:23:48.569: INFO: reached 10.240.0.33 after 0/1 tries
    Mar 23 20:23:48.569: INFO: Breadth first check of 10.240.0.68 on host 10.240.0.56...
    Mar 23 20:23:48.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.68&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:23:48.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:23:48.575: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:23:48.575: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 20:23:48.696: INFO: Waiting for responses: map[]
    Mar 23 20:23:48.696: INFO: reached 10.240.0.68 after 0/1 tries
    Mar 23 20:23:48.696: INFO: Breadth first check of 10.240.0.28 on host 10.240.0.4...
    Mar 23 20:23:48.700: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.240.0.74:9080/dial?request=hostname&protocol=http&host=10.240.0.28&port=8083&tries=1'] Namespace:pod-network-test-6383 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Mar 23 20:23:48.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    Mar 23 20:23:48.701: INFO: ExecWithOptions: Clientset creation
    Mar 23 20:23:48.701: INFO: ExecWithOptions: execute(POST https://10.0.0.1:443/api/v1/namespaces/pod-network-test-6383/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.240.0.74%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.240.0.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Mar 23 20:23:48.841: INFO: Waiting for responses: map[]
    Mar 23 20:23:48.841: INFO: reached 10.240.0.28 after 0/1 tries
    Mar 23 20:23:48.841: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Mar 23 20:23:48.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6383" for this suite. 03/23/23 20:23:48.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:48.858
Mar 23 20:23:48.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename projected 03/23/23 20:23:48.86
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:48.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:48.887
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 03/23/23 20:23:48.89
Mar 23 20:23:48.913: INFO: Waiting up to 5m0s for pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56" in namespace "projected-2122" to be "running and ready"
Mar 23 20:23:48.925: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56": Phase="Pending", Reason="", readiness=false. Elapsed: 11.666474ms
Mar 23 20:23:48.925: INFO: The phase of Pod annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:23:50.930: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56": Phase="Running", Reason="", readiness=true. Elapsed: 2.016492852s
Mar 23 20:23:50.930: INFO: The phase of Pod annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56 is Running (Ready = true)
Mar 23 20:23:50.930: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56" satisfied condition "running and ready"
Mar 23 20:23:51.454: INFO: Successfully updated pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Mar 23 20:23:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2122" for this suite. 03/23/23 20:23:55.482
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":350,"skipped":6484,"failed":0}
------------------------------
• [SLOW TEST] [6.631 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:48.858
    Mar 23 20:23:48.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename projected 03/23/23 20:23:48.86
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:48.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:48.887
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 03/23/23 20:23:48.89
    Mar 23 20:23:48.913: INFO: Waiting up to 5m0s for pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56" in namespace "projected-2122" to be "running and ready"
    Mar 23 20:23:48.925: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56": Phase="Pending", Reason="", readiness=false. Elapsed: 11.666474ms
    Mar 23 20:23:48.925: INFO: The phase of Pod annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:23:50.930: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56": Phase="Running", Reason="", readiness=true. Elapsed: 2.016492852s
    Mar 23 20:23:50.930: INFO: The phase of Pod annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56 is Running (Ready = true)
    Mar 23 20:23:50.930: INFO: Pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56" satisfied condition "running and ready"
    Mar 23 20:23:51.454: INFO: Successfully updated pod "annotationupdatec4d30d74-7d5a-4ebe-8998-2103d3d79e56"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Mar 23 20:23:55.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2122" for this suite. 03/23/23 20:23:55.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:23:55.497
Mar 23 20:23:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:23:55.498
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:55.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:55.523
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Mar 23 20:23:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:24:01.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4692" for this suite. 03/23/23 20:24:01.851
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":351,"skipped":6505,"failed":0}
------------------------------
• [SLOW TEST] [6.366 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:23:55.497
    Mar 23 20:23:55.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:23:55.498
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:23:55.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:23:55.523
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Mar 23 20:23:55.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:24:01.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4692" for this suite. 03/23/23 20:24:01.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:24:01.876
Mar 23 20:24:01.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename kubectl 03/23/23 20:24:01.877
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:24:01.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:24:01.911
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 20:24:01.914
Mar 23 20:24:01.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5362 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Mar 23 20:24:02.052: INFO: stderr: ""
Mar 23 20:24:02.052: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 03/23/23 20:24:02.052
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Mar 23 20:24:02.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5362 delete pods e2e-test-httpd-pod'
Mar 23 20:24:05.971: INFO: stderr: ""
Mar 23 20:24:05.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Mar 23 20:24:05.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5362" for this suite. 03/23/23 20:24:05.977
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":352,"skipped":6554,"failed":0}
------------------------------
• [4.107 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:24:01.876
    Mar 23 20:24:01.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename kubectl 03/23/23 20:24:01.877
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:24:01.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:24:01.911
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 03/23/23 20:24:01.914
    Mar 23 20:24:01.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5362 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Mar 23 20:24:02.052: INFO: stderr: ""
    Mar 23 20:24:02.052: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 03/23/23 20:24:02.052
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Mar 23 20:24:02.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=kubectl-5362 delete pods e2e-test-httpd-pod'
    Mar 23 20:24:05.971: INFO: stderr: ""
    Mar 23 20:24:05.971: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Mar 23 20:24:05.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5362" for this suite. 03/23/23 20:24:05.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:24:05.987
Mar 23 20:24:05.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename statefulset 03/23/23 20:24:05.989
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:24:06.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:24:06.037
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4791 03/23/23 20:24:06.039
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 03/23/23 20:24:06.044
Mar 23 20:24:06.058: INFO: Found 0 stateful pods, waiting for 3
Mar 23 20:24:16.069: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 20:24:16.069: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 20:24:16.069: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Mar 23 20:24:16.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:24:16.274: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:24:16.274: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:24:16.274: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/23/23 20:24:26.294
Mar 23 20:24:26.314: INFO: Updating stateful set ss2
STEP: Creating a new revision 03/23/23 20:24:26.314
STEP: Updating Pods in reverse ordinal order 03/23/23 20:24:36.333
Mar 23 20:24:36.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:24:36.542: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 20:24:36.542: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 20:24:36.542: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 20:24:46.588: INFO: Waiting for StatefulSet statefulset-4791/ss2 to complete update
STEP: Rolling back to a previous revision 03/23/23 20:24:56.597
Mar 23 20:24:56.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Mar 23 20:24:56.824: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Mar 23 20:24:56.824: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Mar 23 20:24:56.824: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Mar 23 20:25:06.897: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 03/23/23 20:25:16.952
Mar 23 20:25:16.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Mar 23 20:25:17.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Mar 23 20:25:17.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Mar 23 20:25:17.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Mar 23 20:25:27.208: INFO: Waiting for StatefulSet statefulset-4791/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Mar 23 20:25:37.216: INFO: Deleting all statefulset in ns statefulset-4791
Mar 23 20:25:37.219: INFO: Scaling statefulset ss2 to 0
Mar 23 20:25:47.236: INFO: Waiting for statefulset status.replicas updated to 0
Mar 23 20:25:47.244: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Mar 23 20:25:47.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4791" for this suite. 03/23/23 20:25:47.28
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":353,"skipped":6573,"failed":0}
------------------------------
• [SLOW TEST] [101.302 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:24:05.987
    Mar 23 20:24:05.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename statefulset 03/23/23 20:24:05.989
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:24:06.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:24:06.037
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4791 03/23/23 20:24:06.039
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 03/23/23 20:24:06.044
    Mar 23 20:24:06.058: INFO: Found 0 stateful pods, waiting for 3
    Mar 23 20:24:16.069: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 20:24:16.069: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 20:24:16.069: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Mar 23 20:24:16.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 20:24:16.274: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 20:24:16.274: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 20:24:16.274: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 03/23/23 20:24:26.294
    Mar 23 20:24:26.314: INFO: Updating stateful set ss2
    STEP: Creating a new revision 03/23/23 20:24:26.314
    STEP: Updating Pods in reverse ordinal order 03/23/23 20:24:36.333
    Mar 23 20:24:36.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 20:24:36.542: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 20:24:36.542: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 20:24:36.542: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 20:24:46.588: INFO: Waiting for StatefulSet statefulset-4791/ss2 to complete update
    STEP: Rolling back to a previous revision 03/23/23 20:24:56.597
    Mar 23 20:24:56.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Mar 23 20:24:56.824: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Mar 23 20:24:56.824: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Mar 23 20:24:56.824: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Mar 23 20:25:06.897: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 03/23/23 20:25:16.952
    Mar 23 20:25:16.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=statefulset-4791 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Mar 23 20:25:17.160: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Mar 23 20:25:17.160: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Mar 23 20:25:17.160: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Mar 23 20:25:27.208: INFO: Waiting for StatefulSet statefulset-4791/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Mar 23 20:25:37.216: INFO: Deleting all statefulset in ns statefulset-4791
    Mar 23 20:25:37.219: INFO: Scaling statefulset ss2 to 0
    Mar 23 20:25:47.236: INFO: Waiting for statefulset status.replicas updated to 0
    Mar 23 20:25:47.244: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Mar 23 20:25:47.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4791" for this suite. 03/23/23 20:25:47.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:25:47.291
Mar 23 20:25:47.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename pods 03/23/23 20:25:47.293
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.319
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 03/23/23 20:25:47.321
STEP: submitting the pod to kubernetes 03/23/23 20:25:47.321
STEP: verifying QOS class is set on the pod 03/23/23 20:25:47.331
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Mar 23 20:25:47.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4954" for this suite. 03/23/23 20:25:47.343
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":354,"skipped":6593,"failed":0}
------------------------------
• [0.059 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:25:47.291
    Mar 23 20:25:47.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename pods 03/23/23 20:25:47.293
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.319
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 03/23/23 20:25:47.321
    STEP: submitting the pod to kubernetes 03/23/23 20:25:47.321
    STEP: verifying QOS class is set on the pod 03/23/23 20:25:47.331
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Mar 23 20:25:47.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4954" for this suite. 03/23/23 20:25:47.343
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:25:47.352
Mar 23 20:25:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename cronjob 03/23/23 20:25:47.354
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.385
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 03/23/23 20:25:47.387
STEP: creating 03/23/23 20:25:47.388
STEP: getting 03/23/23 20:25:47.398
STEP: listing 03/23/23 20:25:47.401
STEP: watching 03/23/23 20:25:47.405
Mar 23 20:25:47.405: INFO: starting watch
STEP: cluster-wide listing 03/23/23 20:25:47.406
STEP: cluster-wide watching 03/23/23 20:25:47.414
Mar 23 20:25:47.415: INFO: starting watch
STEP: patching 03/23/23 20:25:47.416
STEP: updating 03/23/23 20:25:47.422
Mar 23 20:25:47.437: INFO: waiting for watch events with expected annotations
Mar 23 20:25:47.437: INFO: saw patched and updated annotations
STEP: patching /status 03/23/23 20:25:47.437
STEP: updating /status 03/23/23 20:25:47.444
STEP: get /status 03/23/23 20:25:47.452
STEP: deleting 03/23/23 20:25:47.455
STEP: deleting a collection 03/23/23 20:25:47.475
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Mar 23 20:25:47.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9917" for this suite. 03/23/23 20:25:47.49
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":355,"skipped":6607,"failed":0}
------------------------------
• [0.145 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:25:47.352
    Mar 23 20:25:47.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename cronjob 03/23/23 20:25:47.354
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.385
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 03/23/23 20:25:47.387
    STEP: creating 03/23/23 20:25:47.388
    STEP: getting 03/23/23 20:25:47.398
    STEP: listing 03/23/23 20:25:47.401
    STEP: watching 03/23/23 20:25:47.405
    Mar 23 20:25:47.405: INFO: starting watch
    STEP: cluster-wide listing 03/23/23 20:25:47.406
    STEP: cluster-wide watching 03/23/23 20:25:47.414
    Mar 23 20:25:47.415: INFO: starting watch
    STEP: patching 03/23/23 20:25:47.416
    STEP: updating 03/23/23 20:25:47.422
    Mar 23 20:25:47.437: INFO: waiting for watch events with expected annotations
    Mar 23 20:25:47.437: INFO: saw patched and updated annotations
    STEP: patching /status 03/23/23 20:25:47.437
    STEP: updating /status 03/23/23 20:25:47.444
    STEP: get /status 03/23/23 20:25:47.452
    STEP: deleting 03/23/23 20:25:47.455
    STEP: deleting a collection 03/23/23 20:25:47.475
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Mar 23 20:25:47.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9917" for this suite. 03/23/23 20:25:47.49
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:25:47.497
Mar 23 20:25:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename replicaset 03/23/23 20:25:47.499
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.52
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Mar 23 20:25:47.523: INFO: Creating ReplicaSet my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682
Mar 23 20:25:47.534: INFO: Pod name my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Found 0 pods out of 1
Mar 23 20:25:52.539: INFO: Pod name my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Found 1 pods out of 1
Mar 23 20:25:52.539: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682" is running
Mar 23 20:25:52.539: INFO: Waiting up to 5m0s for pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" in namespace "replicaset-1098" to be "running"
Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw": Phase="Running", Reason="", readiness=true. Elapsed: 4.171991ms
Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" satisfied condition "running"
Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:47 +0000 UTC Reason: Message:}])
Mar 23 20:25:52.543: INFO: Trying to dial the pod
Mar 23 20:25:57.559: INFO: Controller my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Got expected result from replica 1 [my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw]: "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Mar 23 20:25:57.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1098" for this suite. 03/23/23 20:25:57.564
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":356,"skipped":6608,"failed":0}
------------------------------
• [SLOW TEST] [10.093 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:25:47.497
    Mar 23 20:25:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename replicaset 03/23/23 20:25:47.499
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:47.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:47.52
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Mar 23 20:25:47.523: INFO: Creating ReplicaSet my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682
    Mar 23 20:25:47.534: INFO: Pod name my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Found 0 pods out of 1
    Mar 23 20:25:52.539: INFO: Pod name my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Found 1 pods out of 1
    Mar 23 20:25:52.539: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682" is running
    Mar 23 20:25:52.539: INFO: Waiting up to 5m0s for pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" in namespace "replicaset-1098" to be "running"
    Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw": Phase="Running", Reason="", readiness=true. Elapsed: 4.171991ms
    Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" satisfied condition "running"
    Mar 23 20:25:52.543: INFO: Pod "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:50 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:50 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-03-23 20:25:47 +0000 UTC Reason: Message:}])
    Mar 23 20:25:52.543: INFO: Trying to dial the pod
    Mar 23 20:25:57.559: INFO: Controller my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682: Got expected result from replica 1 [my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw]: "my-hostname-basic-87ea0575-b1eb-4c87-bfe4-18e60392e682-cwxxw", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Mar 23 20:25:57.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1098" for this suite. 03/23/23 20:25:57.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:25:57.593
Mar 23 20:25:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:25:57.594
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:57.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:57.622
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Mar 23 20:25:57.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Mar 23 20:25:58.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9509" for this suite. 03/23/23 20:25:58.693
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":357,"skipped":6650,"failed":0}
------------------------------
• [1.109 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:25:57.593
    Mar 23 20:25:57.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename custom-resource-definition 03/23/23 20:25:57.594
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:57.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:57.622
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Mar 23 20:25:57.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Mar 23 20:25:58.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9509" for this suite. 03/23/23 20:25:58.693
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:25:58.71
Mar 23 20:25:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename container-probe 03/23/23 20:25:58.712
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:58.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:58.74
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Mar 23 20:26:58.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2852" for this suite. 03/23/23 20:26:58.762
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":358,"skipped":6654,"failed":0}
------------------------------
• [SLOW TEST] [60.059 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:25:58.71
    Mar 23 20:25:58.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename container-probe 03/23/23 20:25:58.712
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:25:58.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:25:58.74
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Mar 23 20:26:58.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2852" for this suite. 03/23/23 20:26:58.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:26:58.775
Mar 23 20:26:58.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename services 03/23/23 20:26:58.777
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:26:58.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:26:58.799
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-8744 03/23/23 20:26:58.801
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[] 03/23/23 20:26:58.833
Mar 23 20:26:58.844: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Mar 23 20:26:59.852: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8744 03/23/23 20:26:59.853
Mar 23 20:26:59.860: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8744" to be "running and ready"
Mar 23 20:26:59.868: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.432683ms
Mar 23 20:26:59.868: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:27:01.874: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013583347s
Mar 23 20:27:01.874: INFO: The phase of Pod pod1 is Running (Ready = true)
Mar 23 20:27:01.874: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod1:[100]] 03/23/23 20:27:01.877
Mar 23 20:27:01.889: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-8744 03/23/23 20:27:01.889
Mar 23 20:27:01.896: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8744" to be "running and ready"
Mar 23 20:27:01.901: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968589ms
Mar 23 20:27:01.901: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Mar 23 20:27:03.910: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014126953s
Mar 23 20:27:03.910: INFO: The phase of Pod pod2 is Running (Ready = true)
Mar 23 20:27:03.910: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod1:[100] pod2:[101]] 03/23/23 20:27:03.915
Mar 23 20:27:03.940: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 03/23/23 20:27:03.94
Mar 23 20:27:03.940: INFO: Creating new exec pod
Mar 23 20:27:03.952: INFO: Waiting up to 5m0s for pod "execpodgsbgd" in namespace "services-8744" to be "running"
Mar 23 20:27:03.960: INFO: Pod "execpodgsbgd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.45728ms
Mar 23 20:27:05.968: INFO: Pod "execpodgsbgd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016301453s
Mar 23 20:27:05.968: INFO: Pod "execpodgsbgd" satisfied condition "running"
Mar 23 20:27:06.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Mar 23 20:27:07.217: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Mar 23 20:27:07.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:27:07.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.215.19 80'
Mar 23 20:27:07.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.215.19 80\nConnection to 10.0.215.19 80 port [tcp/http] succeeded!\n"
Mar 23 20:27:07.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:27:07.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Mar 23 20:27:07.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Mar 23 20:27:07.695: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Mar 23 20:27:07.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.215.19 81'
Mar 23 20:27:07.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.215.19 81\nConnection to 10.0.215.19 81 port [tcp/*] succeeded!\n"
Mar 23 20:27:07.892: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-8744 03/23/23 20:27:07.892
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod2:[101]] 03/23/23 20:27:07.921
Mar 23 20:27:07.938: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-8744 03/23/23 20:27:07.939
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[] 03/23/23 20:27:07.955
Mar 23 20:27:08.969: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Mar 23 20:27:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8744" for this suite. 03/23/23 20:27:09.031
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":359,"skipped":6662,"failed":0}
------------------------------
• [SLOW TEST] [10.267 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:26:58.775
    Mar 23 20:26:58.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename services 03/23/23 20:26:58.777
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:26:58.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:26:58.799
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-8744 03/23/23 20:26:58.801
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[] 03/23/23 20:26:58.833
    Mar 23 20:26:58.844: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Mar 23 20:26:59.852: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8744 03/23/23 20:26:59.853
    Mar 23 20:26:59.860: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8744" to be "running and ready"
    Mar 23 20:26:59.868: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.432683ms
    Mar 23 20:26:59.868: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:27:01.874: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.013583347s
    Mar 23 20:27:01.874: INFO: The phase of Pod pod1 is Running (Ready = true)
    Mar 23 20:27:01.874: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod1:[100]] 03/23/23 20:27:01.877
    Mar 23 20:27:01.889: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-8744 03/23/23 20:27:01.889
    Mar 23 20:27:01.896: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8744" to be "running and ready"
    Mar 23 20:27:01.901: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.968589ms
    Mar 23 20:27:01.901: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Mar 23 20:27:03.910: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014126953s
    Mar 23 20:27:03.910: INFO: The phase of Pod pod2 is Running (Ready = true)
    Mar 23 20:27:03.910: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod1:[100] pod2:[101]] 03/23/23 20:27:03.915
    Mar 23 20:27:03.940: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 03/23/23 20:27:03.94
    Mar 23 20:27:03.940: INFO: Creating new exec pod
    Mar 23 20:27:03.952: INFO: Waiting up to 5m0s for pod "execpodgsbgd" in namespace "services-8744" to be "running"
    Mar 23 20:27:03.960: INFO: Pod "execpodgsbgd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.45728ms
    Mar 23 20:27:05.968: INFO: Pod "execpodgsbgd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016301453s
    Mar 23 20:27:05.968: INFO: Pod "execpodgsbgd" satisfied condition "running"
    Mar 23 20:27:06.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Mar 23 20:27:07.217: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 80\n+ echo hostName\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Mar 23 20:27:07.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:27:07.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.215.19 80'
    Mar 23 20:27:07.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.215.19 80\nConnection to 10.0.215.19 80 port [tcp/http] succeeded!\n"
    Mar 23 20:27:07.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:27:07.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Mar 23 20:27:07.695: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Mar 23 20:27:07.695: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Mar 23 20:27:07.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2420329342 --namespace=services-8744 exec execpodgsbgd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.215.19 81'
    Mar 23 20:27:07.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.215.19 81\nConnection to 10.0.215.19 81 port [tcp/*] succeeded!\n"
    Mar 23 20:27:07.892: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-8744 03/23/23 20:27:07.892
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[pod2:[101]] 03/23/23 20:27:07.921
    Mar 23 20:27:07.938: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-8744 03/23/23 20:27:07.939
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-8744 to expose endpoints map[] 03/23/23 20:27:07.955
    Mar 23 20:27:08.969: INFO: successfully validated that service multi-endpoint-test in namespace services-8744 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Mar 23 20:27:09.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8744" for this suite. 03/23/23 20:27:09.031
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:27:09.05
Mar 23 20:27:09.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename var-expansion 03/23/23 20:27:09.051
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:09.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:09.127
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Mar 23 20:27:09.177: INFO: Waiting up to 2m0s for pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" in namespace "var-expansion-1982" to be "container 0 failed with reason CreateContainerConfigError"
Mar 23 20:27:09.180: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.256192ms
Mar 23 20:27:11.184: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007534401s
Mar 23 20:27:13.185: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008784843s
Mar 23 20:27:15.186: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009411301s
Mar 23 20:27:15.186: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Mar 23 20:27:15.186: INFO: Deleting pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" in namespace "var-expansion-1982"
Mar 23 20:27:15.200: INFO: Wait up to 5m0s for pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Mar 23 20:27:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1982" for this suite. 03/23/23 20:27:17.218
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":360,"skipped":6673,"failed":0}
------------------------------
• [SLOW TEST] [8.176 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:27:09.05
    Mar 23 20:27:09.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename var-expansion 03/23/23 20:27:09.051
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:09.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:09.127
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Mar 23 20:27:09.177: INFO: Waiting up to 2m0s for pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" in namespace "var-expansion-1982" to be "container 0 failed with reason CreateContainerConfigError"
    Mar 23 20:27:09.180: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 3.256192ms
    Mar 23 20:27:11.184: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007534401s
    Mar 23 20:27:13.185: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008784843s
    Mar 23 20:27:15.186: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009411301s
    Mar 23 20:27:15.186: INFO: Pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Mar 23 20:27:15.186: INFO: Deleting pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" in namespace "var-expansion-1982"
    Mar 23 20:27:15.200: INFO: Wait up to 5m0s for pod "var-expansion-8544b4cb-3ca0-433c-b27a-566edbffb006" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Mar 23 20:27:17.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1982" for this suite. 03/23/23 20:27:17.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:27:17.234
Mar 23 20:27:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename security-context-test 03/23/23 20:27:17.236
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:17.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:17.26
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Mar 23 20:27:17.278: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a" in namespace "security-context-test-4414" to be "Succeeded or Failed"
Mar 23 20:27:17.310: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.183328ms
Mar 23 20:27:19.315: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037608734s
Mar 23 20:27:21.315: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Running", Reason="", readiness=false. Elapsed: 4.037555163s
Mar 23 20:27:23.316: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037823392s
Mar 23 20:27:23.316: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a" satisfied condition "Succeeded or Failed"
Mar 23 20:27:23.349: INFO: Got logs for pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Mar 23 20:27:23.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4414" for this suite. 03/23/23 20:27:23.356
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":361,"skipped":6679,"failed":0}
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:27:17.234
    Mar 23 20:27:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename security-context-test 03/23/23 20:27:17.236
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:17.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:17.26
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Mar 23 20:27:17.278: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a" in namespace "security-context-test-4414" to be "Succeeded or Failed"
    Mar 23 20:27:17.310: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Pending", Reason="", readiness=false. Elapsed: 32.183328ms
    Mar 23 20:27:19.315: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037608734s
    Mar 23 20:27:21.315: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Running", Reason="", readiness=false. Elapsed: 4.037555163s
    Mar 23 20:27:23.316: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037823392s
    Mar 23 20:27:23.316: INFO: Pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a" satisfied condition "Succeeded or Failed"
    Mar 23 20:27:23.349: INFO: Got logs for pod "busybox-privileged-false-ae9e2a40-694f-48f4-ba38-2579b002ec5a": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Mar 23 20:27:23.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4414" for this suite. 03/23/23 20:27:23.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 03/23/23 20:27:23.375
Mar 23 20:27:23.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
STEP: Building a namespace api object, basename emptydir 03/23/23 20:27:23.376
STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:23.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:23.399
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 03/23/23 20:27:23.401
Mar 23 20:27:23.418: INFO: Waiting up to 5m0s for pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915" in namespace "emptydir-384" to be "Succeeded or Failed"
Mar 23 20:27:23.439: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Pending", Reason="", readiness=false. Elapsed: 20.224955ms
Mar 23 20:27:25.443: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Running", Reason="", readiness=true. Elapsed: 2.023959876s
Mar 23 20:27:27.443: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Running", Reason="", readiness=false. Elapsed: 4.024729491s
Mar 23 20:27:29.444: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025186595s
STEP: Saw pod success 03/23/23 20:27:29.444
Mar 23 20:27:29.444: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915" satisfied condition "Succeeded or Failed"
Mar 23 20:27:29.447: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 container test-container: <nil>
STEP: delete the pod 03/23/23 20:27:29.453
Mar 23 20:27:29.467: INFO: Waiting for pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 to disappear
Mar 23 20:27:29.470: INFO: Pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Mar 23 20:27:29.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-384" for this suite. 03/23/23 20:27:29.475
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":362,"skipped":6686,"failed":0}
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 03/23/23 20:27:23.375
    Mar 23 20:27:23.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2420329342
    STEP: Building a namespace api object, basename emptydir 03/23/23 20:27:23.376
    STEP: Waiting for a default service account to be provisioned in namespace 03/23/23 20:27:23.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 03/23/23 20:27:23.399
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 03/23/23 20:27:23.401
    Mar 23 20:27:23.418: INFO: Waiting up to 5m0s for pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915" in namespace "emptydir-384" to be "Succeeded or Failed"
    Mar 23 20:27:23.439: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Pending", Reason="", readiness=false. Elapsed: 20.224955ms
    Mar 23 20:27:25.443: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Running", Reason="", readiness=true. Elapsed: 2.023959876s
    Mar 23 20:27:27.443: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Running", Reason="", readiness=false. Elapsed: 4.024729491s
    Mar 23 20:27:29.444: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025186595s
    STEP: Saw pod success 03/23/23 20:27:29.444
    Mar 23 20:27:29.444: INFO: Pod "pod-09855fb5-060f-4fc3-8afd-3376755cb915" satisfied condition "Succeeded or Failed"
    Mar 23 20:27:29.447: INFO: Trying to get logs from node k8s-linuxpool-16392394-1 pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 container test-container: <nil>
    STEP: delete the pod 03/23/23 20:27:29.453
    Mar 23 20:27:29.467: INFO: Waiting for pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 to disappear
    Mar 23 20:27:29.470: INFO: Pod pod-09855fb5-060f-4fc3-8afd-3376755cb915 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Mar 23 20:27:29.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-384" for this suite. 03/23/23 20:27:29.475
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Mar 23 20:27:29.494: INFO: Running AfterSuite actions on all nodes
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Mar 23 20:27:29.494: INFO: Running AfterSuite actions on node 1
Mar 23 20:27:29.494: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 23 20:27:29.494: INFO: Running AfterSuite actions on all nodes
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Mar 23 20:27:29.494: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Mar 23 20:27:29.494: INFO: Running AfterSuite actions on node 1
    Mar 23 20:27:29.494: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.132 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 6800.782 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h53m21.873258952s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

