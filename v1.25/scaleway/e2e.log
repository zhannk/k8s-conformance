I0118 16:58:22.793361      21 e2e.go:116] Starting e2e run "635e59ed-3adc-4183-91fa-918ac4beb641" on Ginkgo node 1
Jan 18 16:58:22.806: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674061102 - will randomize all specs

Will run 362 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 18 16:58:22.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 16:58:22.903: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 18 16:58:22.927: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 18 16:58:22.968: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 18 16:58:22.968: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jan 18 16:58:22.968: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-node' (0 seconds elapsed)
Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Jan 18 16:58:22.980: INFO: e2e test version: v1.25.5
Jan 18 16:58:22.982: INFO: kube-apiserver version: v1.25.5
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 18 16:58:22.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 16:58:22.990: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.090 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 16:58:22.900: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 16:58:22.903: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 18 16:58:22.927: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 18 16:58:22.968: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 18 16:58:22.968: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jan 18 16:58:22.968: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'csi-node' (0 seconds elapsed)
    Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan 18 16:58:22.980: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
    Jan 18 16:58:22.980: INFO: e2e test version: v1.25.5
    Jan 18 16:58:22.982: INFO: kube-apiserver version: v1.25.5
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 18 16:58:22.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 16:58:22.990: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:23.02
Jan 18 16:58:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 16:58:23.021
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:23.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:23.052
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-9a49d232-14c8-4b6b-8120-c9f232feb694 01/18/23 16:58:23.057
STEP: Creating a pod to test consume configMaps 01/18/23 16:58:23.067
Jan 18 16:58:23.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191" in namespace "configmap-552" to be "Succeeded or Failed"
Jan 18 16:58:23.094: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 9.445668ms
Jan 18 16:58:25.102: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017864394s
Jan 18 16:58:27.109: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024174346s
Jan 18 16:58:29.103: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Running", Reason="", readiness=false. Elapsed: 6.018721062s
Jan 18 16:58:31.104: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019896058s
STEP: Saw pod success 01/18/23 16:58:31.104
Jan 18 16:58:31.105: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191" satisfied condition "Succeeded or Failed"
Jan 18 16:58:31.111: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:58:31.147
Jan 18 16:58:31.170: INFO: Waiting for pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 to disappear
Jan 18 16:58:31.178: INFO: Pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 16:58:31.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-552" for this suite. 01/18/23 16:58:31.185
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":1,"skipped":24,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.179 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:23.02
    Jan 18 16:58:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 16:58:23.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:23.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:23.052
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-9a49d232-14c8-4b6b-8120-c9f232feb694 01/18/23 16:58:23.057
    STEP: Creating a pod to test consume configMaps 01/18/23 16:58:23.067
    Jan 18 16:58:23.084: INFO: Waiting up to 5m0s for pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191" in namespace "configmap-552" to be "Succeeded or Failed"
    Jan 18 16:58:23.094: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 9.445668ms
    Jan 18 16:58:25.102: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017864394s
    Jan 18 16:58:27.109: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024174346s
    Jan 18 16:58:29.103: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Running", Reason="", readiness=false. Elapsed: 6.018721062s
    Jan 18 16:58:31.104: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.019896058s
    STEP: Saw pod success 01/18/23 16:58:31.104
    Jan 18 16:58:31.105: INFO: Pod "pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191" satisfied condition "Succeeded or Failed"
    Jan 18 16:58:31.111: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:58:31.147
    Jan 18 16:58:31.170: INFO: Waiting for pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 to disappear
    Jan 18 16:58:31.178: INFO: Pod pod-configmaps-d050e993-e2dc-4974-bb71-5493d337e191 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 16:58:31.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-552" for this suite. 01/18/23 16:58:31.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:31.202
Jan 18 16:58:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 16:58:31.203
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:31.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:31.233
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 18 16:58:31.238: INFO: Creating simple deployment test-new-deployment
Jan 18 16:58:31.262: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jan 18 16:58:33.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:58:35.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 01/18/23 16:58:37.301
STEP: updating a scale subresource 01/18/23 16:58:37.309
STEP: verifying the deployment Spec.Replicas was modified 01/18/23 16:58:37.322
STEP: Patch a scale subresource 01/18/23 16:58:37.329
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 16:58:37.355: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9099  e96ce082-2e7a-44a0-af77-5eccb8c47372 2631789721 3 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002591558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 16:58:35 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 16:58:35 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 16:58:37.364: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9099  69bdd649-1f9a-4b8e-ae51-212146fbb7a9 2631789724 2 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e96ce082-2e7a-44a0-af77-5eccb8c47372 0xc0025919a7 0xc0025919a8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e96ce082-2e7a-44a0-af77-5eccb8c47372\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002591a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 16:58:37.371: INFO: Pod "test-new-deployment-845c8977d9-pflqb" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-pflqb test-new-deployment-845c8977d9- deployment-9099  c1bf9b97-d7b0-4cf7-ae28-a0983fe32485 2631789665 0 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2fd4a191fd5db4ba45fcfe6ef00e2cf98b8cd106e5c6ed45579017196fd8f7a0 cni.projectcalico.org/podIP:10.100.145.134/32 cni.projectcalico.org/podIPs:10.100.145.134/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 69bdd649-1f9a-4b8e-ae51-212146fbb7a9 0xc00237c467 0xc00237c468}] [] [{calico Update v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69bdd649-1f9a-4b8e-ae51-212146fbb7a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 16:58:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqlgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqlgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.134,StartTime:2023-01-18 16:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:58:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4d33545811f7e4322babe3a090c7f9960930226edfa652c5ed3d77a74e11c376,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 16:58:37.371: INFO: Pod "test-new-deployment-845c8977d9-tk6hr" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-tk6hr test-new-deployment-845c8977d9- deployment-9099  662ace29-21d1-4687-98ca-1f4a4d55e4e5 2631789723 0 2023-01-18 16:58:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 69bdd649-1f9a-4b8e-ae51-212146fbb7a9 0xc00237c657 0xc00237c658}] [] [{kube-controller-manager Update v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69bdd649-1f9a-4b8e-ae51-212146fbb7a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qk2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qk2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 16:58:37.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9099" for this suite. 01/18/23 16:58:37.378
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":2,"skipped":54,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.257 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:31.202
    Jan 18 16:58:31.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 16:58:31.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:31.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:31.233
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 18 16:58:31.238: INFO: Creating simple deployment test-new-deployment
    Jan 18 16:58:31.262: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Jan 18 16:58:33.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:58:35.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 01/18/23 16:58:37.301
    STEP: updating a scale subresource 01/18/23 16:58:37.309
    STEP: verifying the deployment Spec.Replicas was modified 01/18/23 16:58:37.322
    STEP: Patch a scale subresource 01/18/23 16:58:37.329
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 16:58:37.355: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9099  e96ce082-2e7a-44a0-af77-5eccb8c47372 2631789721 3 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:58:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002591558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 16:58:35 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-18 16:58:35 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 16:58:37.364: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9099  69bdd649-1f9a-4b8e-ae51-212146fbb7a9 2631789724 2 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment e96ce082-2e7a-44a0-af77-5eccb8c47372 0xc0025919a7 0xc0025919a8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e96ce082-2e7a-44a0-af77-5eccb8c47372\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002591a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 16:58:37.371: INFO: Pod "test-new-deployment-845c8977d9-pflqb" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-pflqb test-new-deployment-845c8977d9- deployment-9099  c1bf9b97-d7b0-4cf7-ae28-a0983fe32485 2631789665 0 2023-01-18 16:58:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2fd4a191fd5db4ba45fcfe6ef00e2cf98b8cd106e5c6ed45579017196fd8f7a0 cni.projectcalico.org/podIP:10.100.145.134/32 cni.projectcalico.org/podIPs:10.100.145.134/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 69bdd649-1f9a-4b8e-ae51-212146fbb7a9 0xc00237c467 0xc00237c468}] [] [{calico Update v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 16:58:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69bdd649-1f9a-4b8e-ae51-212146fbb7a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 16:58:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqlgz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqlgz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.134,StartTime:2023-01-18 16:58:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 16:58:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4d33545811f7e4322babe3a090c7f9960930226edfa652c5ed3d77a74e11c376,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 16:58:37.371: INFO: Pod "test-new-deployment-845c8977d9-tk6hr" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-tk6hr test-new-deployment-845c8977d9- deployment-9099  662ace29-21d1-4687-98ca-1f4a4d55e4e5 2631789723 0 2023-01-18 16:58:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 69bdd649-1f9a-4b8e-ae51-212146fbb7a9 0xc00237c657 0xc00237c658}] [] [{kube-controller-manager Update v1 2023-01-18 16:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69bdd649-1f9a-4b8e-ae51-212146fbb7a9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qk2n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qk2n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 16:58:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 16:58:37.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9099" for this suite. 01/18/23 16:58:37.378
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:37.46
Jan 18 16:58:37.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 16:58:37.462
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:37.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:37.491
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 16:58:37.568
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:58:37.916
STEP: Deploying the webhook pod 01/18/23 16:58:37.932
STEP: Wait for the deployment to be ready 01/18/23 16:58:37.954
Jan 18 16:58:37.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 16:58:39.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 16:58:42.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 16:58:44.003
STEP: Verifying the service has paired with the endpoint 01/18/23 16:58:44.025
Jan 18 16:58:45.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:58:45.033
STEP: create a pod 01/18/23 16:58:45.067
Jan 18 16:58:45.077: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-621" to be "running"
Jan 18 16:58:45.083: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399276ms
Jan 18 16:58:47.091: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014375671s
Jan 18 16:58:47.091: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 16:58:47.091
Jan 18 16:58:47.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=webhook-621 attach --namespace=webhook-621 to-be-attached-pod -i -c=container1'
Jan 18 16:58:47.254: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 16:58:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-621" for this suite. 01/18/23 16:58:47.275
STEP: Destroying namespace "webhook-621-markers" for this suite. 01/18/23 16:58:47.287
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":3,"skipped":54,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.911 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:37.46
    Jan 18 16:58:37.460: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 16:58:37.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:37.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:37.491
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 16:58:37.568
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 16:58:37.916
    STEP: Deploying the webhook pod 01/18/23 16:58:37.932
    STEP: Wait for the deployment to be ready 01/18/23 16:58:37.954
    Jan 18 16:58:37.970: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 16:58:39.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 16:58:42.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 16, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 16, 58, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 16:58:44.003
    STEP: Verifying the service has paired with the endpoint 01/18/23 16:58:44.025
    Jan 18 16:58:45.025: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 16:58:45.033
    STEP: create a pod 01/18/23 16:58:45.067
    Jan 18 16:58:45.077: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-621" to be "running"
    Jan 18 16:58:45.083: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.399276ms
    Jan 18 16:58:47.091: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014375671s
    Jan 18 16:58:47.091: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/18/23 16:58:47.091
    Jan 18 16:58:47.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=webhook-621 attach --namespace=webhook-621 to-be-attached-pod -i -c=container1'
    Jan 18 16:58:47.254: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 16:58:47.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-621" for this suite. 01/18/23 16:58:47.275
    STEP: Destroying namespace "webhook-621-markers" for this suite. 01/18/23 16:58:47.287
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:47.373
Jan 18 16:58:47.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename containers 01/18/23 16:58:47.377
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:47.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:47.408
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/18/23 16:58:47.414
Jan 18 16:58:47.431: INFO: Waiting up to 5m0s for pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf" in namespace "containers-1602" to be "Succeeded or Failed"
Jan 18 16:58:47.437: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.70954ms
Jan 18 16:58:49.445: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014200118s
Jan 18 16:58:51.447: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016109252s
STEP: Saw pod success 01/18/23 16:58:51.447
Jan 18 16:58:51.447: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf" satisfied condition "Succeeded or Failed"
Jan 18 16:58:51.455: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf container agnhost-container: <nil>
STEP: delete the pod 01/18/23 16:58:51.49
Jan 18 16:58:51.510: INFO: Waiting for pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf to disappear
Jan 18 16:58:51.518: INFO: Pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 16:58:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1602" for this suite. 01/18/23 16:58:51.527
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":4,"skipped":94,"failed":0}
------------------------------
â€¢ [4.167 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:47.373
    Jan 18 16:58:47.374: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename containers 01/18/23 16:58:47.377
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:47.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:47.408
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/18/23 16:58:47.414
    Jan 18 16:58:47.431: INFO: Waiting up to 5m0s for pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf" in namespace "containers-1602" to be "Succeeded or Failed"
    Jan 18 16:58:47.437: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.70954ms
    Jan 18 16:58:49.445: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014200118s
    Jan 18 16:58:51.447: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016109252s
    STEP: Saw pod success 01/18/23 16:58:51.447
    Jan 18 16:58:51.447: INFO: Pod "client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf" satisfied condition "Succeeded or Failed"
    Jan 18 16:58:51.455: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 16:58:51.49
    Jan 18 16:58:51.510: INFO: Waiting for pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf to disappear
    Jan 18 16:58:51.518: INFO: Pod client-containers-7c31141d-f956-4c73-ad97-df3fc96044bf no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 16:58:51.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1602" for this suite. 01/18/23 16:58:51.527
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:51.541
Jan 18 16:58:51.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename init-container 01/18/23 16:58:51.542
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:51.566
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:51.572
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/18/23 16:58:51.577
Jan 18 16:58:51.577: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 16:58:57.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9050" for this suite. 01/18/23 16:58:57.283
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":5,"skipped":97,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.759 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:51.541
    Jan 18 16:58:51.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename init-container 01/18/23 16:58:51.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:51.566
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:51.572
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/18/23 16:58:51.577
    Jan 18 16:58:51.577: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 16:58:57.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9050" for this suite. 01/18/23 16:58:57.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:57.302
Jan 18 16:58:57.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename endpointslice 01/18/23 16:58:57.303
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:57.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:57.331
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 16:58:59.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-5637" for this suite. 01/18/23 16:58:59.449
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":6,"skipped":118,"failed":0}
------------------------------
â€¢ [2.160 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:57.302
    Jan 18 16:58:57.302: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 16:58:57.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:57.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:57.331
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 16:58:59.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-5637" for this suite. 01/18/23 16:58:59.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:58:59.466
Jan 18 16:58:59.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 16:58:59.468
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:59.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:59.493
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-e81189ca-8fdc-4df3-8b50-2479191b3ac9 01/18/23 16:58:59.529
STEP: Creating a pod to test consume secrets 01/18/23 16:58:59.537
Jan 18 16:58:59.550: INFO: Waiting up to 5m0s for pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6" in namespace "secrets-7854" to be "Succeeded or Failed"
Jan 18 16:58:59.561: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091249ms
Jan 18 16:59:01.569: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018960528s
Jan 18 16:59:03.571: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020577661s
STEP: Saw pod success 01/18/23 16:59:03.571
Jan 18 16:59:03.571: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6" satisfied condition "Succeeded or Failed"
Jan 18 16:59:03.582: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 16:59:03.596
Jan 18 16:59:03.617: INFO: Waiting for pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 to disappear
Jan 18 16:59:03.624: INFO: Pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 16:59:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7854" for this suite. 01/18/23 16:59:03.633
STEP: Destroying namespace "secret-namespace-7810" for this suite. 01/18/23 16:59:03.647
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":7,"skipped":154,"failed":0}
------------------------------
â€¢ [4.195 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:58:59.466
    Jan 18 16:58:59.466: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 16:58:59.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:58:59.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:58:59.493
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-e81189ca-8fdc-4df3-8b50-2479191b3ac9 01/18/23 16:58:59.529
    STEP: Creating a pod to test consume secrets 01/18/23 16:58:59.537
    Jan 18 16:58:59.550: INFO: Waiting up to 5m0s for pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6" in namespace "secrets-7854" to be "Succeeded or Failed"
    Jan 18 16:58:59.561: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091249ms
    Jan 18 16:59:01.569: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018960528s
    Jan 18 16:59:03.571: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020577661s
    STEP: Saw pod success 01/18/23 16:59:03.571
    Jan 18 16:59:03.571: INFO: Pod "pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6" satisfied condition "Succeeded or Failed"
    Jan 18 16:59:03.582: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 16:59:03.596
    Jan 18 16:59:03.617: INFO: Waiting for pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 to disappear
    Jan 18 16:59:03.624: INFO: Pod pod-secrets-3ca2bb3e-db8e-4601-9881-2bc897933ed6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 16:59:03.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7854" for this suite. 01/18/23 16:59:03.633
    STEP: Destroying namespace "secret-namespace-7810" for this suite. 01/18/23 16:59:03.647
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:59:03.661
Jan 18 16:59:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 16:59:03.663
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:03.687
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:03.692
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 16:59:03.696
Jan 18 16:59:03.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db" in namespace "downward-api-8461" to be "Succeeded or Failed"
Jan 18 16:59:03.719: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.754791ms
Jan 18 16:59:05.727: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013860941s
Jan 18 16:59:07.728: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0146179s
STEP: Saw pod success 01/18/23 16:59:07.728
Jan 18 16:59:07.728: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db" satisfied condition "Succeeded or Failed"
Jan 18 16:59:07.735: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db container client-container: <nil>
STEP: delete the pod 01/18/23 16:59:07.75
Jan 18 16:59:07.771: INFO: Waiting for pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db to disappear
Jan 18 16:59:07.779: INFO: Pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 16:59:07.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8461" for this suite. 01/18/23 16:59:07.786
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":8,"skipped":155,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:59:03.661
    Jan 18 16:59:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 16:59:03.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:03.687
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:03.692
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 16:59:03.696
    Jan 18 16:59:03.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db" in namespace "downward-api-8461" to be "Succeeded or Failed"
    Jan 18 16:59:03.719: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.754791ms
    Jan 18 16:59:05.727: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013860941s
    Jan 18 16:59:07.728: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0146179s
    STEP: Saw pod success 01/18/23 16:59:07.728
    Jan 18 16:59:07.728: INFO: Pod "downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db" satisfied condition "Succeeded or Failed"
    Jan 18 16:59:07.735: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db container client-container: <nil>
    STEP: delete the pod 01/18/23 16:59:07.75
    Jan 18 16:59:07.771: INFO: Waiting for pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db to disappear
    Jan 18 16:59:07.779: INFO: Pod downwardapi-volume-a5508056-dedf-4a1d-8421-dc10d8c113db no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 16:59:07.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8461" for this suite. 01/18/23 16:59:07.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:59:07.801
Jan 18 16:59:07.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 16:59:07.802
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:07.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:07.834
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 16:59:07.839
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/18/23 16:59:07.84
STEP: creating a pod to probe DNS 01/18/23 16:59:07.84
STEP: submitting the pod to kubernetes 01/18/23 16:59:07.84
Jan 18 16:59:07.854: INFO: Waiting up to 15m0s for pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301" in namespace "dns-3101" to be "running"
Jan 18 16:59:07.861: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.958088ms
Jan 18 16:59:09.870: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015592734s
Jan 18 16:59:11.870: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015648199s
Jan 18 16:59:13.871: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016533601s
Jan 18 16:59:15.873: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Running", Reason="", readiness=true. Elapsed: 8.01823054s
Jan 18 16:59:15.873: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301" satisfied condition "running"
STEP: retrieving the pod 01/18/23 16:59:15.873
STEP: looking for the results for each expected name from probers 01/18/23 16:59:15.88
Jan 18 16:59:15.924: INFO: DNS probes using dns-3101/dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301 succeeded

STEP: deleting the pod 01/18/23 16:59:15.924
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 16:59:15.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3101" for this suite. 01/18/23 16:59:15.959
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":9,"skipped":187,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.172 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:59:07.801
    Jan 18 16:59:07.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 16:59:07.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:07.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:07.834
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 16:59:07.839
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/18/23 16:59:07.84
    STEP: creating a pod to probe DNS 01/18/23 16:59:07.84
    STEP: submitting the pod to kubernetes 01/18/23 16:59:07.84
    Jan 18 16:59:07.854: INFO: Waiting up to 15m0s for pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301" in namespace "dns-3101" to be "running"
    Jan 18 16:59:07.861: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.958088ms
    Jan 18 16:59:09.870: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015592734s
    Jan 18 16:59:11.870: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015648199s
    Jan 18 16:59:13.871: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016533601s
    Jan 18 16:59:15.873: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301": Phase="Running", Reason="", readiness=true. Elapsed: 8.01823054s
    Jan 18 16:59:15.873: INFO: Pod "dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 16:59:15.873
    STEP: looking for the results for each expected name from probers 01/18/23 16:59:15.88
    Jan 18 16:59:15.924: INFO: DNS probes using dns-3101/dns-test-6ba50eb0-b492-4f36-9c51-b272bf7b2301 succeeded

    STEP: deleting the pod 01/18/23 16:59:15.924
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 16:59:15.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3101" for this suite. 01/18/23 16:59:15.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:59:15.974
Jan 18 16:59:15.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 16:59:15.976
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:16.005
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/18/23 16:59:16.01
Jan 18 16:59:16.026: INFO: Waiting up to 5m0s for pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174" in namespace "var-expansion-2547" to be "Succeeded or Failed"
Jan 18 16:59:16.033: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07077ms
Jan 18 16:59:18.041: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014634109s
Jan 18 16:59:20.043: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017051274s
Jan 18 16:59:22.044: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017420145s
STEP: Saw pod success 01/18/23 16:59:22.044
Jan 18 16:59:22.044: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174" satisfied condition "Succeeded or Failed"
Jan 18 16:59:22.050: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 container dapi-container: <nil>
STEP: delete the pod 01/18/23 16:59:22.063
Jan 18 16:59:22.083: INFO: Waiting for pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 to disappear
Jan 18 16:59:22.090: INFO: Pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 16:59:22.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2547" for this suite. 01/18/23 16:59:22.098
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":10,"skipped":198,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.137 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:59:15.974
    Jan 18 16:59:15.974: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 16:59:15.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:16.005
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/18/23 16:59:16.01
    Jan 18 16:59:16.026: INFO: Waiting up to 5m0s for pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174" in namespace "var-expansion-2547" to be "Succeeded or Failed"
    Jan 18 16:59:16.033: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07077ms
    Jan 18 16:59:18.041: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014634109s
    Jan 18 16:59:20.043: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017051274s
    Jan 18 16:59:22.044: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017420145s
    STEP: Saw pod success 01/18/23 16:59:22.044
    Jan 18 16:59:22.044: INFO: Pod "var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174" satisfied condition "Succeeded or Failed"
    Jan 18 16:59:22.050: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 16:59:22.063
    Jan 18 16:59:22.083: INFO: Waiting for pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 to disappear
    Jan 18 16:59:22.090: INFO: Pod var-expansion-05b755c3-9d16-44ab-b8ae-274fdd56a174 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 16:59:22.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2547" for this suite. 01/18/23 16:59:22.098
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:59:22.112
Jan 18 16:59:22.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename events 01/18/23 16:59:22.113
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:22.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:22.14
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/18/23 16:59:22.145
STEP: listing all events in all namespaces 01/18/23 16:59:22.17
STEP: patching the test event 01/18/23 16:59:22.184
STEP: fetching the test event 01/18/23 16:59:22.196
STEP: updating the test event 01/18/23 16:59:22.201
STEP: getting the test event 01/18/23 16:59:22.216
STEP: deleting the test event 01/18/23 16:59:22.22
STEP: listing all events in all namespaces 01/18/23 16:59:22.231
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 16:59:22.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7628" for this suite. 01/18/23 16:59:22.25
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":11,"skipped":199,"failed":0}
------------------------------
â€¢ [0.151 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:59:22.112
    Jan 18 16:59:22.112: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename events 01/18/23 16:59:22.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:22.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:22.14
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/18/23 16:59:22.145
    STEP: listing all events in all namespaces 01/18/23 16:59:22.17
    STEP: patching the test event 01/18/23 16:59:22.184
    STEP: fetching the test event 01/18/23 16:59:22.196
    STEP: updating the test event 01/18/23 16:59:22.201
    STEP: getting the test event 01/18/23 16:59:22.216
    STEP: deleting the test event 01/18/23 16:59:22.22
    STEP: listing all events in all namespaces 01/18/23 16:59:22.231
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 16:59:22.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7628" for this suite. 01/18/23 16:59:22.25
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 16:59:22.263
Jan 18 16:59:22.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:59:22.264
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:22.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:22.295
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 16:59:22.325: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 17:00:22.463: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:00:22.471
Jan 18 17:00:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 17:00:22.472
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:22.572
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:22.577
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/18/23 17:00:22.582
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 17:00:22.582
Jan 18 17:00:22.598: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3213" to be "running"
Jan 18 17:00:22.604: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606849ms
Jan 18 17:00:24.614: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016152598s
Jan 18 17:00:24.615: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 17:00:24.622
Jan 18 17:00:24.642: INFO: found a healthy node: scw-conformance125-default-61c39bbf4d81476a8e3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 18 17:00:32.793: INFO: pods created so far: [1 1 1]
Jan 18 17:00:32.793: INFO: length of pods created so far: 3
Jan 18 17:00:34.812: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 18 17:00:41.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3213" for this suite. 01/18/23 17:00:41.827
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:00:41.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9887" for this suite. 01/18/23 17:00:41.913
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":12,"skipped":208,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.735 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 16:59:22.263
    Jan 18 16:59:22.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 16:59:22.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 16:59:22.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 16:59:22.295
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 16:59:22.325: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 17:00:22.463: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:00:22.471
    Jan 18 17:00:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 17:00:22.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:22.572
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:22.577
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/18/23 17:00:22.582
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 17:00:22.582
    Jan 18 17:00:22.598: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3213" to be "running"
    Jan 18 17:00:22.604: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.606849ms
    Jan 18 17:00:24.614: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016152598s
    Jan 18 17:00:24.615: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 17:00:24.622
    Jan 18 17:00:24.642: INFO: found a healthy node: scw-conformance125-default-61c39bbf4d81476a8e3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 18 17:00:32.793: INFO: pods created so far: [1 1 1]
    Jan 18 17:00:32.793: INFO: length of pods created so far: 3
    Jan 18 17:00:34.812: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 18 17:00:41.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3213" for this suite. 01/18/23 17:00:41.827
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:00:41.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9887" for this suite. 01/18/23 17:00:41.913
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:00:41.998
Jan 18 17:00:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename ingress 01/18/23 17:00:42
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:42.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:42.03
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/18/23 17:00:42.035
STEP: getting /apis/networking.k8s.io 01/18/23 17:00:42.04
STEP: getting /apis/networking.k8s.iov1 01/18/23 17:00:42.041
STEP: creating 01/18/23 17:00:42.043
STEP: getting 01/18/23 17:00:42.072
STEP: listing 01/18/23 17:00:42.079
STEP: watching 01/18/23 17:00:42.085
Jan 18 17:00:42.085: INFO: starting watch
STEP: cluster-wide listing 01/18/23 17:00:42.087
STEP: cluster-wide watching 01/18/23 17:00:42.094
Jan 18 17:00:42.094: INFO: starting watch
STEP: patching 01/18/23 17:00:42.096
STEP: updating 01/18/23 17:00:42.106
Jan 18 17:00:42.120: INFO: waiting for watch events with expected annotations
Jan 18 17:00:42.120: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 17:00:42.121
STEP: updating /status 01/18/23 17:00:42.131
STEP: get /status 01/18/23 17:00:42.146
STEP: deleting 01/18/23 17:00:42.153
STEP: deleting a collection 01/18/23 17:00:42.18
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 18 17:00:42.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-551" for this suite. 01/18/23 17:00:42.22
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":13,"skipped":211,"failed":0}
------------------------------
â€¢ [0.234 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:00:41.998
    Jan 18 17:00:41.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename ingress 01/18/23 17:00:42
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:42.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:42.03
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/18/23 17:00:42.035
    STEP: getting /apis/networking.k8s.io 01/18/23 17:00:42.04
    STEP: getting /apis/networking.k8s.iov1 01/18/23 17:00:42.041
    STEP: creating 01/18/23 17:00:42.043
    STEP: getting 01/18/23 17:00:42.072
    STEP: listing 01/18/23 17:00:42.079
    STEP: watching 01/18/23 17:00:42.085
    Jan 18 17:00:42.085: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 17:00:42.087
    STEP: cluster-wide watching 01/18/23 17:00:42.094
    Jan 18 17:00:42.094: INFO: starting watch
    STEP: patching 01/18/23 17:00:42.096
    STEP: updating 01/18/23 17:00:42.106
    Jan 18 17:00:42.120: INFO: waiting for watch events with expected annotations
    Jan 18 17:00:42.120: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 17:00:42.121
    STEP: updating /status 01/18/23 17:00:42.131
    STEP: get /status 01/18/23 17:00:42.146
    STEP: deleting 01/18/23 17:00:42.153
    STEP: deleting a collection 01/18/23 17:00:42.18
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 18 17:00:42.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-551" for this suite. 01/18/23 17:00:42.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:00:42.233
Jan 18 17:00:42.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:00:42.234
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:42.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:42.275
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-3178 01/18/23 17:00:42.28
STEP: creating service affinity-nodeport-transition in namespace services-3178 01/18/23 17:00:42.28
STEP: creating replication controller affinity-nodeport-transition in namespace services-3178 01/18/23 17:00:42.311
I0118 17:00:42.323752      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3178, replica count: 3
I0118 17:00:45.374784      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:00:45.399: INFO: Creating new exec pod
Jan 18 17:00:45.408: INFO: Waiting up to 5m0s for pod "execpod-affinityxn4m4" in namespace "services-3178" to be "running"
Jan 18 17:00:45.415: INFO: Pod "execpod-affinityxn4m4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951287ms
Jan 18 17:00:47.425: INFO: Pod "execpod-affinityxn4m4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016431231s
Jan 18 17:00:47.425: INFO: Pod "execpod-affinityxn4m4" satisfied condition "running"
Jan 18 17:00:48.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 18 17:00:48.644: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 18 17:00:48.644: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:00:48.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.168.235 80'
Jan 18 17:00:48.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.168.235 80\nConnection to 10.96.168.235 80 port [tcp/http] succeeded!\n"
Jan 18 17:00:48.838: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:00:48.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 31640'
Jan 18 17:00:49.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 31640\nConnection to 10.195.74.123 31640 port [tcp/*] succeeded!\n"
Jan 18 17:00:49.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:00:49.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 31640'
Jan 18 17:00:49.233: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 31640\nConnection to 10.195.78.23 31640 port [tcp/*] succeeded!\n"
Jan 18 17:00:49.233: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:00:49.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31640/ ; done'
Jan 18 17:00:49.539: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n"
Jan 18 17:00:49.539: INFO: stdout: "\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-9vqwn\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-9vqwn\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4"
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-9vqwn
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-9vqwn
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
Jan 18 17:00:49.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31640/ ; done'
Jan 18 17:00:49.835: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n"
Jan 18 17:00:49.835: INFO: stdout: "\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft"
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
Jan 18 17:00:49.835: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3178, will wait for the garbage collector to delete the pods 01/18/23 17:00:49.858
Jan 18 17:00:49.930: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.158647ms
Jan 18 17:00:50.031: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.075762ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:00:52.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3178" for this suite. 01/18/23 17:00:52.275
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":14,"skipped":216,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.056 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:00:42.233
    Jan 18 17:00:42.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:00:42.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:42.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:42.275
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-3178 01/18/23 17:00:42.28
    STEP: creating service affinity-nodeport-transition in namespace services-3178 01/18/23 17:00:42.28
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3178 01/18/23 17:00:42.311
    I0118 17:00:42.323752      21 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3178, replica count: 3
    I0118 17:00:45.374784      21 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:00:45.399: INFO: Creating new exec pod
    Jan 18 17:00:45.408: INFO: Waiting up to 5m0s for pod "execpod-affinityxn4m4" in namespace "services-3178" to be "running"
    Jan 18 17:00:45.415: INFO: Pod "execpod-affinityxn4m4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951287ms
    Jan 18 17:00:47.425: INFO: Pod "execpod-affinityxn4m4": Phase="Running", Reason="", readiness=true. Elapsed: 2.016431231s
    Jan 18 17:00:47.425: INFO: Pod "execpod-affinityxn4m4" satisfied condition "running"
    Jan 18 17:00:48.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 18 17:00:48.644: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 17:00:48.644: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:00:48.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.168.235 80'
    Jan 18 17:00:48.838: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.168.235 80\nConnection to 10.96.168.235 80 port [tcp/http] succeeded!\n"
    Jan 18 17:00:48.838: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:00:48.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 31640'
    Jan 18 17:00:49.039: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 31640\nConnection to 10.195.74.123 31640 port [tcp/*] succeeded!\n"
    Jan 18 17:00:49.039: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:00:49.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 31640'
    Jan 18 17:00:49.233: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 31640\nConnection to 10.195.78.23 31640 port [tcp/*] succeeded!\n"
    Jan 18 17:00:49.233: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:00:49.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31640/ ; done'
    Jan 18 17:00:49.539: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n"
    Jan 18 17:00:49.539: INFO: stdout: "\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-9vqwn\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-9vqwn\naffinity-nodeport-transition-j24v4\naffinity-nodeport-transition-j24v4"
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-9vqwn
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-9vqwn
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.539: INFO: Received response from host: affinity-nodeport-transition-j24v4
    Jan 18 17:00:49.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3178 exec execpod-affinityxn4m4 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31640/ ; done'
    Jan 18 17:00:49.835: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31640/\n"
    Jan 18 17:00:49.835: INFO: stdout: "\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft\naffinity-nodeport-transition-blgft"
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Received response from host: affinity-nodeport-transition-blgft
    Jan 18 17:00:49.835: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3178, will wait for the garbage collector to delete the pods 01/18/23 17:00:49.858
    Jan 18 17:00:49.930: INFO: Deleting ReplicationController affinity-nodeport-transition took: 14.158647ms
    Jan 18 17:00:50.031: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.075762ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:00:52.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3178" for this suite. 01/18/23 17:00:52.275
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:00:52.289
Jan 18 17:00:52.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:00:52.291
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:52.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:52.32
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 18 17:00:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:00:59.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2266" for this suite. 01/18/23 17:00:59.421
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":15,"skipped":229,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.147 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:00:52.289
    Jan 18 17:00:52.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:00:52.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:52.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:52.32
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 18 17:00:52.325: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:00:59.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2266" for this suite. 01/18/23 17:00:59.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:00:59.44
Jan 18 17:00:59.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svc-latency 01/18/23 17:00:59.441
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:59.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:59.47
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 18 17:00:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5490 01/18/23 17:00:59.476
I0118 17:00:59.490008      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5490, replica count: 1
I0118 17:01:00.541424      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0118 17:01:01.541921      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:01:01.664: INFO: Created: latency-svc-zdl8n
Jan 18 17:01:01.675: INFO: Got endpoints: latency-svc-zdl8n [33.259446ms]
Jan 18 17:01:01.697: INFO: Created: latency-svc-bxbqk
Jan 18 17:01:01.706: INFO: Created: latency-svc-gg29v
Jan 18 17:01:01.707: INFO: Got endpoints: latency-svc-bxbqk [31.804136ms]
Jan 18 17:01:01.717: INFO: Got endpoints: latency-svc-gg29v [41.981317ms]
Jan 18 17:01:01.717: INFO: Created: latency-svc-jbjbv
Jan 18 17:01:01.759: INFO: Got endpoints: latency-svc-jbjbv [83.775622ms]
Jan 18 17:01:01.766: INFO: Created: latency-svc-g4mjp
Jan 18 17:01:01.775: INFO: Got endpoints: latency-svc-g4mjp [99.80615ms]
Jan 18 17:01:01.778: INFO: Created: latency-svc-p6phj
Jan 18 17:01:01.786: INFO: Got endpoints: latency-svc-p6phj [110.758776ms]
Jan 18 17:01:01.789: INFO: Created: latency-svc-b9fpx
Jan 18 17:01:01.797: INFO: Created: latency-svc-kz4qw
Jan 18 17:01:01.803: INFO: Got endpoints: latency-svc-b9fpx [127.021956ms]
Jan 18 17:01:01.803: INFO: Got endpoints: latency-svc-kz4qw [127.500169ms]
Jan 18 17:01:01.806: INFO: Created: latency-svc-kqbsl
Jan 18 17:01:01.820: INFO: Created: latency-svc-b2d44
Jan 18 17:01:01.861: INFO: Got endpoints: latency-svc-kqbsl [185.013062ms]
Jan 18 17:01:01.869: INFO: Created: latency-svc-n4c8t
Jan 18 17:01:01.870: INFO: Got endpoints: latency-svc-b2d44 [194.529297ms]
Jan 18 17:01:01.878: INFO: Got endpoints: latency-svc-n4c8t [201.985237ms]
Jan 18 17:01:01.880: INFO: Created: latency-svc-mj4t2
Jan 18 17:01:01.890: INFO: Got endpoints: latency-svc-mj4t2 [214.126241ms]
Jan 18 17:01:01.890: INFO: Created: latency-svc-pngwq
Jan 18 17:01:01.901: INFO: Got endpoints: latency-svc-pngwq [224.740684ms]
Jan 18 17:01:01.904: INFO: Created: latency-svc-w4jvn
Jan 18 17:01:01.912: INFO: Got endpoints: latency-svc-w4jvn [236.150722ms]
Jan 18 17:01:01.917: INFO: Created: latency-svc-mglsd
Jan 18 17:01:01.931: INFO: Created: latency-svc-tf6hc
Jan 18 17:01:01.959: INFO: Got endpoints: latency-svc-mglsd [282.651448ms]
Jan 18 17:01:01.966: INFO: Created: latency-svc-vg25j
Jan 18 17:01:01.968: INFO: Got endpoints: latency-svc-tf6hc [292.395485ms]
Jan 18 17:01:01.974: INFO: Got endpoints: latency-svc-vg25j [267.319065ms]
Jan 18 17:01:01.977: INFO: Created: latency-svc-4vh9f
Jan 18 17:01:01.986: INFO: Got endpoints: latency-svc-4vh9f [269.250577ms]
Jan 18 17:01:01.995: INFO: Created: latency-svc-vgxs9
Jan 18 17:01:02.003: INFO: Created: latency-svc-7v8j8
Jan 18 17:01:02.005: INFO: Got endpoints: latency-svc-vgxs9 [245.853388ms]
Jan 18 17:01:02.012: INFO: Created: latency-svc-22hmz
Jan 18 17:01:02.014: INFO: Got endpoints: latency-svc-7v8j8 [238.547037ms]
Jan 18 17:01:02.059: INFO: Got endpoints: latency-svc-22hmz [272.724431ms]
Jan 18 17:01:02.065: INFO: Created: latency-svc-bhz7r
Jan 18 17:01:02.073: INFO: Got endpoints: latency-svc-bhz7r [270.678898ms]
Jan 18 17:01:02.080: INFO: Created: latency-svc-lwbf5
Jan 18 17:01:02.088: INFO: Got endpoints: latency-svc-lwbf5 [285.159396ms]
Jan 18 17:01:02.091: INFO: Created: latency-svc-tt4c5
Jan 18 17:01:02.100: INFO: Got endpoints: latency-svc-tt4c5 [239.025381ms]
Jan 18 17:01:02.104: INFO: Created: latency-svc-hhtjf
Jan 18 17:01:02.114: INFO: Got endpoints: latency-svc-hhtjf [243.208319ms]
Jan 18 17:01:02.117: INFO: Created: latency-svc-mll6z
Jan 18 17:01:02.159: INFO: Got endpoints: latency-svc-mll6z [280.882646ms]
Jan 18 17:01:02.165: INFO: Created: latency-svc-lvjb7
Jan 18 17:01:02.175: INFO: Got endpoints: latency-svc-lvjb7 [285.059275ms]
Jan 18 17:01:02.182: INFO: Created: latency-svc-n6xnr
Jan 18 17:01:02.192: INFO: Got endpoints: latency-svc-n6xnr [290.884384ms]
Jan 18 17:01:02.193: INFO: Created: latency-svc-lvkzh
Jan 18 17:01:02.200: INFO: Got endpoints: latency-svc-lvkzh [287.914094ms]
Jan 18 17:01:02.204: INFO: Created: latency-svc-62p69
Jan 18 17:01:02.216: INFO: Got endpoints: latency-svc-62p69 [256.689561ms]
Jan 18 17:01:02.260: INFO: Created: latency-svc-lbrbh
Jan 18 17:01:02.265: INFO: Created: latency-svc-dqxzv
Jan 18 17:01:02.269: INFO: Got endpoints: latency-svc-lbrbh [300.792961ms]
Jan 18 17:01:02.273: INFO: Got endpoints: latency-svc-dqxzv [298.552816ms]
Jan 18 17:01:02.289: INFO: Created: latency-svc-txmj4
Jan 18 17:01:02.302: INFO: Got endpoints: latency-svc-txmj4 [315.912274ms]
Jan 18 17:01:02.313: INFO: Created: latency-svc-n4wnz
Jan 18 17:01:02.324: INFO: Created: latency-svc-rstgt
Jan 18 17:01:02.332: INFO: Created: latency-svc-7mklb
Jan 18 17:01:02.350: INFO: Created: latency-svc-lxhq2
Jan 18 17:01:02.365: INFO: Created: latency-svc-65l9f
Jan 18 17:01:02.374: INFO: Created: latency-svc-dbjdq
Jan 18 17:01:02.382: INFO: Got endpoints: latency-svc-n4wnz [377.051761ms]
Jan 18 17:01:02.386: INFO: Got endpoints: latency-svc-7mklb [327.370692ms]
Jan 18 17:01:02.387: INFO: Got endpoints: latency-svc-65l9f [298.631326ms]
Jan 18 17:01:02.387: INFO: Got endpoints: latency-svc-rstgt [373.479937ms]
Jan 18 17:01:02.393: INFO: Got endpoints: latency-svc-lxhq2 [319.881701ms]
Jan 18 17:01:02.395: INFO: Created: latency-svc-tcwb9
Jan 18 17:01:02.395: INFO: Got endpoints: latency-svc-dbjdq [294.72392ms]
Jan 18 17:01:02.408: INFO: Got endpoints: latency-svc-tcwb9 [293.969614ms]
Jan 18 17:01:02.459: INFO: Created: latency-svc-4qkcp
Jan 18 17:01:02.466: INFO: Got endpoints: latency-svc-4qkcp [307.357257ms]
Jan 18 17:01:02.466: INFO: Created: latency-svc-gr2qh
Jan 18 17:01:02.477: INFO: Got endpoints: latency-svc-gr2qh [301.545936ms]
Jan 18 17:01:02.481: INFO: Created: latency-svc-2nzkb
Jan 18 17:01:02.489: INFO: Got endpoints: latency-svc-2nzkb [297.226367ms]
Jan 18 17:01:02.494: INFO: Created: latency-svc-8ls5r
Jan 18 17:01:02.501: INFO: Got endpoints: latency-svc-8ls5r [300.893803ms]
Jan 18 17:01:02.506: INFO: Created: latency-svc-ldnbl
Jan 18 17:01:02.516: INFO: Got endpoints: latency-svc-ldnbl [300.133758ms]
Jan 18 17:01:02.559: INFO: Created: latency-svc-xn6c8
Jan 18 17:01:02.571: INFO: Got endpoints: latency-svc-xn6c8 [302.07145ms]
Jan 18 17:01:02.574: INFO: Created: latency-svc-nc7ch
Jan 18 17:01:02.583: INFO: Got endpoints: latency-svc-nc7ch [309.926903ms]
Jan 18 17:01:02.584: INFO: Created: latency-svc-66tsc
Jan 18 17:01:02.594: INFO: Got endpoints: latency-svc-66tsc [291.120166ms]
Jan 18 17:01:02.599: INFO: Created: latency-svc-6rtwn
Jan 18 17:01:02.610: INFO: Created: latency-svc-rr55l
Jan 18 17:01:02.616: INFO: Created: latency-svc-wjkvf
Jan 18 17:01:02.659: INFO: Got endpoints: latency-svc-6rtwn [276.95377ms]
Jan 18 17:01:02.666: INFO: Created: latency-svc-bzvdc
Jan 18 17:01:02.676: INFO: Got endpoints: latency-svc-rr55l [289.326083ms]
Jan 18 17:01:02.679: INFO: Created: latency-svc-jqql6
Jan 18 17:01:02.689: INFO: Created: latency-svc-djd6c
Jan 18 17:01:02.704: INFO: Created: latency-svc-5dg5h
Jan 18 17:01:02.712: INFO: Created: latency-svc-fzkjl
Jan 18 17:01:02.721: INFO: Created: latency-svc-m8nmg
Jan 18 17:01:02.724: INFO: Got endpoints: latency-svc-wjkvf [337.0831ms]
Jan 18 17:01:02.759: INFO: Created: latency-svc-5fc8b
Jan 18 17:01:02.765: INFO: Created: latency-svc-b7ppc
Jan 18 17:01:02.775: INFO: Got endpoints: latency-svc-bzvdc [387.988366ms]
Jan 18 17:01:02.777: INFO: Created: latency-svc-xjf2t
Jan 18 17:01:02.791: INFO: Created: latency-svc-qbnr2
Jan 18 17:01:02.804: INFO: Created: latency-svc-jpqsk
Jan 18 17:01:02.814: INFO: Created: latency-svc-v9sbl
Jan 18 17:01:02.823: INFO: Got endpoints: latency-svc-jqql6 [429.905803ms]
Jan 18 17:01:02.829: INFO: Created: latency-svc-jhgc8
Jan 18 17:01:02.839: INFO: Created: latency-svc-mps5l
Jan 18 17:01:02.849: INFO: Created: latency-svc-2sc9j
Jan 18 17:01:02.861: INFO: Created: latency-svc-fddzw
Jan 18 17:01:02.870: INFO: Created: latency-svc-tnfcz
Jan 18 17:01:02.876: INFO: Got endpoints: latency-svc-djd6c [481.458853ms]
Jan 18 17:01:02.898: INFO: Created: latency-svc-5l2w7
Jan 18 17:01:02.926: INFO: Got endpoints: latency-svc-5dg5h [517.959093ms]
Jan 18 17:01:02.949: INFO: Created: latency-svc-jvpxn
Jan 18 17:01:02.975: INFO: Got endpoints: latency-svc-fzkjl [508.516067ms]
Jan 18 17:01:02.999: INFO: Created: latency-svc-6hpsx
Jan 18 17:01:03.025: INFO: Got endpoints: latency-svc-m8nmg [548.589863ms]
Jan 18 17:01:03.046: INFO: Created: latency-svc-mjvcm
Jan 18 17:01:03.074: INFO: Got endpoints: latency-svc-5fc8b [585.531075ms]
Jan 18 17:01:03.099: INFO: Created: latency-svc-v22wj
Jan 18 17:01:03.124: INFO: Got endpoints: latency-svc-b7ppc [622.702858ms]
Jan 18 17:01:03.147: INFO: Created: latency-svc-lxfb9
Jan 18 17:01:03.174: INFO: Got endpoints: latency-svc-xjf2t [658.175819ms]
Jan 18 17:01:03.194: INFO: Created: latency-svc-kv8lf
Jan 18 17:01:03.225: INFO: Got endpoints: latency-svc-qbnr2 [653.210154ms]
Jan 18 17:01:03.247: INFO: Created: latency-svc-7wb6k
Jan 18 17:01:03.274: INFO: Got endpoints: latency-svc-jpqsk [691.222094ms]
Jan 18 17:01:03.295: INFO: Created: latency-svc-4fgx7
Jan 18 17:01:03.325: INFO: Got endpoints: latency-svc-v9sbl [731.346587ms]
Jan 18 17:01:03.344: INFO: Created: latency-svc-nszql
Jan 18 17:01:03.375: INFO: Got endpoints: latency-svc-jhgc8 [716.024672ms]
Jan 18 17:01:03.400: INFO: Created: latency-svc-df6xb
Jan 18 17:01:03.424: INFO: Got endpoints: latency-svc-mps5l [747.84152ms]
Jan 18 17:01:03.445: INFO: Created: latency-svc-ckt8w
Jan 18 17:01:03.475: INFO: Got endpoints: latency-svc-2sc9j [750.598297ms]
Jan 18 17:01:03.495: INFO: Created: latency-svc-8qgb2
Jan 18 17:01:03.525: INFO: Got endpoints: latency-svc-fddzw [749.006427ms]
Jan 18 17:01:03.547: INFO: Created: latency-svc-9qlsh
Jan 18 17:01:03.577: INFO: Got endpoints: latency-svc-tnfcz [753.461857ms]
Jan 18 17:01:03.599: INFO: Created: latency-svc-n7hb9
Jan 18 17:01:03.628: INFO: Got endpoints: latency-svc-5l2w7 [751.646824ms]
Jan 18 17:01:03.646: INFO: Created: latency-svc-ff4vp
Jan 18 17:01:03.676: INFO: Got endpoints: latency-svc-jvpxn [750.892619ms]
Jan 18 17:01:03.697: INFO: Created: latency-svc-5bz25
Jan 18 17:01:03.726: INFO: Got endpoints: latency-svc-6hpsx [751.630395ms]
Jan 18 17:01:03.750: INFO: Created: latency-svc-vvqqh
Jan 18 17:01:03.776: INFO: Got endpoints: latency-svc-mjvcm [750.118593ms]
Jan 18 17:01:03.799: INFO: Created: latency-svc-wh8n6
Jan 18 17:01:03.826: INFO: Got endpoints: latency-svc-v22wj [750.987239ms]
Jan 18 17:01:03.850: INFO: Created: latency-svc-ccsg4
Jan 18 17:01:03.875: INFO: Got endpoints: latency-svc-lxfb9 [751.23855ms]
Jan 18 17:01:03.895: INFO: Created: latency-svc-h9dj6
Jan 18 17:01:03.926: INFO: Got endpoints: latency-svc-kv8lf [752.111837ms]
Jan 18 17:01:03.947: INFO: Created: latency-svc-zzpm2
Jan 18 17:01:03.974: INFO: Got endpoints: latency-svc-7wb6k [749.52282ms]
Jan 18 17:01:03.998: INFO: Created: latency-svc-t9sjb
Jan 18 17:01:04.027: INFO: Got endpoints: latency-svc-4fgx7 [753.104435ms]
Jan 18 17:01:04.052: INFO: Created: latency-svc-9xxsg
Jan 18 17:01:04.077: INFO: Got endpoints: latency-svc-nszql [752.216968ms]
Jan 18 17:01:04.098: INFO: Created: latency-svc-h4nlr
Jan 18 17:01:04.124: INFO: Got endpoints: latency-svc-df6xb [748.257761ms]
Jan 18 17:01:04.144: INFO: Created: latency-svc-rrgpk
Jan 18 17:01:04.175: INFO: Got endpoints: latency-svc-ckt8w [751.362961ms]
Jan 18 17:01:04.197: INFO: Created: latency-svc-z9mgn
Jan 18 17:01:04.226: INFO: Got endpoints: latency-svc-8qgb2 [750.836928ms]
Jan 18 17:01:04.249: INFO: Created: latency-svc-sv464
Jan 18 17:01:04.273: INFO: Got endpoints: latency-svc-9qlsh [748.079359ms]
Jan 18 17:01:04.294: INFO: Created: latency-svc-knnns
Jan 18 17:01:04.327: INFO: Got endpoints: latency-svc-n7hb9 [750.194164ms]
Jan 18 17:01:04.351: INFO: Created: latency-svc-jww9b
Jan 18 17:01:04.375: INFO: Got endpoints: latency-svc-ff4vp [747.514944ms]
Jan 18 17:01:04.395: INFO: Created: latency-svc-94q87
Jan 18 17:01:04.425: INFO: Got endpoints: latency-svc-5bz25 [748.517111ms]
Jan 18 17:01:04.449: INFO: Created: latency-svc-5tc2d
Jan 18 17:01:04.476: INFO: Got endpoints: latency-svc-vvqqh [749.422489ms]
Jan 18 17:01:04.497: INFO: Created: latency-svc-l2465
Jan 18 17:01:04.524: INFO: Got endpoints: latency-svc-wh8n6 [748.835745ms]
Jan 18 17:01:04.545: INFO: Created: latency-svc-qprsm
Jan 18 17:01:04.576: INFO: Got endpoints: latency-svc-ccsg4 [750.759687ms]
Jan 18 17:01:04.598: INFO: Created: latency-svc-hdgct
Jan 18 17:01:04.623: INFO: Got endpoints: latency-svc-h9dj6 [747.689827ms]
Jan 18 17:01:04.642: INFO: Created: latency-svc-szldd
Jan 18 17:01:04.676: INFO: Got endpoints: latency-svc-zzpm2 [749.950961ms]
Jan 18 17:01:04.696: INFO: Created: latency-svc-7nh9q
Jan 18 17:01:04.724: INFO: Got endpoints: latency-svc-t9sjb [749.863491ms]
Jan 18 17:01:04.745: INFO: Created: latency-svc-lwgrp
Jan 18 17:01:04.774: INFO: Got endpoints: latency-svc-9xxsg [746.771969ms]
Jan 18 17:01:04.796: INFO: Created: latency-svc-2j79k
Jan 18 17:01:04.825: INFO: Got endpoints: latency-svc-h4nlr [747.816778ms]
Jan 18 17:01:04.844: INFO: Created: latency-svc-clt5j
Jan 18 17:01:04.875: INFO: Got endpoints: latency-svc-rrgpk [751.339571ms]
Jan 18 17:01:04.898: INFO: Created: latency-svc-4frlt
Jan 18 17:01:04.925: INFO: Got endpoints: latency-svc-z9mgn [749.54549ms]
Jan 18 17:01:04.947: INFO: Created: latency-svc-pfnzq
Jan 18 17:01:04.975: INFO: Got endpoints: latency-svc-sv464 [749.54328ms]
Jan 18 17:01:04.996: INFO: Created: latency-svc-tx8dt
Jan 18 17:01:05.029: INFO: Got endpoints: latency-svc-knnns [756.174985ms]
Jan 18 17:01:05.050: INFO: Created: latency-svc-lnkfl
Jan 18 17:01:05.075: INFO: Got endpoints: latency-svc-jww9b [747.566738ms]
Jan 18 17:01:05.094: INFO: Created: latency-svc-t9hcb
Jan 18 17:01:05.125: INFO: Got endpoints: latency-svc-94q87 [749.557941ms]
Jan 18 17:01:05.147: INFO: Created: latency-svc-zc5d4
Jan 18 17:01:05.175: INFO: Got endpoints: latency-svc-5tc2d [750.368276ms]
Jan 18 17:01:05.197: INFO: Created: latency-svc-pjmcd
Jan 18 17:01:05.224: INFO: Got endpoints: latency-svc-l2465 [748.493622ms]
Jan 18 17:01:05.246: INFO: Created: latency-svc-5f459
Jan 18 17:01:05.276: INFO: Got endpoints: latency-svc-qprsm [751.32834ms]
Jan 18 17:01:05.296: INFO: Created: latency-svc-8rk9b
Jan 18 17:01:05.325: INFO: Got endpoints: latency-svc-hdgct [748.318201ms]
Jan 18 17:01:05.347: INFO: Created: latency-svc-jmgbt
Jan 18 17:01:05.373: INFO: Got endpoints: latency-svc-szldd [750.229594ms]
Jan 18 17:01:05.398: INFO: Created: latency-svc-kxqwj
Jan 18 17:01:05.424: INFO: Got endpoints: latency-svc-7nh9q [747.897838ms]
Jan 18 17:01:05.442: INFO: Created: latency-svc-2bw8t
Jan 18 17:01:05.475: INFO: Got endpoints: latency-svc-lwgrp [751.036949ms]
Jan 18 17:01:05.496: INFO: Created: latency-svc-hxnr2
Jan 18 17:01:05.525: INFO: Got endpoints: latency-svc-2j79k [750.458214ms]
Jan 18 17:01:05.546: INFO: Created: latency-svc-jgzl7
Jan 18 17:01:05.576: INFO: Got endpoints: latency-svc-clt5j [750.252723ms]
Jan 18 17:01:05.596: INFO: Created: latency-svc-vdppv
Jan 18 17:01:05.627: INFO: Got endpoints: latency-svc-4frlt [751.808013ms]
Jan 18 17:01:05.648: INFO: Created: latency-svc-gq6rs
Jan 18 17:01:05.675: INFO: Got endpoints: latency-svc-pfnzq [749.707239ms]
Jan 18 17:01:05.697: INFO: Created: latency-svc-g2t67
Jan 18 17:01:05.724: INFO: Got endpoints: latency-svc-tx8dt [748.227498ms]
Jan 18 17:01:05.744: INFO: Created: latency-svc-hgqrl
Jan 18 17:01:05.774: INFO: Got endpoints: latency-svc-lnkfl [745.357418ms]
Jan 18 17:01:05.794: INFO: Created: latency-svc-qclx8
Jan 18 17:01:05.825: INFO: Got endpoints: latency-svc-t9hcb [750.642984ms]
Jan 18 17:01:05.847: INFO: Created: latency-svc-t9zfn
Jan 18 17:01:05.876: INFO: Got endpoints: latency-svc-zc5d4 [751.498551ms]
Jan 18 17:01:05.897: INFO: Created: latency-svc-htnt7
Jan 18 17:01:05.925: INFO: Got endpoints: latency-svc-pjmcd [749.489867ms]
Jan 18 17:01:05.947: INFO: Created: latency-svc-rw4kh
Jan 18 17:01:05.975: INFO: Got endpoints: latency-svc-5f459 [750.364673ms]
Jan 18 17:01:05.996: INFO: Created: latency-svc-5r7sx
Jan 18 17:01:06.027: INFO: Got endpoints: latency-svc-8rk9b [751.24147ms]
Jan 18 17:01:06.047: INFO: Created: latency-svc-sdw4s
Jan 18 17:01:06.076: INFO: Got endpoints: latency-svc-jmgbt [751.657022ms]
Jan 18 17:01:06.096: INFO: Created: latency-svc-gz2pf
Jan 18 17:01:06.123: INFO: Got endpoints: latency-svc-kxqwj [749.703629ms]
Jan 18 17:01:06.142: INFO: Created: latency-svc-rqg7z
Jan 18 17:01:06.175: INFO: Got endpoints: latency-svc-2bw8t [751.345749ms]
Jan 18 17:01:06.194: INFO: Created: latency-svc-4tx8d
Jan 18 17:01:06.224: INFO: Got endpoints: latency-svc-hxnr2 [749.213524ms]
Jan 18 17:01:06.248: INFO: Created: latency-svc-pwljc
Jan 18 17:01:06.277: INFO: Got endpoints: latency-svc-jgzl7 [752.384297ms]
Jan 18 17:01:06.298: INFO: Created: latency-svc-jcdpq
Jan 18 17:01:06.327: INFO: Got endpoints: latency-svc-vdppv [751.305717ms]
Jan 18 17:01:06.348: INFO: Created: latency-svc-g5twq
Jan 18 17:01:06.377: INFO: Got endpoints: latency-svc-gq6rs [749.795249ms]
Jan 18 17:01:06.460: INFO: Created: latency-svc-6m8qx
Jan 18 17:01:06.468: INFO: Got endpoints: latency-svc-g2t67 [793.164814ms]
Jan 18 17:01:06.559: INFO: Got endpoints: latency-svc-hgqrl [835.354952ms]
Jan 18 17:01:06.568: INFO: Got endpoints: latency-svc-qclx8 [793.263395ms]
Jan 18 17:01:06.574: INFO: Got endpoints: latency-svc-t9zfn [748.712672ms]
Jan 18 17:01:06.660: INFO: Created: latency-svc-dzdt4
Jan 18 17:01:06.672: INFO: Got endpoints: latency-svc-htnt7 [795.935322ms]
Jan 18 17:01:06.759: INFO: Created: latency-svc-67pcq
Jan 18 17:01:06.768: INFO: Created: latency-svc-p5kf5
Jan 18 17:01:06.769: INFO: Got endpoints: latency-svc-rw4kh [843.739317ms]
Jan 18 17:01:06.775: INFO: Got endpoints: latency-svc-5r7sx [799.826631ms]
Jan 18 17:01:06.859: INFO: Got endpoints: latency-svc-sdw4s [831.532675ms]
Jan 18 17:01:06.866: INFO: Created: latency-svc-cfjj6
Jan 18 17:01:06.874: INFO: Got endpoints: latency-svc-gz2pf [797.13916ms]
Jan 18 17:01:06.877: INFO: Got endpoints: latency-svc-rqg7z [754.252798ms]
Jan 18 17:01:06.879: INFO: Created: latency-svc-bbn4f
Jan 18 17:01:06.893: INFO: Created: latency-svc-9cls5
Jan 18 17:01:06.907: INFO: Created: latency-svc-pgk46
Jan 18 17:01:06.915: INFO: Created: latency-svc-p7r5h
Jan 18 17:01:06.960: INFO: Got endpoints: latency-svc-4tx8d [784.618985ms]
Jan 18 17:01:06.967: INFO: Created: latency-svc-k6hnc
Jan 18 17:01:06.980: INFO: Got endpoints: latency-svc-pwljc [755.357006ms]
Jan 18 17:01:06.983: INFO: Created: latency-svc-5486b
Jan 18 17:01:06.994: INFO: Created: latency-svc-6phlx
Jan 18 17:01:07.011: INFO: Created: latency-svc-mggn6
Jan 18 17:01:07.024: INFO: Got endpoints: latency-svc-jcdpq [746.925089ms]
Jan 18 17:01:07.046: INFO: Created: latency-svc-46wl4
Jan 18 17:01:07.076: INFO: Got endpoints: latency-svc-g5twq [749.095725ms]
Jan 18 17:01:07.096: INFO: Created: latency-svc-4rzh6
Jan 18 17:01:07.128: INFO: Got endpoints: latency-svc-6m8qx [751.362669ms]
Jan 18 17:01:07.152: INFO: Created: latency-svc-2mm95
Jan 18 17:01:07.177: INFO: Got endpoints: latency-svc-dzdt4 [709.127671ms]
Jan 18 17:01:07.197: INFO: Created: latency-svc-kwxml
Jan 18 17:01:07.226: INFO: Got endpoints: latency-svc-67pcq [666.434499ms]
Jan 18 17:01:07.245: INFO: Created: latency-svc-hf4df
Jan 18 17:01:07.278: INFO: Got endpoints: latency-svc-p5kf5 [710.275509ms]
Jan 18 17:01:07.303: INFO: Created: latency-svc-nckfz
Jan 18 17:01:07.324: INFO: Got endpoints: latency-svc-cfjj6 [750.183459ms]
Jan 18 17:01:07.352: INFO: Created: latency-svc-ffhbz
Jan 18 17:01:07.377: INFO: Got endpoints: latency-svc-bbn4f [704.939813ms]
Jan 18 17:01:07.400: INFO: Created: latency-svc-zbtht
Jan 18 17:01:07.428: INFO: Got endpoints: latency-svc-9cls5 [658.661947ms]
Jan 18 17:01:07.451: INFO: Created: latency-svc-vb4hd
Jan 18 17:01:07.475: INFO: Got endpoints: latency-svc-pgk46 [700.498142ms]
Jan 18 17:01:07.494: INFO: Created: latency-svc-zt7k9
Jan 18 17:01:07.525: INFO: Got endpoints: latency-svc-p7r5h [666.069098ms]
Jan 18 17:01:07.544: INFO: Created: latency-svc-m9mrj
Jan 18 17:01:07.577: INFO: Got endpoints: latency-svc-k6hnc [702.846798ms]
Jan 18 17:01:07.600: INFO: Created: latency-svc-l9nhp
Jan 18 17:01:07.626: INFO: Got endpoints: latency-svc-5486b [748.642619ms]
Jan 18 17:01:07.648: INFO: Created: latency-svc-ppfmg
Jan 18 17:01:07.677: INFO: Got endpoints: latency-svc-6phlx [717.173446ms]
Jan 18 17:01:07.695: INFO: Created: latency-svc-nrrcv
Jan 18 17:01:07.726: INFO: Got endpoints: latency-svc-mggn6 [745.907931ms]
Jan 18 17:01:07.748: INFO: Created: latency-svc-7swjd
Jan 18 17:01:07.776: INFO: Got endpoints: latency-svc-46wl4 [752.150864ms]
Jan 18 17:01:07.803: INFO: Created: latency-svc-t6pdz
Jan 18 17:01:07.828: INFO: Got endpoints: latency-svc-4rzh6 [751.398609ms]
Jan 18 17:01:07.847: INFO: Created: latency-svc-pmc9p
Jan 18 17:01:07.876: INFO: Got endpoints: latency-svc-2mm95 [748.378308ms]
Jan 18 17:01:07.900: INFO: Created: latency-svc-hjg9b
Jan 18 17:01:07.933: INFO: Got endpoints: latency-svc-kwxml [755.759438ms]
Jan 18 17:01:07.956: INFO: Created: latency-svc-c47w5
Jan 18 17:01:07.976: INFO: Got endpoints: latency-svc-hf4df [750.637954ms]
Jan 18 17:01:08.001: INFO: Created: latency-svc-5sg6c
Jan 18 17:01:08.029: INFO: Got endpoints: latency-svc-nckfz [750.635804ms]
Jan 18 17:01:08.053: INFO: Created: latency-svc-n82tr
Jan 18 17:01:08.074: INFO: Got endpoints: latency-svc-ffhbz [749.513377ms]
Jan 18 17:01:08.096: INFO: Created: latency-svc-skgwq
Jan 18 17:01:08.125: INFO: Got endpoints: latency-svc-zbtht [747.671033ms]
Jan 18 17:01:08.147: INFO: Created: latency-svc-78mtm
Jan 18 17:01:08.176: INFO: Got endpoints: latency-svc-vb4hd [748.494568ms]
Jan 18 17:01:08.197: INFO: Created: latency-svc-l9gkj
Jan 18 17:01:08.224: INFO: Got endpoints: latency-svc-zt7k9 [748.88346ms]
Jan 18 17:01:08.255: INFO: Created: latency-svc-z8ksb
Jan 18 17:01:08.274: INFO: Got endpoints: latency-svc-m9mrj [749.346963ms]
Jan 18 17:01:08.297: INFO: Created: latency-svc-hkdgc
Jan 18 17:01:08.324: INFO: Got endpoints: latency-svc-l9nhp [747.829362ms]
Jan 18 17:01:08.345: INFO: Created: latency-svc-7w5l5
Jan 18 17:01:08.376: INFO: Got endpoints: latency-svc-ppfmg [749.959767ms]
Jan 18 17:01:08.398: INFO: Created: latency-svc-4lx7s
Jan 18 17:01:08.428: INFO: Got endpoints: latency-svc-nrrcv [750.251658ms]
Jan 18 17:01:08.446: INFO: Created: latency-svc-q62ll
Jan 18 17:01:08.476: INFO: Got endpoints: latency-svc-7swjd [750.082818ms]
Jan 18 17:01:08.497: INFO: Created: latency-svc-54j9b
Jan 18 17:01:08.525: INFO: Got endpoints: latency-svc-t6pdz [749.006651ms]
Jan 18 17:01:08.548: INFO: Created: latency-svc-dpqct
Jan 18 17:01:08.576: INFO: Got endpoints: latency-svc-pmc9p [747.855803ms]
Jan 18 17:01:08.596: INFO: Created: latency-svc-xwk2s
Jan 18 17:01:08.626: INFO: Got endpoints: latency-svc-hjg9b [749.314852ms]
Jan 18 17:01:08.651: INFO: Created: latency-svc-vbkdd
Jan 18 17:01:08.678: INFO: Got endpoints: latency-svc-c47w5 [745.434656ms]
Jan 18 17:01:08.698: INFO: Created: latency-svc-8fjj8
Jan 18 17:01:08.724: INFO: Got endpoints: latency-svc-5sg6c [748.072223ms]
Jan 18 17:01:08.744: INFO: Created: latency-svc-z8vr6
Jan 18 17:01:08.775: INFO: Got endpoints: latency-svc-n82tr [746.816974ms]
Jan 18 17:01:08.796: INFO: Created: latency-svc-rzl8v
Jan 18 17:01:08.823: INFO: Got endpoints: latency-svc-skgwq [749.564353ms]
Jan 18 17:01:08.842: INFO: Created: latency-svc-rt7d7
Jan 18 17:01:08.877: INFO: Got endpoints: latency-svc-78mtm [751.573187ms]
Jan 18 17:01:08.899: INFO: Created: latency-svc-6scwk
Jan 18 17:01:08.925: INFO: Got endpoints: latency-svc-l9gkj [748.98288ms]
Jan 18 17:01:08.949: INFO: Created: latency-svc-5xgj4
Jan 18 17:01:08.974: INFO: Got endpoints: latency-svc-z8ksb [750.022517ms]
Jan 18 17:01:08.995: INFO: Created: latency-svc-jfhkn
Jan 18 17:01:09.026: INFO: Got endpoints: latency-svc-hkdgc [751.367666ms]
Jan 18 17:01:09.046: INFO: Created: latency-svc-s755l
Jan 18 17:01:09.075: INFO: Got endpoints: latency-svc-7w5l5 [750.46184ms]
Jan 18 17:01:09.097: INFO: Created: latency-svc-zltg2
Jan 18 17:01:09.126: INFO: Got endpoints: latency-svc-4lx7s [750.44432ms]
Jan 18 17:01:09.167: INFO: Created: latency-svc-mht25
Jan 18 17:01:09.175: INFO: Got endpoints: latency-svc-q62ll [747.023367ms]
Jan 18 17:01:09.198: INFO: Created: latency-svc-d7sf7
Jan 18 17:01:09.225: INFO: Got endpoints: latency-svc-54j9b [748.752048ms]
Jan 18 17:01:09.247: INFO: Created: latency-svc-4x5td
Jan 18 17:01:09.275: INFO: Got endpoints: latency-svc-dpqct [749.822776ms]
Jan 18 17:01:09.295: INFO: Created: latency-svc-rdrfl
Jan 18 17:01:09.325: INFO: Got endpoints: latency-svc-xwk2s [749.19602ms]
Jan 18 17:01:09.344: INFO: Created: latency-svc-ds7sn
Jan 18 17:01:09.377: INFO: Got endpoints: latency-svc-vbkdd [750.991293ms]
Jan 18 17:01:09.399: INFO: Created: latency-svc-j5kjl
Jan 18 17:01:09.423: INFO: Got endpoints: latency-svc-8fjj8 [744.650791ms]
Jan 18 17:01:09.445: INFO: Created: latency-svc-gzm2k
Jan 18 17:01:09.474: INFO: Got endpoints: latency-svc-z8vr6 [749.512653ms]
Jan 18 17:01:09.492: INFO: Created: latency-svc-kzrrx
Jan 18 17:01:09.526: INFO: Got endpoints: latency-svc-rzl8v [750.899601ms]
Jan 18 17:01:09.573: INFO: Got endpoints: latency-svc-rt7d7 [749.712435ms]
Jan 18 17:01:09.625: INFO: Got endpoints: latency-svc-6scwk [747.630609ms]
Jan 18 17:01:09.679: INFO: Got endpoints: latency-svc-5xgj4 [753.42438ms]
Jan 18 17:01:09.728: INFO: Got endpoints: latency-svc-jfhkn [753.297658ms]
Jan 18 17:01:09.776: INFO: Got endpoints: latency-svc-s755l [750.325888ms]
Jan 18 17:01:09.827: INFO: Got endpoints: latency-svc-zltg2 [751.927999ms]
Jan 18 17:01:09.874: INFO: Got endpoints: latency-svc-mht25 [747.462068ms]
Jan 18 17:01:09.926: INFO: Got endpoints: latency-svc-d7sf7 [751.161004ms]
Jan 18 17:01:09.975: INFO: Got endpoints: latency-svc-4x5td [750.338257ms]
Jan 18 17:01:10.025: INFO: Got endpoints: latency-svc-rdrfl [749.788234ms]
Jan 18 17:01:10.075: INFO: Got endpoints: latency-svc-ds7sn [750.085826ms]
Jan 18 17:01:10.143: INFO: Got endpoints: latency-svc-j5kjl [765.989314ms]
Jan 18 17:01:10.173: INFO: Got endpoints: latency-svc-gzm2k [750.099044ms]
Jan 18 17:01:10.224: INFO: Got endpoints: latency-svc-kzrrx [750.090345ms]
Jan 18 17:01:10.224: INFO: Latencies: [31.804136ms 41.981317ms 83.775622ms 99.80615ms 110.758776ms 127.021956ms 127.500169ms 185.013062ms 194.529297ms 201.985237ms 214.126241ms 224.740684ms 236.150722ms 238.547037ms 239.025381ms 243.208319ms 245.853388ms 256.689561ms 267.319065ms 269.250577ms 270.678898ms 272.724431ms 276.95377ms 280.882646ms 282.651448ms 285.059275ms 285.159396ms 287.914094ms 289.326083ms 290.884384ms 291.120166ms 292.395485ms 293.969614ms 294.72392ms 297.226367ms 298.552816ms 298.631326ms 300.133758ms 300.792961ms 300.893803ms 301.545936ms 302.07145ms 307.357257ms 309.926903ms 315.912274ms 319.881701ms 327.370692ms 337.0831ms 373.479937ms 377.051761ms 387.988366ms 429.905803ms 481.458853ms 508.516067ms 517.959093ms 548.589863ms 585.531075ms 622.702858ms 653.210154ms 658.175819ms 658.661947ms 666.069098ms 666.434499ms 691.222094ms 700.498142ms 702.846798ms 704.939813ms 709.127671ms 710.275509ms 716.024672ms 717.173446ms 731.346587ms 744.650791ms 745.357418ms 745.434656ms 745.907931ms 746.771969ms 746.816974ms 746.925089ms 747.023367ms 747.462068ms 747.514944ms 747.566738ms 747.630609ms 747.671033ms 747.689827ms 747.816778ms 747.829362ms 747.84152ms 747.855803ms 747.897838ms 748.072223ms 748.079359ms 748.227498ms 748.257761ms 748.318201ms 748.378308ms 748.493622ms 748.494568ms 748.517111ms 748.642619ms 748.712672ms 748.752048ms 748.835745ms 748.88346ms 748.98288ms 749.006427ms 749.006651ms 749.095725ms 749.19602ms 749.213524ms 749.314852ms 749.346963ms 749.422489ms 749.489867ms 749.512653ms 749.513377ms 749.52282ms 749.54328ms 749.54549ms 749.557941ms 749.564353ms 749.703629ms 749.707239ms 749.712435ms 749.788234ms 749.795249ms 749.822776ms 749.863491ms 749.950961ms 749.959767ms 750.022517ms 750.082818ms 750.085826ms 750.090345ms 750.099044ms 750.118593ms 750.183459ms 750.194164ms 750.229594ms 750.251658ms 750.252723ms 750.325888ms 750.338257ms 750.364673ms 750.368276ms 750.44432ms 750.458214ms 750.46184ms 750.598297ms 750.635804ms 750.637954ms 750.642984ms 750.759687ms 750.836928ms 750.892619ms 750.899601ms 750.987239ms 750.991293ms 751.036949ms 751.161004ms 751.23855ms 751.24147ms 751.305717ms 751.32834ms 751.339571ms 751.345749ms 751.362669ms 751.362961ms 751.367666ms 751.398609ms 751.498551ms 751.573187ms 751.630395ms 751.646824ms 751.657022ms 751.808013ms 751.927999ms 752.111837ms 752.150864ms 752.216968ms 752.384297ms 753.104435ms 753.297658ms 753.42438ms 753.461857ms 754.252798ms 755.357006ms 755.759438ms 756.174985ms 765.989314ms 784.618985ms 793.164814ms 793.263395ms 795.935322ms 797.13916ms 799.826631ms 831.532675ms 835.354952ms 843.739317ms]
Jan 18 17:01:10.224: INFO: 50 %ile: 748.642619ms
Jan 18 17:01:10.224: INFO: 90 %ile: 752.216968ms
Jan 18 17:01:10.224: INFO: 99 %ile: 835.354952ms
Jan 18 17:01:10.225: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 18 17:01:10.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5490" for this suite. 01/18/23 17:01:10.235
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":16,"skipped":255,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.810 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:00:59.44
    Jan 18 17:00:59.440: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svc-latency 01/18/23 17:00:59.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:00:59.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:00:59.47
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 18 17:00:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-5490 01/18/23 17:00:59.476
    I0118 17:00:59.490008      21 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5490, replica count: 1
    I0118 17:01:00.541424      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0118 17:01:01.541921      21 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:01:01.664: INFO: Created: latency-svc-zdl8n
    Jan 18 17:01:01.675: INFO: Got endpoints: latency-svc-zdl8n [33.259446ms]
    Jan 18 17:01:01.697: INFO: Created: latency-svc-bxbqk
    Jan 18 17:01:01.706: INFO: Created: latency-svc-gg29v
    Jan 18 17:01:01.707: INFO: Got endpoints: latency-svc-bxbqk [31.804136ms]
    Jan 18 17:01:01.717: INFO: Got endpoints: latency-svc-gg29v [41.981317ms]
    Jan 18 17:01:01.717: INFO: Created: latency-svc-jbjbv
    Jan 18 17:01:01.759: INFO: Got endpoints: latency-svc-jbjbv [83.775622ms]
    Jan 18 17:01:01.766: INFO: Created: latency-svc-g4mjp
    Jan 18 17:01:01.775: INFO: Got endpoints: latency-svc-g4mjp [99.80615ms]
    Jan 18 17:01:01.778: INFO: Created: latency-svc-p6phj
    Jan 18 17:01:01.786: INFO: Got endpoints: latency-svc-p6phj [110.758776ms]
    Jan 18 17:01:01.789: INFO: Created: latency-svc-b9fpx
    Jan 18 17:01:01.797: INFO: Created: latency-svc-kz4qw
    Jan 18 17:01:01.803: INFO: Got endpoints: latency-svc-b9fpx [127.021956ms]
    Jan 18 17:01:01.803: INFO: Got endpoints: latency-svc-kz4qw [127.500169ms]
    Jan 18 17:01:01.806: INFO: Created: latency-svc-kqbsl
    Jan 18 17:01:01.820: INFO: Created: latency-svc-b2d44
    Jan 18 17:01:01.861: INFO: Got endpoints: latency-svc-kqbsl [185.013062ms]
    Jan 18 17:01:01.869: INFO: Created: latency-svc-n4c8t
    Jan 18 17:01:01.870: INFO: Got endpoints: latency-svc-b2d44 [194.529297ms]
    Jan 18 17:01:01.878: INFO: Got endpoints: latency-svc-n4c8t [201.985237ms]
    Jan 18 17:01:01.880: INFO: Created: latency-svc-mj4t2
    Jan 18 17:01:01.890: INFO: Got endpoints: latency-svc-mj4t2 [214.126241ms]
    Jan 18 17:01:01.890: INFO: Created: latency-svc-pngwq
    Jan 18 17:01:01.901: INFO: Got endpoints: latency-svc-pngwq [224.740684ms]
    Jan 18 17:01:01.904: INFO: Created: latency-svc-w4jvn
    Jan 18 17:01:01.912: INFO: Got endpoints: latency-svc-w4jvn [236.150722ms]
    Jan 18 17:01:01.917: INFO: Created: latency-svc-mglsd
    Jan 18 17:01:01.931: INFO: Created: latency-svc-tf6hc
    Jan 18 17:01:01.959: INFO: Got endpoints: latency-svc-mglsd [282.651448ms]
    Jan 18 17:01:01.966: INFO: Created: latency-svc-vg25j
    Jan 18 17:01:01.968: INFO: Got endpoints: latency-svc-tf6hc [292.395485ms]
    Jan 18 17:01:01.974: INFO: Got endpoints: latency-svc-vg25j [267.319065ms]
    Jan 18 17:01:01.977: INFO: Created: latency-svc-4vh9f
    Jan 18 17:01:01.986: INFO: Got endpoints: latency-svc-4vh9f [269.250577ms]
    Jan 18 17:01:01.995: INFO: Created: latency-svc-vgxs9
    Jan 18 17:01:02.003: INFO: Created: latency-svc-7v8j8
    Jan 18 17:01:02.005: INFO: Got endpoints: latency-svc-vgxs9 [245.853388ms]
    Jan 18 17:01:02.012: INFO: Created: latency-svc-22hmz
    Jan 18 17:01:02.014: INFO: Got endpoints: latency-svc-7v8j8 [238.547037ms]
    Jan 18 17:01:02.059: INFO: Got endpoints: latency-svc-22hmz [272.724431ms]
    Jan 18 17:01:02.065: INFO: Created: latency-svc-bhz7r
    Jan 18 17:01:02.073: INFO: Got endpoints: latency-svc-bhz7r [270.678898ms]
    Jan 18 17:01:02.080: INFO: Created: latency-svc-lwbf5
    Jan 18 17:01:02.088: INFO: Got endpoints: latency-svc-lwbf5 [285.159396ms]
    Jan 18 17:01:02.091: INFO: Created: latency-svc-tt4c5
    Jan 18 17:01:02.100: INFO: Got endpoints: latency-svc-tt4c5 [239.025381ms]
    Jan 18 17:01:02.104: INFO: Created: latency-svc-hhtjf
    Jan 18 17:01:02.114: INFO: Got endpoints: latency-svc-hhtjf [243.208319ms]
    Jan 18 17:01:02.117: INFO: Created: latency-svc-mll6z
    Jan 18 17:01:02.159: INFO: Got endpoints: latency-svc-mll6z [280.882646ms]
    Jan 18 17:01:02.165: INFO: Created: latency-svc-lvjb7
    Jan 18 17:01:02.175: INFO: Got endpoints: latency-svc-lvjb7 [285.059275ms]
    Jan 18 17:01:02.182: INFO: Created: latency-svc-n6xnr
    Jan 18 17:01:02.192: INFO: Got endpoints: latency-svc-n6xnr [290.884384ms]
    Jan 18 17:01:02.193: INFO: Created: latency-svc-lvkzh
    Jan 18 17:01:02.200: INFO: Got endpoints: latency-svc-lvkzh [287.914094ms]
    Jan 18 17:01:02.204: INFO: Created: latency-svc-62p69
    Jan 18 17:01:02.216: INFO: Got endpoints: latency-svc-62p69 [256.689561ms]
    Jan 18 17:01:02.260: INFO: Created: latency-svc-lbrbh
    Jan 18 17:01:02.265: INFO: Created: latency-svc-dqxzv
    Jan 18 17:01:02.269: INFO: Got endpoints: latency-svc-lbrbh [300.792961ms]
    Jan 18 17:01:02.273: INFO: Got endpoints: latency-svc-dqxzv [298.552816ms]
    Jan 18 17:01:02.289: INFO: Created: latency-svc-txmj4
    Jan 18 17:01:02.302: INFO: Got endpoints: latency-svc-txmj4 [315.912274ms]
    Jan 18 17:01:02.313: INFO: Created: latency-svc-n4wnz
    Jan 18 17:01:02.324: INFO: Created: latency-svc-rstgt
    Jan 18 17:01:02.332: INFO: Created: latency-svc-7mklb
    Jan 18 17:01:02.350: INFO: Created: latency-svc-lxhq2
    Jan 18 17:01:02.365: INFO: Created: latency-svc-65l9f
    Jan 18 17:01:02.374: INFO: Created: latency-svc-dbjdq
    Jan 18 17:01:02.382: INFO: Got endpoints: latency-svc-n4wnz [377.051761ms]
    Jan 18 17:01:02.386: INFO: Got endpoints: latency-svc-7mklb [327.370692ms]
    Jan 18 17:01:02.387: INFO: Got endpoints: latency-svc-65l9f [298.631326ms]
    Jan 18 17:01:02.387: INFO: Got endpoints: latency-svc-rstgt [373.479937ms]
    Jan 18 17:01:02.393: INFO: Got endpoints: latency-svc-lxhq2 [319.881701ms]
    Jan 18 17:01:02.395: INFO: Created: latency-svc-tcwb9
    Jan 18 17:01:02.395: INFO: Got endpoints: latency-svc-dbjdq [294.72392ms]
    Jan 18 17:01:02.408: INFO: Got endpoints: latency-svc-tcwb9 [293.969614ms]
    Jan 18 17:01:02.459: INFO: Created: latency-svc-4qkcp
    Jan 18 17:01:02.466: INFO: Got endpoints: latency-svc-4qkcp [307.357257ms]
    Jan 18 17:01:02.466: INFO: Created: latency-svc-gr2qh
    Jan 18 17:01:02.477: INFO: Got endpoints: latency-svc-gr2qh [301.545936ms]
    Jan 18 17:01:02.481: INFO: Created: latency-svc-2nzkb
    Jan 18 17:01:02.489: INFO: Got endpoints: latency-svc-2nzkb [297.226367ms]
    Jan 18 17:01:02.494: INFO: Created: latency-svc-8ls5r
    Jan 18 17:01:02.501: INFO: Got endpoints: latency-svc-8ls5r [300.893803ms]
    Jan 18 17:01:02.506: INFO: Created: latency-svc-ldnbl
    Jan 18 17:01:02.516: INFO: Got endpoints: latency-svc-ldnbl [300.133758ms]
    Jan 18 17:01:02.559: INFO: Created: latency-svc-xn6c8
    Jan 18 17:01:02.571: INFO: Got endpoints: latency-svc-xn6c8 [302.07145ms]
    Jan 18 17:01:02.574: INFO: Created: latency-svc-nc7ch
    Jan 18 17:01:02.583: INFO: Got endpoints: latency-svc-nc7ch [309.926903ms]
    Jan 18 17:01:02.584: INFO: Created: latency-svc-66tsc
    Jan 18 17:01:02.594: INFO: Got endpoints: latency-svc-66tsc [291.120166ms]
    Jan 18 17:01:02.599: INFO: Created: latency-svc-6rtwn
    Jan 18 17:01:02.610: INFO: Created: latency-svc-rr55l
    Jan 18 17:01:02.616: INFO: Created: latency-svc-wjkvf
    Jan 18 17:01:02.659: INFO: Got endpoints: latency-svc-6rtwn [276.95377ms]
    Jan 18 17:01:02.666: INFO: Created: latency-svc-bzvdc
    Jan 18 17:01:02.676: INFO: Got endpoints: latency-svc-rr55l [289.326083ms]
    Jan 18 17:01:02.679: INFO: Created: latency-svc-jqql6
    Jan 18 17:01:02.689: INFO: Created: latency-svc-djd6c
    Jan 18 17:01:02.704: INFO: Created: latency-svc-5dg5h
    Jan 18 17:01:02.712: INFO: Created: latency-svc-fzkjl
    Jan 18 17:01:02.721: INFO: Created: latency-svc-m8nmg
    Jan 18 17:01:02.724: INFO: Got endpoints: latency-svc-wjkvf [337.0831ms]
    Jan 18 17:01:02.759: INFO: Created: latency-svc-5fc8b
    Jan 18 17:01:02.765: INFO: Created: latency-svc-b7ppc
    Jan 18 17:01:02.775: INFO: Got endpoints: latency-svc-bzvdc [387.988366ms]
    Jan 18 17:01:02.777: INFO: Created: latency-svc-xjf2t
    Jan 18 17:01:02.791: INFO: Created: latency-svc-qbnr2
    Jan 18 17:01:02.804: INFO: Created: latency-svc-jpqsk
    Jan 18 17:01:02.814: INFO: Created: latency-svc-v9sbl
    Jan 18 17:01:02.823: INFO: Got endpoints: latency-svc-jqql6 [429.905803ms]
    Jan 18 17:01:02.829: INFO: Created: latency-svc-jhgc8
    Jan 18 17:01:02.839: INFO: Created: latency-svc-mps5l
    Jan 18 17:01:02.849: INFO: Created: latency-svc-2sc9j
    Jan 18 17:01:02.861: INFO: Created: latency-svc-fddzw
    Jan 18 17:01:02.870: INFO: Created: latency-svc-tnfcz
    Jan 18 17:01:02.876: INFO: Got endpoints: latency-svc-djd6c [481.458853ms]
    Jan 18 17:01:02.898: INFO: Created: latency-svc-5l2w7
    Jan 18 17:01:02.926: INFO: Got endpoints: latency-svc-5dg5h [517.959093ms]
    Jan 18 17:01:02.949: INFO: Created: latency-svc-jvpxn
    Jan 18 17:01:02.975: INFO: Got endpoints: latency-svc-fzkjl [508.516067ms]
    Jan 18 17:01:02.999: INFO: Created: latency-svc-6hpsx
    Jan 18 17:01:03.025: INFO: Got endpoints: latency-svc-m8nmg [548.589863ms]
    Jan 18 17:01:03.046: INFO: Created: latency-svc-mjvcm
    Jan 18 17:01:03.074: INFO: Got endpoints: latency-svc-5fc8b [585.531075ms]
    Jan 18 17:01:03.099: INFO: Created: latency-svc-v22wj
    Jan 18 17:01:03.124: INFO: Got endpoints: latency-svc-b7ppc [622.702858ms]
    Jan 18 17:01:03.147: INFO: Created: latency-svc-lxfb9
    Jan 18 17:01:03.174: INFO: Got endpoints: latency-svc-xjf2t [658.175819ms]
    Jan 18 17:01:03.194: INFO: Created: latency-svc-kv8lf
    Jan 18 17:01:03.225: INFO: Got endpoints: latency-svc-qbnr2 [653.210154ms]
    Jan 18 17:01:03.247: INFO: Created: latency-svc-7wb6k
    Jan 18 17:01:03.274: INFO: Got endpoints: latency-svc-jpqsk [691.222094ms]
    Jan 18 17:01:03.295: INFO: Created: latency-svc-4fgx7
    Jan 18 17:01:03.325: INFO: Got endpoints: latency-svc-v9sbl [731.346587ms]
    Jan 18 17:01:03.344: INFO: Created: latency-svc-nszql
    Jan 18 17:01:03.375: INFO: Got endpoints: latency-svc-jhgc8 [716.024672ms]
    Jan 18 17:01:03.400: INFO: Created: latency-svc-df6xb
    Jan 18 17:01:03.424: INFO: Got endpoints: latency-svc-mps5l [747.84152ms]
    Jan 18 17:01:03.445: INFO: Created: latency-svc-ckt8w
    Jan 18 17:01:03.475: INFO: Got endpoints: latency-svc-2sc9j [750.598297ms]
    Jan 18 17:01:03.495: INFO: Created: latency-svc-8qgb2
    Jan 18 17:01:03.525: INFO: Got endpoints: latency-svc-fddzw [749.006427ms]
    Jan 18 17:01:03.547: INFO: Created: latency-svc-9qlsh
    Jan 18 17:01:03.577: INFO: Got endpoints: latency-svc-tnfcz [753.461857ms]
    Jan 18 17:01:03.599: INFO: Created: latency-svc-n7hb9
    Jan 18 17:01:03.628: INFO: Got endpoints: latency-svc-5l2w7 [751.646824ms]
    Jan 18 17:01:03.646: INFO: Created: latency-svc-ff4vp
    Jan 18 17:01:03.676: INFO: Got endpoints: latency-svc-jvpxn [750.892619ms]
    Jan 18 17:01:03.697: INFO: Created: latency-svc-5bz25
    Jan 18 17:01:03.726: INFO: Got endpoints: latency-svc-6hpsx [751.630395ms]
    Jan 18 17:01:03.750: INFO: Created: latency-svc-vvqqh
    Jan 18 17:01:03.776: INFO: Got endpoints: latency-svc-mjvcm [750.118593ms]
    Jan 18 17:01:03.799: INFO: Created: latency-svc-wh8n6
    Jan 18 17:01:03.826: INFO: Got endpoints: latency-svc-v22wj [750.987239ms]
    Jan 18 17:01:03.850: INFO: Created: latency-svc-ccsg4
    Jan 18 17:01:03.875: INFO: Got endpoints: latency-svc-lxfb9 [751.23855ms]
    Jan 18 17:01:03.895: INFO: Created: latency-svc-h9dj6
    Jan 18 17:01:03.926: INFO: Got endpoints: latency-svc-kv8lf [752.111837ms]
    Jan 18 17:01:03.947: INFO: Created: latency-svc-zzpm2
    Jan 18 17:01:03.974: INFO: Got endpoints: latency-svc-7wb6k [749.52282ms]
    Jan 18 17:01:03.998: INFO: Created: latency-svc-t9sjb
    Jan 18 17:01:04.027: INFO: Got endpoints: latency-svc-4fgx7 [753.104435ms]
    Jan 18 17:01:04.052: INFO: Created: latency-svc-9xxsg
    Jan 18 17:01:04.077: INFO: Got endpoints: latency-svc-nszql [752.216968ms]
    Jan 18 17:01:04.098: INFO: Created: latency-svc-h4nlr
    Jan 18 17:01:04.124: INFO: Got endpoints: latency-svc-df6xb [748.257761ms]
    Jan 18 17:01:04.144: INFO: Created: latency-svc-rrgpk
    Jan 18 17:01:04.175: INFO: Got endpoints: latency-svc-ckt8w [751.362961ms]
    Jan 18 17:01:04.197: INFO: Created: latency-svc-z9mgn
    Jan 18 17:01:04.226: INFO: Got endpoints: latency-svc-8qgb2 [750.836928ms]
    Jan 18 17:01:04.249: INFO: Created: latency-svc-sv464
    Jan 18 17:01:04.273: INFO: Got endpoints: latency-svc-9qlsh [748.079359ms]
    Jan 18 17:01:04.294: INFO: Created: latency-svc-knnns
    Jan 18 17:01:04.327: INFO: Got endpoints: latency-svc-n7hb9 [750.194164ms]
    Jan 18 17:01:04.351: INFO: Created: latency-svc-jww9b
    Jan 18 17:01:04.375: INFO: Got endpoints: latency-svc-ff4vp [747.514944ms]
    Jan 18 17:01:04.395: INFO: Created: latency-svc-94q87
    Jan 18 17:01:04.425: INFO: Got endpoints: latency-svc-5bz25 [748.517111ms]
    Jan 18 17:01:04.449: INFO: Created: latency-svc-5tc2d
    Jan 18 17:01:04.476: INFO: Got endpoints: latency-svc-vvqqh [749.422489ms]
    Jan 18 17:01:04.497: INFO: Created: latency-svc-l2465
    Jan 18 17:01:04.524: INFO: Got endpoints: latency-svc-wh8n6 [748.835745ms]
    Jan 18 17:01:04.545: INFO: Created: latency-svc-qprsm
    Jan 18 17:01:04.576: INFO: Got endpoints: latency-svc-ccsg4 [750.759687ms]
    Jan 18 17:01:04.598: INFO: Created: latency-svc-hdgct
    Jan 18 17:01:04.623: INFO: Got endpoints: latency-svc-h9dj6 [747.689827ms]
    Jan 18 17:01:04.642: INFO: Created: latency-svc-szldd
    Jan 18 17:01:04.676: INFO: Got endpoints: latency-svc-zzpm2 [749.950961ms]
    Jan 18 17:01:04.696: INFO: Created: latency-svc-7nh9q
    Jan 18 17:01:04.724: INFO: Got endpoints: latency-svc-t9sjb [749.863491ms]
    Jan 18 17:01:04.745: INFO: Created: latency-svc-lwgrp
    Jan 18 17:01:04.774: INFO: Got endpoints: latency-svc-9xxsg [746.771969ms]
    Jan 18 17:01:04.796: INFO: Created: latency-svc-2j79k
    Jan 18 17:01:04.825: INFO: Got endpoints: latency-svc-h4nlr [747.816778ms]
    Jan 18 17:01:04.844: INFO: Created: latency-svc-clt5j
    Jan 18 17:01:04.875: INFO: Got endpoints: latency-svc-rrgpk [751.339571ms]
    Jan 18 17:01:04.898: INFO: Created: latency-svc-4frlt
    Jan 18 17:01:04.925: INFO: Got endpoints: latency-svc-z9mgn [749.54549ms]
    Jan 18 17:01:04.947: INFO: Created: latency-svc-pfnzq
    Jan 18 17:01:04.975: INFO: Got endpoints: latency-svc-sv464 [749.54328ms]
    Jan 18 17:01:04.996: INFO: Created: latency-svc-tx8dt
    Jan 18 17:01:05.029: INFO: Got endpoints: latency-svc-knnns [756.174985ms]
    Jan 18 17:01:05.050: INFO: Created: latency-svc-lnkfl
    Jan 18 17:01:05.075: INFO: Got endpoints: latency-svc-jww9b [747.566738ms]
    Jan 18 17:01:05.094: INFO: Created: latency-svc-t9hcb
    Jan 18 17:01:05.125: INFO: Got endpoints: latency-svc-94q87 [749.557941ms]
    Jan 18 17:01:05.147: INFO: Created: latency-svc-zc5d4
    Jan 18 17:01:05.175: INFO: Got endpoints: latency-svc-5tc2d [750.368276ms]
    Jan 18 17:01:05.197: INFO: Created: latency-svc-pjmcd
    Jan 18 17:01:05.224: INFO: Got endpoints: latency-svc-l2465 [748.493622ms]
    Jan 18 17:01:05.246: INFO: Created: latency-svc-5f459
    Jan 18 17:01:05.276: INFO: Got endpoints: latency-svc-qprsm [751.32834ms]
    Jan 18 17:01:05.296: INFO: Created: latency-svc-8rk9b
    Jan 18 17:01:05.325: INFO: Got endpoints: latency-svc-hdgct [748.318201ms]
    Jan 18 17:01:05.347: INFO: Created: latency-svc-jmgbt
    Jan 18 17:01:05.373: INFO: Got endpoints: latency-svc-szldd [750.229594ms]
    Jan 18 17:01:05.398: INFO: Created: latency-svc-kxqwj
    Jan 18 17:01:05.424: INFO: Got endpoints: latency-svc-7nh9q [747.897838ms]
    Jan 18 17:01:05.442: INFO: Created: latency-svc-2bw8t
    Jan 18 17:01:05.475: INFO: Got endpoints: latency-svc-lwgrp [751.036949ms]
    Jan 18 17:01:05.496: INFO: Created: latency-svc-hxnr2
    Jan 18 17:01:05.525: INFO: Got endpoints: latency-svc-2j79k [750.458214ms]
    Jan 18 17:01:05.546: INFO: Created: latency-svc-jgzl7
    Jan 18 17:01:05.576: INFO: Got endpoints: latency-svc-clt5j [750.252723ms]
    Jan 18 17:01:05.596: INFO: Created: latency-svc-vdppv
    Jan 18 17:01:05.627: INFO: Got endpoints: latency-svc-4frlt [751.808013ms]
    Jan 18 17:01:05.648: INFO: Created: latency-svc-gq6rs
    Jan 18 17:01:05.675: INFO: Got endpoints: latency-svc-pfnzq [749.707239ms]
    Jan 18 17:01:05.697: INFO: Created: latency-svc-g2t67
    Jan 18 17:01:05.724: INFO: Got endpoints: latency-svc-tx8dt [748.227498ms]
    Jan 18 17:01:05.744: INFO: Created: latency-svc-hgqrl
    Jan 18 17:01:05.774: INFO: Got endpoints: latency-svc-lnkfl [745.357418ms]
    Jan 18 17:01:05.794: INFO: Created: latency-svc-qclx8
    Jan 18 17:01:05.825: INFO: Got endpoints: latency-svc-t9hcb [750.642984ms]
    Jan 18 17:01:05.847: INFO: Created: latency-svc-t9zfn
    Jan 18 17:01:05.876: INFO: Got endpoints: latency-svc-zc5d4 [751.498551ms]
    Jan 18 17:01:05.897: INFO: Created: latency-svc-htnt7
    Jan 18 17:01:05.925: INFO: Got endpoints: latency-svc-pjmcd [749.489867ms]
    Jan 18 17:01:05.947: INFO: Created: latency-svc-rw4kh
    Jan 18 17:01:05.975: INFO: Got endpoints: latency-svc-5f459 [750.364673ms]
    Jan 18 17:01:05.996: INFO: Created: latency-svc-5r7sx
    Jan 18 17:01:06.027: INFO: Got endpoints: latency-svc-8rk9b [751.24147ms]
    Jan 18 17:01:06.047: INFO: Created: latency-svc-sdw4s
    Jan 18 17:01:06.076: INFO: Got endpoints: latency-svc-jmgbt [751.657022ms]
    Jan 18 17:01:06.096: INFO: Created: latency-svc-gz2pf
    Jan 18 17:01:06.123: INFO: Got endpoints: latency-svc-kxqwj [749.703629ms]
    Jan 18 17:01:06.142: INFO: Created: latency-svc-rqg7z
    Jan 18 17:01:06.175: INFO: Got endpoints: latency-svc-2bw8t [751.345749ms]
    Jan 18 17:01:06.194: INFO: Created: latency-svc-4tx8d
    Jan 18 17:01:06.224: INFO: Got endpoints: latency-svc-hxnr2 [749.213524ms]
    Jan 18 17:01:06.248: INFO: Created: latency-svc-pwljc
    Jan 18 17:01:06.277: INFO: Got endpoints: latency-svc-jgzl7 [752.384297ms]
    Jan 18 17:01:06.298: INFO: Created: latency-svc-jcdpq
    Jan 18 17:01:06.327: INFO: Got endpoints: latency-svc-vdppv [751.305717ms]
    Jan 18 17:01:06.348: INFO: Created: latency-svc-g5twq
    Jan 18 17:01:06.377: INFO: Got endpoints: latency-svc-gq6rs [749.795249ms]
    Jan 18 17:01:06.460: INFO: Created: latency-svc-6m8qx
    Jan 18 17:01:06.468: INFO: Got endpoints: latency-svc-g2t67 [793.164814ms]
    Jan 18 17:01:06.559: INFO: Got endpoints: latency-svc-hgqrl [835.354952ms]
    Jan 18 17:01:06.568: INFO: Got endpoints: latency-svc-qclx8 [793.263395ms]
    Jan 18 17:01:06.574: INFO: Got endpoints: latency-svc-t9zfn [748.712672ms]
    Jan 18 17:01:06.660: INFO: Created: latency-svc-dzdt4
    Jan 18 17:01:06.672: INFO: Got endpoints: latency-svc-htnt7 [795.935322ms]
    Jan 18 17:01:06.759: INFO: Created: latency-svc-67pcq
    Jan 18 17:01:06.768: INFO: Created: latency-svc-p5kf5
    Jan 18 17:01:06.769: INFO: Got endpoints: latency-svc-rw4kh [843.739317ms]
    Jan 18 17:01:06.775: INFO: Got endpoints: latency-svc-5r7sx [799.826631ms]
    Jan 18 17:01:06.859: INFO: Got endpoints: latency-svc-sdw4s [831.532675ms]
    Jan 18 17:01:06.866: INFO: Created: latency-svc-cfjj6
    Jan 18 17:01:06.874: INFO: Got endpoints: latency-svc-gz2pf [797.13916ms]
    Jan 18 17:01:06.877: INFO: Got endpoints: latency-svc-rqg7z [754.252798ms]
    Jan 18 17:01:06.879: INFO: Created: latency-svc-bbn4f
    Jan 18 17:01:06.893: INFO: Created: latency-svc-9cls5
    Jan 18 17:01:06.907: INFO: Created: latency-svc-pgk46
    Jan 18 17:01:06.915: INFO: Created: latency-svc-p7r5h
    Jan 18 17:01:06.960: INFO: Got endpoints: latency-svc-4tx8d [784.618985ms]
    Jan 18 17:01:06.967: INFO: Created: latency-svc-k6hnc
    Jan 18 17:01:06.980: INFO: Got endpoints: latency-svc-pwljc [755.357006ms]
    Jan 18 17:01:06.983: INFO: Created: latency-svc-5486b
    Jan 18 17:01:06.994: INFO: Created: latency-svc-6phlx
    Jan 18 17:01:07.011: INFO: Created: latency-svc-mggn6
    Jan 18 17:01:07.024: INFO: Got endpoints: latency-svc-jcdpq [746.925089ms]
    Jan 18 17:01:07.046: INFO: Created: latency-svc-46wl4
    Jan 18 17:01:07.076: INFO: Got endpoints: latency-svc-g5twq [749.095725ms]
    Jan 18 17:01:07.096: INFO: Created: latency-svc-4rzh6
    Jan 18 17:01:07.128: INFO: Got endpoints: latency-svc-6m8qx [751.362669ms]
    Jan 18 17:01:07.152: INFO: Created: latency-svc-2mm95
    Jan 18 17:01:07.177: INFO: Got endpoints: latency-svc-dzdt4 [709.127671ms]
    Jan 18 17:01:07.197: INFO: Created: latency-svc-kwxml
    Jan 18 17:01:07.226: INFO: Got endpoints: latency-svc-67pcq [666.434499ms]
    Jan 18 17:01:07.245: INFO: Created: latency-svc-hf4df
    Jan 18 17:01:07.278: INFO: Got endpoints: latency-svc-p5kf5 [710.275509ms]
    Jan 18 17:01:07.303: INFO: Created: latency-svc-nckfz
    Jan 18 17:01:07.324: INFO: Got endpoints: latency-svc-cfjj6 [750.183459ms]
    Jan 18 17:01:07.352: INFO: Created: latency-svc-ffhbz
    Jan 18 17:01:07.377: INFO: Got endpoints: latency-svc-bbn4f [704.939813ms]
    Jan 18 17:01:07.400: INFO: Created: latency-svc-zbtht
    Jan 18 17:01:07.428: INFO: Got endpoints: latency-svc-9cls5 [658.661947ms]
    Jan 18 17:01:07.451: INFO: Created: latency-svc-vb4hd
    Jan 18 17:01:07.475: INFO: Got endpoints: latency-svc-pgk46 [700.498142ms]
    Jan 18 17:01:07.494: INFO: Created: latency-svc-zt7k9
    Jan 18 17:01:07.525: INFO: Got endpoints: latency-svc-p7r5h [666.069098ms]
    Jan 18 17:01:07.544: INFO: Created: latency-svc-m9mrj
    Jan 18 17:01:07.577: INFO: Got endpoints: latency-svc-k6hnc [702.846798ms]
    Jan 18 17:01:07.600: INFO: Created: latency-svc-l9nhp
    Jan 18 17:01:07.626: INFO: Got endpoints: latency-svc-5486b [748.642619ms]
    Jan 18 17:01:07.648: INFO: Created: latency-svc-ppfmg
    Jan 18 17:01:07.677: INFO: Got endpoints: latency-svc-6phlx [717.173446ms]
    Jan 18 17:01:07.695: INFO: Created: latency-svc-nrrcv
    Jan 18 17:01:07.726: INFO: Got endpoints: latency-svc-mggn6 [745.907931ms]
    Jan 18 17:01:07.748: INFO: Created: latency-svc-7swjd
    Jan 18 17:01:07.776: INFO: Got endpoints: latency-svc-46wl4 [752.150864ms]
    Jan 18 17:01:07.803: INFO: Created: latency-svc-t6pdz
    Jan 18 17:01:07.828: INFO: Got endpoints: latency-svc-4rzh6 [751.398609ms]
    Jan 18 17:01:07.847: INFO: Created: latency-svc-pmc9p
    Jan 18 17:01:07.876: INFO: Got endpoints: latency-svc-2mm95 [748.378308ms]
    Jan 18 17:01:07.900: INFO: Created: latency-svc-hjg9b
    Jan 18 17:01:07.933: INFO: Got endpoints: latency-svc-kwxml [755.759438ms]
    Jan 18 17:01:07.956: INFO: Created: latency-svc-c47w5
    Jan 18 17:01:07.976: INFO: Got endpoints: latency-svc-hf4df [750.637954ms]
    Jan 18 17:01:08.001: INFO: Created: latency-svc-5sg6c
    Jan 18 17:01:08.029: INFO: Got endpoints: latency-svc-nckfz [750.635804ms]
    Jan 18 17:01:08.053: INFO: Created: latency-svc-n82tr
    Jan 18 17:01:08.074: INFO: Got endpoints: latency-svc-ffhbz [749.513377ms]
    Jan 18 17:01:08.096: INFO: Created: latency-svc-skgwq
    Jan 18 17:01:08.125: INFO: Got endpoints: latency-svc-zbtht [747.671033ms]
    Jan 18 17:01:08.147: INFO: Created: latency-svc-78mtm
    Jan 18 17:01:08.176: INFO: Got endpoints: latency-svc-vb4hd [748.494568ms]
    Jan 18 17:01:08.197: INFO: Created: latency-svc-l9gkj
    Jan 18 17:01:08.224: INFO: Got endpoints: latency-svc-zt7k9 [748.88346ms]
    Jan 18 17:01:08.255: INFO: Created: latency-svc-z8ksb
    Jan 18 17:01:08.274: INFO: Got endpoints: latency-svc-m9mrj [749.346963ms]
    Jan 18 17:01:08.297: INFO: Created: latency-svc-hkdgc
    Jan 18 17:01:08.324: INFO: Got endpoints: latency-svc-l9nhp [747.829362ms]
    Jan 18 17:01:08.345: INFO: Created: latency-svc-7w5l5
    Jan 18 17:01:08.376: INFO: Got endpoints: latency-svc-ppfmg [749.959767ms]
    Jan 18 17:01:08.398: INFO: Created: latency-svc-4lx7s
    Jan 18 17:01:08.428: INFO: Got endpoints: latency-svc-nrrcv [750.251658ms]
    Jan 18 17:01:08.446: INFO: Created: latency-svc-q62ll
    Jan 18 17:01:08.476: INFO: Got endpoints: latency-svc-7swjd [750.082818ms]
    Jan 18 17:01:08.497: INFO: Created: latency-svc-54j9b
    Jan 18 17:01:08.525: INFO: Got endpoints: latency-svc-t6pdz [749.006651ms]
    Jan 18 17:01:08.548: INFO: Created: latency-svc-dpqct
    Jan 18 17:01:08.576: INFO: Got endpoints: latency-svc-pmc9p [747.855803ms]
    Jan 18 17:01:08.596: INFO: Created: latency-svc-xwk2s
    Jan 18 17:01:08.626: INFO: Got endpoints: latency-svc-hjg9b [749.314852ms]
    Jan 18 17:01:08.651: INFO: Created: latency-svc-vbkdd
    Jan 18 17:01:08.678: INFO: Got endpoints: latency-svc-c47w5 [745.434656ms]
    Jan 18 17:01:08.698: INFO: Created: latency-svc-8fjj8
    Jan 18 17:01:08.724: INFO: Got endpoints: latency-svc-5sg6c [748.072223ms]
    Jan 18 17:01:08.744: INFO: Created: latency-svc-z8vr6
    Jan 18 17:01:08.775: INFO: Got endpoints: latency-svc-n82tr [746.816974ms]
    Jan 18 17:01:08.796: INFO: Created: latency-svc-rzl8v
    Jan 18 17:01:08.823: INFO: Got endpoints: latency-svc-skgwq [749.564353ms]
    Jan 18 17:01:08.842: INFO: Created: latency-svc-rt7d7
    Jan 18 17:01:08.877: INFO: Got endpoints: latency-svc-78mtm [751.573187ms]
    Jan 18 17:01:08.899: INFO: Created: latency-svc-6scwk
    Jan 18 17:01:08.925: INFO: Got endpoints: latency-svc-l9gkj [748.98288ms]
    Jan 18 17:01:08.949: INFO: Created: latency-svc-5xgj4
    Jan 18 17:01:08.974: INFO: Got endpoints: latency-svc-z8ksb [750.022517ms]
    Jan 18 17:01:08.995: INFO: Created: latency-svc-jfhkn
    Jan 18 17:01:09.026: INFO: Got endpoints: latency-svc-hkdgc [751.367666ms]
    Jan 18 17:01:09.046: INFO: Created: latency-svc-s755l
    Jan 18 17:01:09.075: INFO: Got endpoints: latency-svc-7w5l5 [750.46184ms]
    Jan 18 17:01:09.097: INFO: Created: latency-svc-zltg2
    Jan 18 17:01:09.126: INFO: Got endpoints: latency-svc-4lx7s [750.44432ms]
    Jan 18 17:01:09.167: INFO: Created: latency-svc-mht25
    Jan 18 17:01:09.175: INFO: Got endpoints: latency-svc-q62ll [747.023367ms]
    Jan 18 17:01:09.198: INFO: Created: latency-svc-d7sf7
    Jan 18 17:01:09.225: INFO: Got endpoints: latency-svc-54j9b [748.752048ms]
    Jan 18 17:01:09.247: INFO: Created: latency-svc-4x5td
    Jan 18 17:01:09.275: INFO: Got endpoints: latency-svc-dpqct [749.822776ms]
    Jan 18 17:01:09.295: INFO: Created: latency-svc-rdrfl
    Jan 18 17:01:09.325: INFO: Got endpoints: latency-svc-xwk2s [749.19602ms]
    Jan 18 17:01:09.344: INFO: Created: latency-svc-ds7sn
    Jan 18 17:01:09.377: INFO: Got endpoints: latency-svc-vbkdd [750.991293ms]
    Jan 18 17:01:09.399: INFO: Created: latency-svc-j5kjl
    Jan 18 17:01:09.423: INFO: Got endpoints: latency-svc-8fjj8 [744.650791ms]
    Jan 18 17:01:09.445: INFO: Created: latency-svc-gzm2k
    Jan 18 17:01:09.474: INFO: Got endpoints: latency-svc-z8vr6 [749.512653ms]
    Jan 18 17:01:09.492: INFO: Created: latency-svc-kzrrx
    Jan 18 17:01:09.526: INFO: Got endpoints: latency-svc-rzl8v [750.899601ms]
    Jan 18 17:01:09.573: INFO: Got endpoints: latency-svc-rt7d7 [749.712435ms]
    Jan 18 17:01:09.625: INFO: Got endpoints: latency-svc-6scwk [747.630609ms]
    Jan 18 17:01:09.679: INFO: Got endpoints: latency-svc-5xgj4 [753.42438ms]
    Jan 18 17:01:09.728: INFO: Got endpoints: latency-svc-jfhkn [753.297658ms]
    Jan 18 17:01:09.776: INFO: Got endpoints: latency-svc-s755l [750.325888ms]
    Jan 18 17:01:09.827: INFO: Got endpoints: latency-svc-zltg2 [751.927999ms]
    Jan 18 17:01:09.874: INFO: Got endpoints: latency-svc-mht25 [747.462068ms]
    Jan 18 17:01:09.926: INFO: Got endpoints: latency-svc-d7sf7 [751.161004ms]
    Jan 18 17:01:09.975: INFO: Got endpoints: latency-svc-4x5td [750.338257ms]
    Jan 18 17:01:10.025: INFO: Got endpoints: latency-svc-rdrfl [749.788234ms]
    Jan 18 17:01:10.075: INFO: Got endpoints: latency-svc-ds7sn [750.085826ms]
    Jan 18 17:01:10.143: INFO: Got endpoints: latency-svc-j5kjl [765.989314ms]
    Jan 18 17:01:10.173: INFO: Got endpoints: latency-svc-gzm2k [750.099044ms]
    Jan 18 17:01:10.224: INFO: Got endpoints: latency-svc-kzrrx [750.090345ms]
    Jan 18 17:01:10.224: INFO: Latencies: [31.804136ms 41.981317ms 83.775622ms 99.80615ms 110.758776ms 127.021956ms 127.500169ms 185.013062ms 194.529297ms 201.985237ms 214.126241ms 224.740684ms 236.150722ms 238.547037ms 239.025381ms 243.208319ms 245.853388ms 256.689561ms 267.319065ms 269.250577ms 270.678898ms 272.724431ms 276.95377ms 280.882646ms 282.651448ms 285.059275ms 285.159396ms 287.914094ms 289.326083ms 290.884384ms 291.120166ms 292.395485ms 293.969614ms 294.72392ms 297.226367ms 298.552816ms 298.631326ms 300.133758ms 300.792961ms 300.893803ms 301.545936ms 302.07145ms 307.357257ms 309.926903ms 315.912274ms 319.881701ms 327.370692ms 337.0831ms 373.479937ms 377.051761ms 387.988366ms 429.905803ms 481.458853ms 508.516067ms 517.959093ms 548.589863ms 585.531075ms 622.702858ms 653.210154ms 658.175819ms 658.661947ms 666.069098ms 666.434499ms 691.222094ms 700.498142ms 702.846798ms 704.939813ms 709.127671ms 710.275509ms 716.024672ms 717.173446ms 731.346587ms 744.650791ms 745.357418ms 745.434656ms 745.907931ms 746.771969ms 746.816974ms 746.925089ms 747.023367ms 747.462068ms 747.514944ms 747.566738ms 747.630609ms 747.671033ms 747.689827ms 747.816778ms 747.829362ms 747.84152ms 747.855803ms 747.897838ms 748.072223ms 748.079359ms 748.227498ms 748.257761ms 748.318201ms 748.378308ms 748.493622ms 748.494568ms 748.517111ms 748.642619ms 748.712672ms 748.752048ms 748.835745ms 748.88346ms 748.98288ms 749.006427ms 749.006651ms 749.095725ms 749.19602ms 749.213524ms 749.314852ms 749.346963ms 749.422489ms 749.489867ms 749.512653ms 749.513377ms 749.52282ms 749.54328ms 749.54549ms 749.557941ms 749.564353ms 749.703629ms 749.707239ms 749.712435ms 749.788234ms 749.795249ms 749.822776ms 749.863491ms 749.950961ms 749.959767ms 750.022517ms 750.082818ms 750.085826ms 750.090345ms 750.099044ms 750.118593ms 750.183459ms 750.194164ms 750.229594ms 750.251658ms 750.252723ms 750.325888ms 750.338257ms 750.364673ms 750.368276ms 750.44432ms 750.458214ms 750.46184ms 750.598297ms 750.635804ms 750.637954ms 750.642984ms 750.759687ms 750.836928ms 750.892619ms 750.899601ms 750.987239ms 750.991293ms 751.036949ms 751.161004ms 751.23855ms 751.24147ms 751.305717ms 751.32834ms 751.339571ms 751.345749ms 751.362669ms 751.362961ms 751.367666ms 751.398609ms 751.498551ms 751.573187ms 751.630395ms 751.646824ms 751.657022ms 751.808013ms 751.927999ms 752.111837ms 752.150864ms 752.216968ms 752.384297ms 753.104435ms 753.297658ms 753.42438ms 753.461857ms 754.252798ms 755.357006ms 755.759438ms 756.174985ms 765.989314ms 784.618985ms 793.164814ms 793.263395ms 795.935322ms 797.13916ms 799.826631ms 831.532675ms 835.354952ms 843.739317ms]
    Jan 18 17:01:10.224: INFO: 50 %ile: 748.642619ms
    Jan 18 17:01:10.224: INFO: 90 %ile: 752.216968ms
    Jan 18 17:01:10.224: INFO: 99 %ile: 835.354952ms
    Jan 18 17:01:10.225: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 18 17:01:10.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-5490" for this suite. 01/18/23 17:01:10.235
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:01:10.251
Jan 18 17:01:10.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context-test 01/18/23 17:01:10.253
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:10.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:10.301
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 18 17:01:10.319: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473" in namespace "security-context-test-5514" to be "Succeeded or Failed"
Jan 18 17:01:10.326: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Pending", Reason="", readiness=false. Elapsed: 6.672985ms
Jan 18 17:01:12.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127681s
Jan 18 17:01:14.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014999852s
Jan 18 17:01:14.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473" satisfied condition "Succeeded or Failed"
Jan 18 17:01:14.366: INFO: Got logs for pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 17:01:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5514" for this suite. 01/18/23 17:01:14.374
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":258,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:01:10.251
    Jan 18 17:01:10.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 17:01:10.253
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:10.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:10.301
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 18 17:01:10.319: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473" in namespace "security-context-test-5514" to be "Succeeded or Failed"
    Jan 18 17:01:10.326: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Pending", Reason="", readiness=false. Elapsed: 6.672985ms
    Jan 18 17:01:12.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015127681s
    Jan 18 17:01:14.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014999852s
    Jan 18 17:01:14.334: INFO: Pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473" satisfied condition "Succeeded or Failed"
    Jan 18 17:01:14.366: INFO: Got logs for pod "busybox-privileged-false-64d4f10e-6e5c-45f4-9d17-db2b94306473": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 17:01:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5514" for this suite. 01/18/23 17:01:14.374
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:01:14.389
Jan 18 17:01:14.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:01:14.39
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:14.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:14.42
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:01:14.451
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:01:14.658
STEP: Deploying the webhook pod 01/18/23 17:01:14.675
STEP: Wait for the deployment to be ready 01/18/23 17:01:14.696
Jan 18 17:01:14.709: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 17:01:16.728
STEP: Verifying the service has paired with the endpoint 01/18/23 17:01:16.749
Jan 18 17:01:17.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:18.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:19.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:20.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:21.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:22.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:23.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:24.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:25.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
Jan 18 17:01:26.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/18/23 17:01:26.758
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.794
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 17:01:26.816
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.838
STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 17:01:26.861
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.875
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:01:26.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5528" for this suite. 01/18/23 17:01:26.901
STEP: Destroying namespace "webhook-5528-markers" for this suite. 01/18/23 17:01:26.916
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":18,"skipped":261,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.609 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:01:14.389
    Jan 18 17:01:14.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:01:14.39
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:14.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:14.42
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:01:14.451
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:01:14.658
    STEP: Deploying the webhook pod 01/18/23 17:01:14.675
    STEP: Wait for the deployment to be ready 01/18/23 17:01:14.696
    Jan 18 17:01:14.709: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 17:01:16.728
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:01:16.749
    Jan 18 17:01:17.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:18.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:19.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:20.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:21.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:22.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:23.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:24.751: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:25.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    Jan 18 17:01:26.750: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/18/23 17:01:26.758
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.794
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/18/23 17:01:26.816
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.838
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/18/23 17:01:26.861
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 17:01:26.875
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:01:26.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5528" for this suite. 01/18/23 17:01:26.901
    STEP: Destroying namespace "webhook-5528-markers" for this suite. 01/18/23 17:01:26.916
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:01:27.003
Jan 18 17:01:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:01:27.004
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:27.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:27.034
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 17:01:27.073
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:01:27.083
Jan 18 17:01:27.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:01:27.096: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:01:28.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:01:28.113: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:01:29.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:01:29.114: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:01:30.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:01:30.112: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:01:31.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:01:31.115: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:01:32.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:01:32.113: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 17:01:32.12
Jan 18 17:01:32.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:01:32.163: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:01:33.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:01:33.181: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:01:34.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:01:34.180: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 17:01:34.18
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:01:34.193
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-749, will wait for the garbage collector to delete the pods 01/18/23 17:01:34.194
Jan 18 17:01:34.264: INFO: Deleting DaemonSet.extensions daemon-set took: 13.002537ms
Jan 18 17:01:34.365: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756262ms
Jan 18 17:01:36.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:01:36.774: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 17:01:36.781: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631799075"},"items":null}

Jan 18 17:01:36.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631799075"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:01:36.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-749" for this suite. 01/18/23 17:01:36.819
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":19,"skipped":314,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.829 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:01:27.003
    Jan 18 17:01:27.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:01:27.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:27.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:27.034
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/18/23 17:01:27.073
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:01:27.083
    Jan 18 17:01:27.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:01:27.096: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:01:28.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:01:28.113: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:01:29.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:01:29.114: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:01:30.112: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:01:30.112: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:01:31.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:01:31.115: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:01:32.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:01:32.113: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/18/23 17:01:32.12
    Jan 18 17:01:32.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:01:32.163: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:01:33.181: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:01:33.181: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:01:34.179: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:01:34.180: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/18/23 17:01:34.18
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:01:34.193
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-749, will wait for the garbage collector to delete the pods 01/18/23 17:01:34.194
    Jan 18 17:01:34.264: INFO: Deleting DaemonSet.extensions daemon-set took: 13.002537ms
    Jan 18 17:01:34.365: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756262ms
    Jan 18 17:01:36.774: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:01:36.774: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 17:01:36.781: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631799075"},"items":null}

    Jan 18 17:01:36.789: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631799075"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:01:36.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-749" for this suite. 01/18/23 17:01:36.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:01:36.833
Jan 18 17:01:36.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename subpath 01/18/23 17:01:36.835
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:36.857
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:36.862
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 17:01:36.867
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-v2sh 01/18/23 17:01:36.885
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:01:36.885
Jan 18 17:01:36.901: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-v2sh" in namespace "subpath-6251" to be "Succeeded or Failed"
Jan 18 17:01:36.907: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249005ms
Jan 18 17:01:38.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 2.014345122s
Jan 18 17:01:40.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 4.013022153s
Jan 18 17:01:42.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 6.014427591s
Jan 18 17:01:44.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 8.014224573s
Jan 18 17:01:46.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 10.014463868s
Jan 18 17:01:48.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 12.013823206s
Jan 18 17:01:50.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 14.013418565s
Jan 18 17:01:52.917: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015345078s
Jan 18 17:01:54.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 18.013252108s
Jan 18 17:01:56.917: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 20.015038524s
Jan 18 17:01:58.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=false. Elapsed: 22.013255655s
Jan 18 17:02:00.944: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042335945s
STEP: Saw pod success 01/18/23 17:02:00.944
Jan 18 17:02:00.944: INFO: Pod "pod-subpath-test-downwardapi-v2sh" satisfied condition "Succeeded or Failed"
Jan 18 17:02:00.951: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod pod-subpath-test-downwardapi-v2sh container test-container-subpath-downwardapi-v2sh: <nil>
STEP: delete the pod 01/18/23 17:02:00.967
Jan 18 17:02:00.992: INFO: Waiting for pod pod-subpath-test-downwardapi-v2sh to disappear
Jan 18 17:02:00.999: INFO: Pod pod-subpath-test-downwardapi-v2sh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-v2sh 01/18/23 17:02:00.999
Jan 18 17:02:00.999: INFO: Deleting pod "pod-subpath-test-downwardapi-v2sh" in namespace "subpath-6251"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 17:02:01.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6251" for this suite. 01/18/23 17:02:01.015
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":20,"skipped":319,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.196 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:01:36.833
    Jan 18 17:01:36.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename subpath 01/18/23 17:01:36.835
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:01:36.857
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:01:36.862
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 17:01:36.867
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-v2sh 01/18/23 17:01:36.885
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:01:36.885
    Jan 18 17:01:36.901: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-v2sh" in namespace "subpath-6251" to be "Succeeded or Failed"
    Jan 18 17:01:36.907: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Pending", Reason="", readiness=false. Elapsed: 5.249005ms
    Jan 18 17:01:38.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 2.014345122s
    Jan 18 17:01:40.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 4.013022153s
    Jan 18 17:01:42.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 6.014427591s
    Jan 18 17:01:44.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 8.014224573s
    Jan 18 17:01:46.916: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 10.014463868s
    Jan 18 17:01:48.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 12.013823206s
    Jan 18 17:01:50.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 14.013418565s
    Jan 18 17:01:52.917: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 16.015345078s
    Jan 18 17:01:54.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 18.013252108s
    Jan 18 17:01:56.917: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=true. Elapsed: 20.015038524s
    Jan 18 17:01:58.915: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Running", Reason="", readiness=false. Elapsed: 22.013255655s
    Jan 18 17:02:00.944: INFO: Pod "pod-subpath-test-downwardapi-v2sh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.042335945s
    STEP: Saw pod success 01/18/23 17:02:00.944
    Jan 18 17:02:00.944: INFO: Pod "pod-subpath-test-downwardapi-v2sh" satisfied condition "Succeeded or Failed"
    Jan 18 17:02:00.951: INFO: Trying to get logs from node scw-conformance125-default-788643c0f7cd4128a5c pod pod-subpath-test-downwardapi-v2sh container test-container-subpath-downwardapi-v2sh: <nil>
    STEP: delete the pod 01/18/23 17:02:00.967
    Jan 18 17:02:00.992: INFO: Waiting for pod pod-subpath-test-downwardapi-v2sh to disappear
    Jan 18 17:02:00.999: INFO: Pod pod-subpath-test-downwardapi-v2sh no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-v2sh 01/18/23 17:02:00.999
    Jan 18 17:02:00.999: INFO: Deleting pod "pod-subpath-test-downwardapi-v2sh" in namespace "subpath-6251"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 17:02:01.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6251" for this suite. 01/18/23 17:02:01.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:02:01.031
Jan 18 17:02:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:02:01.032
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:01.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:01.06
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 17:02:01.094
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:02:01.104
Jan 18 17:02:01.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:02:01.115: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:02:02.133: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:02:02.133: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:02:03.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:02:03.132: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 01/18/23 17:02:03.138
STEP: DeleteCollection of the DaemonSets 01/18/23 17:02:03.146
STEP: Verify that ReplicaSets have been deleted 01/18/23 17:02:03.161
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 18 17:02:03.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631799919"},"items":null}

Jan 18 17:02:03.191: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631799919"},"items":[{"metadata":{"name":"daemon-set-656cr","generateName":"daemon-set-","namespace":"daemonsets-7242","uid":"7f9fce18-8fe9-4eeb-a48a-c52e75d2dfd8","resourceVersion":"2631799889","creationTimestamp":"2023-01-18T17:02:01Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7ae1ca3dd15fd57bff505205fdbdbd31017313cea03f12e8d072df7cd4de7d82","cni.projectcalico.org/podIP":"10.100.145.144/32","cni.projectcalico.org/podIPs":"10.100.145.144/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"10414514-acd6-413d-8c17-85ae7833d320","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10414514-acd6-413d-8c17-85ae7833d320\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sznqd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sznqd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"scw-conformance125-default-61c39bbf4d81476a8e3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["scw-conformance125-default-61c39bbf4d81476a8e3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"}],"hostIP":"10.195.74.123","podIP":"10.100.145.144","podIPs":[{"ip":"10.100.145.144"}],"startTime":"2023-01-18T17:02:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T17:02:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e79ecdd040917a8dc32bf21e35d48f07d793e17dfbdda8ff2ca23d13ed3e0343","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-btpb6","generateName":"daemon-set-","namespace":"daemonsets-7242","uid":"4e933b75-da98-4682-be09-b6c7c99cd8da","resourceVersion":"2631799904","creationTimestamp":"2023-01-18T17:02:01Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c223e692fc730edaf971fef774243166d21c1ddc14c7deec89adc1d7458ef7d8","cni.projectcalico.org/podIP":"10.100.158.19/32","cni.projectcalico.org/podIPs":"10.100.158.19/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"10414514-acd6-413d-8c17-85ae7833d320","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10414514-acd6-413d-8c17-85ae7833d320\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rqgn2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rqgn2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"scw-conformance125-default-788643c0f7cd4128a5c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["scw-conformance125-default-788643c0f7cd4128a5c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"}],"hostIP":"10.195.78.23","podIP":"10.100.158.19","podIPs":[{"ip":"10.100.158.19"}],"startTime":"2023-01-18T17:02:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T17:02:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://fd8720799d1a98852ee3c91db201ebf973d34385597975acc726a70d19216536","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:02:03.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7242" for this suite. 01/18/23 17:02:03.218
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":21,"skipped":337,"failed":0}
------------------------------
â€¢ [2.197 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:02:01.031
    Jan 18 17:02:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:02:01.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:01.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:01.06
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 17:02:01.094
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:02:01.104
    Jan 18 17:02:01.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:02:01.115: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:02:02.133: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:02:02.133: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:02:03.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:02:03.132: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 01/18/23 17:02:03.138
    STEP: DeleteCollection of the DaemonSets 01/18/23 17:02:03.146
    STEP: Verify that ReplicaSets have been deleted 01/18/23 17:02:03.161
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 18 17:02:03.185: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631799919"},"items":null}

    Jan 18 17:02:03.191: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631799919"},"items":[{"metadata":{"name":"daemon-set-656cr","generateName":"daemon-set-","namespace":"daemonsets-7242","uid":"7f9fce18-8fe9-4eeb-a48a-c52e75d2dfd8","resourceVersion":"2631799889","creationTimestamp":"2023-01-18T17:02:01Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7ae1ca3dd15fd57bff505205fdbdbd31017313cea03f12e8d072df7cd4de7d82","cni.projectcalico.org/podIP":"10.100.145.144/32","cni.projectcalico.org/podIPs":"10.100.145.144/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"10414514-acd6-413d-8c17-85ae7833d320","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10414514-acd6-413d-8c17-85ae7833d320\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sznqd","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sznqd","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"scw-conformance125-default-61c39bbf4d81476a8e3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["scw-conformance125-default-61c39bbf4d81476a8e3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"}],"hostIP":"10.195.74.123","podIP":"10.100.145.144","podIPs":[{"ip":"10.100.145.144"}],"startTime":"2023-01-18T17:02:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T17:02:02Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://e79ecdd040917a8dc32bf21e35d48f07d793e17dfbdda8ff2ca23d13ed3e0343","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-btpb6","generateName":"daemon-set-","namespace":"daemonsets-7242","uid":"4e933b75-da98-4682-be09-b6c7c99cd8da","resourceVersion":"2631799904","creationTimestamp":"2023-01-18T17:02:01Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c223e692fc730edaf971fef774243166d21c1ddc14c7deec89adc1d7458ef7d8","cni.projectcalico.org/podIP":"10.100.158.19/32","cni.projectcalico.org/podIPs":"10.100.158.19/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"10414514-acd6-413d-8c17-85ae7833d320","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"10414514-acd6-413d-8c17-85ae7833d320\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-18T17:02:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rqgn2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rqgn2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"scw-conformance125-default-788643c0f7cd4128a5c","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["scw-conformance125-default-788643c0f7cd4128a5c"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:02Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-18T17:02:01Z"}],"hostIP":"10.195.78.23","podIP":"10.100.158.19","podIPs":[{"ip":"10.100.158.19"}],"startTime":"2023-01-18T17:02:01Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-18T17:02:01Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://fd8720799d1a98852ee3c91db201ebf973d34385597975acc726a70d19216536","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:02:03.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7242" for this suite. 01/18/23 17:02:03.218
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:02:03.23
Jan 18 17:02:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:02:03.231
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:03.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:03.257
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/18/23 17:02:03.261
Jan 18 17:02:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 create -f -'
Jan 18 17:02:04.724: INFO: stderr: ""
Jan 18 17:02:04.724: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/18/23 17:02:04.724
Jan 18 17:02:04.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 diff -f -'
Jan 18 17:02:04.976: INFO: rc: 1
Jan 18 17:02:04.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 delete -f -'
Jan 18 17:02:05.079: INFO: stderr: ""
Jan 18 17:02:05.079: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:02:05.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4138" for this suite. 01/18/23 17:02:05.087
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":22,"skipped":339,"failed":0}
------------------------------
â€¢ [1.869 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:02:03.23
    Jan 18 17:02:03.230: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:02:03.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:03.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:03.257
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/18/23 17:02:03.261
    Jan 18 17:02:03.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 create -f -'
    Jan 18 17:02:04.724: INFO: stderr: ""
    Jan 18 17:02:04.724: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/18/23 17:02:04.724
    Jan 18 17:02:04.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 diff -f -'
    Jan 18 17:02:04.976: INFO: rc: 1
    Jan 18 17:02:04.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-4138 delete -f -'
    Jan 18 17:02:05.079: INFO: stderr: ""
    Jan 18 17:02:05.079: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:02:05.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4138" for this suite. 01/18/23 17:02:05.087
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:02:05.099
Jan 18 17:02:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 17:02:05.1
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:05.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:05.124
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 18 17:02:05.129: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 17:03:05.165: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 18 17:03:05.172: INFO: Starting informer...
STEP: Starting pods... 01/18/23 17:03:05.172
Jan 18 17:03:05.406: INFO: Pod1 is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
Jan 18 17:03:05.623: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1025" to be "running"
Jan 18 17:03:05.630: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.587594ms
Jan 18 17:03:07.639: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933254s
Jan 18 17:03:07.639: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 18 17:03:07.639: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1025" to be "running"
Jan 18 17:03:07.649: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.739205ms
Jan 18 17:03:07.649: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 18 17:03:07.649: INFO: Pod2 is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 17:03:07.649
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:03:07.674
STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 17:03:07.681
Jan 18 17:03:13.596: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 18 17:03:33.602: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:03:33.627
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:03:33.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1025" for this suite. 01/18/23 17:03:33.643
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":23,"skipped":341,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.555 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:02:05.099
    Jan 18 17:02:05.099: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename taint-multiple-pods 01/18/23 17:02:05.1
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:02:05.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:02:05.124
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 18 17:02:05.129: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 17:03:05.165: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 18 17:03:05.172: INFO: Starting informer...
    STEP: Starting pods... 01/18/23 17:03:05.172
    Jan 18 17:03:05.406: INFO: Pod1 is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
    Jan 18 17:03:05.623: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-1025" to be "running"
    Jan 18 17:03:05.630: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.587594ms
    Jan 18 17:03:07.639: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015933254s
    Jan 18 17:03:07.639: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 18 17:03:07.639: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-1025" to be "running"
    Jan 18 17:03:07.649: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 9.739205ms
    Jan 18 17:03:07.649: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 18 17:03:07.649: INFO: Pod2 is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 17:03:07.649
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:03:07.674
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/18/23 17:03:07.681
    Jan 18 17:03:13.596: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 18 17:03:33.602: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:03:33.627
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:03:33.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-1025" for this suite. 01/18/23 17:03:33.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:33.657
Jan 18 17:03:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:03:33.658
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:33.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:33.691
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:03:33.722
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:03:34.242
STEP: Deploying the webhook pod 01/18/23 17:03:34.253
STEP: Wait for the deployment to be ready 01/18/23 17:03:34.278
Jan 18 17:03:34.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:03:36.315
STEP: Verifying the service has paired with the endpoint 01/18/23 17:03:36.337
Jan 18 17:03:37.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 17:03:37.345
STEP: create a configmap that should be updated by the webhook 01/18/23 17:03:37.379
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:03:37.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8383" for this suite. 01/18/23 17:03:37.428
STEP: Destroying namespace "webhook-8383-markers" for this suite. 01/18/23 17:03:37.442
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":24,"skipped":362,"failed":0}
------------------------------
â€¢ [3.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:33.657
    Jan 18 17:03:33.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:03:33.658
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:33.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:33.691
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:03:33.722
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:03:34.242
    STEP: Deploying the webhook pod 01/18/23 17:03:34.253
    STEP: Wait for the deployment to be ready 01/18/23 17:03:34.278
    Jan 18 17:03:34.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:03:36.315
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:03:36.337
    Jan 18 17:03:37.338: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/18/23 17:03:37.345
    STEP: create a configmap that should be updated by the webhook 01/18/23 17:03:37.379
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:03:37.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8383" for this suite. 01/18/23 17:03:37.428
    STEP: Destroying namespace "webhook-8383-markers" for this suite. 01/18/23 17:03:37.442
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:37.524
Jan 18 17:03:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:03:37.525
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:37.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:37.589
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/18/23 17:03:37.593
STEP: Creating a ResourceQuota 01/18/23 17:03:42.602
STEP: Ensuring resource quota status is calculated 01/18/23 17:03:42.612
STEP: Creating a ReplicaSet 01/18/23 17:03:44.622
STEP: Ensuring resource quota status captures replicaset creation 01/18/23 17:03:44.642
STEP: Deleting a ReplicaSet 01/18/23 17:03:46.652
STEP: Ensuring resource quota status released usage 01/18/23 17:03:46.667
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:03:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2061" for this suite. 01/18/23 17:03:48.694
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":25,"skipped":396,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.183 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:37.524
    Jan 18 17:03:37.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:03:37.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:37.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:37.589
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/18/23 17:03:37.593
    STEP: Creating a ResourceQuota 01/18/23 17:03:42.602
    STEP: Ensuring resource quota status is calculated 01/18/23 17:03:42.612
    STEP: Creating a ReplicaSet 01/18/23 17:03:44.622
    STEP: Ensuring resource quota status captures replicaset creation 01/18/23 17:03:44.642
    STEP: Deleting a ReplicaSet 01/18/23 17:03:46.652
    STEP: Ensuring resource quota status released usage 01/18/23 17:03:46.667
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:03:48.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2061" for this suite. 01/18/23 17:03:48.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:48.716
Jan 18 17:03:48.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:03:48.717
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:48.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:48.75
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-5d5dbd9c-78d8-4d17-93fe-a03ee7d057e6 01/18/23 17:03:48.756
STEP: Creating a pod to test consume secrets 01/18/23 17:03:48.765
Jan 18 17:03:48.782: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f" in namespace "projected-5929" to be "Succeeded or Failed"
Jan 18 17:03:48.792: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.780594ms
Jan 18 17:03:50.800: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018080201s
Jan 18 17:03:52.801: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018884072s
STEP: Saw pod success 01/18/23 17:03:52.801
Jan 18 17:03:52.801: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f" satisfied condition "Succeeded or Failed"
Jan 18 17:03:52.809: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:03:52.839
Jan 18 17:03:52.857: INFO: Waiting for pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f to disappear
Jan 18 17:03:52.863: INFO: Pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 17:03:52.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5929" for this suite. 01/18/23 17:03:52.871
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":26,"skipped":509,"failed":0}
------------------------------
â€¢ [4.168 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:48.716
    Jan 18 17:03:48.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:03:48.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:48.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:48.75
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-5d5dbd9c-78d8-4d17-93fe-a03ee7d057e6 01/18/23 17:03:48.756
    STEP: Creating a pod to test consume secrets 01/18/23 17:03:48.765
    Jan 18 17:03:48.782: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f" in namespace "projected-5929" to be "Succeeded or Failed"
    Jan 18 17:03:48.792: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.780594ms
    Jan 18 17:03:50.800: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018080201s
    Jan 18 17:03:52.801: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018884072s
    STEP: Saw pod success 01/18/23 17:03:52.801
    Jan 18 17:03:52.801: INFO: Pod "pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f" satisfied condition "Succeeded or Failed"
    Jan 18 17:03:52.809: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:03:52.839
    Jan 18 17:03:52.857: INFO: Waiting for pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f to disappear
    Jan 18 17:03:52.863: INFO: Pod pod-projected-secrets-f7b077f1-70c6-4883-9f45-cfd285791a0f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 17:03:52.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5929" for this suite. 01/18/23 17:03:52.871
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:52.884
Jan 18 17:03:52.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:03:52.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:52.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:52.913
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/18/23 17:03:52.918
STEP: submitting the pod to kubernetes 01/18/23 17:03:52.918
Jan 18 17:03:52.931: INFO: Waiting up to 5m0s for pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" in namespace "pods-2436" to be "running and ready"
Jan 18 17:03:52.938: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595593ms
Jan 18 17:03:52.938: INFO: The phase of Pod pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:03:54.960: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Running", Reason="", readiness=true. Elapsed: 2.028449637s
Jan 18 17:03:54.960: INFO: The phase of Pod pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084 is Running (Ready = true)
Jan 18 17:03:54.960: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 17:03:54.967
STEP: updating the pod 01/18/23 17:03:54.974
Jan 18 17:03:55.501: INFO: Successfully updated pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084"
Jan 18 17:03:55.501: INFO: Waiting up to 5m0s for pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" in namespace "pods-2436" to be "running"
Jan 18 17:03:55.507: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Running", Reason="", readiness=true. Elapsed: 6.838045ms
Jan 18 17:03:55.507: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/18/23 17:03:55.508
Jan 18 17:03:55.515: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:03:55.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2436" for this suite. 01/18/23 17:03:55.524
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":27,"skipped":510,"failed":0}
------------------------------
â€¢ [2.654 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:52.884
    Jan 18 17:03:52.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:03:52.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:52.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:52.913
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/18/23 17:03:52.918
    STEP: submitting the pod to kubernetes 01/18/23 17:03:52.918
    Jan 18 17:03:52.931: INFO: Waiting up to 5m0s for pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" in namespace "pods-2436" to be "running and ready"
    Jan 18 17:03:52.938: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Pending", Reason="", readiness=false. Elapsed: 6.595593ms
    Jan 18 17:03:52.938: INFO: The phase of Pod pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:03:54.960: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Running", Reason="", readiness=true. Elapsed: 2.028449637s
    Jan 18 17:03:54.960: INFO: The phase of Pod pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084 is Running (Ready = true)
    Jan 18 17:03:54.960: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 17:03:54.967
    STEP: updating the pod 01/18/23 17:03:54.974
    Jan 18 17:03:55.501: INFO: Successfully updated pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084"
    Jan 18 17:03:55.501: INFO: Waiting up to 5m0s for pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" in namespace "pods-2436" to be "running"
    Jan 18 17:03:55.507: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084": Phase="Running", Reason="", readiness=true. Elapsed: 6.838045ms
    Jan 18 17:03:55.507: INFO: Pod "pod-update-2e5504b2-5e57-4581-87e6-7e1abcffa084" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/18/23 17:03:55.508
    Jan 18 17:03:55.515: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:03:55.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2436" for this suite. 01/18/23 17:03:55.524
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:55.54
Jan 18 17:03:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:03:55.542
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:55.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:55.575
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:03:55.58
Jan 18 17:03:55.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796" in namespace "projected-4552" to be "Succeeded or Failed"
Jan 18 17:03:55.606: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Pending", Reason="", readiness=false. Elapsed: 8.274964ms
Jan 18 17:03:57.613: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015638035s
Jan 18 17:03:59.615: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0169042s
STEP: Saw pod success 01/18/23 17:03:59.615
Jan 18 17:03:59.615: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796" satisfied condition "Succeeded or Failed"
Jan 18 17:03:59.621: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 container client-container: <nil>
STEP: delete the pod 01/18/23 17:03:59.638
Jan 18 17:03:59.659: INFO: Waiting for pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 to disappear
Jan 18 17:03:59.667: INFO: Pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:03:59.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4552" for this suite. 01/18/23 17:03:59.675
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":28,"skipped":512,"failed":0}
------------------------------
â€¢ [4.146 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:55.54
    Jan 18 17:03:55.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:03:55.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:55.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:55.575
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:03:55.58
    Jan 18 17:03:55.598: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796" in namespace "projected-4552" to be "Succeeded or Failed"
    Jan 18 17:03:55.606: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Pending", Reason="", readiness=false. Elapsed: 8.274964ms
    Jan 18 17:03:57.613: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015638035s
    Jan 18 17:03:59.615: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0169042s
    STEP: Saw pod success 01/18/23 17:03:59.615
    Jan 18 17:03:59.615: INFO: Pod "downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796" satisfied condition "Succeeded or Failed"
    Jan 18 17:03:59.621: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:03:59.638
    Jan 18 17:03:59.659: INFO: Waiting for pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 to disappear
    Jan 18 17:03:59.667: INFO: Pod downwardapi-volume-54b12f3b-edb2-4004-9c1a-150f3d3f8796 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:03:59.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4552" for this suite. 01/18/23 17:03:59.675
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:03:59.687
Jan 18 17:03:59.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:03:59.688
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:59.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:59.716
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-3656 01/18/23 17:03:59.721
STEP: creating service affinity-nodeport in namespace services-3656 01/18/23 17:03:59.721
STEP: creating replication controller affinity-nodeport in namespace services-3656 01/18/23 17:03:59.752
I0118 17:03:59.766084      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3656, replica count: 3
I0118 17:04:02.818275      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:04:02.839: INFO: Creating new exec pod
Jan 18 17:04:02.848: INFO: Waiting up to 5m0s for pod "execpod-affinityvtkb2" in namespace "services-3656" to be "running"
Jan 18 17:04:02.853: INFO: Pod "execpod-affinityvtkb2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023573ms
Jan 18 17:04:04.865: INFO: Pod "execpod-affinityvtkb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016436681s
Jan 18 17:04:04.865: INFO: Pod "execpod-affinityvtkb2" satisfied condition "running"
Jan 18 17:04:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 18 17:04:06.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 18 17:04:06.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.145.53 80'
Jan 18 17:04:06.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.145.53 80\nConnection to 10.96.145.53 80 port [tcp/http] succeeded!\n"
Jan 18 17:04:06.262: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:06.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 30757'
Jan 18 17:04:06.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 30757\nConnection to 10.195.74.123 30757 port [tcp/*] succeeded!\n"
Jan 18 17:04:06.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:06.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 30757'
Jan 18 17:04:06.632: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 30757\nConnection to 10.195.78.23 30757 port [tcp/*] succeeded!\n"
Jan 18 17:04:06.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:06.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:30757/ ; done'
Jan 18 17:04:06.933: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n"
Jan 18 17:04:06.933: INFO: stdout: "\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh"
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
Jan 18 17:04:06.933: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-3656, will wait for the garbage collector to delete the pods 01/18/23 17:04:06.953
Jan 18 17:04:07.024: INFO: Deleting ReplicationController affinity-nodeport took: 13.70022ms
Jan 18 17:04:07.124: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.518767ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:04:09.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3656" for this suite. 01/18/23 17:04:09.277
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":29,"skipped":515,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.601 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:03:59.687
    Jan 18 17:03:59.687: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:03:59.688
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:03:59.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:03:59.716
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-3656 01/18/23 17:03:59.721
    STEP: creating service affinity-nodeport in namespace services-3656 01/18/23 17:03:59.721
    STEP: creating replication controller affinity-nodeport in namespace services-3656 01/18/23 17:03:59.752
    I0118 17:03:59.766084      21 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-3656, replica count: 3
    I0118 17:04:02.818275      21 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:04:02.839: INFO: Creating new exec pod
    Jan 18 17:04:02.848: INFO: Waiting up to 5m0s for pod "execpod-affinityvtkb2" in namespace "services-3656" to be "running"
    Jan 18 17:04:02.853: INFO: Pod "execpod-affinityvtkb2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023573ms
    Jan 18 17:04:04.865: INFO: Pod "execpod-affinityvtkb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.016436681s
    Jan 18 17:04:04.865: INFO: Pod "execpod-affinityvtkb2" satisfied condition "running"
    Jan 18 17:04:05.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 18 17:04:06.077: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 18 17:04:06.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:06.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.145.53 80'
    Jan 18 17:04:06.262: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.145.53 80\nConnection to 10.96.145.53 80 port [tcp/http] succeeded!\n"
    Jan 18 17:04:06.262: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:06.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 30757'
    Jan 18 17:04:06.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 30757\nConnection to 10.195.74.123 30757 port [tcp/*] succeeded!\n"
    Jan 18 17:04:06.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:06.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 30757'
    Jan 18 17:04:06.632: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 30757\nConnection to 10.195.78.23 30757 port [tcp/*] succeeded!\n"
    Jan 18 17:04:06.632: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:06.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3656 exec execpod-affinityvtkb2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:30757/ ; done'
    Jan 18 17:04:06.933: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:30757/\n"
    Jan 18 17:04:06.933: INFO: stdout: "\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh\naffinity-nodeport-brzwh"
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Received response from host: affinity-nodeport-brzwh
    Jan 18 17:04:06.933: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-3656, will wait for the garbage collector to delete the pods 01/18/23 17:04:06.953
    Jan 18 17:04:07.024: INFO: Deleting ReplicationController affinity-nodeport took: 13.70022ms
    Jan 18 17:04:07.124: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.518767ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:04:09.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3656" for this suite. 01/18/23 17:04:09.277
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:04:09.292
Jan 18 17:04:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:04:09.293
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:09.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:09.32
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-9349 01/18/23 17:04:09.325
Jan 18 17:04:09.339: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9349" to be "running and ready"
Jan 18 17:04:09.345: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.997638ms
Jan 18 17:04:09.345: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:04:11.354: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014588753s
Jan 18 17:04:11.354: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 17:04:11.354: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 17:04:11.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 17:04:11.584: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 17:04:11.584: INFO: stdout: "iptables"
Jan 18 17:04:11.584: INFO: proxyMode: iptables
Jan 18 17:04:11.605: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 17:04:11.613: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-nodeport-timeout in namespace services-9349 01/18/23 17:04:11.613
STEP: creating replication controller affinity-nodeport-timeout in namespace services-9349 01/18/23 17:04:11.645
I0118 17:04:11.657296      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9349, replica count: 3
I0118 17:04:14.709024      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:04:14.765: INFO: Creating new exec pod
Jan 18 17:04:14.777: INFO: Waiting up to 5m0s for pod "execpod-affinity2wkx8" in namespace "services-9349" to be "running"
Jan 18 17:04:14.786: INFO: Pod "execpod-affinity2wkx8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.785117ms
Jan 18 17:04:16.793: INFO: Pod "execpod-affinity2wkx8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016154011s
Jan 18 17:04:16.793: INFO: Pod "execpod-affinity2wkx8" satisfied condition "running"
Jan 18 17:04:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 18 17:04:18.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 17:04:18.008: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:18.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.99.246 80'
Jan 18 17:04:18.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.99.246 80\nConnection to 10.96.99.246 80 port [tcp/http] succeeded!\n"
Jan 18 17:04:18.208: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:18.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 31904'
Jan 18 17:04:18.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 31904\nConnection to 10.195.74.123 31904 port [tcp/*] succeeded!\n"
Jan 18 17:04:18.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:18.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 31904'
Jan 18 17:04:18.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 31904\nConnection to 10.195.78.23 31904 port [tcp/*] succeeded!\n"
Jan 18 17:04:18.625: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:04:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31904/ ; done'
Jan 18 17:04:18.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
Jan 18 17:04:18.868: INFO: stdout: "\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw"
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
Jan 18 17:04:18.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.195.74.123:31904/'
Jan 18 17:04:19.064: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
Jan 18 17:04:19.064: INFO: stdout: "affinity-nodeport-timeout-qm8tw"
Jan 18 17:04:39.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.195.74.123:31904/'
Jan 18 17:04:39.270: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
Jan 18 17:04:39.270: INFO: stdout: "affinity-nodeport-timeout-vj6ph"
Jan 18 17:04:39.270: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9349, will wait for the garbage collector to delete the pods 01/18/23 17:04:39.293
Jan 18 17:04:39.365: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.552537ms
Jan 18 17:04:39.465: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.478124ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:04:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9349" for this suite. 01/18/23 17:04:42.012
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":30,"skipped":570,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.733 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:04:09.292
    Jan 18 17:04:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:04:09.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:09.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:09.32
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-9349 01/18/23 17:04:09.325
    Jan 18 17:04:09.339: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-9349" to be "running and ready"
    Jan 18 17:04:09.345: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 5.997638ms
    Jan 18 17:04:09.345: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:04:11.354: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.014588753s
    Jan 18 17:04:11.354: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 17:04:11.354: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 17:04:11.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 17:04:11.584: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 17:04:11.584: INFO: stdout: "iptables"
    Jan 18 17:04:11.584: INFO: proxyMode: iptables
    Jan 18 17:04:11.605: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 17:04:11.613: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-nodeport-timeout in namespace services-9349 01/18/23 17:04:11.613
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-9349 01/18/23 17:04:11.645
    I0118 17:04:11.657296      21 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-9349, replica count: 3
    I0118 17:04:14.709024      21 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:04:14.765: INFO: Creating new exec pod
    Jan 18 17:04:14.777: INFO: Waiting up to 5m0s for pod "execpod-affinity2wkx8" in namespace "services-9349" to be "running"
    Jan 18 17:04:14.786: INFO: Pod "execpod-affinity2wkx8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.785117ms
    Jan 18 17:04:16.793: INFO: Pod "execpod-affinity2wkx8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016154011s
    Jan 18 17:04:16.793: INFO: Pod "execpod-affinity2wkx8" satisfied condition "running"
    Jan 18 17:04:17.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 18 17:04:18.008: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 17:04:18.008: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:18.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.99.246 80'
    Jan 18 17:04:18.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.99.246 80\nConnection to 10.96.99.246 80 port [tcp/http] succeeded!\n"
    Jan 18 17:04:18.208: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:18.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 31904'
    Jan 18 17:04:18.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 31904\nConnection to 10.195.74.123 31904 port [tcp/*] succeeded!\n"
    Jan 18 17:04:18.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:18.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 31904'
    Jan 18 17:04:18.625: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 31904\nConnection to 10.195.78.23 31904 port [tcp/*] succeeded!\n"
    Jan 18 17:04:18.625: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:04:18.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.195.74.123:31904/ ; done'
    Jan 18 17:04:18.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
    Jan 18 17:04:18.868: INFO: stdout: "\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw\naffinity-nodeport-timeout-qm8tw"
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Received response from host: affinity-nodeport-timeout-qm8tw
    Jan 18 17:04:18.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.195.74.123:31904/'
    Jan 18 17:04:19.064: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
    Jan 18 17:04:19.064: INFO: stdout: "affinity-nodeport-timeout-qm8tw"
    Jan 18 17:04:39.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9349 exec execpod-affinity2wkx8 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.195.74.123:31904/'
    Jan 18 17:04:39.270: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.195.74.123:31904/\n"
    Jan 18 17:04:39.270: INFO: stdout: "affinity-nodeport-timeout-vj6ph"
    Jan 18 17:04:39.270: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-9349, will wait for the garbage collector to delete the pods 01/18/23 17:04:39.293
    Jan 18 17:04:39.365: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 13.552537ms
    Jan 18 17:04:39.465: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 100.478124ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:04:42.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9349" for this suite. 01/18/23 17:04:42.012
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:04:42.03
Jan 18 17:04:42.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 17:04:42.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:42.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:42.061
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/18/23 17:04:42.065
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local;sleep 1; done
 01/18/23 17:04:42.073
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local;sleep 1; done
 01/18/23 17:04:42.074
STEP: creating a pod to probe DNS 01/18/23 17:04:42.074
STEP: submitting the pod to kubernetes 01/18/23 17:04:42.074
Jan 18 17:04:42.089: INFO: Waiting up to 15m0s for pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49" in namespace "dns-6817" to be "running"
Jan 18 17:04:42.095: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.755184ms
Jan 18 17:04:44.105: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016309016s
Jan 18 17:04:46.104: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015322471s
Jan 18 17:04:48.107: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017991995s
Jan 18 17:04:50.106: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Running", Reason="", readiness=true. Elapsed: 8.017228857s
Jan 18 17:04:50.106: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49" satisfied condition "running"
STEP: retrieving the pod 01/18/23 17:04:50.106
STEP: looking for the results for each expected name from probers 01/18/23 17:04:50.113
Jan 18 17:04:50.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.138: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.147: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.160: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.170: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.179: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.189: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.199: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
Jan 18 17:04:50.199: INFO: Lookups using dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local]

Jan 18 17:04:55.280: INFO: DNS probes using dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49 succeeded

STEP: deleting the pod 01/18/23 17:04:55.28
STEP: deleting the test headless service 01/18/23 17:04:55.299
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 17:04:55.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6817" for this suite. 01/18/23 17:04:55.33
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":31,"skipped":575,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.310 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:04:42.03
    Jan 18 17:04:42.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 17:04:42.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:42.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:42.061
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/18/23 17:04:42.065
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local;sleep 1; done
     01/18/23 17:04:42.073
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6817.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local;sleep 1; done
     01/18/23 17:04:42.074
    STEP: creating a pod to probe DNS 01/18/23 17:04:42.074
    STEP: submitting the pod to kubernetes 01/18/23 17:04:42.074
    Jan 18 17:04:42.089: INFO: Waiting up to 15m0s for pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49" in namespace "dns-6817" to be "running"
    Jan 18 17:04:42.095: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.755184ms
    Jan 18 17:04:44.105: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016309016s
    Jan 18 17:04:46.104: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015322471s
    Jan 18 17:04:48.107: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017991995s
    Jan 18 17:04:50.106: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49": Phase="Running", Reason="", readiness=true. Elapsed: 8.017228857s
    Jan 18 17:04:50.106: INFO: Pod "dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 17:04:50.106
    STEP: looking for the results for each expected name from probers 01/18/23 17:04:50.113
    Jan 18 17:04:50.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.138: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.147: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.160: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.170: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.179: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.189: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.199: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local from pod dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49: the server could not find the requested resource (get pods dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49)
    Jan 18 17:04:50.199: INFO: Lookups using dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local wheezy_udp@dns-test-service-2.dns-6817.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-6817.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6817.svc.cluster.local jessie_udp@dns-test-service-2.dns-6817.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6817.svc.cluster.local]

    Jan 18 17:04:55.280: INFO: DNS probes using dns-6817/dns-test-6686bec3-134e-4d9a-aae6-9c13e5787e49 succeeded

    STEP: deleting the pod 01/18/23 17:04:55.28
    STEP: deleting the test headless service 01/18/23 17:04:55.299
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 17:04:55.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6817" for this suite. 01/18/23 17:04:55.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:04:55.344
Jan 18 17:04:55.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:04:55.345
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:55.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:55.374
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:04:55.401
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:04:55.862
STEP: Deploying the webhook pod 01/18/23 17:04:55.879
STEP: Wait for the deployment to be ready 01/18/23 17:04:55.898
Jan 18 17:04:55.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:04:57.938
STEP: Verifying the service has paired with the endpoint 01/18/23 17:04:57.961
Jan 18 17:04:58.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/18/23 17:04:58.97
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 17:04:58.973
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 17:04:58.973
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 17:04:58.973
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 17:04:58.975
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 17:04:58.975
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 17:04:58.978
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:04:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7450" for this suite. 01/18/23 17:04:58.988
STEP: Destroying namespace "webhook-7450-markers" for this suite. 01/18/23 17:04:59.004
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":32,"skipped":638,"failed":0}
------------------------------
â€¢ [3.742 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:04:55.344
    Jan 18 17:04:55.344: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:04:55.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:55.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:55.374
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:04:55.401
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:04:55.862
    STEP: Deploying the webhook pod 01/18/23 17:04:55.879
    STEP: Wait for the deployment to be ready 01/18/23 17:04:55.898
    Jan 18 17:04:55.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:04:57.938
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:04:57.961
    Jan 18 17:04:58.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/18/23 17:04:58.97
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/18/23 17:04:58.973
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 17:04:58.973
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/18/23 17:04:58.973
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/18/23 17:04:58.975
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 17:04:58.975
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/18/23 17:04:58.978
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:04:58.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7450" for this suite. 01/18/23 17:04:58.988
    STEP: Destroying namespace "webhook-7450-markers" for this suite. 01/18/23 17:04:59.004
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:04:59.087
Jan 18 17:04:59.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:04:59.088
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:59.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:59.117
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-6616 01/18/23 17:04:59.121
STEP: creating a selector 01/18/23 17:04:59.121
STEP: Creating the service pods in kubernetes 01/18/23 17:04:59.121
Jan 18 17:04:59.121: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 17:04:59.170: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6616" to be "running and ready"
Jan 18 17:04:59.179: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.129379ms
Jan 18 17:04:59.179: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:05:01.187: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01704539s
Jan 18 17:05:01.187: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:03.190: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01987769s
Jan 18 17:05:03.190: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:05.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018334168s
Jan 18 17:05:05.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:07.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020973497s
Jan 18 17:05:07.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:09.187: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016911279s
Jan 18 17:05:09.187: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:11.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018486295s
Jan 18 17:05:11.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:13.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020819516s
Jan 18 17:05:13.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:15.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018176426s
Jan 18 17:05:15.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:17.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01953505s
Jan 18 17:05:17.189: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:19.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018642333s
Jan 18 17:05:19.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:05:21.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018845567s
Jan 18 17:05:21.189: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 17:05:21.189: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 17:05:21.196: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6616" to be "running and ready"
Jan 18 17:05:21.202: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.885088ms
Jan 18 17:05:21.202: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 17:05:21.202: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 17:05:21.208
Jan 18 17:05:21.230: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6616" to be "running"
Jan 18 17:05:21.237: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.993555ms
Jan 18 17:05:23.246: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857972s
Jan 18 17:05:23.246: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 17:05:23.252: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6616" to be "running"
Jan 18 17:05:23.258: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.106799ms
Jan 18 17:05:23.258: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 17:05:23.265: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 17:05:23.265: INFO: Going to poll 10.100.145.159 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 17:05:23.271: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.145.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:05:23.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:05:23.272: INFO: ExecWithOptions: Clientset creation
Jan 18 17:05:23.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6616/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.145.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 17:05:24.383: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 17:05:24.383: INFO: Going to poll 10.100.158.26 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Jan 18 17:05:24.389: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.158.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:05:24.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:05:24.390: INFO: ExecWithOptions: Clientset creation
Jan 18 17:05:24.390: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6616/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.158.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 17:05:25.497: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 17:05:25.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6616" for this suite. 01/18/23 17:05:25.507
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":33,"skipped":657,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.434 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:04:59.087
    Jan 18 17:04:59.087: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:04:59.088
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:04:59.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:04:59.117
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-6616 01/18/23 17:04:59.121
    STEP: creating a selector 01/18/23 17:04:59.121
    STEP: Creating the service pods in kubernetes 01/18/23 17:04:59.121
    Jan 18 17:04:59.121: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 17:04:59.170: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6616" to be "running and ready"
    Jan 18 17:04:59.179: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.129379ms
    Jan 18 17:04:59.179: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:05:01.187: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01704539s
    Jan 18 17:05:01.187: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:03.190: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01987769s
    Jan 18 17:05:03.190: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:05.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018334168s
    Jan 18 17:05:05.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:07.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.020973497s
    Jan 18 17:05:07.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:09.187: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016911279s
    Jan 18 17:05:09.187: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:11.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.018486295s
    Jan 18 17:05:11.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:13.191: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.020819516s
    Jan 18 17:05:13.191: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:15.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.018176426s
    Jan 18 17:05:15.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:17.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01953505s
    Jan 18 17:05:17.189: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:19.188: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.018642333s
    Jan 18 17:05:19.188: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:05:21.189: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018845567s
    Jan 18 17:05:21.189: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 17:05:21.189: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 17:05:21.196: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6616" to be "running and ready"
    Jan 18 17:05:21.202: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 5.885088ms
    Jan 18 17:05:21.202: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 17:05:21.202: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 17:05:21.208
    Jan 18 17:05:21.230: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6616" to be "running"
    Jan 18 17:05:21.237: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.993555ms
    Jan 18 17:05:23.246: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015857972s
    Jan 18 17:05:23.246: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 17:05:23.252: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-6616" to be "running"
    Jan 18 17:05:23.258: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.106799ms
    Jan 18 17:05:23.258: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 17:05:23.265: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 17:05:23.265: INFO: Going to poll 10.100.145.159 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 17:05:23.271: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.145.159 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:05:23.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:05:23.272: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:05:23.272: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6616/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.145.159+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 17:05:24.383: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 17:05:24.383: INFO: Going to poll 10.100.158.26 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 17:05:24.389: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.100.158.26 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6616 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:05:24.389: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:05:24.390: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:05:24.390: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6616/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.100.158.26+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 17:05:25.497: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 17:05:25.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6616" for this suite. 01/18/23 17:05:25.507
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:05:25.522
Jan 18 17:05:25.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename aggregator 01/18/23 17:05:25.523
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:25.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:25.553
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 18 17:05:25.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/18/23 17:05:25.558
Jan 18 17:05:25.950: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 18 17:05:28.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:30.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:32.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:34.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:36.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:38.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:40.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:42.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:44.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:46.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:48.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:05:50.195: INFO: Waited 138.484779ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 17:05:50.275
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 17:05:50.286
STEP: List APIServices 01/18/23 17:05:50.296
Jan 18 17:05:50.306: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 18 17:05:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5273" for this suite. 01/18/23 17:05:50.865
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":34,"skipped":659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.359 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:05:25.522
    Jan 18 17:05:25.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename aggregator 01/18/23 17:05:25.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:25.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:25.553
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 18 17:05:25.557: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/18/23 17:05:25.558
    Jan 18 17:05:25.950: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 18 17:05:28.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:30.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:32.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:34.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:36.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:38.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:40.040: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:42.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:44.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:46.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:48.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 5, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:05:50.195: INFO: Waited 138.484779ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/18/23 17:05:50.275
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/18/23 17:05:50.286
    STEP: List APIServices 01/18/23 17:05:50.296
    Jan 18 17:05:50.306: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 18 17:05:50.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-5273" for this suite. 01/18/23 17:05:50.865
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:05:50.882
Jan 18 17:05:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:05:50.883
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:50.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:50.962
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:05:50.966
Jan 18 17:05:50.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 17:05:51.070: INFO: stderr: ""
Jan 18 17:05:51.070: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/18/23 17:05:51.07
Jan 18 17:05:51.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 18 17:05:52.588: INFO: stderr: ""
Jan 18 17:05:52.588: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:05:52.588
Jan 18 17:05:52.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 delete pods e2e-test-httpd-pod'
Jan 18 17:05:55.086: INFO: stderr: ""
Jan 18 17:05:55.086: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:05:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6009" for this suite. 01/18/23 17:05:55.094
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":35,"skipped":662,"failed":0}
------------------------------
â€¢ [4.226 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:05:50.882
    Jan 18 17:05:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:05:50.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:50.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:50.962
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:05:50.966
    Jan 18 17:05:50.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 17:05:51.070: INFO: stderr: ""
    Jan 18 17:05:51.070: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/18/23 17:05:51.07
    Jan 18 17:05:51.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 18 17:05:52.588: INFO: stderr: ""
    Jan 18 17:05:52.588: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:05:52.588
    Jan 18 17:05:52.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6009 delete pods e2e-test-httpd-pod'
    Jan 18 17:05:55.086: INFO: stderr: ""
    Jan 18 17:05:55.086: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:05:55.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6009" for this suite. 01/18/23 17:05:55.094
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:05:55.108
Jan 18 17:05:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 17:05:55.11
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:55.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:55.134
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 18 17:05:55.139: INFO: Creating ReplicaSet my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f
Jan 18 17:05:55.165: INFO: Pod name my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Found 0 pods out of 1
Jan 18 17:06:00.176: INFO: Pod name my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Found 1 pods out of 1
Jan 18 17:06:00.176: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f" is running
Jan 18 17:06:00.176: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" in namespace "replicaset-5774" to be "running"
Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb": Phase="Running", Reason="", readiness=true. Elapsed: 7.282266ms
Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" satisfied condition "running"
Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:55 +0000 UTC Reason: Message:}])
Jan 18 17:06:00.183: INFO: Trying to dial the pod
Jan 18 17:06:05.214: INFO: Controller my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Got expected result from replica 1 [my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb]: "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 17:06:05.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5774" for this suite. 01/18/23 17:06:05.224
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":36,"skipped":665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.128 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:05:55.108
    Jan 18 17:05:55.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 17:05:55.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:05:55.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:05:55.134
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 18 17:05:55.139: INFO: Creating ReplicaSet my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f
    Jan 18 17:05:55.165: INFO: Pod name my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Found 0 pods out of 1
    Jan 18 17:06:00.176: INFO: Pod name my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Found 1 pods out of 1
    Jan 18 17:06:00.176: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f" is running
    Jan 18 17:06:00.176: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" in namespace "replicaset-5774" to be "running"
    Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb": Phase="Running", Reason="", readiness=true. Elapsed: 7.282266ms
    Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" satisfied condition "running"
    Jan 18 17:06:00.183: INFO: Pod "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:05:55 +0000 UTC Reason: Message:}])
    Jan 18 17:06:00.183: INFO: Trying to dial the pod
    Jan 18 17:06:05.214: INFO: Controller my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f: Got expected result from replica 1 [my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb]: "my-hostname-basic-a9eca1c9-9c54-4e4f-8bb6-a01b6ca9123f-dv6pb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 17:06:05.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5774" for this suite. 01/18/23 17:06:05.224
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:05.237
Jan 18 17:06:05.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:06:05.239
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:05.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:05.271
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:06:05.276
Jan 18 17:06:05.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c" in namespace "downward-api-7709" to be "Succeeded or Failed"
Jan 18 17:06:05.298: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.451911ms
Jan 18 17:06:07.305: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01327483s
Jan 18 17:06:09.307: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01523254s
STEP: Saw pod success 01/18/23 17:06:09.307
Jan 18 17:06:09.307: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c" satisfied condition "Succeeded or Failed"
Jan 18 17:06:09.313: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c container client-container: <nil>
STEP: delete the pod 01/18/23 17:06:09.344
Jan 18 17:06:09.375: INFO: Waiting for pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c to disappear
Jan 18 17:06:09.380: INFO: Pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 17:06:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7709" for this suite. 01/18/23 17:06:09.389
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":37,"skipped":666,"failed":0}
------------------------------
â€¢ [4.167 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:05.237
    Jan 18 17:06:05.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:06:05.239
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:05.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:05.271
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:06:05.276
    Jan 18 17:06:05.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c" in namespace "downward-api-7709" to be "Succeeded or Failed"
    Jan 18 17:06:05.298: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.451911ms
    Jan 18 17:06:07.305: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01327483s
    Jan 18 17:06:09.307: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01523254s
    STEP: Saw pod success 01/18/23 17:06:09.307
    Jan 18 17:06:09.307: INFO: Pod "downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c" satisfied condition "Succeeded or Failed"
    Jan 18 17:06:09.313: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c container client-container: <nil>
    STEP: delete the pod 01/18/23 17:06:09.344
    Jan 18 17:06:09.375: INFO: Waiting for pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c to disappear
    Jan 18 17:06:09.380: INFO: Pod downwardapi-volume-c964722b-4e8d-438d-bb49-b03f1da7816c no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 17:06:09.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7709" for this suite. 01/18/23 17:06:09.389
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:09.404
Jan 18 17:06:09.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:06:09.405
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:09.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:09.433
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-3c81fcfa-56a8-40c5-b61e-f8acfd800245 01/18/23 17:06:09.437
STEP: Creating a pod to test consume secrets 01/18/23 17:06:09.447
Jan 18 17:06:09.463: INFO: Waiting up to 5m0s for pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae" in namespace "secrets-4908" to be "Succeeded or Failed"
Jan 18 17:06:09.469: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281811ms
Jan 18 17:06:11.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016425074s
Jan 18 17:06:13.478: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014467973s
Jan 18 17:06:15.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016404678s
STEP: Saw pod success 01/18/23 17:06:15.48
Jan 18 17:06:15.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae" satisfied condition "Succeeded or Failed"
Jan 18 17:06:15.488: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae container secret-env-test: <nil>
STEP: delete the pod 01/18/23 17:06:15.503
Jan 18 17:06:15.524: INFO: Waiting for pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae to disappear
Jan 18 17:06:15.531: INFO: Pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:06:15.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4908" for this suite. 01/18/23 17:06:15.541
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":38,"skipped":679,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.150 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:09.404
    Jan 18 17:06:09.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:06:09.405
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:09.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:09.433
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-3c81fcfa-56a8-40c5-b61e-f8acfd800245 01/18/23 17:06:09.437
    STEP: Creating a pod to test consume secrets 01/18/23 17:06:09.447
    Jan 18 17:06:09.463: INFO: Waiting up to 5m0s for pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae" in namespace "secrets-4908" to be "Succeeded or Failed"
    Jan 18 17:06:09.469: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281811ms
    Jan 18 17:06:11.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016425074s
    Jan 18 17:06:13.478: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014467973s
    Jan 18 17:06:15.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016404678s
    STEP: Saw pod success 01/18/23 17:06:15.48
    Jan 18 17:06:15.480: INFO: Pod "pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae" satisfied condition "Succeeded or Failed"
    Jan 18 17:06:15.488: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae container secret-env-test: <nil>
    STEP: delete the pod 01/18/23 17:06:15.503
    Jan 18 17:06:15.524: INFO: Waiting for pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae to disappear
    Jan 18 17:06:15.531: INFO: Pod pod-secrets-e952d806-8c32-4d50-9bf9-f351bbc6a1ae no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:06:15.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4908" for this suite. 01/18/23 17:06:15.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:15.555
Jan 18 17:06:15.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:06:15.556
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:15.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:15.586
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-f64aa35b-9522-4625-8648-090d00f1a968 01/18/23 17:06:15.59
STEP: Creating a pod to test consume secrets 01/18/23 17:06:15.599
Jan 18 17:06:15.615: INFO: Waiting up to 5m0s for pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702" in namespace "secrets-1782" to be "Succeeded or Failed"
Jan 18 17:06:15.625: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34677ms
Jan 18 17:06:17.633: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017473681s
Jan 18 17:06:19.635: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019147843s
STEP: Saw pod success 01/18/23 17:06:19.635
Jan 18 17:06:19.635: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702" satisfied condition "Succeeded or Failed"
Jan 18 17:06:19.643: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:06:19.658
Jan 18 17:06:19.679: INFO: Waiting for pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 to disappear
Jan 18 17:06:19.685: INFO: Pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:06:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1782" for this suite. 01/18/23 17:06:19.698
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":39,"skipped":689,"failed":0}
------------------------------
â€¢ [4.156 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:15.555
    Jan 18 17:06:15.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:06:15.556
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:15.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:15.586
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-f64aa35b-9522-4625-8648-090d00f1a968 01/18/23 17:06:15.59
    STEP: Creating a pod to test consume secrets 01/18/23 17:06:15.599
    Jan 18 17:06:15.615: INFO: Waiting up to 5m0s for pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702" in namespace "secrets-1782" to be "Succeeded or Failed"
    Jan 18 17:06:15.625: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Pending", Reason="", readiness=false. Elapsed: 9.34677ms
    Jan 18 17:06:17.633: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017473681s
    Jan 18 17:06:19.635: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019147843s
    STEP: Saw pod success 01/18/23 17:06:19.635
    Jan 18 17:06:19.635: INFO: Pod "pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702" satisfied condition "Succeeded or Failed"
    Jan 18 17:06:19.643: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:06:19.658
    Jan 18 17:06:19.679: INFO: Waiting for pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 to disappear
    Jan 18 17:06:19.685: INFO: Pod pod-secrets-89ae7d1f-b4d6-4a7c-b25e-da448f6f3702 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:06:19.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1782" for this suite. 01/18/23 17:06:19.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:19.712
Jan 18 17:06:19.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename podtemplate 01/18/23 17:06:19.713
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:19.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:19.744
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/18/23 17:06:19.749
Jan 18 17:06:19.758: INFO: created test-podtemplate-1
Jan 18 17:06:19.768: INFO: created test-podtemplate-2
Jan 18 17:06:19.778: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/18/23 17:06:19.778
STEP: delete collection of pod templates 01/18/23 17:06:19.785
Jan 18 17:06:19.785: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/18/23 17:06:19.823
Jan 18 17:06:19.824: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 17:06:19.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7920" for this suite. 01/18/23 17:06:19.84
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":40,"skipped":694,"failed":0}
------------------------------
â€¢ [0.140 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:19.712
    Jan 18 17:06:19.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 17:06:19.713
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:19.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:19.744
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/18/23 17:06:19.749
    Jan 18 17:06:19.758: INFO: created test-podtemplate-1
    Jan 18 17:06:19.768: INFO: created test-podtemplate-2
    Jan 18 17:06:19.778: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/18/23 17:06:19.778
    STEP: delete collection of pod templates 01/18/23 17:06:19.785
    Jan 18 17:06:19.785: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/18/23 17:06:19.823
    Jan 18 17:06:19.824: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 17:06:19.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7920" for this suite. 01/18/23 17:06:19.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:19.859
Jan 18 17:06:19.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename events 01/18/23 17:06:19.861
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:19.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:19.891
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/18/23 17:06:19.896
STEP: get a list of Events with a label in the current namespace 01/18/23 17:06:19.925
STEP: delete a list of events 01/18/23 17:06:19.93
Jan 18 17:06:19.930: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 17:06:19.963
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 17:06:19.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-605" for this suite. 01/18/23 17:06:19.975
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":41,"skipped":735,"failed":0}
------------------------------
â€¢ [0.131 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:19.859
    Jan 18 17:06:19.860: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename events 01/18/23 17:06:19.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:19.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:19.891
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/18/23 17:06:19.896
    STEP: get a list of Events with a label in the current namespace 01/18/23 17:06:19.925
    STEP: delete a list of events 01/18/23 17:06:19.93
    Jan 18 17:06:19.930: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 17:06:19.963
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 17:06:19.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-605" for this suite. 01/18/23 17:06:19.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:19.992
Jan 18 17:06:19.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:06:19.993
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:20.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:20.025
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-f04e7520-0cbb-4122-bd20-c0b464d9a98f 01/18/23 17:06:20.031
STEP: Creating a pod to test consume secrets 01/18/23 17:06:20.044
Jan 18 17:06:20.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b" in namespace "projected-219" to be "Succeeded or Failed"
Jan 18 17:06:20.069: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.317827ms
Jan 18 17:06:22.079: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020545109s
Jan 18 17:06:24.078: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019076107s
STEP: Saw pod success 01/18/23 17:06:24.078
Jan 18 17:06:24.078: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b" satisfied condition "Succeeded or Failed"
Jan 18 17:06:24.087: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:06:24.105
Jan 18 17:06:24.127: INFO: Waiting for pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b to disappear
Jan 18 17:06:24.132: INFO: Pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 17:06:24.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-219" for this suite. 01/18/23 17:06:24.141
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":42,"skipped":758,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:19.992
    Jan 18 17:06:19.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:06:19.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:20.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:20.025
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-f04e7520-0cbb-4122-bd20-c0b464d9a98f 01/18/23 17:06:20.031
    STEP: Creating a pod to test consume secrets 01/18/23 17:06:20.044
    Jan 18 17:06:20.059: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b" in namespace "projected-219" to be "Succeeded or Failed"
    Jan 18 17:06:20.069: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.317827ms
    Jan 18 17:06:22.079: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020545109s
    Jan 18 17:06:24.078: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019076107s
    STEP: Saw pod success 01/18/23 17:06:24.078
    Jan 18 17:06:24.078: INFO: Pod "pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b" satisfied condition "Succeeded or Failed"
    Jan 18 17:06:24.087: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:06:24.105
    Jan 18 17:06:24.127: INFO: Waiting for pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b to disappear
    Jan 18 17:06:24.132: INFO: Pod pod-projected-secrets-8d958b67-6900-4a03-9900-f2fd2027ba8b no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 17:06:24.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-219" for this suite. 01/18/23 17:06:24.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:24.157
Jan 18 17:06:24.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename podtemplate 01/18/23 17:06:24.158
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:24.183
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:24.188
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/18/23 17:06:24.193
STEP: Replace a pod template 01/18/23 17:06:24.203
Jan 18 17:06:24.219: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 17:06:24.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9663" for this suite. 01/18/23 17:06:24.226
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":43,"skipped":763,"failed":0}
------------------------------
â€¢ [0.082 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:24.157
    Jan 18 17:06:24.157: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 17:06:24.158
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:24.183
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:24.188
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/18/23 17:06:24.193
    STEP: Replace a pod template 01/18/23 17:06:24.203
    Jan 18 17:06:24.219: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 17:06:24.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9663" for this suite. 01/18/23 17:06:24.226
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:06:24.241
Jan 18 17:06:24.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename taint-single-pod 01/18/23 17:06:24.242
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:24.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:24.272
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 18 17:06:24.277: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 17:07:24.304: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 18 17:07:24.312: INFO: Starting informer...
STEP: Starting pod... 01/18/23 17:07:24.312
Jan 18 17:07:24.539: INFO: Pod is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
STEP: Trying to apply a taint on the Node 01/18/23 17:07:24.539
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:07:24.567
STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 17:07:24.575
Jan 18 17:07:24.575: INFO: Pod wasn't evicted. Proceeding
Jan 18 17:07:24.575: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:07:24.601
STEP: Waiting some time to make sure that toleration time passed. 01/18/23 17:07:24.607
Jan 18 17:08:39.607: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:08:39.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5742" for this suite. 01/18/23 17:08:39.617
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":44,"skipped":778,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.391 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:06:24.241
    Jan 18 17:06:24.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename taint-single-pod 01/18/23 17:06:24.242
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:06:24.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:06:24.272
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 18 17:06:24.277: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 17:07:24.304: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 18 17:07:24.312: INFO: Starting informer...
    STEP: Starting pod... 01/18/23 17:07:24.312
    Jan 18 17:07:24.539: INFO: Pod is running on scw-conformance125-default-61c39bbf4d81476a8e3. Tainting Node
    STEP: Trying to apply a taint on the Node 01/18/23 17:07:24.539
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:07:24.567
    STEP: Waiting short time to make sure Pod is queued for deletion 01/18/23 17:07:24.575
    Jan 18 17:07:24.575: INFO: Pod wasn't evicted. Proceeding
    Jan 18 17:07:24.575: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/18/23 17:07:24.601
    STEP: Waiting some time to make sure that toleration time passed. 01/18/23 17:07:24.607
    Jan 18 17:08:39.607: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:08:39.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-5742" for this suite. 01/18/23 17:08:39.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:08:39.632
Jan 18 17:08:39.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename hostport 01/18/23 17:08:39.634
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:39.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:39.664
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 17:08:39.677
Jan 18 17:08:39.691: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8988" to be "running and ready"
Jan 18 17:08:39.698: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717582ms
Jan 18 17:08:39.698: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:08:41.706: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015059999s
Jan 18 17:08:41.706: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 17:08:41.706: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.195.74.123 on the node which pod1 resides and expect scheduled 01/18/23 17:08:41.706
Jan 18 17:08:41.716: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8988" to be "running and ready"
Jan 18 17:08:41.723: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007968ms
Jan 18 17:08:41.723: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:08:43.731: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014905475s
Jan 18 17:08:43.731: INFO: The phase of Pod pod2 is Running (Ready = false)
Jan 18 17:08:45.731: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014137982s
Jan 18 17:08:45.731: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 17:08:45.731: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.195.74.123 but use UDP protocol on the node which pod2 resides 01/18/23 17:08:45.731
Jan 18 17:08:45.743: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8988" to be "running and ready"
Jan 18 17:08:45.750: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.291896ms
Jan 18 17:08:45.750: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:08:47.762: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018689876s
Jan 18 17:08:47.762: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 18 17:08:47.762: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 18 17:08:47.774: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8988" to be "running and ready"
Jan 18 17:08:47.782: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.99815ms
Jan 18 17:08:47.782: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:08:49.789: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015355441s
Jan 18 17:08:49.789: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 18 17:08:49.789: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 17:08:49.797
Jan 18 17:08:49.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.195.74.123 http://127.0.0.1:54323/hostname] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:08:49.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:08:49.798: INFO: ExecWithOptions: Clientset creation
Jan 18 17:08:49.798: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.195.74.123+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.195.74.123, port: 54323 01/18/23 17:08:49.906
Jan 18 17:08:49.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.195.74.123:54323/hostname] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:08:49.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:08:49.907: INFO: ExecWithOptions: Clientset creation
Jan 18 17:08:49.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.195.74.123%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.195.74.123, port: 54323 UDP 01/18/23 17:08:50.006
Jan 18 17:08:50.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.195.74.123 54323] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:08:50.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:08:50.007: INFO: ExecWithOptions: Clientset creation
Jan 18 17:08:50.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.195.74.123+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 18 17:08:55.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-8988" for this suite. 01/18/23 17:08:55.113
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":45,"skipped":790,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.498 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:08:39.632
    Jan 18 17:08:39.633: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename hostport 01/18/23 17:08:39.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:39.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:39.664
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/18/23 17:08:39.677
    Jan 18 17:08:39.691: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8988" to be "running and ready"
    Jan 18 17:08:39.698: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.717582ms
    Jan 18 17:08:39.698: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:08:41.706: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.015059999s
    Jan 18 17:08:41.706: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 17:08:41.706: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.195.74.123 on the node which pod1 resides and expect scheduled 01/18/23 17:08:41.706
    Jan 18 17:08:41.716: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8988" to be "running and ready"
    Jan 18 17:08:41.723: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007968ms
    Jan 18 17:08:41.723: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:08:43.731: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.014905475s
    Jan 18 17:08:43.731: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jan 18 17:08:45.731: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.014137982s
    Jan 18 17:08:45.731: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 17:08:45.731: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.195.74.123 but use UDP protocol on the node which pod2 resides 01/18/23 17:08:45.731
    Jan 18 17:08:45.743: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8988" to be "running and ready"
    Jan 18 17:08:45.750: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.291896ms
    Jan 18 17:08:45.750: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:08:47.762: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018689876s
    Jan 18 17:08:47.762: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 18 17:08:47.762: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 18 17:08:47.774: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8988" to be "running and ready"
    Jan 18 17:08:47.782: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.99815ms
    Jan 18 17:08:47.782: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:08:49.789: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.015355441s
    Jan 18 17:08:49.789: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 18 17:08:49.789: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/18/23 17:08:49.797
    Jan 18 17:08:49.797: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.195.74.123 http://127.0.0.1:54323/hostname] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:08:49.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:08:49.798: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:08:49.798: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.195.74.123+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.195.74.123, port: 54323 01/18/23 17:08:49.906
    Jan 18 17:08:49.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.195.74.123:54323/hostname] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:08:49.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:08:49.907: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:08:49.907: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.195.74.123%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.195.74.123, port: 54323 UDP 01/18/23 17:08:50.006
    Jan 18 17:08:50.006: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.195.74.123 54323] Namespace:hostport-8988 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:08:50.006: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:08:50.007: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:08:50.007: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8988/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.195.74.123+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 18 17:08:55.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-8988" for this suite. 01/18/23 17:08:55.113
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:08:55.13
Jan 18 17:08:55.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:08:55.132
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:55.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:55.163
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b5ac1371-ba3c-4999-9ecd-4f2756f44383 01/18/23 17:08:55.174
STEP: Creating the pod 01/18/23 17:08:55.184
Jan 18 17:08:55.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a" in namespace "projected-2752" to be "running and ready"
Jan 18 17:08:55.208: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643432ms
Jan 18 17:08:55.208: INFO: The phase of Pod pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:08:57.217: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015218715s
Jan 18 17:08:57.217: INFO: The phase of Pod pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a is Running (Ready = true)
Jan 18 17:08:57.217: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-b5ac1371-ba3c-4999-9ecd-4f2756f44383 01/18/23 17:08:57.258
STEP: waiting to observe update in volume 01/18/23 17:08:57.267
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:08:59.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2752" for this suite. 01/18/23 17:08:59.307
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":46,"skipped":792,"failed":0}
------------------------------
â€¢ [4.192 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:08:55.13
    Jan 18 17:08:55.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:08:55.132
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:55.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:55.163
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-b5ac1371-ba3c-4999-9ecd-4f2756f44383 01/18/23 17:08:55.174
    STEP: Creating the pod 01/18/23 17:08:55.184
    Jan 18 17:08:55.201: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a" in namespace "projected-2752" to be "running and ready"
    Jan 18 17:08:55.208: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.643432ms
    Jan 18 17:08:55.208: INFO: The phase of Pod pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:08:57.217: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a": Phase="Running", Reason="", readiness=true. Elapsed: 2.015218715s
    Jan 18 17:08:57.217: INFO: The phase of Pod pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a is Running (Ready = true)
    Jan 18 17:08:57.217: INFO: Pod "pod-projected-configmaps-16533c03-f122-4107-b2fb-55cb808a416a" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-b5ac1371-ba3c-4999-9ecd-4f2756f44383 01/18/23 17:08:57.258
    STEP: waiting to observe update in volume 01/18/23 17:08:57.267
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:08:59.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2752" for this suite. 01/18/23 17:08:59.307
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:08:59.324
Jan 18 17:08:59.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:08:59.325
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:59.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:59.35
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 17:08:59.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9187" for this suite. 01/18/23 17:08:59.401
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":47,"skipped":816,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:08:59.324
    Jan 18 17:08:59.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:08:59.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:59.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:59.35
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 17:08:59.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9187" for this suite. 01/18/23 17:08:59.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:08:59.417
Jan 18 17:08:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 17:08:59.418
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:59.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:59.446
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/18/23 17:08:59.451
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 17:08:59.463
STEP: delete the deployment 01/18/23 17:08:59.987
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 17:09:00.001
STEP: Gathering metrics 01/18/23 17:09:00.546
W0118 17:09:00.556628      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 17:09:00.556: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 17:09:00.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7158" for this suite. 01/18/23 17:09:00.563
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":48,"skipped":828,"failed":0}
------------------------------
â€¢ [1.158 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:08:59.417
    Jan 18 17:08:59.417: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 17:08:59.418
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:08:59.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:08:59.446
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/18/23 17:08:59.451
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 17:08:59.463
    STEP: delete the deployment 01/18/23 17:08:59.987
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/18/23 17:09:00.001
    STEP: Gathering metrics 01/18/23 17:09:00.546
    W0118 17:09:00.556628      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 17:09:00.556: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 17:09:00.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7158" for this suite. 01/18/23 17:09:00.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:00.576
Jan 18 17:09:00.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 17:09:00.577
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:00.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:00.605
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/18/23 17:09:00.617
Jan 18 17:09:00.617: INFO: Creating simple deployment test-deployment-pbjrr
Jan 18 17:09:00.638: INFO: deployment "test-deployment-pbjrr" doesn't have the required revision set
Jan 18 17:09:02.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 17:09:04.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/18/23 17:09:06.676
Jan 18 17:09:06.686: INFO: Deployment test-deployment-pbjrr has Conditions: [{Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/18/23 17:09:06.686
Jan 18 17:09:06.705: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/18/23 17:09:06.705
Jan 18 17:09:06.708: INFO: Observed &Deployment event: ADDED
Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
Jan 18 17:09:06.708: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 17:09:06.708: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pbjrr-777898ffcc" is progressing.}
Jan 18 17:09:06.709: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
Jan 18 17:09:06.709: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
Jan 18 17:09:06.709: INFO: Found Deployment test-deployment-pbjrr in namespace deployment-9791 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 17:09:06.709: INFO: Deployment test-deployment-pbjrr has an updated status
STEP: patching the Statefulset Status 01/18/23 17:09:06.709
Jan 18 17:09:06.709: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 17:09:06.720: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/18/23 17:09:06.721
Jan 18 17:09:06.724: INFO: Observed &Deployment event: ADDED
Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
Jan 18 17:09:06.724: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pbjrr-777898ffcc" is progressing.}
Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
Jan 18 17:09:06.725: INFO: Found deployment test-deployment-pbjrr in namespace deployment-9791 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 17:09:06.725: INFO: Deployment test-deployment-pbjrr has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 17:09:06.734: INFO: Deployment "test-deployment-pbjrr":
&Deployment{ObjectMeta:{test-deployment-pbjrr  deployment-9791  42af3b75-9d24-4ec5-be43-565edc0cf259 2631815079 1 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 17:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:09:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039214b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pbjrr-777898ffcc",LastUpdateTime:2023-01-18 17:09:06 +0000 UTC,LastTransitionTime:2023-01-18 17:09:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 17:09:06.740: INFO: New ReplicaSet "test-deployment-pbjrr-777898ffcc" of Deployment "test-deployment-pbjrr":
&ReplicaSet{ObjectMeta:{test-deployment-pbjrr-777898ffcc  deployment-9791  0daa270d-58db-4100-ac7f-1ea483d40e2d 2631814993 1 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pbjrr 42af3b75-9d24-4ec5-be43-565edc0cf259 0xc003921897 0xc003921898}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42af3b75-9d24-4ec5-be43-565edc0cf259\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:09:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003921948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 17:09:06.747: INFO: Pod "test-deployment-pbjrr-777898ffcc-kmcvk" is available:
&Pod{ObjectMeta:{test-deployment-pbjrr-777898ffcc-kmcvk test-deployment-pbjrr-777898ffcc- deployment-9791  5761c6ef-abb6-4359-924e-be406a462a95 2631814992 0 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:741ebac1ae822d6fe3e2e5cdc96167b1350cca06005313d36b3e0dcd0c64c9fa cni.projectcalico.org/podIP:10.100.145.174/32 cni.projectcalico.org/podIPs:10.100.145.174/32] [{apps/v1 ReplicaSet test-deployment-pbjrr-777898ffcc 0daa270d-58db-4100-ac7f-1ea483d40e2d 0xc003921d07 0xc003921d08}] [] [{kube-controller-manager Update v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0daa270d-58db-4100-ac7f-1ea483d40e2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsns2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsns2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.174,StartTime:2023-01-18 17:09:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:09:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ed2698a387f21b93a2b63023c55e7b7bef847c152c41a4bb044734f7e87795f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 17:09:06.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9791" for this suite. 01/18/23 17:09:06.756
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":49,"skipped":837,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.193 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:00.576
    Jan 18 17:09:00.576: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 17:09:00.577
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:00.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:00.605
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/18/23 17:09:00.617
    Jan 18 17:09:00.617: INFO: Creating simple deployment test-deployment-pbjrr
    Jan 18 17:09:00.638: INFO: deployment "test-deployment-pbjrr" doesn't have the required revision set
    Jan 18 17:09:02.661: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 17:09:04.670: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/18/23 17:09:06.676
    Jan 18 17:09:06.686: INFO: Deployment test-deployment-pbjrr has Conditions: [{Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/18/23 17:09:06.686
    Jan 18 17:09:06.705: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 9, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 9, 0, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-pbjrr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/18/23 17:09:06.705
    Jan 18 17:09:06.708: INFO: Observed &Deployment event: ADDED
    Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
    Jan 18 17:09:06.708: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
    Jan 18 17:09:06.708: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 17:09:06.708: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pbjrr-777898ffcc" is progressing.}
    Jan 18 17:09:06.709: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
    Jan 18 17:09:06.709: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 17:09:06.709: INFO: Observed Deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
    Jan 18 17:09:06.709: INFO: Found Deployment test-deployment-pbjrr in namespace deployment-9791 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 17:09:06.709: INFO: Deployment test-deployment-pbjrr has an updated status
    STEP: patching the Statefulset Status 01/18/23 17:09:06.709
    Jan 18 17:09:06.709: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 17:09:06.720: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/18/23 17:09:06.721
    Jan 18 17:09:06.724: INFO: Observed &Deployment event: ADDED
    Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
    Jan 18 17:09:06.724: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-pbjrr-777898ffcc"}
    Jan 18 17:09:06.724: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:00 +0000 UTC 2023-01-18 17:09:00 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-pbjrr-777898ffcc" is progressing.}
    Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
    Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-18 17:09:05 +0000 UTC 2023-01-18 17:09:00 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-pbjrr-777898ffcc" has successfully progressed.}
    Jan 18 17:09:06.725: INFO: Observed deployment test-deployment-pbjrr in namespace deployment-9791 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 17:09:06.725: INFO: Observed &Deployment event: MODIFIED
    Jan 18 17:09:06.725: INFO: Found deployment test-deployment-pbjrr in namespace deployment-9791 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 17:09:06.725: INFO: Deployment test-deployment-pbjrr has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 17:09:06.734: INFO: Deployment "test-deployment-pbjrr":
    &Deployment{ObjectMeta:{test-deployment-pbjrr  deployment-9791  42af3b75-9d24-4ec5-be43-565edc0cf259 2631815079 1 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-18 17:09:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:09:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039214b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-pbjrr-777898ffcc",LastUpdateTime:2023-01-18 17:09:06 +0000 UTC,LastTransitionTime:2023-01-18 17:09:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 17:09:06.740: INFO: New ReplicaSet "test-deployment-pbjrr-777898ffcc" of Deployment "test-deployment-pbjrr":
    &ReplicaSet{ObjectMeta:{test-deployment-pbjrr-777898ffcc  deployment-9791  0daa270d-58db-4100-ac7f-1ea483d40e2d 2631814993 1 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-pbjrr 42af3b75-9d24-4ec5-be43-565edc0cf259 0xc003921897 0xc003921898}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"42af3b75-9d24-4ec5-be43-565edc0cf259\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:09:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003921948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 17:09:06.747: INFO: Pod "test-deployment-pbjrr-777898ffcc-kmcvk" is available:
    &Pod{ObjectMeta:{test-deployment-pbjrr-777898ffcc-kmcvk test-deployment-pbjrr-777898ffcc- deployment-9791  5761c6ef-abb6-4359-924e-be406a462a95 2631814992 0 2023-01-18 17:09:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:741ebac1ae822d6fe3e2e5cdc96167b1350cca06005313d36b3e0dcd0c64c9fa cni.projectcalico.org/podIP:10.100.145.174/32 cni.projectcalico.org/podIPs:10.100.145.174/32] [{apps/v1 ReplicaSet test-deployment-pbjrr-777898ffcc 0daa270d-58db-4100-ac7f-1ea483d40e2d 0xc003921d07 0xc003921d08}] [] [{kube-controller-manager Update v1 2023-01-18 17:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0daa270d-58db-4100-ac7f-1ea483d40e2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:09:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:09:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsns2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsns2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:09:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.174,StartTime:2023-01-18 17:09:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:09:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ed2698a387f21b93a2b63023c55e7b7bef847c152c41a4bb044734f7e87795f6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 17:09:06.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9791" for this suite. 01/18/23 17:09:06.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:06.772
Jan 18 17:09:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:09:06.773
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:06.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:06.799
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 18 17:09:06.817: INFO: Waiting up to 5m0s for pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b" in namespace "pods-9981" to be "running and ready"
Jan 18 17:09:06.825: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.457753ms
Jan 18 17:09:06.825: INFO: The phase of Pod server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:09:08.833: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016312573s
Jan 18 17:09:08.833: INFO: The phase of Pod server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b is Running (Ready = true)
Jan 18 17:09:08.833: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b" satisfied condition "running and ready"
Jan 18 17:09:08.874: INFO: Waiting up to 5m0s for pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383" in namespace "pods-9981" to be "Succeeded or Failed"
Jan 18 17:09:08.881: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Pending", Reason="", readiness=false. Elapsed: 7.165565ms
Jan 18 17:09:10.891: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017426599s
Jan 18 17:09:12.889: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01469426s
STEP: Saw pod success 01/18/23 17:09:12.889
Jan 18 17:09:12.889: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383" satisfied condition "Succeeded or Failed"
Jan 18 17:09:12.895: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 container env3cont: <nil>
STEP: delete the pod 01/18/23 17:09:12.909
Jan 18 17:09:12.929: INFO: Waiting for pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 to disappear
Jan 18 17:09:12.936: INFO: Pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:09:12.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9981" for this suite. 01/18/23 17:09:12.944
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":50,"skipped":884,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.185 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:06.772
    Jan 18 17:09:06.772: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:09:06.773
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:06.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:06.799
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 18 17:09:06.817: INFO: Waiting up to 5m0s for pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b" in namespace "pods-9981" to be "running and ready"
    Jan 18 17:09:06.825: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.457753ms
    Jan 18 17:09:06.825: INFO: The phase of Pod server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:09:08.833: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016312573s
    Jan 18 17:09:08.833: INFO: The phase of Pod server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b is Running (Ready = true)
    Jan 18 17:09:08.833: INFO: Pod "server-envvars-379e4d08-79bb-45cf-990f-75c8c891137b" satisfied condition "running and ready"
    Jan 18 17:09:08.874: INFO: Waiting up to 5m0s for pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383" in namespace "pods-9981" to be "Succeeded or Failed"
    Jan 18 17:09:08.881: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Pending", Reason="", readiness=false. Elapsed: 7.165565ms
    Jan 18 17:09:10.891: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017426599s
    Jan 18 17:09:12.889: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01469426s
    STEP: Saw pod success 01/18/23 17:09:12.889
    Jan 18 17:09:12.889: INFO: Pod "client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383" satisfied condition "Succeeded or Failed"
    Jan 18 17:09:12.895: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 container env3cont: <nil>
    STEP: delete the pod 01/18/23 17:09:12.909
    Jan 18 17:09:12.929: INFO: Waiting for pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 to disappear
    Jan 18 17:09:12.936: INFO: Pod client-envvars-d562cebb-04ca-469b-ada6-a86898b3f383 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:09:12.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9981" for this suite. 01/18/23 17:09:12.944
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:12.958
Jan 18 17:09:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:09:12.96
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:12.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:12.987
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/18/23 17:09:12.992
Jan 18 17:09:12.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 18 17:09:13.097: INFO: stderr: ""
Jan 18 17:09:13.097: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/18/23 17:09:13.097
Jan 18 17:09:13.097: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 18 17:09:13.097: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2155" to be "running and ready, or succeeded"
Jan 18 17:09:13.102: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207373ms
Jan 18 17:09:13.102: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'scw-conformance125-default-61c39bbf4d81476a8e3' to be 'Running' but was 'Pending'
Jan 18 17:09:15.112: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014832318s
Jan 18 17:09:15.112: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 18 17:09:15.112: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/18/23 17:09:15.112
Jan 18 17:09:15.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator'
Jan 18 17:09:15.223: INFO: stderr: ""
Jan 18 17:09:15.223: INFO: stdout: "I0118 17:09:14.010553       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dxsp 308\nI0118 17:09:14.210736       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/45hp 504\nI0118 17:09:14.411384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/mt7w 279\nI0118 17:09:14.611231       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/s2qn 373\nI0118 17:09:14.811565       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/47nb 434\nI0118 17:09:15.010925       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/kmg5 328\nI0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
STEP: limiting log lines 01/18/23 17:09:15.223
Jan 18 17:09:15.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --tail=1'
Jan 18 17:09:15.332: INFO: stderr: ""
Jan 18 17:09:15.333: INFO: stdout: "I0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
Jan 18 17:09:15.333: INFO: got output "I0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
STEP: limiting log bytes 01/18/23 17:09:15.333
Jan 18 17:09:15.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --limit-bytes=1'
Jan 18 17:09:15.477: INFO: stderr: ""
Jan 18 17:09:15.477: INFO: stdout: "I"
Jan 18 17:09:15.477: INFO: got output "I"
STEP: exposing timestamps 01/18/23 17:09:15.477
Jan 18 17:09:15.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 18 17:09:15.585: INFO: stderr: ""
Jan 18 17:09:15.585: INFO: stdout: "2023-01-18T17:09:15.410930650Z I0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\n"
Jan 18 17:09:15.585: INFO: got output "2023-01-18T17:09:15.410930650Z I0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\n"
STEP: restricting to a time range 01/18/23 17:09:15.585
Jan 18 17:09:18.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --since=1s'
Jan 18 17:09:18.201: INFO: stderr: ""
Jan 18 17:09:18.201: INFO: stdout: "I0118 17:09:17.211101       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/64z 344\nI0118 17:09:17.411464       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gg8 487\nI0118 17:09:17.610779       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2n6 230\nI0118 17:09:17.811127       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/j6rx 392\nI0118 17:09:18.011503       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2qh 408\n"
Jan 18 17:09:18.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --since=24h'
Jan 18 17:09:18.310: INFO: stderr: ""
Jan 18 17:09:18.310: INFO: stdout: "I0118 17:09:14.010553       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dxsp 308\nI0118 17:09:14.210736       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/45hp 504\nI0118 17:09:14.411384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/mt7w 279\nI0118 17:09:14.611231       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/s2qn 373\nI0118 17:09:14.811565       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/47nb 434\nI0118 17:09:15.010925       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/kmg5 328\nI0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\nI0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\nI0118 17:09:15.611087       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/wc6 203\nI0118 17:09:15.811539       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/8n95 373\nI0118 17:09:16.010835       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6hn 564\nI0118 17:09:16.211223       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/rzx 305\nI0118 17:09:16.411649       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/f7tk 554\nI0118 17:09:16.610999       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/27z 440\nI0118 17:09:16.811464       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/pdt 591\nI0118 17:09:17.010702       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/jjkc 548\nI0118 17:09:17.211101       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/64z 344\nI0118 17:09:17.411464       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gg8 487\nI0118 17:09:17.610779       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2n6 230\nI0118 17:09:17.811127       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/j6rx 392\nI0118 17:09:18.011503       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2qh 408\nI0118 17:09:18.210729       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/rr9l 315\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 18 17:09:18.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 delete pod logs-generator'
Jan 18 17:09:19.657: INFO: stderr: ""
Jan 18 17:09:19.657: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:09:19.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2155" for this suite. 01/18/23 17:09:19.664
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":51,"skipped":898,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.721 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:12.958
    Jan 18 17:09:12.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:09:12.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:12.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:12.987
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/18/23 17:09:12.992
    Jan 18 17:09:12.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 18 17:09:13.097: INFO: stderr: ""
    Jan 18 17:09:13.097: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/18/23 17:09:13.097
    Jan 18 17:09:13.097: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 18 17:09:13.097: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2155" to be "running and ready, or succeeded"
    Jan 18 17:09:13.102: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.207373ms
    Jan 18 17:09:13.102: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'scw-conformance125-default-61c39bbf4d81476a8e3' to be 'Running' but was 'Pending'
    Jan 18 17:09:15.112: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.014832318s
    Jan 18 17:09:15.112: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 18 17:09:15.112: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/18/23 17:09:15.112
    Jan 18 17:09:15.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator'
    Jan 18 17:09:15.223: INFO: stderr: ""
    Jan 18 17:09:15.223: INFO: stdout: "I0118 17:09:14.010553       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dxsp 308\nI0118 17:09:14.210736       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/45hp 504\nI0118 17:09:14.411384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/mt7w 279\nI0118 17:09:14.611231       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/s2qn 373\nI0118 17:09:14.811565       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/47nb 434\nI0118 17:09:15.010925       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/kmg5 328\nI0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
    STEP: limiting log lines 01/18/23 17:09:15.223
    Jan 18 17:09:15.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --tail=1'
    Jan 18 17:09:15.332: INFO: stderr: ""
    Jan 18 17:09:15.333: INFO: stdout: "I0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
    Jan 18 17:09:15.333: INFO: got output "I0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\n"
    STEP: limiting log bytes 01/18/23 17:09:15.333
    Jan 18 17:09:15.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --limit-bytes=1'
    Jan 18 17:09:15.477: INFO: stderr: ""
    Jan 18 17:09:15.477: INFO: stdout: "I"
    Jan 18 17:09:15.477: INFO: got output "I"
    STEP: exposing timestamps 01/18/23 17:09:15.477
    Jan 18 17:09:15.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 18 17:09:15.585: INFO: stderr: ""
    Jan 18 17:09:15.585: INFO: stdout: "2023-01-18T17:09:15.410930650Z I0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\n"
    Jan 18 17:09:15.585: INFO: got output "2023-01-18T17:09:15.410930650Z I0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\n"
    STEP: restricting to a time range 01/18/23 17:09:15.585
    Jan 18 17:09:18.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --since=1s'
    Jan 18 17:09:18.201: INFO: stderr: ""
    Jan 18 17:09:18.201: INFO: stdout: "I0118 17:09:17.211101       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/64z 344\nI0118 17:09:17.411464       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gg8 487\nI0118 17:09:17.610779       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2n6 230\nI0118 17:09:17.811127       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/j6rx 392\nI0118 17:09:18.011503       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2qh 408\n"
    Jan 18 17:09:18.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 logs logs-generator logs-generator --since=24h'
    Jan 18 17:09:18.310: INFO: stderr: ""
    Jan 18 17:09:18.310: INFO: stdout: "I0118 17:09:14.010553       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/dxsp 308\nI0118 17:09:14.210736       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/45hp 504\nI0118 17:09:14.411384       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/mt7w 279\nI0118 17:09:14.611231       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/s2qn 373\nI0118 17:09:14.811565       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/47nb 434\nI0118 17:09:15.010925       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/kmg5 328\nI0118 17:09:15.211436       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/vk6p 589\nI0118 17:09:15.410709       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/hz86 233\nI0118 17:09:15.611087       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/wc6 203\nI0118 17:09:15.811539       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/8n95 373\nI0118 17:09:16.010835       1 logs_generator.go:76] 10 GET /api/v1/namespaces/kube-system/pods/6hn 564\nI0118 17:09:16.211223       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/rzx 305\nI0118 17:09:16.411649       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/f7tk 554\nI0118 17:09:16.610999       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/27z 440\nI0118 17:09:16.811464       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/pdt 591\nI0118 17:09:17.010702       1 logs_generator.go:76] 15 POST /api/v1/namespaces/ns/pods/jjkc 548\nI0118 17:09:17.211101       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/64z 344\nI0118 17:09:17.411464       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gg8 487\nI0118 17:09:17.610779       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/2n6 230\nI0118 17:09:17.811127       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/j6rx 392\nI0118 17:09:18.011503       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/2qh 408\nI0118 17:09:18.210729       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/rr9l 315\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 18 17:09:18.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2155 delete pod logs-generator'
    Jan 18 17:09:19.657: INFO: stderr: ""
    Jan 18 17:09:19.657: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:09:19.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2155" for this suite. 01/18/23 17:09:19.664
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:19.68
Jan 18 17:09:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:09:19.681
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:19.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:19.708
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 17:09:19.712
Jan 18 17:09:19.727: INFO: Waiting up to 5m0s for pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df" in namespace "emptydir-7693" to be "Succeeded or Failed"
Jan 18 17:09:19.732: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.81554ms
Jan 18 17:09:21.740: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Running", Reason="", readiness=false. Elapsed: 2.012669036s
Jan 18 17:09:23.738: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01108495s
STEP: Saw pod success 01/18/23 17:09:23.738
Jan 18 17:09:23.738: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df" satisfied condition "Succeeded or Failed"
Jan 18 17:09:23.746: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-61fc6a00-9a77-445e-bcad-cd80673634df container test-container: <nil>
STEP: delete the pod 01/18/23 17:09:23.764
Jan 18 17:09:23.826: INFO: Waiting for pod pod-61fc6a00-9a77-445e-bcad-cd80673634df to disappear
Jan 18 17:09:23.831: INFO: Pod pod-61fc6a00-9a77-445e-bcad-cd80673634df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:09:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7693" for this suite. 01/18/23 17:09:23.839
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":52,"skipped":909,"failed":0}
------------------------------
â€¢ [4.169 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:19.68
    Jan 18 17:09:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:09:19.681
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:19.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:19.708
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 17:09:19.712
    Jan 18 17:09:19.727: INFO: Waiting up to 5m0s for pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df" in namespace "emptydir-7693" to be "Succeeded or Failed"
    Jan 18 17:09:19.732: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Pending", Reason="", readiness=false. Elapsed: 4.81554ms
    Jan 18 17:09:21.740: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Running", Reason="", readiness=false. Elapsed: 2.012669036s
    Jan 18 17:09:23.738: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01108495s
    STEP: Saw pod success 01/18/23 17:09:23.738
    Jan 18 17:09:23.738: INFO: Pod "pod-61fc6a00-9a77-445e-bcad-cd80673634df" satisfied condition "Succeeded or Failed"
    Jan 18 17:09:23.746: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-61fc6a00-9a77-445e-bcad-cd80673634df container test-container: <nil>
    STEP: delete the pod 01/18/23 17:09:23.764
    Jan 18 17:09:23.826: INFO: Waiting for pod pod-61fc6a00-9a77-445e-bcad-cd80673634df to disappear
    Jan 18 17:09:23.831: INFO: Pod pod-61fc6a00-9a77-445e-bcad-cd80673634df no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:09:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7693" for this suite. 01/18/23 17:09:23.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:23.85
Jan 18 17:09:23.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:09:23.851
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:23.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:23.877
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/18/23 17:09:23.882
Jan 18 17:09:23.903: INFO: Waiting up to 5m0s for pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2" in namespace "downward-api-5937" to be "Succeeded or Failed"
Jan 18 17:09:23.915: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476759ms
Jan 18 17:09:25.925: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021851166s
Jan 18 17:09:27.924: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020760854s
STEP: Saw pod success 01/18/23 17:09:27.924
Jan 18 17:09:27.924: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2" satisfied condition "Succeeded or Failed"
Jan 18 17:09:27.931: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:09:27.947
Jan 18 17:09:27.966: INFO: Waiting for pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 to disappear
Jan 18 17:09:27.972: INFO: Pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 17:09:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5937" for this suite. 01/18/23 17:09:27.979
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":53,"skipped":923,"failed":0}
------------------------------
â€¢ [4.141 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:23.85
    Jan 18 17:09:23.850: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:09:23.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:23.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:23.877
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/18/23 17:09:23.882
    Jan 18 17:09:23.903: INFO: Waiting up to 5m0s for pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2" in namespace "downward-api-5937" to be "Succeeded or Failed"
    Jan 18 17:09:23.915: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476759ms
    Jan 18 17:09:25.925: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021851166s
    Jan 18 17:09:27.924: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020760854s
    STEP: Saw pod success 01/18/23 17:09:27.924
    Jan 18 17:09:27.924: INFO: Pod "downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2" satisfied condition "Succeeded or Failed"
    Jan 18 17:09:27.931: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:09:27.947
    Jan 18 17:09:27.966: INFO: Waiting for pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 to disappear
    Jan 18 17:09:27.972: INFO: Pod downward-api-d64de6b9-fef6-4538-bbc4-2ff62d603df2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 17:09:27.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5937" for this suite. 01/18/23 17:09:27.979
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:27.992
Jan 18 17:09:27.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:09:27.993
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:28.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:28.023
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-3fd56c1d-edba-4ec3-b489-a4d5fe6c4dae 01/18/23 17:09:28.028
STEP: Creating a pod to test consume configMaps 01/18/23 17:09:28.036
Jan 18 17:09:28.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116" in namespace "projected-6018" to be "Succeeded or Failed"
Jan 18 17:09:28.059: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Pending", Reason="", readiness=false. Elapsed: 7.113804ms
Jan 18 17:09:30.067: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014519001s
Jan 18 17:09:32.068: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015757376s
STEP: Saw pod success 01/18/23 17:09:32.068
Jan 18 17:09:32.068: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116" satisfied condition "Succeeded or Failed"
Jan 18 17:09:32.075: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:09:32.091
Jan 18 17:09:32.111: INFO: Waiting for pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 to disappear
Jan 18 17:09:32.116: INFO: Pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:09:32.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6018" for this suite. 01/18/23 17:09:32.124
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":54,"skipped":925,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:27.992
    Jan 18 17:09:27.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:09:27.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:28.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:28.023
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-3fd56c1d-edba-4ec3-b489-a4d5fe6c4dae 01/18/23 17:09:28.028
    STEP: Creating a pod to test consume configMaps 01/18/23 17:09:28.036
    Jan 18 17:09:28.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116" in namespace "projected-6018" to be "Succeeded or Failed"
    Jan 18 17:09:28.059: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Pending", Reason="", readiness=false. Elapsed: 7.113804ms
    Jan 18 17:09:30.067: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014519001s
    Jan 18 17:09:32.068: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015757376s
    STEP: Saw pod success 01/18/23 17:09:32.068
    Jan 18 17:09:32.068: INFO: Pod "pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116" satisfied condition "Succeeded or Failed"
    Jan 18 17:09:32.075: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:09:32.091
    Jan 18 17:09:32.111: INFO: Waiting for pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 to disappear
    Jan 18 17:09:32.116: INFO: Pod pod-projected-configmaps-4dbadee0-1e92-4061-b4be-7c907af03116 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:09:32.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6018" for this suite. 01/18/23 17:09:32.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:09:32.136
Jan 18 17:09:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:09:32.137
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:32.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:32.164
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa in namespace container-probe-7620 01/18/23 17:09:32.168
Jan 18 17:09:32.193: INFO: Waiting up to 5m0s for pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa" in namespace "container-probe-7620" to be "not pending"
Jan 18 17:09:32.199: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311893ms
Jan 18 17:09:34.208: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.014971112s
Jan 18 17:09:34.208: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa" satisfied condition "not pending"
Jan 18 17:09:34.208: INFO: Started pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa in namespace container-probe-7620
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:09:34.208
Jan 18 17:09:34.216: INFO: Initial restart count of pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa is 0
STEP: deleting the pod 01/18/23 17:13:35.334
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:13:35.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7620" for this suite. 01/18/23 17:13:35.369
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":55,"skipped":941,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.246 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:09:32.136
    Jan 18 17:09:32.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:09:32.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:09:32.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:09:32.164
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa in namespace container-probe-7620 01/18/23 17:09:32.168
    Jan 18 17:09:32.193: INFO: Waiting up to 5m0s for pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa" in namespace "container-probe-7620" to be "not pending"
    Jan 18 17:09:32.199: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.311893ms
    Jan 18 17:09:34.208: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa": Phase="Running", Reason="", readiness=true. Elapsed: 2.014971112s
    Jan 18 17:09:34.208: INFO: Pod "busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa" satisfied condition "not pending"
    Jan 18 17:09:34.208: INFO: Started pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa in namespace container-probe-7620
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:09:34.208
    Jan 18 17:09:34.216: INFO: Initial restart count of pod busybox-9a1dbc4c-79a0-43a7-8f1a-74782d96faaa is 0
    STEP: deleting the pod 01/18/23 17:13:35.334
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:13:35.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7620" for this suite. 01/18/23 17:13:35.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:13:35.383
Jan 18 17:13:35.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:13:35.384
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:13:35.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:13:35.411
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 in namespace container-probe-5408 01/18/23 17:13:35.415
Jan 18 17:13:35.431: INFO: Waiting up to 5m0s for pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31" in namespace "container-probe-5408" to be "not pending"
Jan 18 17:13:35.438: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.374106ms
Jan 18 17:13:37.447: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31": Phase="Running", Reason="", readiness=true. Elapsed: 2.016458258s
Jan 18 17:13:37.447: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31" satisfied condition "not pending"
Jan 18 17:13:37.447: INFO: Started pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 in namespace container-probe-5408
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:13:37.447
Jan 18 17:13:37.455: INFO: Initial restart count of pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 is 0
Jan 18 17:14:27.695: INFO: Restart count of pod container-probe-5408/busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 is now 1 (50.240334588s elapsed)
STEP: deleting the pod 01/18/23 17:14:27.695
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:14:27.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5408" for this suite. 01/18/23 17:14:27.725
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":56,"skipped":957,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.357 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:13:35.383
    Jan 18 17:13:35.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:13:35.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:13:35.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:13:35.411
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 in namespace container-probe-5408 01/18/23 17:13:35.415
    Jan 18 17:13:35.431: INFO: Waiting up to 5m0s for pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31" in namespace "container-probe-5408" to be "not pending"
    Jan 18 17:13:35.438: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.374106ms
    Jan 18 17:13:37.447: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31": Phase="Running", Reason="", readiness=true. Elapsed: 2.016458258s
    Jan 18 17:13:37.447: INFO: Pod "busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31" satisfied condition "not pending"
    Jan 18 17:13:37.447: INFO: Started pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 in namespace container-probe-5408
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:13:37.447
    Jan 18 17:13:37.455: INFO: Initial restart count of pod busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 is 0
    Jan 18 17:14:27.695: INFO: Restart count of pod container-probe-5408/busybox-3663c69f-7bb7-4903-9d4d-6934dfa7fc31 is now 1 (50.240334588s elapsed)
    STEP: deleting the pod 01/18/23 17:14:27.695
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:14:27.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5408" for this suite. 01/18/23 17:14:27.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:14:27.741
Jan 18 17:14:27.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:14:27.743
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:14:27.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:14:27.771
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3105 01/18/23 17:14:27.775
STEP: creating a selector 01/18/23 17:14:27.775
STEP: Creating the service pods in kubernetes 01/18/23 17:14:27.776
Jan 18 17:14:27.776: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 17:14:27.811: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3105" to be "running and ready"
Jan 18 17:14:27.818: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005643ms
Jan 18 17:14:27.818: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:14:29.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015953681s
Jan 18 17:14:29.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:14:31.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016427796s
Jan 18 17:14:31.828: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:14:33.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015622582s
Jan 18 17:14:33.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:14:35.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018236237s
Jan 18 17:14:35.830: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:14:37.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015547989s
Jan 18 17:14:37.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:14:39.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018218632s
Jan 18 17:14:39.830: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 17:14:39.830: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 17:14:39.837: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3105" to be "running and ready"
Jan 18 17:14:39.844: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.795282ms
Jan 18 17:14:39.844: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 17:14:39.844: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 17:14:39.851
Jan 18 17:14:39.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3105" to be "running"
Jan 18 17:14:39.872: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.987545ms
Jan 18 17:14:41.881: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017980134s
Jan 18 17:14:41.881: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 17:14:41.888: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 17:14:41.888: INFO: Breadth first check of 10.100.145.183 on host 10.195.74.123...
Jan 18 17:14:41.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.184:9080/dial?request=hostname&protocol=http&host=10.100.145.183&port=8083&tries=1'] Namespace:pod-network-test-3105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:14:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:14:41.895: INFO: ExecWithOptions: Clientset creation
Jan 18 17:14:41.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3105/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.184%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.145.183%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 17:14:42.009: INFO: Waiting for responses: map[]
Jan 18 17:14:42.009: INFO: reached 10.100.145.183 after 0/1 tries
Jan 18 17:14:42.009: INFO: Breadth first check of 10.100.158.28 on host 10.195.78.23...
Jan 18 17:14:42.016: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.184:9080/dial?request=hostname&protocol=http&host=10.100.158.28&port=8083&tries=1'] Namespace:pod-network-test-3105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:14:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:14:42.017: INFO: ExecWithOptions: Clientset creation
Jan 18 17:14:42.017: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3105/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.184%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.158.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 17:14:42.111: INFO: Waiting for responses: map[]
Jan 18 17:14:42.111: INFO: reached 10.100.158.28 after 0/1 tries
Jan 18 17:14:42.111: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 17:14:42.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3105" for this suite. 01/18/23 17:14:42.119
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":57,"skipped":969,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.393 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:14:27.741
    Jan 18 17:14:27.742: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:14:27.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:14:27.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:14:27.771
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3105 01/18/23 17:14:27.775
    STEP: creating a selector 01/18/23 17:14:27.775
    STEP: Creating the service pods in kubernetes 01/18/23 17:14:27.776
    Jan 18 17:14:27.776: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 17:14:27.811: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3105" to be "running and ready"
    Jan 18 17:14:27.818: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.005643ms
    Jan 18 17:14:27.818: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:14:29.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.015953681s
    Jan 18 17:14:29.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:14:31.828: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016427796s
    Jan 18 17:14:31.828: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:14:33.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.015622582s
    Jan 18 17:14:33.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:14:35.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.018236237s
    Jan 18 17:14:35.830: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:14:37.827: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.015547989s
    Jan 18 17:14:37.827: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:14:39.830: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.018218632s
    Jan 18 17:14:39.830: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 17:14:39.830: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 17:14:39.837: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3105" to be "running and ready"
    Jan 18 17:14:39.844: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.795282ms
    Jan 18 17:14:39.844: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 17:14:39.844: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 17:14:39.851
    Jan 18 17:14:39.862: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3105" to be "running"
    Jan 18 17:14:39.872: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.987545ms
    Jan 18 17:14:41.881: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.017980134s
    Jan 18 17:14:41.881: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 17:14:41.888: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 17:14:41.888: INFO: Breadth first check of 10.100.145.183 on host 10.195.74.123...
    Jan 18 17:14:41.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.184:9080/dial?request=hostname&protocol=http&host=10.100.145.183&port=8083&tries=1'] Namespace:pod-network-test-3105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:14:41.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:14:41.895: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:14:41.895: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3105/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.184%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.145.183%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 17:14:42.009: INFO: Waiting for responses: map[]
    Jan 18 17:14:42.009: INFO: reached 10.100.145.183 after 0/1 tries
    Jan 18 17:14:42.009: INFO: Breadth first check of 10.100.158.28 on host 10.195.78.23...
    Jan 18 17:14:42.016: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.184:9080/dial?request=hostname&protocol=http&host=10.100.158.28&port=8083&tries=1'] Namespace:pod-network-test-3105 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:14:42.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:14:42.017: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:14:42.017: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3105/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.184%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.100.158.28%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 17:14:42.111: INFO: Waiting for responses: map[]
    Jan 18 17:14:42.111: INFO: reached 10.100.158.28 after 0/1 tries
    Jan 18 17:14:42.111: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 17:14:42.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3105" for this suite. 01/18/23 17:14:42.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:14:42.136
Jan 18 17:14:42.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:14:42.137
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:14:42.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:14:42.162
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 in namespace container-probe-3401 01/18/23 17:14:42.166
Jan 18 17:14:42.183: INFO: Waiting up to 5m0s for pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4" in namespace "container-probe-3401" to be "not pending"
Jan 18 17:14:42.191: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.736884ms
Jan 18 17:14:44.201: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018284775s
Jan 18 17:14:44.201: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4" satisfied condition "not pending"
Jan 18 17:14:44.201: INFO: Started pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 in namespace container-probe-3401
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:14:44.201
Jan 18 17:14:44.208: INFO: Initial restart count of pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 is 0
STEP: deleting the pod 01/18/23 17:18:45.314
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:18:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3401" for this suite. 01/18/23 17:18:45.346
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":58,"skipped":1000,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.223 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:14:42.136
    Jan 18 17:14:42.136: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:14:42.137
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:14:42.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:14:42.162
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 in namespace container-probe-3401 01/18/23 17:14:42.166
    Jan 18 17:14:42.183: INFO: Waiting up to 5m0s for pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4" in namespace "container-probe-3401" to be "not pending"
    Jan 18 17:14:42.191: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.736884ms
    Jan 18 17:14:44.201: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018284775s
    Jan 18 17:14:44.201: INFO: Pod "test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4" satisfied condition "not pending"
    Jan 18 17:14:44.201: INFO: Started pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 in namespace container-probe-3401
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:14:44.201
    Jan 18 17:14:44.208: INFO: Initial restart count of pod test-webserver-fd8d0a41-d54d-469b-b203-818b3c156ce4 is 0
    STEP: deleting the pod 01/18/23 17:18:45.314
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:18:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3401" for this suite. 01/18/23 17:18:45.346
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:18:45.36
Jan 18 17:18:45.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:18:45.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:45.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:45.389
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-d570c294-2f94-4a04-9936-a7a46b4a3140 01/18/23 17:18:45.393
STEP: Creating a pod to test consume configMaps 01/18/23 17:18:45.401
Jan 18 17:18:45.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a" in namespace "configmap-9501" to be "Succeeded or Failed"
Jan 18 17:18:45.424: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.987094ms
Jan 18 17:18:47.433: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018237893s
Jan 18 17:18:49.434: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019147412s
STEP: Saw pod success 01/18/23 17:18:49.434
Jan 18 17:18:49.434: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a" satisfied condition "Succeeded or Failed"
Jan 18 17:18:49.443: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:18:49.475
Jan 18 17:18:49.497: INFO: Waiting for pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a to disappear
Jan 18 17:18:49.504: INFO: Pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:18:49.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9501" for this suite. 01/18/23 17:18:49.511
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":59,"skipped":1002,"failed":0}
------------------------------
â€¢ [4.165 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:18:45.36
    Jan 18 17:18:45.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:18:45.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:45.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:45.389
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-d570c294-2f94-4a04-9936-a7a46b4a3140 01/18/23 17:18:45.393
    STEP: Creating a pod to test consume configMaps 01/18/23 17:18:45.401
    Jan 18 17:18:45.415: INFO: Waiting up to 5m0s for pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a" in namespace "configmap-9501" to be "Succeeded or Failed"
    Jan 18 17:18:45.424: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.987094ms
    Jan 18 17:18:47.433: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018237893s
    Jan 18 17:18:49.434: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019147412s
    STEP: Saw pod success 01/18/23 17:18:49.434
    Jan 18 17:18:49.434: INFO: Pod "pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a" satisfied condition "Succeeded or Failed"
    Jan 18 17:18:49.443: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:18:49.475
    Jan 18 17:18:49.497: INFO: Waiting for pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a to disappear
    Jan 18 17:18:49.504: INFO: Pod pod-configmaps-a7240a03-51cf-4dd5-871c-2b297cf7441a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:18:49.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9501" for this suite. 01/18/23 17:18:49.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:18:49.526
Jan 18 17:18:49.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:18:49.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:49.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:49.558
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/18/23 17:18:49.577
Jan 18 17:18:49.578: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007" in namespace "kubelet-test-7749" to be "completed"
Jan 18 17:18:49.583: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Pending", Reason="", readiness=false. Elapsed: 5.428442ms
Jan 18 17:18:51.593: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015130604s
Jan 18 17:18:53.590: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012705214s
Jan 18 17:18:53.590: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 17:18:53.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7749" for this suite. 01/18/23 17:18:53.617
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":60,"skipped":1014,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:18:49.526
    Jan 18 17:18:49.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:18:49.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:49.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:49.558
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/18/23 17:18:49.577
    Jan 18 17:18:49.578: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007" in namespace "kubelet-test-7749" to be "completed"
    Jan 18 17:18:49.583: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Pending", Reason="", readiness=false. Elapsed: 5.428442ms
    Jan 18 17:18:51.593: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015130604s
    Jan 18 17:18:53.590: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012705214s
    Jan 18 17:18:53.590: INFO: Pod "agnhost-host-aliases2a99139d-1af2-40b6-8712-2f487d81c007" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 17:18:53.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7749" for this suite. 01/18/23 17:18:53.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:18:53.634
Jan 18 17:18:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:18:53.635
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:53.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:53.668
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/18/23 17:19:10.682
STEP: Creating a ResourceQuota 01/18/23 17:19:15.691
STEP: Ensuring resource quota status is calculated 01/18/23 17:19:15.701
STEP: Creating a ConfigMap 01/18/23 17:19:17.708
STEP: Ensuring resource quota status captures configMap creation 01/18/23 17:19:17.73
STEP: Deleting a ConfigMap 01/18/23 17:19:19.74
STEP: Ensuring resource quota status released usage 01/18/23 17:19:19.753
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:19:21.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1947" for this suite. 01/18/23 17:19:21.771
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":61,"skipped":1043,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.151 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:18:53.634
    Jan 18 17:18:53.634: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:18:53.635
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:18:53.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:18:53.668
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/18/23 17:19:10.682
    STEP: Creating a ResourceQuota 01/18/23 17:19:15.691
    STEP: Ensuring resource quota status is calculated 01/18/23 17:19:15.701
    STEP: Creating a ConfigMap 01/18/23 17:19:17.708
    STEP: Ensuring resource quota status captures configMap creation 01/18/23 17:19:17.73
    STEP: Deleting a ConfigMap 01/18/23 17:19:19.74
    STEP: Ensuring resource quota status released usage 01/18/23 17:19:19.753
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:19:21.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1947" for this suite. 01/18/23 17:19:21.771
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:21.785
Jan 18 17:19:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-runtime 01/18/23 17:19:21.786
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:21.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:21.817
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/18/23 17:19:21.822
STEP: wait for the container to reach Succeeded 01/18/23 17:19:21.839
STEP: get the container status 01/18/23 17:19:25.878
STEP: the container should be terminated 01/18/23 17:19:25.886
STEP: the termination message should be set 01/18/23 17:19:25.886
Jan 18 17:19:25.886: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 17:19:25.886
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 17:19:25.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1781" for this suite. 01/18/23 17:19:25.922
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":62,"skipped":1043,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:21.785
    Jan 18 17:19:21.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 17:19:21.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:21.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:21.817
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/18/23 17:19:21.822
    STEP: wait for the container to reach Succeeded 01/18/23 17:19:21.839
    STEP: get the container status 01/18/23 17:19:25.878
    STEP: the container should be terminated 01/18/23 17:19:25.886
    STEP: the termination message should be set 01/18/23 17:19:25.886
    Jan 18 17:19:25.886: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 17:19:25.886
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 17:19:25.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1781" for this suite. 01/18/23 17:19:25.922
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:25.936
Jan 18 17:19:25.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:19:25.938
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:25.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:25.966
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:19:25.978
Jan 18 17:19:25.993: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5808" to be "running and ready"
Jan 18 17:19:26.000: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551559ms
Jan 18 17:19:26.000: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:19:28.009: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015375537s
Jan 18 17:19:28.009: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 17:19:28.009: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/18/23 17:19:28.015
Jan 18 17:19:28.027: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5808" to be "running and ready"
Jan 18 17:19:28.036: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.298036ms
Jan 18 17:19:28.036: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:19:30.046: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01887542s
Jan 18 17:19:30.046: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 18 17:19:30.046: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 17:19:30.053
STEP: delete the pod with lifecycle hook 01/18/23 17:19:30.069
Jan 18 17:19:30.081: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 17:19:30.088: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 17:19:32.088: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 17:19:32.097: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 18 17:19:34.089: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 18 17:19:34.098: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 17:19:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5808" for this suite. 01/18/23 17:19:34.107
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":63,"skipped":1045,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.184 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:25.936
    Jan 18 17:19:25.936: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:19:25.938
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:25.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:25.966
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:19:25.978
    Jan 18 17:19:25.993: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5808" to be "running and ready"
    Jan 18 17:19:26.000: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.551559ms
    Jan 18 17:19:26.000: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:19:28.009: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015375537s
    Jan 18 17:19:28.009: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 17:19:28.009: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/18/23 17:19:28.015
    Jan 18 17:19:28.027: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5808" to be "running and ready"
    Jan 18 17:19:28.036: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 9.298036ms
    Jan 18 17:19:28.036: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:19:30.046: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01887542s
    Jan 18 17:19:30.046: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 18 17:19:30.046: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 17:19:30.053
    STEP: delete the pod with lifecycle hook 01/18/23 17:19:30.069
    Jan 18 17:19:30.081: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 17:19:30.088: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 17:19:32.088: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 17:19:32.097: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 18 17:19:34.089: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 18 17:19:34.098: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 17:19:34.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5808" for this suite. 01/18/23 17:19:34.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:34.122
Jan 18 17:19:34.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:19:34.123
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:34.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:34.153
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:19:34.157
Jan 18 17:19:34.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7" in namespace "downward-api-6743" to be "Succeeded or Failed"
Jan 18 17:19:34.179: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033446ms
Jan 18 17:19:36.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Running", Reason="", readiness=false. Elapsed: 2.013699641s
Jan 18 17:19:38.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013974883s
STEP: Saw pod success 01/18/23 17:19:38.187
Jan 18 17:19:38.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7" satisfied condition "Succeeded or Failed"
Jan 18 17:19:38.195: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 container client-container: <nil>
STEP: delete the pod 01/18/23 17:19:38.208
Jan 18 17:19:38.225: INFO: Waiting for pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 to disappear
Jan 18 17:19:38.230: INFO: Pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 17:19:38.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6743" for this suite. 01/18/23 17:19:38.236
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":64,"skipped":1054,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:34.122
    Jan 18 17:19:34.122: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:19:34.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:34.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:34.153
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:19:34.157
    Jan 18 17:19:34.173: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7" in namespace "downward-api-6743" to be "Succeeded or Failed"
    Jan 18 17:19:34.179: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033446ms
    Jan 18 17:19:36.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Running", Reason="", readiness=false. Elapsed: 2.013699641s
    Jan 18 17:19:38.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013974883s
    STEP: Saw pod success 01/18/23 17:19:38.187
    Jan 18 17:19:38.187: INFO: Pod "downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7" satisfied condition "Succeeded or Failed"
    Jan 18 17:19:38.195: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:19:38.208
    Jan 18 17:19:38.225: INFO: Waiting for pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 to disappear
    Jan 18 17:19:38.230: INFO: Pod downwardapi-volume-2a4758bd-5849-40a4-8f02-4f600fb4eda7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 17:19:38.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6743" for this suite. 01/18/23 17:19:38.236
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:38.255
Jan 18 17:19:38.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-webhook 01/18/23 17:19:38.256
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:38.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:38.278
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 17:19:38.283
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 17:19:38.7
STEP: Deploying the custom resource conversion webhook pod 01/18/23 17:19:38.717
STEP: Wait for the deployment to be ready 01/18/23 17:19:38.741
Jan 18 17:19:38.757: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:19:40.781
STEP: Verifying the service has paired with the endpoint 01/18/23 17:19:40.803
Jan 18 17:19:41.803: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 18 17:19:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Creating a v1 custom resource 01/18/23 17:19:44.523
STEP: v2 custom resource should be converted 01/18/23 17:19:44.534
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:19:45.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3552" for this suite. 01/18/23 17:19:45.082
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":65,"skipped":1055,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.916 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:38.255
    Jan 18 17:19:38.255: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 17:19:38.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:38.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:38.278
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 17:19:38.283
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 17:19:38.7
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 17:19:38.717
    STEP: Wait for the deployment to be ready 01/18/23 17:19:38.741
    Jan 18 17:19:38.757: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:19:40.781
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:19:40.803
    Jan 18 17:19:41.803: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 18 17:19:41.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Creating a v1 custom resource 01/18/23 17:19:44.523
    STEP: v2 custom resource should be converted 01/18/23 17:19:44.534
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:19:45.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3552" for this suite. 01/18/23 17:19:45.082
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:45.172
Jan 18 17:19:45.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:19:45.173
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:45.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:45.205
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan 18 17:19:45.225: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d" in namespace "kubelet-test-3538" to be "running and ready"
Jan 18 17:19:45.232: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.660811ms
Jan 18 17:19:45.232: INFO: The phase of Pod busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:19:47.242: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016650551s
Jan 18 17:19:47.242: INFO: The phase of Pod busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d is Running (Ready = true)
Jan 18 17:19:47.242: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 17:19:47.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3538" for this suite. 01/18/23 17:19:47.275
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":66,"skipped":1062,"failed":0}
------------------------------
â€¢ [2.116 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:45.172
    Jan 18 17:19:45.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:19:45.173
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:45.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:45.205
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan 18 17:19:45.225: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d" in namespace "kubelet-test-3538" to be "running and ready"
    Jan 18 17:19:45.232: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.660811ms
    Jan 18 17:19:45.232: INFO: The phase of Pod busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:19:47.242: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d": Phase="Running", Reason="", readiness=true. Elapsed: 2.016650551s
    Jan 18 17:19:47.242: INFO: The phase of Pod busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d is Running (Ready = true)
    Jan 18 17:19:47.242: INFO: Pod "busybox-readonly-fsc75a0044-d16b-444c-8770-78e27351d74d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 17:19:47.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3538" for this suite. 01/18/23 17:19:47.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:47.289
Jan 18 17:19:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:19:47.29
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:47.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:47.32
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 18 17:19:47.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 17:19:49.745
Jan 18 17:19:49.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 create -f -'
Jan 18 17:19:51.152: INFO: stderr: ""
Jan 18 17:19:51.152: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 17:19:51.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-8178-crds test-cr'
Jan 18 17:19:51.258: INFO: stderr: ""
Jan 18 17:19:51.258: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 18 17:19:51.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 apply -f -'
Jan 18 17:19:51.552: INFO: stderr: ""
Jan 18 17:19:51.552: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 18 17:19:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-8178-crds test-cr'
Jan 18 17:19:51.659: INFO: stderr: ""
Jan 18 17:19:51.659: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 17:19:51.659
Jan 18 17:19:51.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 explain e2e-test-crd-publish-openapi-8178-crds'
Jan 18 17:19:51.929: INFO: stderr: ""
Jan 18 17:19:51.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8178-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:19:54.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-192" for this suite. 01/18/23 17:19:54.772
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":67,"skipped":1095,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.496 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:47.289
    Jan 18 17:19:47.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:19:47.29
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:47.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:47.32
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 18 17:19:47.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 17:19:49.745
    Jan 18 17:19:49.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 create -f -'
    Jan 18 17:19:51.152: INFO: stderr: ""
    Jan 18 17:19:51.152: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 17:19:51.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-8178-crds test-cr'
    Jan 18 17:19:51.258: INFO: stderr: ""
    Jan 18 17:19:51.258: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 18 17:19:51.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 apply -f -'
    Jan 18 17:19:51.552: INFO: stderr: ""
    Jan 18 17:19:51.552: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 18 17:19:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 --namespace=crd-publish-openapi-192 delete e2e-test-crd-publish-openapi-8178-crds test-cr'
    Jan 18 17:19:51.659: INFO: stderr: ""
    Jan 18 17:19:51.659: INFO: stdout: "e2e-test-crd-publish-openapi-8178-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 17:19:51.659
    Jan 18 17:19:51.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-192 explain e2e-test-crd-publish-openapi-8178-crds'
    Jan 18 17:19:51.929: INFO: stderr: ""
    Jan 18 17:19:51.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8178-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:19:54.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-192" for this suite. 01/18/23 17:19:54.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:54.787
Jan 18 17:19:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:19:54.788
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:54.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:54.815
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 17:19:54.819
Jan 18 17:19:54.832: INFO: Waiting up to 5m0s for pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5" in namespace "emptydir-1559" to be "Succeeded or Failed"
Jan 18 17:19:54.838: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328407ms
Jan 18 17:19:56.846: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014123407s
Jan 18 17:19:58.848: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016232555s
STEP: Saw pod success 01/18/23 17:19:58.848
Jan 18 17:19:58.849: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5" satisfied condition "Succeeded or Failed"
Jan 18 17:19:58.855: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 container test-container: <nil>
STEP: delete the pod 01/18/23 17:19:58.872
Jan 18 17:19:58.898: INFO: Waiting for pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 to disappear
Jan 18 17:19:58.904: INFO: Pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:19:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1559" for this suite. 01/18/23 17:19:58.911
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":68,"skipped":1116,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:54.787
    Jan 18 17:19:54.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:19:54.788
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:54.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:54.815
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 17:19:54.819
    Jan 18 17:19:54.832: INFO: Waiting up to 5m0s for pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5" in namespace "emptydir-1559" to be "Succeeded or Failed"
    Jan 18 17:19:54.838: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328407ms
    Jan 18 17:19:56.846: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014123407s
    Jan 18 17:19:58.848: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016232555s
    STEP: Saw pod success 01/18/23 17:19:58.848
    Jan 18 17:19:58.849: INFO: Pod "pod-ace7a78b-2005-4b27-a534-ef5d41896ec5" satisfied condition "Succeeded or Failed"
    Jan 18 17:19:58.855: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:19:58.872
    Jan 18 17:19:58.898: INFO: Waiting for pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 to disappear
    Jan 18 17:19:58.904: INFO: Pod pod-ace7a78b-2005-4b27-a534-ef5d41896ec5 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:19:58.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1559" for this suite. 01/18/23 17:19:58.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:19:58.927
Jan 18 17:19:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-runtime 01/18/23 17:19:58.929
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:58.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:58.965
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/18/23 17:19:58.97
STEP: wait for the container to reach Failed 01/18/23 17:19:58.986
STEP: get the container status 01/18/23 17:20:03.043
STEP: the container should be terminated 01/18/23 17:20:03.054
STEP: the termination message should be set 01/18/23 17:20:03.054
Jan 18 17:20:03.054: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/18/23 17:20:03.054
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 17:20:03.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3092" for this suite. 01/18/23 17:20:03.089
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":69,"skipped":1132,"failed":0}
------------------------------
â€¢ [4.174 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:19:58.927
    Jan 18 17:19:58.928: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 17:19:58.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:19:58.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:19:58.965
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/18/23 17:19:58.97
    STEP: wait for the container to reach Failed 01/18/23 17:19:58.986
    STEP: get the container status 01/18/23 17:20:03.043
    STEP: the container should be terminated 01/18/23 17:20:03.054
    STEP: the termination message should be set 01/18/23 17:20:03.054
    Jan 18 17:20:03.054: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/18/23 17:20:03.054
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 17:20:03.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3092" for this suite. 01/18/23 17:20:03.089
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:03.102
Jan 18 17:20:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:20:03.103
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:03.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:03.127
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-4255 01/18/23 17:20:03.13
STEP: creating a selector 01/18/23 17:20:03.13
STEP: Creating the service pods in kubernetes 01/18/23 17:20:03.13
Jan 18 17:20:03.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 17:20:03.165: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4255" to be "running and ready"
Jan 18 17:20:03.173: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604195ms
Jan 18 17:20:03.173: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:20:05.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014756138s
Jan 18 17:20:05.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:07.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01671459s
Jan 18 17:20:07.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:09.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018324195s
Jan 18 17:20:09.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:11.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016922931s
Jan 18 17:20:11.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:13.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018141065s
Jan 18 17:20:13.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:15.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016828771s
Jan 18 17:20:15.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:17.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015527963s
Jan 18 17:20:17.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:19.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015901348s
Jan 18 17:20:19.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:21.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016149909s
Jan 18 17:20:21.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:23.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015623056s
Jan 18 17:20:23.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 17:20:25.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018721486s
Jan 18 17:20:25.184: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 17:20:25.184: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 17:20:25.191: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4255" to be "running and ready"
Jan 18 17:20:25.197: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.309717ms
Jan 18 17:20:25.197: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 17:20:25.197: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 17:20:25.205
Jan 18 17:20:25.225: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4255" to be "running"
Jan 18 17:20:25.231: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665913ms
Jan 18 17:20:27.240: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014775659s
Jan 18 17:20:27.240: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 17:20:27.247: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4255" to be "running"
Jan 18 17:20:27.255: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.736987ms
Jan 18 17:20:27.255: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 18 17:20:27.262: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 17:20:27.262: INFO: Going to poll 10.100.145.138 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 17:20:27.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.145.138:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4255 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:20:27.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:20:27.270: INFO: ExecWithOptions: Clientset creation
Jan 18 17:20:27.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4255/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.145.138%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 17:20:27.381: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 18 17:20:27.381: INFO: Going to poll 10.100.158.29 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Jan 18 17:20:27.390: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.158.29:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4255 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:20:27.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:20:27.391: INFO: ExecWithOptions: Clientset creation
Jan 18 17:20:27.391: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4255/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.158.29%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 17:20:27.495: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 17:20:27.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4255" for this suite. 01/18/23 17:20:27.502
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.415 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:03.102
    Jan 18 17:20:03.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 17:20:03.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:03.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:03.127
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-4255 01/18/23 17:20:03.13
    STEP: creating a selector 01/18/23 17:20:03.13
    STEP: Creating the service pods in kubernetes 01/18/23 17:20:03.13
    Jan 18 17:20:03.130: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 17:20:03.165: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4255" to be "running and ready"
    Jan 18 17:20:03.173: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604195ms
    Jan 18 17:20:03.173: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:20:05.180: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014756138s
    Jan 18 17:20:05.180: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:07.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01671459s
    Jan 18 17:20:07.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:09.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.018324195s
    Jan 18 17:20:09.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:11.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016922931s
    Jan 18 17:20:11.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:13.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018141065s
    Jan 18 17:20:13.184: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:15.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016828771s
    Jan 18 17:20:15.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:17.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.015527963s
    Jan 18 17:20:17.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:19.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.015901348s
    Jan 18 17:20:19.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:21.182: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.016149909s
    Jan 18 17:20:21.182: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:23.181: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.015623056s
    Jan 18 17:20:23.181: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 17:20:25.184: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.018721486s
    Jan 18 17:20:25.184: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 17:20:25.184: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 17:20:25.191: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4255" to be "running and ready"
    Jan 18 17:20:25.197: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 6.309717ms
    Jan 18 17:20:25.197: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 17:20:25.197: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 17:20:25.205
    Jan 18 17:20:25.225: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4255" to be "running"
    Jan 18 17:20:25.231: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.665913ms
    Jan 18 17:20:27.240: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.014775659s
    Jan 18 17:20:27.240: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 17:20:27.247: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4255" to be "running"
    Jan 18 17:20:27.255: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 7.736987ms
    Jan 18 17:20:27.255: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 18 17:20:27.262: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 17:20:27.262: INFO: Going to poll 10.100.145.138 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 17:20:27.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.145.138:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4255 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:20:27.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:20:27.270: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:20:27.270: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4255/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.145.138%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 17:20:27.381: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 18 17:20:27.381: INFO: Going to poll 10.100.158.29 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Jan 18 17:20:27.390: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.100.158.29:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4255 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:20:27.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:20:27.391: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:20:27.391: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4255/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.100.158.29%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 17:20:27.495: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 17:20:27.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4255" for this suite. 01/18/23 17:20:27.502
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:27.521
Jan 18 17:20:27.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:20:27.522
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:27.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:27.549
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/18/23 17:20:27.554
STEP: Getting a ResourceQuota 01/18/23 17:20:27.563
STEP: Updating a ResourceQuota 01/18/23 17:20:27.573
STEP: Verifying a ResourceQuota was modified 01/18/23 17:20:27.584
STEP: Deleting a ResourceQuota 01/18/23 17:20:27.591
STEP: Verifying the deleted ResourceQuota 01/18/23 17:20:27.604
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:20:27.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1892" for this suite. 01/18/23 17:20:27.617
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":71,"skipped":1138,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:27.521
    Jan 18 17:20:27.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:20:27.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:27.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:27.549
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/18/23 17:20:27.554
    STEP: Getting a ResourceQuota 01/18/23 17:20:27.563
    STEP: Updating a ResourceQuota 01/18/23 17:20:27.573
    STEP: Verifying a ResourceQuota was modified 01/18/23 17:20:27.584
    STEP: Deleting a ResourceQuota 01/18/23 17:20:27.591
    STEP: Verifying the deleted ResourceQuota 01/18/23 17:20:27.604
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:20:27.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1892" for this suite. 01/18/23 17:20:27.617
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:27.631
Jan 18 17:20:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:20:27.633
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:27.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:27.663
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 18 17:20:27.683: INFO: Waiting up to 2m0s for pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" in namespace "var-expansion-4035" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 17:20:27.693: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110001ms
Jan 18 17:20:29.704: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020482733s
Jan 18 17:20:29.704: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 17:20:29.704: INFO: Deleting pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" in namespace "var-expansion-4035"
Jan 18 17:20:29.718: INFO: Wait up to 5m0s for pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:20:33.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4035" for this suite. 01/18/23 17:20:33.742
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":72,"skipped":1141,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.123 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:27.631
    Jan 18 17:20:27.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:20:27.633
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:27.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:27.663
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 18 17:20:27.683: INFO: Waiting up to 2m0s for pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" in namespace "var-expansion-4035" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 17:20:27.693: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110001ms
    Jan 18 17:20:29.704: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020482733s
    Jan 18 17:20:29.704: INFO: Pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 17:20:29.704: INFO: Deleting pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" in namespace "var-expansion-4035"
    Jan 18 17:20:29.718: INFO: Wait up to 5m0s for pod "var-expansion-e2b89079-ad13-4679-b73e-bdd63076018a" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:20:33.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4035" for this suite. 01/18/23 17:20:33.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:33.756
Jan 18 17:20:33.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:20:33.757
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:33.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:33.783
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-1d486001-4abc-4831-b3e6-b1d0bb9d5b11 01/18/23 17:20:33.788
STEP: Creating a pod to test consume configMaps 01/18/23 17:20:33.796
Jan 18 17:20:33.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632" in namespace "configmap-6466" to be "Succeeded or Failed"
Jan 18 17:20:33.825: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Pending", Reason="", readiness=false. Elapsed: 10.407272ms
Jan 18 17:20:35.835: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020325568s
Jan 18 17:20:37.833: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018726816s
STEP: Saw pod success 01/18/23 17:20:37.834
Jan 18 17:20:37.834: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632" satisfied condition "Succeeded or Failed"
Jan 18 17:20:37.842: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:20:37.857
Jan 18 17:20:37.876: INFO: Waiting for pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 to disappear
Jan 18 17:20:37.883: INFO: Pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:20:37.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6466" for this suite. 01/18/23 17:20:37.893
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":73,"skipped":1162,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:33.756
    Jan 18 17:20:33.756: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:20:33.757
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:33.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:33.783
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-1d486001-4abc-4831-b3e6-b1d0bb9d5b11 01/18/23 17:20:33.788
    STEP: Creating a pod to test consume configMaps 01/18/23 17:20:33.796
    Jan 18 17:20:33.815: INFO: Waiting up to 5m0s for pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632" in namespace "configmap-6466" to be "Succeeded or Failed"
    Jan 18 17:20:33.825: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Pending", Reason="", readiness=false. Elapsed: 10.407272ms
    Jan 18 17:20:35.835: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020325568s
    Jan 18 17:20:37.833: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018726816s
    STEP: Saw pod success 01/18/23 17:20:37.834
    Jan 18 17:20:37.834: INFO: Pod "pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632" satisfied condition "Succeeded or Failed"
    Jan 18 17:20:37.842: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:20:37.857
    Jan 18 17:20:37.876: INFO: Waiting for pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 to disappear
    Jan 18 17:20:37.883: INFO: Pod pod-configmaps-bc4db0d9-2149-482e-b14c-ec5d9806f632 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:20:37.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6466" for this suite. 01/18/23 17:20:37.893
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:37.911
Jan 18 17:20:37.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename init-container 01/18/23 17:20:37.912
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:37.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:37.944
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/18/23 17:20:37.948
Jan 18 17:20:37.948: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 17:20:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7611" for this suite. 01/18/23 17:20:41.405
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":74,"skipped":1163,"failed":0}
------------------------------
â€¢ [3.510 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:37.911
    Jan 18 17:20:37.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename init-container 01/18/23 17:20:37.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:37.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:37.944
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/18/23 17:20:37.948
    Jan 18 17:20:37.948: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 17:20:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7611" for this suite. 01/18/23 17:20:41.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:41.433
Jan 18 17:20:41.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption 01/18/23 17:20:41.434
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:41.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:41.463
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/18/23 17:20:41.468
STEP: Waiting for the pdb to be processed 01/18/23 17:20:41.477
STEP: updating the pdb 01/18/23 17:20:43.491
STEP: Waiting for the pdb to be processed 01/18/23 17:20:43.508
STEP: patching the pdb 01/18/23 17:20:45.523
STEP: Waiting for the pdb to be processed 01/18/23 17:20:45.547
STEP: Waiting for the pdb to be deleted 01/18/23 17:20:47.575
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 17:20:47.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2750" for this suite. 01/18/23 17:20:47.593
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":75,"skipped":1206,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.173 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:41.433
    Jan 18 17:20:41.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption 01/18/23 17:20:41.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:41.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:41.463
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/18/23 17:20:41.468
    STEP: Waiting for the pdb to be processed 01/18/23 17:20:41.477
    STEP: updating the pdb 01/18/23 17:20:43.491
    STEP: Waiting for the pdb to be processed 01/18/23 17:20:43.508
    STEP: patching the pdb 01/18/23 17:20:45.523
    STEP: Waiting for the pdb to be processed 01/18/23 17:20:45.547
    STEP: Waiting for the pdb to be deleted 01/18/23 17:20:47.575
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 17:20:47.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2750" for this suite. 01/18/23 17:20:47.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:20:47.609
Jan 18 17:20:47.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:20:47.61
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:47.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:47.642
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jan 18 17:20:47.663: INFO: Waiting up to 5m0s for pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d" in namespace "container-probe-6892" to be "running and ready"
Jan 18 17:20:47.673: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635937ms
Jan 18 17:20:47.673: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:20:49.682: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 2.018027432s
Jan 18 17:20:49.682: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:20:51.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 4.017415541s
Jan 18 17:20:51.681: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:20:53.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 6.019929767s
Jan 18 17:20:53.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:20:55.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 8.019271266s
Jan 18 17:20:55.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:20:57.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 10.017627804s
Jan 18 17:20:57.681: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:20:59.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 12.019611136s
Jan 18 17:20:59.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:21:01.686: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 14.02224809s
Jan 18 17:21:01.686: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:21:03.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 16.017987884s
Jan 18 17:21:03.682: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:21:05.684: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 18.020983188s
Jan 18 17:21:05.685: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:21:07.684: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 20.02029072s
Jan 18 17:21:07.684: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
Jan 18 17:21:09.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=true. Elapsed: 22.019332468s
Jan 18 17:21:09.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = true)
Jan 18 17:21:09.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d" satisfied condition "running and ready"
Jan 18 17:21:09.690: INFO: Container started at 2023-01-18 17:20:48 +0000 UTC, pod became ready at 2023-01-18 17:21:08 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:21:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6892" for this suite. 01/18/23 17:21:09.699
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":76,"skipped":1223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.103 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:20:47.609
    Jan 18 17:20:47.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:20:47.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:20:47.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:20:47.642
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jan 18 17:20:47.663: INFO: Waiting up to 5m0s for pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d" in namespace "container-probe-6892" to be "running and ready"
    Jan 18 17:20:47.673: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.635937ms
    Jan 18 17:20:47.673: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:20:49.682: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 2.018027432s
    Jan 18 17:20:49.682: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:20:51.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 4.017415541s
    Jan 18 17:20:51.681: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:20:53.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 6.019929767s
    Jan 18 17:20:53.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:20:55.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 8.019271266s
    Jan 18 17:20:55.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:20:57.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 10.017627804s
    Jan 18 17:20:57.681: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:20:59.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 12.019611136s
    Jan 18 17:20:59.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:21:01.686: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 14.02224809s
    Jan 18 17:21:01.686: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:21:03.681: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 16.017987884s
    Jan 18 17:21:03.682: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:21:05.684: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 18.020983188s
    Jan 18 17:21:05.685: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:21:07.684: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=false. Elapsed: 20.02029072s
    Jan 18 17:21:07.684: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = false)
    Jan 18 17:21:09.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d": Phase="Running", Reason="", readiness=true. Elapsed: 22.019332468s
    Jan 18 17:21:09.683: INFO: The phase of Pod test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d is Running (Ready = true)
    Jan 18 17:21:09.683: INFO: Pod "test-webserver-460a0bf3-8ba0-4878-8205-a57691a6233d" satisfied condition "running and ready"
    Jan 18 17:21:09.690: INFO: Container started at 2023-01-18 17:20:48 +0000 UTC, pod became ready at 2023-01-18 17:21:08 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:21:09.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6892" for this suite. 01/18/23 17:21:09.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:21:09.712
Jan 18 17:21:09.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:21:09.714
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:09.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:09.747
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:21:09.753
Jan 18 17:21:09.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b" in namespace "projected-3356" to be "Succeeded or Failed"
Jan 18 17:21:09.778: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868935ms
Jan 18 17:21:11.787: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01555449s
Jan 18 17:21:13.785: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01325269s
STEP: Saw pod success 01/18/23 17:21:13.785
Jan 18 17:21:13.785: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b" satisfied condition "Succeeded or Failed"
Jan 18 17:21:13.793: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b container client-container: <nil>
STEP: delete the pod 01/18/23 17:21:13.809
Jan 18 17:21:13.832: INFO: Waiting for pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b to disappear
Jan 18 17:21:13.840: INFO: Pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:21:13.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3356" for this suite. 01/18/23 17:21:13.846
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":77,"skipped":1236,"failed":0}
------------------------------
â€¢ [4.146 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:21:09.712
    Jan 18 17:21:09.713: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:21:09.714
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:09.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:09.747
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:21:09.753
    Jan 18 17:21:09.772: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b" in namespace "projected-3356" to be "Succeeded or Failed"
    Jan 18 17:21:09.778: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868935ms
    Jan 18 17:21:11.787: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01555449s
    Jan 18 17:21:13.785: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01325269s
    STEP: Saw pod success 01/18/23 17:21:13.785
    Jan 18 17:21:13.785: INFO: Pod "downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b" satisfied condition "Succeeded or Failed"
    Jan 18 17:21:13.793: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b container client-container: <nil>
    STEP: delete the pod 01/18/23 17:21:13.809
    Jan 18 17:21:13.832: INFO: Waiting for pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b to disappear
    Jan 18 17:21:13.840: INFO: Pod downwardapi-volume-27b8b740-9efc-44e9-8954-beb57575129b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:21:13.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3356" for this suite. 01/18/23 17:21:13.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:21:13.861
Jan 18 17:21:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename certificates 01/18/23 17:21:13.862
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:13.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:13.893
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/18/23 17:21:14.368
STEP: getting /apis/certificates.k8s.io 01/18/23 17:21:14.374
STEP: getting /apis/certificates.k8s.io/v1 01/18/23 17:21:14.377
STEP: creating 01/18/23 17:21:14.379
STEP: getting 01/18/23 17:21:14.414
STEP: listing 01/18/23 17:21:14.421
STEP: watching 01/18/23 17:21:14.429
Jan 18 17:21:14.429: INFO: starting watch
STEP: patching 01/18/23 17:21:14.431
STEP: updating 01/18/23 17:21:14.443
Jan 18 17:21:14.458: INFO: waiting for watch events with expected annotations
Jan 18 17:21:14.458: INFO: saw patched and updated annotations
STEP: getting /approval 01/18/23 17:21:14.458
STEP: patching /approval 01/18/23 17:21:14.464
STEP: updating /approval 01/18/23 17:21:14.477
STEP: getting /status 01/18/23 17:21:14.486
STEP: patching /status 01/18/23 17:21:14.495
STEP: updating /status 01/18/23 17:21:14.508
STEP: deleting 01/18/23 17:21:14.521
STEP: deleting a collection 01/18/23 17:21:14.544
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:21:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6592" for this suite. 01/18/23 17:21:14.583
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":78,"skipped":1257,"failed":0}
------------------------------
â€¢ [0.737 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:21:13.861
    Jan 18 17:21:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename certificates 01/18/23 17:21:13.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:13.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:13.893
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/18/23 17:21:14.368
    STEP: getting /apis/certificates.k8s.io 01/18/23 17:21:14.374
    STEP: getting /apis/certificates.k8s.io/v1 01/18/23 17:21:14.377
    STEP: creating 01/18/23 17:21:14.379
    STEP: getting 01/18/23 17:21:14.414
    STEP: listing 01/18/23 17:21:14.421
    STEP: watching 01/18/23 17:21:14.429
    Jan 18 17:21:14.429: INFO: starting watch
    STEP: patching 01/18/23 17:21:14.431
    STEP: updating 01/18/23 17:21:14.443
    Jan 18 17:21:14.458: INFO: waiting for watch events with expected annotations
    Jan 18 17:21:14.458: INFO: saw patched and updated annotations
    STEP: getting /approval 01/18/23 17:21:14.458
    STEP: patching /approval 01/18/23 17:21:14.464
    STEP: updating /approval 01/18/23 17:21:14.477
    STEP: getting /status 01/18/23 17:21:14.486
    STEP: patching /status 01/18/23 17:21:14.495
    STEP: updating /status 01/18/23 17:21:14.508
    STEP: deleting 01/18/23 17:21:14.521
    STEP: deleting a collection 01/18/23 17:21:14.544
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:21:14.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-6592" for this suite. 01/18/23 17:21:14.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:21:14.599
Jan 18 17:21:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:21:14.6
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:14.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:14.632
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-8066 01/18/23 17:21:14.637
Jan 18 17:21:14.652: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8066" to be "running and ready"
Jan 18 17:21:14.662: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.778949ms
Jan 18 17:21:14.662: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:21:16.669: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.017420278s
Jan 18 17:21:16.669: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 18 17:21:16.669: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 18 17:21:16.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 18 17:21:16.891: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
Jan 18 17:21:16.891: INFO: stdout: "iptables"
Jan 18 17:21:16.891: INFO: proxyMode: iptables
Jan 18 17:21:16.913: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 18 17:21:16.920: INFO: Pod kube-proxy-mode-detector no longer exists
STEP: creating service affinity-clusterip-timeout in namespace services-8066 01/18/23 17:21:16.92
STEP: creating replication controller affinity-clusterip-timeout in namespace services-8066 01/18/23 17:21:16.944
I0118 17:21:16.953627      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8066, replica count: 3
I0118 17:21:20.004048      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:21:20.018: INFO: Creating new exec pod
Jan 18 17:21:20.030: INFO: Waiting up to 5m0s for pod "execpod-affinity95rq9" in namespace "services-8066" to be "running"
Jan 18 17:21:20.038: INFO: Pod "execpod-affinity95rq9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.20967ms
Jan 18 17:21:22.047: INFO: Pod "execpod-affinity95rq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017443399s
Jan 18 17:21:22.047: INFO: Pod "execpod-affinity95rq9" satisfied condition "running"
Jan 18 17:21:23.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 18 17:21:23.255: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 18 17:21:23.255: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:21:23.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.142.230 80'
Jan 18 17:21:23.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.142.230 80\nConnection to 10.96.142.230 80 port [tcp/http] succeeded!\n"
Jan 18 17:21:23.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:21:23.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.142.230:80/ ; done'
Jan 18 17:21:23.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
Jan 18 17:21:23.739: INFO: stdout: "\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq"
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
Jan 18 17:21:23.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
Jan 18 17:21:23.954: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
Jan 18 17:21:23.954: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
Jan 18 17:21:43.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
Jan 18 17:21:44.153: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
Jan 18 17:21:44.153: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
Jan 18 17:22:04.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
Jan 18 17:22:04.367: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
Jan 18 17:22:04.367: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
Jan 18 17:22:24.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
Jan 18 17:22:24.568: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
Jan 18 17:22:24.568: INFO: stdout: "affinity-clusterip-timeout-dfpwx"
Jan 18 17:22:24.568: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8066, will wait for the garbage collector to delete the pods 01/18/23 17:22:24.594
Jan 18 17:22:24.663: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.943251ms
Jan 18 17:22:24.763: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.334698ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:22:26.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8066" for this suite. 01/18/23 17:22:27.002
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":79,"skipped":1262,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.415 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:21:14.599
    Jan 18 17:21:14.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:21:14.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:21:14.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:21:14.632
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-8066 01/18/23 17:21:14.637
    Jan 18 17:21:14.652: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-8066" to be "running and ready"
    Jan 18 17:21:14.662: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 9.778949ms
    Jan 18 17:21:14.662: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:21:16.669: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.017420278s
    Jan 18 17:21:16.669: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 18 17:21:16.669: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 18 17:21:16.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 18 17:21:16.891: INFO: stderr: "+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode\n"
    Jan 18 17:21:16.891: INFO: stdout: "iptables"
    Jan 18 17:21:16.891: INFO: proxyMode: iptables
    Jan 18 17:21:16.913: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 18 17:21:16.920: INFO: Pod kube-proxy-mode-detector no longer exists
    STEP: creating service affinity-clusterip-timeout in namespace services-8066 01/18/23 17:21:16.92
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-8066 01/18/23 17:21:16.944
    I0118 17:21:16.953627      21 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-8066, replica count: 3
    I0118 17:21:20.004048      21 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:21:20.018: INFO: Creating new exec pod
    Jan 18 17:21:20.030: INFO: Waiting up to 5m0s for pod "execpod-affinity95rq9" in namespace "services-8066" to be "running"
    Jan 18 17:21:20.038: INFO: Pod "execpod-affinity95rq9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.20967ms
    Jan 18 17:21:22.047: INFO: Pod "execpod-affinity95rq9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017443399s
    Jan 18 17:21:22.047: INFO: Pod "execpod-affinity95rq9" satisfied condition "running"
    Jan 18 17:21:23.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 18 17:21:23.255: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 18 17:21:23.255: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:21:23.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.142.230 80'
    Jan 18 17:21:23.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.142.230 80\nConnection to 10.96.142.230 80 port [tcp/http] succeeded!\n"
    Jan 18 17:21:23.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:21:23.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.142.230:80/ ; done'
    Jan 18 17:21:23.739: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
    Jan 18 17:21:23.739: INFO: stdout: "\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq\naffinity-clusterip-timeout-2cvnq"
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Received response from host: affinity-clusterip-timeout-2cvnq
    Jan 18 17:21:23.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
    Jan 18 17:21:23.954: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
    Jan 18 17:21:23.954: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
    Jan 18 17:21:43.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
    Jan 18 17:21:44.153: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
    Jan 18 17:21:44.153: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
    Jan 18 17:22:04.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
    Jan 18 17:22:04.367: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
    Jan 18 17:22:04.367: INFO: stdout: "affinity-clusterip-timeout-2cvnq"
    Jan 18 17:22:24.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8066 exec execpod-affinity95rq9 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.96.142.230:80/'
    Jan 18 17:22:24.568: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.96.142.230:80/\n"
    Jan 18 17:22:24.568: INFO: stdout: "affinity-clusterip-timeout-dfpwx"
    Jan 18 17:22:24.568: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-8066, will wait for the garbage collector to delete the pods 01/18/23 17:22:24.594
    Jan 18 17:22:24.663: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 11.943251ms
    Jan 18 17:22:24.763: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.334698ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:22:26.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8066" for this suite. 01/18/23 17:22:27.002
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:27.015
Jan 18 17:22:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 17:22:27.016
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:27.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:27.047
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/18/23 17:22:27.059
STEP: Verify that the required pods have come up. 01/18/23 17:22:27.067
Jan 18 17:22:27.077: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 17:22:32.088: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 17:22:32.088
STEP: Getting /status 01/18/23 17:22:32.089
Jan 18 17:22:32.096: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/18/23 17:22:32.096
Jan 18 17:22:32.110: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/18/23 17:22:32.11
Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: ADDED
Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.114: INFO: Found replicaset test-rs in namespace replicaset-8155 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 17:22:32.114: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/18/23 17:22:32.114
Jan 18 17:22:32.115: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 17:22:32.127: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/18/23 17:22:32.127
Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: ADDED
Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.130: INFO: Observed replicaset test-rs in namespace replicaset-8155 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
Jan 18 17:22:32.130: INFO: Found replicaset test-rs in namespace replicaset-8155 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 18 17:22:32.130: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 17:22:32.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8155" for this suite. 01/18/23 17:22:32.139
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":80,"skipped":1272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.140 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:27.015
    Jan 18 17:22:27.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 17:22:27.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:27.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:27.047
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/18/23 17:22:27.059
    STEP: Verify that the required pods have come up. 01/18/23 17:22:27.067
    Jan 18 17:22:27.077: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 17:22:32.088: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 17:22:32.088
    STEP: Getting /status 01/18/23 17:22:32.089
    Jan 18 17:22:32.096: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/18/23 17:22:32.096
    Jan 18 17:22:32.110: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/18/23 17:22:32.11
    Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.114: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.114: INFO: Found replicaset test-rs in namespace replicaset-8155 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 17:22:32.114: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/18/23 17:22:32.114
    Jan 18 17:22:32.115: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 17:22:32.127: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/18/23 17:22:32.127
    Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: ADDED
    Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.130: INFO: Observed replicaset test-rs in namespace replicaset-8155 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 17:22:32.130: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 18 17:22:32.130: INFO: Found replicaset test-rs in namespace replicaset-8155 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 18 17:22:32.130: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 17:22:32.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8155" for this suite. 01/18/23 17:22:32.139
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:32.155
Jan 18 17:22:32.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:22:32.156
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:32.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:32.185
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 18 17:22:32.211: INFO: Waiting up to 5m0s for pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b" in namespace "svcaccounts-4913" to be "running"
Jan 18 17:22:32.217: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006506ms
Jan 18 17:22:34.227: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016212825s
Jan 18 17:22:34.227: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b" satisfied condition "running"
STEP: reading a file in the container 01/18/23 17:22:34.227
Jan 18 17:22:34.227: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/18/23 17:22:34.425
Jan 18 17:22:34.426: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/18/23 17:22:34.595
Jan 18 17:22:34.595: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 18 17:22:34.797: INFO: Got root ca configmap in namespace "svcaccounts-4913"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 17:22:34.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4913" for this suite. 01/18/23 17:22:34.808
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":81,"skipped":1272,"failed":0}
------------------------------
â€¢ [2.665 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:32.155
    Jan 18 17:22:32.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:22:32.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:32.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:32.185
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 18 17:22:32.211: INFO: Waiting up to 5m0s for pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b" in namespace "svcaccounts-4913" to be "running"
    Jan 18 17:22:32.217: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006506ms
    Jan 18 17:22:34.227: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016212825s
    Jan 18 17:22:34.227: INFO: Pod "pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b" satisfied condition "running"
    STEP: reading a file in the container 01/18/23 17:22:34.227
    Jan 18 17:22:34.227: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/18/23 17:22:34.425
    Jan 18 17:22:34.426: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/18/23 17:22:34.595
    Jan 18 17:22:34.595: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4913 pod-service-account-03a9f467-d424-4b90-a651-e566fa70e97b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 18 17:22:34.797: INFO: Got root ca configmap in namespace "svcaccounts-4913"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 17:22:34.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4913" for this suite. 01/18/23 17:22:34.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:34.821
Jan 18 17:22:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:22:34.822
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:34.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:34.849
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:22:34.854
Jan 18 17:22:34.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763" in namespace "projected-8152" to be "Succeeded or Failed"
Jan 18 17:22:34.875: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459363ms
Jan 18 17:22:36.882: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013172976s
Jan 18 17:22:38.883: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01423272s
STEP: Saw pod success 01/18/23 17:22:38.883
Jan 18 17:22:38.884: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763" satisfied condition "Succeeded or Failed"
Jan 18 17:22:38.891: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 container client-container: <nil>
STEP: delete the pod 01/18/23 17:22:38.907
Jan 18 17:22:38.928: INFO: Waiting for pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 to disappear
Jan 18 17:22:38.934: INFO: Pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:22:38.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8152" for this suite. 01/18/23 17:22:38.943
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":82,"skipped":1282,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:34.821
    Jan 18 17:22:34.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:22:34.822
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:34.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:34.849
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:22:34.854
    Jan 18 17:22:34.869: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763" in namespace "projected-8152" to be "Succeeded or Failed"
    Jan 18 17:22:34.875: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Pending", Reason="", readiness=false. Elapsed: 5.459363ms
    Jan 18 17:22:36.882: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013172976s
    Jan 18 17:22:38.883: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01423272s
    STEP: Saw pod success 01/18/23 17:22:38.883
    Jan 18 17:22:38.884: INFO: Pod "downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763" satisfied condition "Succeeded or Failed"
    Jan 18 17:22:38.891: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:22:38.907
    Jan 18 17:22:38.928: INFO: Waiting for pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 to disappear
    Jan 18 17:22:38.934: INFO: Pod downwardapi-volume-f3d95c5f-d12b-4c81-8f38-3c87e1a15763 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:22:38.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8152" for this suite. 01/18/23 17:22:38.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:38.955
Jan 18 17:22:38.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:22:38.956
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:38.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:38.987
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:22:38.992
Jan 18 17:22:39.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81" in namespace "downward-api-482" to be "Succeeded or Failed"
Jan 18 17:22:39.015: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Pending", Reason="", readiness=false. Elapsed: 7.964708ms
Jan 18 17:22:41.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016275384s
Jan 18 17:22:43.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016333262s
STEP: Saw pod success 01/18/23 17:22:43.024
Jan 18 17:22:43.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81" satisfied condition "Succeeded or Failed"
Jan 18 17:22:43.031: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 container client-container: <nil>
STEP: delete the pod 01/18/23 17:22:43.047
Jan 18 17:22:43.075: INFO: Waiting for pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 to disappear
Jan 18 17:22:43.081: INFO: Pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 17:22:43.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-482" for this suite. 01/18/23 17:22:43.089
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":83,"skipped":1290,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:38.955
    Jan 18 17:22:38.955: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:22:38.956
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:38.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:38.987
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:22:38.992
    Jan 18 17:22:39.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81" in namespace "downward-api-482" to be "Succeeded or Failed"
    Jan 18 17:22:39.015: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Pending", Reason="", readiness=false. Elapsed: 7.964708ms
    Jan 18 17:22:41.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016275384s
    Jan 18 17:22:43.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016333262s
    STEP: Saw pod success 01/18/23 17:22:43.024
    Jan 18 17:22:43.024: INFO: Pod "downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81" satisfied condition "Succeeded or Failed"
    Jan 18 17:22:43.031: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:22:43.047
    Jan 18 17:22:43.075: INFO: Waiting for pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 to disappear
    Jan 18 17:22:43.081: INFO: Pod downwardapi-volume-2ef708d1-a20a-46d1-981f-360469446d81 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 17:22:43.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-482" for this suite. 01/18/23 17:22:43.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:43.104
Jan 18 17:22:43.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:22:43.105
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:43.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:43.131
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:22:43.136
Jan 18 17:22:43.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-83 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 18 17:22:43.242: INFO: stderr: ""
Jan 18 17:22:43.243: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 17:22:43.243
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 18 17:22:43.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-83 delete pods e2e-test-httpd-pod'
Jan 18 17:22:45.778: INFO: stderr: ""
Jan 18 17:22:45.778: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:22:45.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-83" for this suite. 01/18/23 17:22:45.788
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":84,"skipped":1315,"failed":0}
------------------------------
â€¢ [2.696 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:43.104
    Jan 18 17:22:43.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:22:43.105
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:43.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:43.131
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:22:43.136
    Jan 18 17:22:43.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-83 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 18 17:22:43.242: INFO: stderr: ""
    Jan 18 17:22:43.243: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 17:22:43.243
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 18 17:22:43.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-83 delete pods e2e-test-httpd-pod'
    Jan 18 17:22:45.778: INFO: stderr: ""
    Jan 18 17:22:45.778: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:22:45.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-83" for this suite. 01/18/23 17:22:45.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:45.8
Jan 18 17:22:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:22:45.801
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:45.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:45.829
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/18/23 17:22:45.834
Jan 18 17:22:45.849: INFO: Waiting up to 5m0s for pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7" in namespace "downward-api-4129" to be "Succeeded or Failed"
Jan 18 17:22:45.855: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714804ms
Jan 18 17:22:47.864: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015082364s
Jan 18 17:22:49.863: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014084783s
STEP: Saw pod success 01/18/23 17:22:49.863
Jan 18 17:22:49.863: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7" satisfied condition "Succeeded or Failed"
Jan 18 17:22:49.871: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:22:49.886
Jan 18 17:22:49.908: INFO: Waiting for pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 to disappear
Jan 18 17:22:49.913: INFO: Pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 17:22:49.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4129" for this suite. 01/18/23 17:22:49.922
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":85,"skipped":1323,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:45.8
    Jan 18 17:22:45.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:22:45.801
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:45.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:45.829
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/18/23 17:22:45.834
    Jan 18 17:22:45.849: INFO: Waiting up to 5m0s for pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7" in namespace "downward-api-4129" to be "Succeeded or Failed"
    Jan 18 17:22:45.855: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714804ms
    Jan 18 17:22:47.864: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015082364s
    Jan 18 17:22:49.863: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014084783s
    STEP: Saw pod success 01/18/23 17:22:49.863
    Jan 18 17:22:49.863: INFO: Pod "downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7" satisfied condition "Succeeded or Failed"
    Jan 18 17:22:49.871: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:22:49.886
    Jan 18 17:22:49.908: INFO: Waiting for pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 to disappear
    Jan 18 17:22:49.913: INFO: Pod downward-api-fba4e785-0506-4ae1-a934-2752cc4fdda7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 17:22:49.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4129" for this suite. 01/18/23 17:22:49.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:49.935
Jan 18 17:22:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 17:22:49.936
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:49.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:49.965
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 18 17:22:49.969: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan 18 17:22:49.986: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 17:22:54.995: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 17:22:54.995
Jan 18 17:22:54.995: INFO: Creating deployment "test-rolling-update-deployment"
Jan 18 17:22:55.004: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 18 17:22:55.016: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 18 17:22:57.034: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 18 17:22:57.041: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 17:22:57.063: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3177  1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 2631844604 1 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029dcc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 17:22:55 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 17:22:56 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 17:22:57.069: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3177  f8b3cf0b-6c2e-429e-a9d1-78f054119a62 2631844594 1 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 0xc004d41db7 0xc004d41db8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f9b4705-abcc-4129-a515-eb0a5d2d6eb6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d41e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 17:22:57.069: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 18 17:22:57.069: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3177  5fb21e8e-3b0c-47df-b0d6-bb5aac1f4257 2631844603 2 2023-01-18 17:22:49 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 0xc004d41c87 0xc004d41c88}] [] [{e2e.test Update apps/v1 2023-01-18 17:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f9b4705-abcc-4129-a515-eb0a5d2d6eb6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d41d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 17:22:57.077: INFO: Pod "test-rolling-update-deployment-78f575d8ff-jnshs" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-jnshs test-rolling-update-deployment-78f575d8ff- deployment-3177  273dd8cd-a344-4a37-86de-549dcf7b5078 2631844593 0 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:027418191e0a2f3c23d8a8a5730ae0daa8d5d14542ed2b31890183803032ec82 cni.projectcalico.org/podIP:10.100.145.153/32 cni.projectcalico.org/podIPs:10.100.145.153/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f8b3cf0b-6c2e-429e-a9d1-78f054119a62 0xc0032b88a7 0xc0032b88a8}] [] [{calico Update v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8b3cf0b-6c2e-429e-a9d1-78f054119a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5b84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5b84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.153,StartTime:2023-01-18 17:22:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:22:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://66f6505c16ea2172e941364d5a1e77007395aff4b43cffa924100e06d7a914ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 17:22:57.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3177" for this suite. 01/18/23 17:22:57.086
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":86,"skipped":1345,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.164 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:49.935
    Jan 18 17:22:49.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 17:22:49.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:49.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:49.965
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 18 17:22:49.969: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan 18 17:22:49.986: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 17:22:54.995: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 17:22:54.995
    Jan 18 17:22:54.995: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 18 17:22:55.004: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 18 17:22:55.016: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 18 17:22:57.034: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 18 17:22:57.041: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 17:22:57.063: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-3177  1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 2631844604 1 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0029dcc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 17:22:55 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-18 17:22:56 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 17:22:57.069: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-3177  f8b3cf0b-6c2e-429e-a9d1-78f054119a62 2631844594 1 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 0xc004d41db7 0xc004d41db8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f9b4705-abcc-4129-a515-eb0a5d2d6eb6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d41e68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 17:22:57.069: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 18 17:22:57.069: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-3177  5fb21e8e-3b0c-47df-b0d6-bb5aac1f4257 2631844603 2 2023-01-18 17:22:49 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 1f9b4705-abcc-4129-a515-eb0a5d2d6eb6 0xc004d41c87 0xc004d41c88}] [] [{e2e.test Update apps/v1 2023-01-18 17:22:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1f9b4705-abcc-4129-a515-eb0a5d2d6eb6\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d41d48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 17:22:57.077: INFO: Pod "test-rolling-update-deployment-78f575d8ff-jnshs" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-jnshs test-rolling-update-deployment-78f575d8ff- deployment-3177  273dd8cd-a344-4a37-86de-549dcf7b5078 2631844593 0 2023-01-18 17:22:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:027418191e0a2f3c23d8a8a5730ae0daa8d5d14542ed2b31890183803032ec82 cni.projectcalico.org/podIP:10.100.145.153/32 cni.projectcalico.org/podIPs:10.100.145.153/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff f8b3cf0b-6c2e-429e-a9d1-78f054119a62 0xc0032b88a7 0xc0032b88a8}] [] [{calico Update v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 17:22:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f8b3cf0b-6c2e-429e-a9d1-78f054119a62\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:22:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5b84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5b84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:22:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.153,StartTime:2023-01-18 17:22:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:22:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://66f6505c16ea2172e941364d5a1e77007395aff4b43cffa924100e06d7a914ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 17:22:57.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3177" for this suite. 01/18/23 17:22:57.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:22:57.102
Jan 18 17:22:57.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:22:57.103
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:57.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:57.129
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 17:22:57.164
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:22:57.174
Jan 18 17:22:57.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:22:57.188: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:22:58.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:22:58.262: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:22:59.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:22:59.206: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 01/18/23 17:22:59.215
Jan 18 17:22:59.221: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/18/23 17:22:59.221
Jan 18 17:22:59.239: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/18/23 17:22:59.239
Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: ADDED
Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.244: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.244: INFO: Found daemon set daemon-set in namespace daemonsets-1812 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 17:22:59.244: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/18/23 17:22:59.244
STEP: watching for the daemon set status to be patched 01/18/23 17:22:59.255
Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: ADDED
Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.258: INFO: Observed daemon set daemon-set in namespace daemonsets-1812 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 17:22:59.259: INFO: Observed &DaemonSet event: MODIFIED
Jan 18 17:22:59.259: INFO: Found daemon set daemon-set in namespace daemonsets-1812 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 18 17:22:59.259: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:22:59.267
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1812, will wait for the garbage collector to delete the pods 01/18/23 17:22:59.267
Jan 18 17:22:59.338: INFO: Deleting DaemonSet.extensions daemon-set took: 13.642301ms
Jan 18 17:22:59.438: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.127026ms
Jan 18 17:23:02.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:23:02.044: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 17:23:02.049: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631844889"},"items":null}

Jan 18 17:23:02.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631844889"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:23:02.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1812" for this suite. 01/18/23 17:23:02.084
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":87,"skipped":1373,"failed":0}
------------------------------
â€¢ [4.995 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:22:57.102
    Jan 18 17:22:57.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:22:57.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:22:57.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:22:57.129
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 17:22:57.164
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:22:57.174
    Jan 18 17:22:57.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:22:57.188: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:22:58.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:22:58.262: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:22:59.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:22:59.206: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 01/18/23 17:22:59.215
    Jan 18 17:22:59.221: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/18/23 17:22:59.221
    Jan 18 17:22:59.239: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/18/23 17:22:59.239
    Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: ADDED
    Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.243: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.244: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.244: INFO: Found daemon set daemon-set in namespace daemonsets-1812 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 17:22:59.244: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/18/23 17:22:59.244
    STEP: watching for the daemon set status to be patched 01/18/23 17:22:59.255
    Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: ADDED
    Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.258: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.258: INFO: Observed daemon set daemon-set in namespace daemonsets-1812 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 17:22:59.259: INFO: Observed &DaemonSet event: MODIFIED
    Jan 18 17:22:59.259: INFO: Found daemon set daemon-set in namespace daemonsets-1812 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 18 17:22:59.259: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:22:59.267
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1812, will wait for the garbage collector to delete the pods 01/18/23 17:22:59.267
    Jan 18 17:22:59.338: INFO: Deleting DaemonSet.extensions daemon-set took: 13.642301ms
    Jan 18 17:22:59.438: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.127026ms
    Jan 18 17:23:02.044: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:23:02.044: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 17:23:02.049: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631844889"},"items":null}

    Jan 18 17:23:02.056: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631844889"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:23:02.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1812" for this suite. 01/18/23 17:23:02.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:02.098
Jan 18 17:23:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:23:02.099
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:02.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:02.13
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:23:02.134
Jan 18 17:23:02.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a" in namespace "projected-8144" to be "Succeeded or Failed"
Jan 18 17:23:02.152: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698269ms
Jan 18 17:23:04.161: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893499s
Jan 18 17:23:06.160: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012588945s
STEP: Saw pod success 01/18/23 17:23:06.16
Jan 18 17:23:06.160: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a" satisfied condition "Succeeded or Failed"
Jan 18 17:23:06.168: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a container client-container: <nil>
STEP: delete the pod 01/18/23 17:23:06.185
Jan 18 17:23:06.207: INFO: Waiting for pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a to disappear
Jan 18 17:23:06.213: INFO: Pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:23:06.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8144" for this suite. 01/18/23 17:23:06.221
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":88,"skipped":1392,"failed":0}
------------------------------
â€¢ [4.135 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:02.098
    Jan 18 17:23:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:23:02.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:02.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:02.13
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:23:02.134
    Jan 18 17:23:02.148: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a" in namespace "projected-8144" to be "Succeeded or Failed"
    Jan 18 17:23:02.152: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698269ms
    Jan 18 17:23:04.161: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012893499s
    Jan 18 17:23:06.160: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012588945s
    STEP: Saw pod success 01/18/23 17:23:06.16
    Jan 18 17:23:06.160: INFO: Pod "downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a" satisfied condition "Succeeded or Failed"
    Jan 18 17:23:06.168: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a container client-container: <nil>
    STEP: delete the pod 01/18/23 17:23:06.185
    Jan 18 17:23:06.207: INFO: Waiting for pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a to disappear
    Jan 18 17:23:06.213: INFO: Pod downwardapi-volume-4f4872eb-700d-4fe9-912e-3aea76980e0a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:23:06.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8144" for this suite. 01/18/23 17:23:06.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:06.233
Jan 18 17:23:06.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename namespaces 01/18/23 17:23:06.234
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.263
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/18/23 17:23:06.268
Jan 18 17:23:06.275: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/18/23 17:23:06.275
Jan 18 17:23:06.285: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/18/23 17:23:06.286
Jan 18 17:23:06.301: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:23:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1145" for this suite. 01/18/23 17:23:06.31
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":89,"skipped":1397,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:06.233
    Jan 18 17:23:06.233: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename namespaces 01/18/23 17:23:06.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.263
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/18/23 17:23:06.268
    Jan 18 17:23:06.275: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/18/23 17:23:06.275
    Jan 18 17:23:06.285: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/18/23 17:23:06.286
    Jan 18 17:23:06.301: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:23:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1145" for this suite. 01/18/23 17:23:06.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:06.326
Jan 18 17:23:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 17:23:06.327
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.363
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/18/23 17:23:06.369
STEP: getting /apis/storage.k8s.io 01/18/23 17:23:06.372
STEP: getting /apis/storage.k8s.io/v1 01/18/23 17:23:06.374
STEP: creating 01/18/23 17:23:06.376
STEP: watching 01/18/23 17:23:06.402
Jan 18 17:23:06.402: INFO: starting watch
STEP: getting 01/18/23 17:23:06.413
STEP: listing in namespace 01/18/23 17:23:06.419
STEP: listing across namespaces 01/18/23 17:23:06.424
STEP: patching 01/18/23 17:23:06.431
STEP: updating 01/18/23 17:23:06.439
Jan 18 17:23:06.451: INFO: waiting for watch events with expected annotations in namespace
Jan 18 17:23:06.451: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/18/23 17:23:06.451
STEP: deleting a collection 01/18/23 17:23:06.474
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 18 17:23:06.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-4970" for this suite. 01/18/23 17:23:06.51
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":90,"skipped":1440,"failed":0}
------------------------------
â€¢ [0.198 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:06.326
    Jan 18 17:23:06.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename csistoragecapacity 01/18/23 17:23:06.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.363
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/18/23 17:23:06.369
    STEP: getting /apis/storage.k8s.io 01/18/23 17:23:06.372
    STEP: getting /apis/storage.k8s.io/v1 01/18/23 17:23:06.374
    STEP: creating 01/18/23 17:23:06.376
    STEP: watching 01/18/23 17:23:06.402
    Jan 18 17:23:06.402: INFO: starting watch
    STEP: getting 01/18/23 17:23:06.413
    STEP: listing in namespace 01/18/23 17:23:06.419
    STEP: listing across namespaces 01/18/23 17:23:06.424
    STEP: patching 01/18/23 17:23:06.431
    STEP: updating 01/18/23 17:23:06.439
    Jan 18 17:23:06.451: INFO: waiting for watch events with expected annotations in namespace
    Jan 18 17:23:06.451: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/18/23 17:23:06.451
    STEP: deleting a collection 01/18/23 17:23:06.474
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 18 17:23:06.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-4970" for this suite. 01/18/23 17:23:06.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:06.525
Jan 18 17:23:06.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:23:06.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.553
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:23:06.58
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:23:06.92
STEP: Deploying the webhook pod 01/18/23 17:23:06.936
STEP: Wait for the deployment to be ready 01/18/23 17:23:06.961
Jan 18 17:23:06.984: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:23:09.005
STEP: Verifying the service has paired with the endpoint 01/18/23 17:23:09.024
Jan 18 17:23:10.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 18 17:23:10.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6018-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:23:10.554
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 17:23:10.587
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:23:13.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4648" for this suite. 01/18/23 17:23:13.245
STEP: Destroying namespace "webhook-4648-markers" for this suite. 01/18/23 17:23:13.261
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":91,"skipped":1459,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.820 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:06.525
    Jan 18 17:23:06.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:23:06.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:06.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:06.553
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:23:06.58
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:23:06.92
    STEP: Deploying the webhook pod 01/18/23 17:23:06.936
    STEP: Wait for the deployment to be ready 01/18/23 17:23:06.961
    Jan 18 17:23:06.984: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:23:09.005
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:23:09.024
    Jan 18 17:23:10.024: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 18 17:23:10.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6018-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:23:10.554
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 17:23:10.587
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:23:13.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4648" for this suite. 01/18/23 17:23:13.245
    STEP: Destroying namespace "webhook-4648-markers" for this suite. 01/18/23 17:23:13.261
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:13.346
Jan 18 17:23:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename containers 01/18/23 17:23:13.347
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:13.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:13.376
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/18/23 17:23:13.381
Jan 18 17:23:13.402: INFO: Waiting up to 5m0s for pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8" in namespace "containers-1231" to be "Succeeded or Failed"
Jan 18 17:23:13.409: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509458ms
Jan 18 17:23:15.418: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015927031s
Jan 18 17:23:17.419: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016726085s
STEP: Saw pod success 01/18/23 17:23:17.419
Jan 18 17:23:17.419: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8" satisfied condition "Succeeded or Failed"
Jan 18 17:23:17.428: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:23:17.45
Jan 18 17:23:17.471: INFO: Waiting for pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 to disappear
Jan 18 17:23:17.479: INFO: Pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 17:23:17.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1231" for this suite. 01/18/23 17:23:17.489
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":92,"skipped":1460,"failed":0}
------------------------------
â€¢ [4.156 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:13.346
    Jan 18 17:23:13.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename containers 01/18/23 17:23:13.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:13.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:13.376
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/18/23 17:23:13.381
    Jan 18 17:23:13.402: INFO: Waiting up to 5m0s for pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8" in namespace "containers-1231" to be "Succeeded or Failed"
    Jan 18 17:23:13.409: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.509458ms
    Jan 18 17:23:15.418: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015927031s
    Jan 18 17:23:17.419: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016726085s
    STEP: Saw pod success 01/18/23 17:23:17.419
    Jan 18 17:23:17.419: INFO: Pod "client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8" satisfied condition "Succeeded or Failed"
    Jan 18 17:23:17.428: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:23:17.45
    Jan 18 17:23:17.471: INFO: Waiting for pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 to disappear
    Jan 18 17:23:17.479: INFO: Pod client-containers-8d3332b9-2cb8-491d-aef2-8d0ebde63ce8 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 17:23:17.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1231" for this suite. 01/18/23 17:23:17.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:17.505
Jan 18 17:23:17.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:23:17.506
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:17.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:17.535
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/18/23 17:23:17.54
Jan 18 17:23:17.540: INFO: Creating e2e-svc-a-pt44f
Jan 18 17:23:17.565: INFO: Creating e2e-svc-b-rdvbf
Jan 18 17:23:17.586: INFO: Creating e2e-svc-c-mgr5w
STEP: deleting service collection 01/18/23 17:23:17.607
Jan 18 17:23:17.667: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:23:17.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1723" for this suite. 01/18/23 17:23:17.673
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":93,"skipped":1496,"failed":0}
------------------------------
â€¢ [0.182 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:17.505
    Jan 18 17:23:17.505: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:23:17.506
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:17.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:17.535
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/18/23 17:23:17.54
    Jan 18 17:23:17.540: INFO: Creating e2e-svc-a-pt44f
    Jan 18 17:23:17.565: INFO: Creating e2e-svc-b-rdvbf
    Jan 18 17:23:17.586: INFO: Creating e2e-svc-c-mgr5w
    STEP: deleting service collection 01/18/23 17:23:17.607
    Jan 18 17:23:17.667: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:23:17.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1723" for this suite. 01/18/23 17:23:17.673
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:17.688
Jan 18 17:23:17.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:23:17.689
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:17.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:17.714
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-65b34c31-6169-438e-8d5a-4819565de304 01/18/23 17:23:17.726
STEP: Creating the pod 01/18/23 17:23:17.734
Jan 18 17:23:17.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210" in namespace "configmap-3743" to be "running and ready"
Jan 18 17:23:17.758: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737779ms
Jan 18 17:23:17.758: INFO: The phase of Pod pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:23:19.769: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210": Phase="Running", Reason="", readiness=true. Elapsed: 2.017066994s
Jan 18 17:23:19.769: INFO: The phase of Pod pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210 is Running (Ready = true)
Jan 18 17:23:19.769: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-65b34c31-6169-438e-8d5a-4819565de304 01/18/23 17:23:19.793
STEP: waiting to observe update in volume 01/18/23 17:23:19.802
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:23:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3743" for this suite. 01/18/23 17:23:21.842
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":94,"skipped":1510,"failed":0}
------------------------------
â€¢ [4.168 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:17.688
    Jan 18 17:23:17.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:23:17.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:17.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:17.714
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-65b34c31-6169-438e-8d5a-4819565de304 01/18/23 17:23:17.726
    STEP: Creating the pod 01/18/23 17:23:17.734
    Jan 18 17:23:17.751: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210" in namespace "configmap-3743" to be "running and ready"
    Jan 18 17:23:17.758: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210": Phase="Pending", Reason="", readiness=false. Elapsed: 6.737779ms
    Jan 18 17:23:17.758: INFO: The phase of Pod pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:23:19.769: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210": Phase="Running", Reason="", readiness=true. Elapsed: 2.017066994s
    Jan 18 17:23:19.769: INFO: The phase of Pod pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210 is Running (Ready = true)
    Jan 18 17:23:19.769: INFO: Pod "pod-configmaps-bf5dc309-49bf-408c-8660-57faffa6f210" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-65b34c31-6169-438e-8d5a-4819565de304 01/18/23 17:23:19.793
    STEP: waiting to observe update in volume 01/18/23 17:23:19.802
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:23:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3743" for this suite. 01/18/23 17:23:21.842
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:21.857
Jan 18 17:23:21.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:23:21.858
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:21.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:21.889
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/18/23 17:23:21.893
STEP: Counting existing ResourceQuota 01/18/23 17:23:26.901
STEP: Creating a ResourceQuota 01/18/23 17:23:31.91
STEP: Ensuring resource quota status is calculated 01/18/23 17:23:31.92
STEP: Creating a Secret 01/18/23 17:23:33.93
STEP: Ensuring resource quota status captures secret creation 01/18/23 17:23:33.952
STEP: Deleting a secret 01/18/23 17:23:35.962
STEP: Ensuring resource quota status released usage 01/18/23 17:23:35.974
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:23:37.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1448" for this suite. 01/18/23 17:23:37.991
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":95,"skipped":1513,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.149 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:21.857
    Jan 18 17:23:21.857: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:23:21.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:21.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:21.889
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/18/23 17:23:21.893
    STEP: Counting existing ResourceQuota 01/18/23 17:23:26.901
    STEP: Creating a ResourceQuota 01/18/23 17:23:31.91
    STEP: Ensuring resource quota status is calculated 01/18/23 17:23:31.92
    STEP: Creating a Secret 01/18/23 17:23:33.93
    STEP: Ensuring resource quota status captures secret creation 01/18/23 17:23:33.952
    STEP: Deleting a secret 01/18/23 17:23:35.962
    STEP: Ensuring resource quota status released usage 01/18/23 17:23:35.974
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:23:37.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1448" for this suite. 01/18/23 17:23:37.991
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:38.006
Jan 18 17:23:38.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 17:23:38.008
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:38.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:38.038
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 17:23:38.043
Jan 18 17:23:38.060: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8023  543048b4-467d-4e37-bced-6399911aa1d6 2631846399 0 2023-01-18 17:23:38 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 17:23:38 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds6d7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds6d7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:23:38.060: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8023" to be "running and ready"
Jan 18 17:23:38.068: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.656149ms
Jan 18 17:23:38.068: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:23:40.077: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017104428s
Jan 18 17:23:40.077: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 18 17:23:40.077: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 17:23:40.078
Jan 18 17:23:40.078: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8023 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:23:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:23:40.079: INFO: ExecWithOptions: Clientset creation
Jan 18 17:23:40.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8023/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/18/23 17:23:40.195
Jan 18 17:23:40.196: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8023 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:23:40.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:23:40.196: INFO: ExecWithOptions: Clientset creation
Jan 18 17:23:40.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8023/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 18 17:23:40.318: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 17:23:40.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8023" for this suite. 01/18/23 17:23:40.346
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":96,"skipped":1516,"failed":0}
------------------------------
â€¢ [2.353 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:38.006
    Jan 18 17:23:38.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 17:23:38.008
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:38.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:38.038
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/18/23 17:23:38.043
    Jan 18 17:23:38.060: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-8023  543048b4-467d-4e37-bced-6399911aa1d6 2631846399 0 2023-01-18 17:23:38 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-18 17:23:38 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ds6d7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ds6d7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:23:38.060: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-8023" to be "running and ready"
    Jan 18 17:23:38.068: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 7.656149ms
    Jan 18 17:23:38.068: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:23:40.077: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.017104428s
    Jan 18 17:23:40.077: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 18 17:23:40.077: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/18/23 17:23:40.078
    Jan 18 17:23:40.078: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-8023 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:23:40.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:23:40.079: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:23:40.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8023/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/18/23 17:23:40.195
    Jan 18 17:23:40.196: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-8023 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:23:40.196: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:23:40.196: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:23:40.197: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-8023/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 18 17:23:40.318: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 17:23:40.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8023" for this suite. 01/18/23 17:23:40.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:40.36
Jan 18 17:23:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replication-controller 01/18/23 17:23:40.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:40.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:40.39
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 18 17:23:40.394: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 17:23:41.421
STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 17:23:41.434
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 17:23:42.449
Jan 18 17:23:42.466: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/18/23 17:23:42.466
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 17:23:43.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3351" for this suite. 01/18/23 17:23:43.489
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":97,"skipped":1526,"failed":0}
------------------------------
â€¢ [3.142 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:40.36
    Jan 18 17:23:40.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 17:23:40.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:40.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:40.39
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 18 17:23:40.394: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/18/23 17:23:41.421
    STEP: Checking rc "condition-test" has the desired failure condition set 01/18/23 17:23:41.434
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/18/23 17:23:42.449
    Jan 18 17:23:42.466: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/18/23 17:23:42.466
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 17:23:43.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3351" for this suite. 01/18/23 17:23:43.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:43.503
Jan 18 17:23:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:23:43.504
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:43.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:43.529
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:23:43.534
Jan 18 17:23:43.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 18 17:23:43.641: INFO: stderr: ""
Jan 18 17:23:43.641: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 17:23:43.641
STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 17:23:48.692
Jan 18 17:23:48.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 get pod e2e-test-httpd-pod -o json'
Jan 18 17:23:48.787: INFO: stderr: ""
Jan 18 17:23:48.787: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"afdab9399cb9ae9f5d3eede58c40c9fb36869d14cadcea04f001aa70eaa1adce\",\n            \"cni.projectcalico.org/podIP\": \"10.100.145.159/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.145.159/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T17:23:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3930\",\n        \"resourceVersion\": \"2631846657\",\n        \"uid\": \"53c56e48-dc7c-4b06-ae61-fbd1c84ed8cd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gkvdd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"scw-conformance125-default-61c39bbf4d81476a8e3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gkvdd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7dda77129aff885420ff2a43d773662c25b1b2afffce4a6f2d0f5812ccccb381\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T17:23:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.195.74.123\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.145.159\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.145.159\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T17:23:43Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/18/23 17:23:48.787
Jan 18 17:23:48.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 replace -f -'
Jan 18 17:23:50.637: INFO: stderr: ""
Jan 18 17:23:50.637: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 17:23:50.638
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 18 17:23:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 delete pods e2e-test-httpd-pod'
Jan 18 17:23:52.033: INFO: stderr: ""
Jan 18 17:23:52.033: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:23:52.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3930" for this suite. 01/18/23 17:23:52.04
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":98,"skipped":1533,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.551 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:43.503
    Jan 18 17:23:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:23:43.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:43.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:43.529
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/18/23 17:23:43.534
    Jan 18 17:23:43.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 18 17:23:43.641: INFO: stderr: ""
    Jan 18 17:23:43.641: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/18/23 17:23:43.641
    STEP: verifying the pod e2e-test-httpd-pod was created 01/18/23 17:23:48.692
    Jan 18 17:23:48.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 get pod e2e-test-httpd-pod -o json'
    Jan 18 17:23:48.787: INFO: stderr: ""
    Jan 18 17:23:48.787: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"afdab9399cb9ae9f5d3eede58c40c9fb36869d14cadcea04f001aa70eaa1adce\",\n            \"cni.projectcalico.org/podIP\": \"10.100.145.159/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.100.145.159/32\"\n        },\n        \"creationTimestamp\": \"2023-01-18T17:23:43Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3930\",\n        \"resourceVersion\": \"2631846657\",\n        \"uid\": \"53c56e48-dc7c-4b06-ae61-fbd1c84ed8cd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-gkvdd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"scw-conformance125-default-61c39bbf4d81476a8e3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-gkvdd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:43Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:44Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:44Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-18T17:23:43Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7dda77129aff885420ff2a43d773662c25b1b2afffce4a6f2d0f5812ccccb381\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-18T17:23:44Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.195.74.123\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.100.145.159\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.100.145.159\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-18T17:23:43Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/18/23 17:23:48.787
    Jan 18 17:23:48.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 replace -f -'
    Jan 18 17:23:50.637: INFO: stderr: ""
    Jan 18 17:23:50.637: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/18/23 17:23:50.638
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 18 17:23:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3930 delete pods e2e-test-httpd-pod'
    Jan 18 17:23:52.033: INFO: stderr: ""
    Jan 18 17:23:52.033: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:23:52.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3930" for this suite. 01/18/23 17:23:52.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:23:52.056
Jan 18 17:23:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:23:52.057
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:52.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:52.086
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:23:52.112
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:23:52.385
STEP: Deploying the webhook pod 01/18/23 17:23:52.403
STEP: Wait for the deployment to be ready 01/18/23 17:23:52.425
Jan 18 17:23:52.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 17:23:54.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 17:23:56.476
STEP: Verifying the service has paired with the endpoint 01/18/23 17:23:56.498
Jan 18 17:23:57.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 18 17:23:57.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5974-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:23:58.04
STEP: Creating a custom resource while v1 is storage version 01/18/23 17:23:58.068
STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 17:24:00.188
STEP: Patching the custom resource while v2 is storage version 01/18/23 17:24:00.214
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:24:00.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4386" for this suite. 01/18/23 17:24:00.871
STEP: Destroying namespace "webhook-4386-markers" for this suite. 01/18/23 17:24:01.059
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":99,"skipped":1560,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.227 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:23:52.056
    Jan 18 17:23:52.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:23:52.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:23:52.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:23:52.086
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:23:52.112
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:23:52.385
    STEP: Deploying the webhook pod 01/18/23 17:23:52.403
    STEP: Wait for the deployment to be ready 01/18/23 17:23:52.425
    Jan 18 17:23:52.439: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 17:23:54.465: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 17, 23, 52, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 17:23:56.476
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:23:56.498
    Jan 18 17:23:57.499: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 18 17:23:57.508: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5974-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:23:58.04
    STEP: Creating a custom resource while v1 is storage version 01/18/23 17:23:58.068
    STEP: Patching Custom Resource Definition to set v2 as storage 01/18/23 17:24:00.188
    STEP: Patching the custom resource while v2 is storage version 01/18/23 17:24:00.214
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:24:00.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4386" for this suite. 01/18/23 17:24:00.871
    STEP: Destroying namespace "webhook-4386-markers" for this suite. 01/18/23 17:24:01.059
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:01.285
Jan 18 17:24:01.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:24:01.286
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:01.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:01.368
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-8f71ad37-521d-4450-80fe-a4c699c13122 01/18/23 17:24:01.383
STEP: Creating configMap with name cm-test-opt-upd-f4ca80c3-ded3-4e0a-93c2-b65e68b87dab 01/18/23 17:24:01.393
STEP: Creating the pod 01/18/23 17:24:01.402
Jan 18 17:24:01.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d" in namespace "configmap-7421" to be "running and ready"
Jan 18 17:24:01.475: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649525ms
Jan 18 17:24:01.475: INFO: The phase of Pod pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:24:03.486: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017684613s
Jan 18 17:24:03.486: INFO: The phase of Pod pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d is Running (Ready = true)
Jan 18 17:24:03.486: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-8f71ad37-521d-4450-80fe-a4c699c13122 01/18/23 17:24:03.546
STEP: Updating configmap cm-test-opt-upd-f4ca80c3-ded3-4e0a-93c2-b65e68b87dab 01/18/23 17:24:03.559
STEP: Creating configMap with name cm-test-opt-create-b925e3e1-1c4b-496f-899e-049b3d90d57e 01/18/23 17:24:03.569
STEP: waiting to observe update in volume 01/18/23 17:24:03.577
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:24:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7421" for this suite. 01/18/23 17:24:05.645
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":100,"skipped":1572,"failed":0}
------------------------------
â€¢ [4.372 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:01.285
    Jan 18 17:24:01.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:24:01.286
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:01.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:01.368
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-8f71ad37-521d-4450-80fe-a4c699c13122 01/18/23 17:24:01.383
    STEP: Creating configMap with name cm-test-opt-upd-f4ca80c3-ded3-4e0a-93c2-b65e68b87dab 01/18/23 17:24:01.393
    STEP: Creating the pod 01/18/23 17:24:01.402
    Jan 18 17:24:01.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d" in namespace "configmap-7421" to be "running and ready"
    Jan 18 17:24:01.475: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649525ms
    Jan 18 17:24:01.475: INFO: The phase of Pod pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:24:03.486: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d": Phase="Running", Reason="", readiness=true. Elapsed: 2.017684613s
    Jan 18 17:24:03.486: INFO: The phase of Pod pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d is Running (Ready = true)
    Jan 18 17:24:03.486: INFO: Pod "pod-configmaps-95b56be3-ff0b-4e3b-a546-11895ee2af7d" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-8f71ad37-521d-4450-80fe-a4c699c13122 01/18/23 17:24:03.546
    STEP: Updating configmap cm-test-opt-upd-f4ca80c3-ded3-4e0a-93c2-b65e68b87dab 01/18/23 17:24:03.559
    STEP: Creating configMap with name cm-test-opt-create-b925e3e1-1c4b-496f-899e-049b3d90d57e 01/18/23 17:24:03.569
    STEP: waiting to observe update in volume 01/18/23 17:24:03.577
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:24:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7421" for this suite. 01/18/23 17:24:05.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:05.661
Jan 18 17:24:05.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:24:05.662
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:05.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:05.694
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/18/23 17:24:05.708
STEP: watching for Pod to be ready 01/18/23 17:24:05.726
Jan 18 17:24:05.728: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 18 17:24:05.736: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
Jan 18 17:24:05.757: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
Jan 18 17:24:06.369: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
Jan 18 17:24:07.058: INFO: Found Pod pod-test in namespace pods-4004 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/18/23 17:24:07.068
STEP: getting the Pod and ensuring that it's patched 01/18/23 17:24:07.087
STEP: replacing the Pod's status Ready condition to False 01/18/23 17:24:07.094
STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 17:24:07.115
STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 17:24:07.115
STEP: watching for the Pod to be deleted 01/18/23 17:24:07.134
Jan 18 17:24:07.137: INFO: observed event type MODIFIED
Jan 18 17:24:09.071: INFO: observed event type MODIFIED
Jan 18 17:24:09.280: INFO: observed event type MODIFIED
Jan 18 17:24:10.067: INFO: observed event type MODIFIED
Jan 18 17:24:10.080: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:24:10.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4004" for this suite. 01/18/23 17:24:10.1
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":101,"skipped":1637,"failed":0}
------------------------------
â€¢ [4.453 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:05.661
    Jan 18 17:24:05.661: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:24:05.662
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:05.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:05.694
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/18/23 17:24:05.708
    STEP: watching for Pod to be ready 01/18/23 17:24:05.726
    Jan 18 17:24:05.728: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 18 17:24:05.736: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
    Jan 18 17:24:05.757: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
    Jan 18 17:24:06.369: INFO: observed Pod pod-test in namespace pods-4004 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
    Jan 18 17:24:07.058: INFO: Found Pod pod-test in namespace pods-4004 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:24:05 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/18/23 17:24:07.068
    STEP: getting the Pod and ensuring that it's patched 01/18/23 17:24:07.087
    STEP: replacing the Pod's status Ready condition to False 01/18/23 17:24:07.094
    STEP: check the Pod again to ensure its Ready conditions are False 01/18/23 17:24:07.115
    STEP: deleting the Pod via a Collection with a LabelSelector 01/18/23 17:24:07.115
    STEP: watching for the Pod to be deleted 01/18/23 17:24:07.134
    Jan 18 17:24:07.137: INFO: observed event type MODIFIED
    Jan 18 17:24:09.071: INFO: observed event type MODIFIED
    Jan 18 17:24:09.280: INFO: observed event type MODIFIED
    Jan 18 17:24:10.067: INFO: observed event type MODIFIED
    Jan 18 17:24:10.080: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:24:10.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4004" for this suite. 01/18/23 17:24:10.1
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:10.114
Jan 18 17:24:10.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:24:10.115
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:10.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:10.15
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 17:24:10.155
Jan 18 17:24:10.170: INFO: Waiting up to 5m0s for pod "pod-35c76fb3-894d-42f7-913a-35c700898543" in namespace "emptydir-6956" to be "Succeeded or Failed"
Jan 18 17:24:10.176: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20639ms
Jan 18 17:24:12.184: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Running", Reason="", readiness=false. Elapsed: 2.013882648s
Jan 18 17:24:14.185: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015410773s
STEP: Saw pod success 01/18/23 17:24:14.185
Jan 18 17:24:14.185: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543" satisfied condition "Succeeded or Failed"
Jan 18 17:24:14.193: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-35c76fb3-894d-42f7-913a-35c700898543 container test-container: <nil>
STEP: delete the pod 01/18/23 17:24:14.205
Jan 18 17:24:14.229: INFO: Waiting for pod pod-35c76fb3-894d-42f7-913a-35c700898543 to disappear
Jan 18 17:24:14.236: INFO: Pod pod-35c76fb3-894d-42f7-913a-35c700898543 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:24:14.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6956" for this suite. 01/18/23 17:24:14.244
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":102,"skipped":1638,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:10.114
    Jan 18 17:24:10.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:24:10.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:10.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:10.15
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 17:24:10.155
    Jan 18 17:24:10.170: INFO: Waiting up to 5m0s for pod "pod-35c76fb3-894d-42f7-913a-35c700898543" in namespace "emptydir-6956" to be "Succeeded or Failed"
    Jan 18 17:24:10.176: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Pending", Reason="", readiness=false. Elapsed: 6.20639ms
    Jan 18 17:24:12.184: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Running", Reason="", readiness=false. Elapsed: 2.013882648s
    Jan 18 17:24:14.185: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015410773s
    STEP: Saw pod success 01/18/23 17:24:14.185
    Jan 18 17:24:14.185: INFO: Pod "pod-35c76fb3-894d-42f7-913a-35c700898543" satisfied condition "Succeeded or Failed"
    Jan 18 17:24:14.193: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-35c76fb3-894d-42f7-913a-35c700898543 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:24:14.205
    Jan 18 17:24:14.229: INFO: Waiting for pod pod-35c76fb3-894d-42f7-913a-35c700898543 to disappear
    Jan 18 17:24:14.236: INFO: Pod pod-35c76fb3-894d-42f7-913a-35c700898543 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:24:14.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6956" for this suite. 01/18/23 17:24:14.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:14.261
Jan 18 17:24:14.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename containers 01/18/23 17:24:14.262
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:14.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:14.287
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 18 17:24:14.306: INFO: Waiting up to 5m0s for pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc" in namespace "containers-5670" to be "running"
Jan 18 17:24:14.314: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.609875ms
Jan 18 17:24:16.321: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015136677s
Jan 18 17:24:16.321: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 17:24:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5670" for this suite. 01/18/23 17:24:16.346
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":103,"skipped":1658,"failed":0}
------------------------------
â€¢ [2.096 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:14.261
    Jan 18 17:24:14.261: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename containers 01/18/23 17:24:14.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:14.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:14.287
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 18 17:24:14.306: INFO: Waiting up to 5m0s for pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc" in namespace "containers-5670" to be "running"
    Jan 18 17:24:14.314: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.609875ms
    Jan 18 17:24:16.321: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015136677s
    Jan 18 17:24:16.321: INFO: Pod "client-containers-e5024adc-fea9-4e54-aad5-6a44214ec3fc" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 17:24:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-5670" for this suite. 01/18/23 17:24:16.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:16.36
Jan 18 17:24:16.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:24:16.361
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:16.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:16.391
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:24:16.418
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:24:16.733
STEP: Deploying the webhook pod 01/18/23 17:24:16.742
STEP: Wait for the deployment to be ready 01/18/23 17:24:16.762
Jan 18 17:24:16.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:24:18.798
STEP: Verifying the service has paired with the endpoint 01/18/23 17:24:18.82
Jan 18 17:24:19.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 17:24:19.832
STEP: create a namespace for the webhook 01/18/23 17:24:19.863
STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 17:24:19.878
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:24:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8902" for this suite. 01/18/23 17:24:19.949
STEP: Destroying namespace "webhook-8902-markers" for this suite. 01/18/23 17:24:19.962
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":104,"skipped":1678,"failed":0}
------------------------------
â€¢ [3.690 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:16.36
    Jan 18 17:24:16.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:24:16.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:16.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:16.391
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:24:16.418
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:24:16.733
    STEP: Deploying the webhook pod 01/18/23 17:24:16.742
    STEP: Wait for the deployment to be ready 01/18/23 17:24:16.762
    Jan 18 17:24:16.775: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:24:18.798
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:24:18.82
    Jan 18 17:24:19.824: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/18/23 17:24:19.832
    STEP: create a namespace for the webhook 01/18/23 17:24:19.863
    STEP: create a configmap should be unconditionally rejected by the webhook 01/18/23 17:24:19.878
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:24:19.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8902" for this suite. 01/18/23 17:24:19.949
    STEP: Destroying namespace "webhook-8902-markers" for this suite. 01/18/23 17:24:19.962
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:20.053
Jan 18 17:24:20.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 17:24:20.054
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:20.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:20.084
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 17:24:20.088
Jan 18 17:24:20.104: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 17:24:25.110: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 17:24:25.11
STEP: getting scale subresource 01/18/23 17:24:25.11
STEP: updating a scale subresource 01/18/23 17:24:25.115
STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 17:24:25.127
STEP: Patch a scale subresource 01/18/23 17:24:25.133
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 17:24:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7079" for this suite. 01/18/23 17:24:25.174
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":105,"skipped":1708,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.138 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:20.053
    Jan 18 17:24:20.053: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 17:24:20.054
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:20.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:20.084
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/18/23 17:24:20.088
    Jan 18 17:24:20.104: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 17:24:25.110: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 17:24:25.11
    STEP: getting scale subresource 01/18/23 17:24:25.11
    STEP: updating a scale subresource 01/18/23 17:24:25.115
    STEP: verifying the replicaset Spec.Replicas was modified 01/18/23 17:24:25.127
    STEP: Patch a scale subresource 01/18/23 17:24:25.133
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 17:24:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7079" for this suite. 01/18/23 17:24:25.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:25.193
Jan 18 17:24:25.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:24:25.194
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:25.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:25.281
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 18 17:24:25.305: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7420 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 17:24:25.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7420" for this suite. 01/18/23 17:24:25.38
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":106,"skipped":1735,"failed":0}
------------------------------
â€¢ [0.196 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:25.193
    Jan 18 17:24:25.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:24:25.194
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:25.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:25.281
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 18 17:24:25.305: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7420 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 17:24:25.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7420" for this suite. 01/18/23 17:24:25.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:25.39
Jan 18 17:24:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:24:25.391
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:25.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:25.468
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-f2a251b3-3d20-4a65-ac93-9ae693f921ee 01/18/23 17:24:25.471
STEP: Creating a pod to test consume secrets 01/18/23 17:24:25.481
Jan 18 17:24:25.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f" in namespace "projected-9025" to be "Succeeded or Failed"
Jan 18 17:24:25.499: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362719ms
Jan 18 17:24:27.508: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012676249s
Jan 18 17:24:29.509: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014171418s
STEP: Saw pod success 01/18/23 17:24:29.509
Jan 18 17:24:29.509: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f" satisfied condition "Succeeded or Failed"
Jan 18 17:24:29.517: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:24:29.533
Jan 18 17:24:29.550: INFO: Waiting for pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f to disappear
Jan 18 17:24:29.556: INFO: Pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 17:24:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9025" for this suite. 01/18/23 17:24:29.565
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":107,"skipped":1749,"failed":0}
------------------------------
â€¢ [4.188 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:25.39
    Jan 18 17:24:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:24:25.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:25.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:25.468
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-f2a251b3-3d20-4a65-ac93-9ae693f921ee 01/18/23 17:24:25.471
    STEP: Creating a pod to test consume secrets 01/18/23 17:24:25.481
    Jan 18 17:24:25.495: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f" in namespace "projected-9025" to be "Succeeded or Failed"
    Jan 18 17:24:25.499: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.362719ms
    Jan 18 17:24:27.508: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012676249s
    Jan 18 17:24:29.509: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014171418s
    STEP: Saw pod success 01/18/23 17:24:29.509
    Jan 18 17:24:29.509: INFO: Pod "pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f" satisfied condition "Succeeded or Failed"
    Jan 18 17:24:29.517: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:24:29.533
    Jan 18 17:24:29.550: INFO: Waiting for pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f to disappear
    Jan 18 17:24:29.556: INFO: Pod pod-projected-secrets-215c83be-272e-44f1-b459-fe9d9cc41b8f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 17:24:29.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9025" for this suite. 01/18/23 17:24:29.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:29.581
Jan 18 17:24:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-webhook 01/18/23 17:24:29.582
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:29.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:29.61
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/18/23 17:24:29.615
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 17:24:30.031
STEP: Deploying the custom resource conversion webhook pod 01/18/23 17:24:30.047
STEP: Wait for the deployment to be ready 01/18/23 17:24:30.068
Jan 18 17:24:30.081: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:24:32.103
STEP: Verifying the service has paired with the endpoint 01/18/23 17:24:32.127
Jan 18 17:24:33.129: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 18 17:24:33.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Creating a v1 custom resource 01/18/23 17:24:35.8
STEP: Create a v2 custom resource 01/18/23 17:24:35.825
STEP: List CRs in v1 01/18/23 17:24:35.959
STEP: List CRs in v2 01/18/23 17:24:35.97
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:24:36.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5969" for this suite. 01/18/23 17:24:36.571
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":108,"skipped":1798,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.303 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:29.581
    Jan 18 17:24:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-webhook 01/18/23 17:24:29.582
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:29.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:29.61
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/18/23 17:24:29.615
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/18/23 17:24:30.031
    STEP: Deploying the custom resource conversion webhook pod 01/18/23 17:24:30.047
    STEP: Wait for the deployment to be ready 01/18/23 17:24:30.068
    Jan 18 17:24:30.081: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:24:32.103
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:24:32.127
    Jan 18 17:24:33.129: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 18 17:24:33.143: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Creating a v1 custom resource 01/18/23 17:24:35.8
    STEP: Create a v2 custom resource 01/18/23 17:24:35.825
    STEP: List CRs in v1 01/18/23 17:24:35.959
    STEP: List CRs in v2 01/18/23 17:24:35.97
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:24:36.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5969" for this suite. 01/18/23 17:24:36.571
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:36.885
Jan 18 17:24:36.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:24:36.886
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:36.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:36.972
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-7853 01/18/23 17:24:36.978
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[] 01/18/23 17:24:36.999
Jan 18 17:24:37.058: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 18 17:24:38.074: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7853 01/18/23 17:24:38.074
Jan 18 17:24:38.090: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7853" to be "running and ready"
Jan 18 17:24:38.096: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038306ms
Jan 18 17:24:38.096: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:24:40.104: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014754741s
Jan 18 17:24:40.104: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 17:24:40.104: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod1:[100]] 01/18/23 17:24:40.113
Jan 18 17:24:40.140: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7853 01/18/23 17:24:40.14
Jan 18 17:24:40.152: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7853" to be "running and ready"
Jan 18 17:24:40.159: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540152ms
Jan 18 17:24:40.159: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:24:42.166: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014669832s
Jan 18 17:24:42.166: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 17:24:42.166: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 17:24:42.174
Jan 18 17:24:42.202: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/18/23 17:24:42.202
Jan 18 17:24:42.202: INFO: Creating new exec pod
Jan 18 17:24:42.212: INFO: Waiting up to 5m0s for pod "execpodchrnk" in namespace "services-7853" to be "running"
Jan 18 17:24:42.217: INFO: Pod "execpodchrnk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.306169ms
Jan 18 17:24:44.227: INFO: Pod "execpodchrnk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014908756s
Jan 18 17:24:44.227: INFO: Pod "execpodchrnk" satisfied condition "running"
Jan 18 17:24:45.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 18 17:24:45.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 18 17:24:45.428: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:24:45.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.24.73 80'
Jan 18 17:24:45.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.24.73 80\nConnection to 10.96.24.73 80 port [tcp/http] succeeded!\n"
Jan 18 17:24:45.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:24:45.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 18 17:24:45.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 18 17:24:45.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:24:45.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.24.73 81'
Jan 18 17:24:46.046: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.24.73 81\nConnection to 10.96.24.73 81 port [tcp/*] succeeded!\n"
Jan 18 17:24:46.046: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-7853 01/18/23 17:24:46.046
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod2:[101]] 01/18/23 17:24:46.064
Jan 18 17:24:46.085: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7853 01/18/23 17:24:46.085
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[] 01/18/23 17:24:46.103
Jan 18 17:24:46.118: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:24:46.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7853" for this suite. 01/18/23 17:24:46.164
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":109,"skipped":1815,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.291 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:36.885
    Jan 18 17:24:36.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:24:36.886
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:36.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:36.972
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-7853 01/18/23 17:24:36.978
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[] 01/18/23 17:24:36.999
    Jan 18 17:24:37.058: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 18 17:24:38.074: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7853 01/18/23 17:24:38.074
    Jan 18 17:24:38.090: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7853" to be "running and ready"
    Jan 18 17:24:38.096: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038306ms
    Jan 18 17:24:38.096: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:24:40.104: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014754741s
    Jan 18 17:24:40.104: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 17:24:40.104: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod1:[100]] 01/18/23 17:24:40.113
    Jan 18 17:24:40.140: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7853 01/18/23 17:24:40.14
    Jan 18 17:24:40.152: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7853" to be "running and ready"
    Jan 18 17:24:40.159: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540152ms
    Jan 18 17:24:40.159: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:24:42.166: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014669832s
    Jan 18 17:24:42.166: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 17:24:42.166: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod1:[100] pod2:[101]] 01/18/23 17:24:42.174
    Jan 18 17:24:42.202: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/18/23 17:24:42.202
    Jan 18 17:24:42.202: INFO: Creating new exec pod
    Jan 18 17:24:42.212: INFO: Waiting up to 5m0s for pod "execpodchrnk" in namespace "services-7853" to be "running"
    Jan 18 17:24:42.217: INFO: Pod "execpodchrnk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.306169ms
    Jan 18 17:24:44.227: INFO: Pod "execpodchrnk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014908756s
    Jan 18 17:24:44.227: INFO: Pod "execpodchrnk" satisfied condition "running"
    Jan 18 17:24:45.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 18 17:24:45.428: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 18 17:24:45.428: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:24:45.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.24.73 80'
    Jan 18 17:24:45.640: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.24.73 80\nConnection to 10.96.24.73 80 port [tcp/http] succeeded!\n"
    Jan 18 17:24:45.640: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:24:45.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 18 17:24:45.843: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 18 17:24:45.843: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:24:45.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-7853 exec execpodchrnk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.24.73 81'
    Jan 18 17:24:46.046: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.24.73 81\nConnection to 10.96.24.73 81 port [tcp/*] succeeded!\n"
    Jan 18 17:24:46.046: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-7853 01/18/23 17:24:46.046
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[pod2:[101]] 01/18/23 17:24:46.064
    Jan 18 17:24:46.085: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7853 01/18/23 17:24:46.085
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7853 to expose endpoints map[] 01/18/23 17:24:46.103
    Jan 18 17:24:46.118: INFO: successfully validated that service multi-endpoint-test in namespace services-7853 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:24:46.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7853" for this suite. 01/18/23 17:24:46.164
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:46.18
Jan 18 17:24:46.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:24:46.18
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:46.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:46.208
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/18/23 17:24:46.213
STEP: Creating a ResourceQuota 01/18/23 17:24:51.22
STEP: Ensuring resource quota status is calculated 01/18/23 17:24:51.228
STEP: Creating a Service 01/18/23 17:24:53.237
STEP: Creating a NodePort Service 01/18/23 17:24:53.271
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 17:24:53.313
STEP: Ensuring resource quota status captures service creation 01/18/23 17:24:53.355
STEP: Deleting Services 01/18/23 17:24:55.366
STEP: Ensuring resource quota status released usage 01/18/23 17:24:55.433
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:24:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-377" for this suite. 01/18/23 17:24:57.449
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":110,"skipped":1862,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.283 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:46.18
    Jan 18 17:24:46.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:24:46.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:46.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:46.208
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/18/23 17:24:46.213
    STEP: Creating a ResourceQuota 01/18/23 17:24:51.22
    STEP: Ensuring resource quota status is calculated 01/18/23 17:24:51.228
    STEP: Creating a Service 01/18/23 17:24:53.237
    STEP: Creating a NodePort Service 01/18/23 17:24:53.271
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/18/23 17:24:53.313
    STEP: Ensuring resource quota status captures service creation 01/18/23 17:24:53.355
    STEP: Deleting Services 01/18/23 17:24:55.366
    STEP: Ensuring resource quota status released usage 01/18/23 17:24:55.433
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:24:57.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-377" for this suite. 01/18/23 17:24:57.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:24:57.464
Jan 18 17:24:57.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 17:24:57.465
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:57.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:57.492
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/18/23 17:24:57.504
STEP: delete the rc 01/18/23 17:25:02.562
STEP: wait for the rc to be deleted 01/18/23 17:25:02.579
Jan 18 17:25:03.692: INFO: 80 pods remaining
Jan 18 17:25:03.693: INFO: 80 pods has nil DeletionTimestamp
Jan 18 17:25:03.693: INFO: 
Jan 18 17:25:04.596: INFO: 74 pods remaining
Jan 18 17:25:04.596: INFO: 74 pods has nil DeletionTimestamp
Jan 18 17:25:04.596: INFO: 
Jan 18 17:25:05.760: INFO: 60 pods remaining
Jan 18 17:25:05.760: INFO: 60 pods has nil DeletionTimestamp
Jan 18 17:25:05.760: INFO: 
Jan 18 17:25:06.679: INFO: 40 pods remaining
Jan 18 17:25:06.679: INFO: 40 pods has nil DeletionTimestamp
Jan 18 17:25:06.679: INFO: 
Jan 18 17:25:07.600: INFO: 34 pods remaining
Jan 18 17:25:07.600: INFO: 34 pods has nil DeletionTimestamp
Jan 18 17:25:07.600: INFO: 
Jan 18 17:25:08.599: INFO: 20 pods remaining
Jan 18 17:25:08.599: INFO: 20 pods has nil DeletionTimestamp
Jan 18 17:25:08.599: INFO: 
STEP: Gathering metrics 01/18/23 17:25:09.658
W0118 17:25:09.669517      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 17:25:09.669: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 17:25:09.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9698" for this suite. 01/18/23 17:25:09.677
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":111,"skipped":1871,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.225 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:24:57.464
    Jan 18 17:24:57.464: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 17:24:57.465
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:24:57.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:24:57.492
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/18/23 17:24:57.504
    STEP: delete the rc 01/18/23 17:25:02.562
    STEP: wait for the rc to be deleted 01/18/23 17:25:02.579
    Jan 18 17:25:03.692: INFO: 80 pods remaining
    Jan 18 17:25:03.693: INFO: 80 pods has nil DeletionTimestamp
    Jan 18 17:25:03.693: INFO: 
    Jan 18 17:25:04.596: INFO: 74 pods remaining
    Jan 18 17:25:04.596: INFO: 74 pods has nil DeletionTimestamp
    Jan 18 17:25:04.596: INFO: 
    Jan 18 17:25:05.760: INFO: 60 pods remaining
    Jan 18 17:25:05.760: INFO: 60 pods has nil DeletionTimestamp
    Jan 18 17:25:05.760: INFO: 
    Jan 18 17:25:06.679: INFO: 40 pods remaining
    Jan 18 17:25:06.679: INFO: 40 pods has nil DeletionTimestamp
    Jan 18 17:25:06.679: INFO: 
    Jan 18 17:25:07.600: INFO: 34 pods remaining
    Jan 18 17:25:07.600: INFO: 34 pods has nil DeletionTimestamp
    Jan 18 17:25:07.600: INFO: 
    Jan 18 17:25:08.599: INFO: 20 pods remaining
    Jan 18 17:25:08.599: INFO: 20 pods has nil DeletionTimestamp
    Jan 18 17:25:08.599: INFO: 
    STEP: Gathering metrics 01/18/23 17:25:09.658
    W0118 17:25:09.669517      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 17:25:09.669: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 17:25:09.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9698" for this suite. 01/18/23 17:25:09.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:09.689
Jan 18 17:25:09.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 17:25:09.69
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:09.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:09.783
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3555 01/18/23 17:25:09.789
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jan 18 17:25:09.863: INFO: Found 0 stateful pods, waiting for 1
Jan 18 17:25:19.873: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jan 18 17:25:29.872: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/18/23 17:25:29.886
W0118 17:25:29.906839      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 17:25:29.918: INFO: Found 1 stateful pods, waiting for 2
Jan 18 17:25:39.927: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 17:25:39.927: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/18/23 17:25:39.94
STEP: Delete all of the StatefulSets 01/18/23 17:25:39.949
STEP: Verify that StatefulSets have been deleted 01/18/23 17:25:39.966
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 17:25:39.973: INFO: Deleting all statefulset in ns statefulset-3555
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 17:25:39.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3555" for this suite. 01/18/23 17:25:40.001
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":112,"skipped":1881,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.325 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:09.689
    Jan 18 17:25:09.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 17:25:09.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:09.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:09.783
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3555 01/18/23 17:25:09.789
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jan 18 17:25:09.863: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 17:25:19.873: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Jan 18 17:25:29.872: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/18/23 17:25:29.886
    W0118 17:25:29.906839      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 17:25:29.918: INFO: Found 1 stateful pods, waiting for 2
    Jan 18 17:25:39.927: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 17:25:39.927: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/18/23 17:25:39.94
    STEP: Delete all of the StatefulSets 01/18/23 17:25:39.949
    STEP: Verify that StatefulSets have been deleted 01/18/23 17:25:39.966
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 17:25:39.973: INFO: Deleting all statefulset in ns statefulset-3555
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 17:25:39.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3555" for this suite. 01/18/23 17:25:40.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:40.022
Jan 18 17:25:40.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:25:40.023
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:40.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:40.091
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/18/23 17:25:40.096
Jan 18 17:25:40.096: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8386 proxy --unix-socket=/tmp/kubectl-proxy-unix4214711926/test'
STEP: retrieving proxy /api/ output 01/18/23 17:25:40.164
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:25:40.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8386" for this suite. 01/18/23 17:25:40.174
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":113,"skipped":1898,"failed":0}
------------------------------
â€¢ [0.166 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:40.022
    Jan 18 17:25:40.022: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:25:40.023
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:40.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:40.091
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/18/23 17:25:40.096
    Jan 18 17:25:40.096: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8386 proxy --unix-socket=/tmp/kubectl-proxy-unix4214711926/test'
    STEP: retrieving proxy /api/ output 01/18/23 17:25:40.164
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:25:40.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8386" for this suite. 01/18/23 17:25:40.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:40.19
Jan 18 17:25:40.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:25:40.191
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:40.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:40.219
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/18/23 17:25:40.224
Jan 18 17:25:40.247: INFO: Waiting up to 5m0s for pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b" in namespace "pods-9729" to be "running and ready"
Jan 18 17:25:40.255: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.402938ms
Jan 18 17:25:40.255: INFO: The phase of Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:25:42.265: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018543084s
Jan 18 17:25:42.265: INFO: The phase of Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b is Running (Ready = true)
Jan 18 17:25:42.265: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b" satisfied condition "running and ready"
Jan 18 17:25:42.280: INFO: Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b has hostIP: 10.195.74.123
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:25:42.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9729" for this suite. 01/18/23 17:25:42.289
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":114,"skipped":1940,"failed":0}
------------------------------
â€¢ [2.114 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:40.19
    Jan 18 17:25:40.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:25:40.191
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:40.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:40.219
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/18/23 17:25:40.224
    Jan 18 17:25:40.247: INFO: Waiting up to 5m0s for pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b" in namespace "pods-9729" to be "running and ready"
    Jan 18 17:25:40.255: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.402938ms
    Jan 18 17:25:40.255: INFO: The phase of Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:25:42.265: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b": Phase="Running", Reason="", readiness=true. Elapsed: 2.018543084s
    Jan 18 17:25:42.265: INFO: The phase of Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b is Running (Ready = true)
    Jan 18 17:25:42.265: INFO: Pod "pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b" satisfied condition "running and ready"
    Jan 18 17:25:42.280: INFO: Pod pod-hostip-40735b14-461f-4a76-9953-aa7b3b82b12b has hostIP: 10.195.74.123
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:25:42.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9729" for this suite. 01/18/23 17:25:42.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:42.307
Jan 18 17:25:42.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:25:42.308
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:42.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:42.334
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 18 17:25:42.371: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 17:25:42.381
Jan 18 17:25:42.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:42.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 17:25:42.389
Jan 18 17:25:42.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:42.429: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:25:43.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:43.437: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:25:44.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:25:44.438: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 17:25:44.445
Jan 18 17:25:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:25:44.487: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 18 17:25:45.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:45.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 17:25:45.494
Jan 18 17:25:45.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:45.514: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:25:46.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:46.523: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:25:47.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:47.520: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:25:48.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:25:48.523: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:25:48.537
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1606, will wait for the garbage collector to delete the pods 01/18/23 17:25:48.537
Jan 18 17:25:48.607: INFO: Deleting DaemonSet.extensions daemon-set took: 11.904884ms
Jan 18 17:25:48.708: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756462ms
Jan 18 17:25:50.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:25:50.716: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 17:25:50.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631852323"},"items":null}

Jan 18 17:25:50.732: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631852323"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:25:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1606" for this suite. 01/18/23 17:25:50.777
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":115,"skipped":1980,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.482 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:42.307
    Jan 18 17:25:42.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:25:42.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:42.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:42.334
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 18 17:25:42.371: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/18/23 17:25:42.381
    Jan 18 17:25:42.388: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:42.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/18/23 17:25:42.389
    Jan 18 17:25:42.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:42.429: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:25:43.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:43.437: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:25:44.438: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:25:44.438: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/18/23 17:25:44.445
    Jan 18 17:25:44.486: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:25:44.487: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 18 17:25:45.494: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:45.494: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/18/23 17:25:45.494
    Jan 18 17:25:45.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:45.514: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:25:46.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:46.523: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:25:47.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:47.520: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:25:48.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:25:48.523: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:25:48.537
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1606, will wait for the garbage collector to delete the pods 01/18/23 17:25:48.537
    Jan 18 17:25:48.607: INFO: Deleting DaemonSet.extensions daemon-set took: 11.904884ms
    Jan 18 17:25:48.708: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.756462ms
    Jan 18 17:25:50.716: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:25:50.716: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 17:25:50.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631852323"},"items":null}

    Jan 18 17:25:50.732: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631852323"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:25:50.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1606" for this suite. 01/18/23 17:25:50.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:50.79
Jan 18 17:25:50.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:25:50.791
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:50.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:50.824
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/18/23 17:25:50.828
STEP: getting /apis/node.k8s.io 01/18/23 17:25:50.831
STEP: getting /apis/node.k8s.io/v1 01/18/23 17:25:50.833
STEP: creating 01/18/23 17:25:50.834
STEP: watching 01/18/23 17:25:50.866
Jan 18 17:25:50.866: INFO: starting watch
STEP: getting 01/18/23 17:25:50.877
STEP: listing 01/18/23 17:25:50.882
STEP: patching 01/18/23 17:25:50.889
STEP: updating 01/18/23 17:25:50.9
Jan 18 17:25:50.909: INFO: waiting for watch events with expected annotations
STEP: deleting 01/18/23 17:25:50.91
STEP: deleting a collection 01/18/23 17:25:50.941
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 17:25:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3082" for this suite. 01/18/23 17:25:50.982
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":116,"skipped":1990,"failed":0}
------------------------------
â€¢ [0.203 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:50.79
    Jan 18 17:25:50.790: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:25:50.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:50.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:50.824
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/18/23 17:25:50.828
    STEP: getting /apis/node.k8s.io 01/18/23 17:25:50.831
    STEP: getting /apis/node.k8s.io/v1 01/18/23 17:25:50.833
    STEP: creating 01/18/23 17:25:50.834
    STEP: watching 01/18/23 17:25:50.866
    Jan 18 17:25:50.866: INFO: starting watch
    STEP: getting 01/18/23 17:25:50.877
    STEP: listing 01/18/23 17:25:50.882
    STEP: patching 01/18/23 17:25:50.889
    STEP: updating 01/18/23 17:25:50.9
    Jan 18 17:25:50.909: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/18/23 17:25:50.91
    STEP: deleting a collection 01/18/23 17:25:50.941
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 17:25:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3082" for this suite. 01/18/23 17:25:50.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:50.997
Jan 18 17:25:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:25:50.998
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.027
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 17:25:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-1436" for this suite. 01/18/23 17:25:51.055
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":117,"skipped":2037,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:50.997
    Jan 18 17:25:50.997: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:25:50.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.027
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 17:25:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-1436" for this suite. 01/18/23 17:25:51.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:51.07
Jan 18 17:25:51.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:25:51.071
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.099
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/18/23 17:25:51.103
Jan 18 17:25:51.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6704 cluster-info'
Jan 18 17:25:51.199: INFO: stderr: ""
Jan 18 17:25:51.199: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:25:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6704" for this suite. 01/18/23 17:25:51.207
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":118,"skipped":2075,"failed":0}
------------------------------
â€¢ [0.151 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:51.07
    Jan 18 17:25:51.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:25:51.071
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.099
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/18/23 17:25:51.103
    Jan 18 17:25:51.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6704 cluster-info'
    Jan 18 17:25:51.199: INFO: stderr: ""
    Jan 18 17:25:51.199: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:25:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6704" for this suite. 01/18/23 17:25:51.207
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:51.222
Jan 18 17:25:51.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:25:51.223
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:25:51.28
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:25:52.07
STEP: Deploying the webhook pod 01/18/23 17:25:52.087
STEP: Wait for the deployment to be ready 01/18/23 17:25:52.111
Jan 18 17:25:52.126: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:25:54.149
STEP: Verifying the service has paired with the endpoint 01/18/23 17:25:54.171
Jan 18 17:25:55.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 18 17:25:55.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4594-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:25:55.7
STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 17:25:55.735
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:25:58.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1041" for this suite. 01/18/23 17:25:58.359
STEP: Destroying namespace "webhook-1041-markers" for this suite. 01/18/23 17:25:58.374
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":119,"skipped":2086,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.231 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:51.222
    Jan 18 17:25:51.222: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:25:51.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:51.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:51.252
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:25:51.28
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:25:52.07
    STEP: Deploying the webhook pod 01/18/23 17:25:52.087
    STEP: Wait for the deployment to be ready 01/18/23 17:25:52.111
    Jan 18 17:25:52.126: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:25:54.149
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:25:54.171
    Jan 18 17:25:55.172: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 18 17:25:55.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4594-crds.webhook.example.com via the AdmissionRegistration API 01/18/23 17:25:55.7
    STEP: Creating a custom resource that should be mutated by the webhook 01/18/23 17:25:55.735
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:25:58.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1041" for this suite. 01/18/23 17:25:58.359
    STEP: Destroying namespace "webhook-1041-markers" for this suite. 01/18/23 17:25:58.374
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:25:58.456
Jan 18 17:25:58.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:25:58.457
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:58.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:58.488
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:25:58.493
Jan 18 17:25:58.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f" in namespace "downward-api-6614" to be "Succeeded or Failed"
Jan 18 17:25:58.519: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717501ms
Jan 18 17:26:00.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017247122s
Jan 18 17:26:02.530: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018974843s
Jan 18 17:26:04.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016874116s
STEP: Saw pod success 01/18/23 17:26:04.528
Jan 18 17:26:04.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f" satisfied condition "Succeeded or Failed"
Jan 18 17:26:04.533: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f container client-container: <nil>
STEP: delete the pod 01/18/23 17:26:04.561
Jan 18 17:26:04.579: INFO: Waiting for pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f to disappear
Jan 18 17:26:04.585: INFO: Pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 17:26:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6614" for this suite. 01/18/23 17:26:04.593
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":120,"skipped":2138,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.150 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:25:58.456
    Jan 18 17:25:58.456: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:25:58.457
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:25:58.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:25:58.488
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:25:58.493
    Jan 18 17:25:58.511: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f" in namespace "downward-api-6614" to be "Succeeded or Failed"
    Jan 18 17:25:58.519: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717501ms
    Jan 18 17:26:00.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017247122s
    Jan 18 17:26:02.530: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018974843s
    Jan 18 17:26:04.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016874116s
    STEP: Saw pod success 01/18/23 17:26:04.528
    Jan 18 17:26:04.528: INFO: Pod "downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f" satisfied condition "Succeeded or Failed"
    Jan 18 17:26:04.533: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f container client-container: <nil>
    STEP: delete the pod 01/18/23 17:26:04.561
    Jan 18 17:26:04.579: INFO: Waiting for pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f to disappear
    Jan 18 17:26:04.585: INFO: Pod downwardapi-volume-bfca5653-ef94-4914-86eb-d89dd79e6a2f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 17:26:04.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6614" for this suite. 01/18/23 17:26:04.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:04.609
Jan 18 17:26:04.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:26:04.61
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:04.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:04.637
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/18/23 17:26:04.641
Jan 18 17:26:04.656: INFO: created test-pod-1
Jan 18 17:26:04.667: INFO: created test-pod-2
Jan 18 17:26:04.683: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/18/23 17:26:04.683
Jan 18 17:26:04.683: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9358' to be running and ready
Jan 18 17:26:04.703: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 17:26:04.703: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 17:26:04.703: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 18 17:26:04.703: INFO: 0 / 3 pods in namespace 'pods-9358' are running and ready (0 seconds elapsed)
Jan 18 17:26:04.703: INFO: expected 0 pod replicas in namespace 'pods-9358', 0 are Running and Ready.
Jan 18 17:26:04.703: INFO: POD         NODE                                            PHASE    GRACE  CONDITIONS
Jan 18 17:26:04.703: INFO: test-pod-1  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
Jan 18 17:26:04.703: INFO: test-pod-2  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
Jan 18 17:26:04.703: INFO: test-pod-3  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
Jan 18 17:26:04.703: INFO: 
Jan 18 17:26:06.723: INFO: 3 / 3 pods in namespace 'pods-9358' are running and ready (2 seconds elapsed)
Jan 18 17:26:06.723: INFO: expected 0 pod replicas in namespace 'pods-9358', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/18/23 17:26:06.763
Jan 18 17:26:06.770: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 17:26:07.781: INFO: Pod quantity 3 is different from expected quantity 0
Jan 18 17:26:08.783: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:26:09.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9358" for this suite. 01/18/23 17:26:09.784
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":121,"skipped":2156,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.188 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:04.609
    Jan 18 17:26:04.609: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:26:04.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:04.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:04.637
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/18/23 17:26:04.641
    Jan 18 17:26:04.656: INFO: created test-pod-1
    Jan 18 17:26:04.667: INFO: created test-pod-2
    Jan 18 17:26:04.683: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/18/23 17:26:04.683
    Jan 18 17:26:04.683: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9358' to be running and ready
    Jan 18 17:26:04.703: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 17:26:04.703: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 17:26:04.703: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 18 17:26:04.703: INFO: 0 / 3 pods in namespace 'pods-9358' are running and ready (0 seconds elapsed)
    Jan 18 17:26:04.703: INFO: expected 0 pod replicas in namespace 'pods-9358', 0 are Running and Ready.
    Jan 18 17:26:04.703: INFO: POD         NODE                                            PHASE    GRACE  CONDITIONS
    Jan 18 17:26:04.703: INFO: test-pod-1  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
    Jan 18 17:26:04.703: INFO: test-pod-2  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
    Jan 18 17:26:04.703: INFO: test-pod-3  scw-conformance125-default-61c39bbf4d81476a8e3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:26:04 +0000 UTC  }]
    Jan 18 17:26:04.703: INFO: 
    Jan 18 17:26:06.723: INFO: 3 / 3 pods in namespace 'pods-9358' are running and ready (2 seconds elapsed)
    Jan 18 17:26:06.723: INFO: expected 0 pod replicas in namespace 'pods-9358', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/18/23 17:26:06.763
    Jan 18 17:26:06.770: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 17:26:07.781: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 18 17:26:08.783: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:26:09.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9358" for this suite. 01/18/23 17:26:09.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:09.797
Jan 18 17:26:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename namespaces 01/18/23 17:26:09.798
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:09.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:09.827
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/18/23 17:26:09.832
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:09.853
STEP: Creating a pod in the namespace 01/18/23 17:26:09.861
STEP: Waiting for the pod to have running status 01/18/23 17:26:09.875
Jan 18 17:26:09.875: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8000" to be "running"
Jan 18 17:26:09.880: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.559847ms
Jan 18 17:26:11.887: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01203392s
Jan 18 17:26:11.887: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/18/23 17:26:11.887
STEP: Waiting for the namespace to be removed. 01/18/23 17:26:11.9
STEP: Recreating the namespace 01/18/23 17:26:23.907
STEP: Verifying there are no pods in the namespace 01/18/23 17:26:23.931
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:26:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5430" for this suite. 01/18/23 17:26:23.945
STEP: Destroying namespace "nsdeletetest-8000" for this suite. 01/18/23 17:26:23.958
Jan 18 17:26:23.966: INFO: Namespace nsdeletetest-8000 was already deleted
STEP: Destroying namespace "nsdeletetest-737" for this suite. 01/18/23 17:26:23.966
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":122,"skipped":2165,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.181 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:09.797
    Jan 18 17:26:09.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename namespaces 01/18/23 17:26:09.798
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:09.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:09.827
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/18/23 17:26:09.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:09.853
    STEP: Creating a pod in the namespace 01/18/23 17:26:09.861
    STEP: Waiting for the pod to have running status 01/18/23 17:26:09.875
    Jan 18 17:26:09.875: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8000" to be "running"
    Jan 18 17:26:09.880: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.559847ms
    Jan 18 17:26:11.887: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01203392s
    Jan 18 17:26:11.887: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/18/23 17:26:11.887
    STEP: Waiting for the namespace to be removed. 01/18/23 17:26:11.9
    STEP: Recreating the namespace 01/18/23 17:26:23.907
    STEP: Verifying there are no pods in the namespace 01/18/23 17:26:23.931
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:26:23.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-5430" for this suite. 01/18/23 17:26:23.945
    STEP: Destroying namespace "nsdeletetest-8000" for this suite. 01/18/23 17:26:23.958
    Jan 18 17:26:23.966: INFO: Namespace nsdeletetest-8000 was already deleted
    STEP: Destroying namespace "nsdeletetest-737" for this suite. 01/18/23 17:26:23.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:23.98
Jan 18 17:26:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 17:26:23.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:24.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:24.013
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/18/23 17:26:24.018
STEP: Creating hostNetwork=false pod 01/18/23 17:26:24.018
Jan 18 17:26:24.036: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-262" to be "running and ready"
Jan 18 17:26:24.042: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.461827ms
Jan 18 17:26:24.042: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:26:26.051: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015502973s
Jan 18 17:26:26.051: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 18 17:26:26.051: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/18/23 17:26:26.059
Jan 18 17:26:26.068: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-262" to be "running and ready"
Jan 18 17:26:26.080: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.219985ms
Jan 18 17:26:26.080: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:26:28.091: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023229473s
Jan 18 17:26:28.091: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 18 17:26:28.091: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/18/23 17:26:28.098
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 17:26:28.098
Jan 18 17:26:28.098: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.099: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.099: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 17:26:28.210: INFO: Exec stderr: ""
Jan 18 17:26:28.210: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.211: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.211: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 17:26:28.305: INFO: Exec stderr: ""
Jan 18 17:26:28.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.305: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.305: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 17:26:28.393: INFO: Exec stderr: ""
Jan 18 17:26:28.393: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.394: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.394: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 17:26:28.499: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 17:26:28.499
Jan 18 17:26:28.500: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.500: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.500: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 17:26:28.595: INFO: Exec stderr: ""
Jan 18 17:26:28.596: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.596: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.596: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 18 17:26:28.702: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 17:26:28.702
Jan 18 17:26:28.702: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.703: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.703: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 17:26:28.807: INFO: Exec stderr: ""
Jan 18 17:26:28.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.808: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.808: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 18 17:26:28.899: INFO: Exec stderr: ""
Jan 18 17:26:28.899: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:28.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:28.900: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:28.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 17:26:29.007: INFO: Exec stderr: ""
Jan 18 17:26:29.007: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:26:29.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:26:29.008: INFO: ExecWithOptions: Clientset creation
Jan 18 17:26:29.008: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 18 17:26:29.141: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 18 17:26:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-262" for this suite. 01/18/23 17:26:29.148
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":123,"skipped":2177,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.182 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:23.98
    Jan 18 17:26:23.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/18/23 17:26:23.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:24.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:24.013
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/18/23 17:26:24.018
    STEP: Creating hostNetwork=false pod 01/18/23 17:26:24.018
    Jan 18 17:26:24.036: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-262" to be "running and ready"
    Jan 18 17:26:24.042: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.461827ms
    Jan 18 17:26:24.042: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:26:26.051: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015502973s
    Jan 18 17:26:26.051: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 18 17:26:26.051: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/18/23 17:26:26.059
    Jan 18 17:26:26.068: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-262" to be "running and ready"
    Jan 18 17:26:26.080: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.219985ms
    Jan 18 17:26:26.080: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:26:28.091: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.023229473s
    Jan 18 17:26:28.091: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 18 17:26:28.091: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/18/23 17:26:28.098
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/18/23 17:26:28.098
    Jan 18 17:26:28.098: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.099: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.099: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 17:26:28.210: INFO: Exec stderr: ""
    Jan 18 17:26:28.210: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.211: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.211: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 17:26:28.305: INFO: Exec stderr: ""
    Jan 18 17:26:28.305: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.305: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.305: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 17:26:28.393: INFO: Exec stderr: ""
    Jan 18 17:26:28.393: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.393: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.394: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.394: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 17:26:28.499: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/18/23 17:26:28.499
    Jan 18 17:26:28.500: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.500: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.500: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 17:26:28.595: INFO: Exec stderr: ""
    Jan 18 17:26:28.596: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.596: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.596: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 18 17:26:28.702: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/18/23 17:26:28.702
    Jan 18 17:26:28.702: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.703: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.703: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 17:26:28.807: INFO: Exec stderr: ""
    Jan 18 17:26:28.807: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.807: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.808: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.808: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 18 17:26:28.899: INFO: Exec stderr: ""
    Jan 18 17:26:28.899: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:28.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:28.900: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:28.900: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 17:26:29.007: INFO: Exec stderr: ""
    Jan 18 17:26:29.007: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-262 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:26:29.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:26:29.008: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:26:29.008: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-262/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 18 17:26:29.141: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 18 17:26:29.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-262" for this suite. 01/18/23 17:26:29.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:29.165
Jan 18 17:26:29.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:26:29.167
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:29.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:29.194
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/18/23 17:26:29.199
Jan 18 17:26:29.218: INFO: Waiting up to 5m0s for pod "downward-api-1a758335-b966-46b2-8f32-413192d58492" in namespace "downward-api-9256" to be "Succeeded or Failed"
Jan 18 17:26:29.264: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Pending", Reason="", readiness=false. Elapsed: 46.204879ms
Jan 18 17:26:31.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055111059s
Jan 18 17:26:33.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054976011s
STEP: Saw pod success 01/18/23 17:26:33.273
Jan 18 17:26:33.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492" satisfied condition "Succeeded or Failed"
Jan 18 17:26:33.283: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-1a758335-b966-46b2-8f32-413192d58492 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:26:33.298
Jan 18 17:26:33.319: INFO: Waiting for pod downward-api-1a758335-b966-46b2-8f32-413192d58492 to disappear
Jan 18 17:26:33.325: INFO: Pod downward-api-1a758335-b966-46b2-8f32-413192d58492 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 17:26:33.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9256" for this suite. 01/18/23 17:26:33.333
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":124,"skipped":2224,"failed":0}
------------------------------
â€¢ [4.181 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:29.165
    Jan 18 17:26:29.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:26:29.167
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:29.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:29.194
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/18/23 17:26:29.199
    Jan 18 17:26:29.218: INFO: Waiting up to 5m0s for pod "downward-api-1a758335-b966-46b2-8f32-413192d58492" in namespace "downward-api-9256" to be "Succeeded or Failed"
    Jan 18 17:26:29.264: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Pending", Reason="", readiness=false. Elapsed: 46.204879ms
    Jan 18 17:26:31.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055111059s
    Jan 18 17:26:33.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054976011s
    STEP: Saw pod success 01/18/23 17:26:33.273
    Jan 18 17:26:33.273: INFO: Pod "downward-api-1a758335-b966-46b2-8f32-413192d58492" satisfied condition "Succeeded or Failed"
    Jan 18 17:26:33.283: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-1a758335-b966-46b2-8f32-413192d58492 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:26:33.298
    Jan 18 17:26:33.319: INFO: Waiting for pod downward-api-1a758335-b966-46b2-8f32-413192d58492 to disappear
    Jan 18 17:26:33.325: INFO: Pod downward-api-1a758335-b966-46b2-8f32-413192d58492 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 17:26:33.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9256" for this suite. 01/18/23 17:26:33.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:33.348
Jan 18 17:26:33.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:26:33.35
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:33.376
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:33.382
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6727 01/18/23 17:26:33.387
STEP: changing the ExternalName service to type=NodePort 01/18/23 17:26:33.399
STEP: creating replication controller externalname-service in namespace services-6727 01/18/23 17:26:33.442
I0118 17:26:33.453159      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6727, replica count: 2
I0118 17:26:36.505823      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:26:36.505: INFO: Creating new exec pod
Jan 18 17:26:36.518: INFO: Waiting up to 5m0s for pod "execpodwkftx" in namespace "services-6727" to be "running"
Jan 18 17:26:36.525: INFO: Pod "execpodwkftx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.373875ms
Jan 18 17:26:38.536: INFO: Pod "execpodwkftx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018332431s
Jan 18 17:26:38.536: INFO: Pod "execpodwkftx" satisfied condition "running"
Jan 18 17:26:39.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 17:26:39.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 17:26:39.755: INFO: stdout: "externalname-service-df8vd"
Jan 18 17:26:39.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.85.122 80'
Jan 18 17:26:39.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.85.122 80\nConnection to 10.96.85.122 80 port [tcp/http] succeeded!\n"
Jan 18 17:26:39.952: INFO: stdout: ""
Jan 18 17:26:40.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.85.122 80'
Jan 18 17:26:41.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.85.122 80\nConnection to 10.96.85.122 80 port [tcp/http] succeeded!\n"
Jan 18 17:26:41.137: INFO: stdout: "externalname-service-lrdhs"
Jan 18 17:26:41.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 32626'
Jan 18 17:26:41.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 32626\nConnection to 10.195.74.123 32626 port [tcp/*] succeeded!\n"
Jan 18 17:26:41.323: INFO: stdout: "externalname-service-lrdhs"
Jan 18 17:26:41.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 32626'
Jan 18 17:26:41.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 32626\nConnection to 10.195.78.23 32626 port [tcp/*] succeeded!\n"
Jan 18 17:26:41.533: INFO: stdout: "externalname-service-df8vd"
Jan 18 17:26:41.533: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:26:41.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6727" for this suite. 01/18/23 17:26:41.583
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":125,"skipped":2229,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.248 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:33.348
    Jan 18 17:26:33.348: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:26:33.35
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:33.376
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:33.382
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6727 01/18/23 17:26:33.387
    STEP: changing the ExternalName service to type=NodePort 01/18/23 17:26:33.399
    STEP: creating replication controller externalname-service in namespace services-6727 01/18/23 17:26:33.442
    I0118 17:26:33.453159      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6727, replica count: 2
    I0118 17:26:36.505823      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:26:36.505: INFO: Creating new exec pod
    Jan 18 17:26:36.518: INFO: Waiting up to 5m0s for pod "execpodwkftx" in namespace "services-6727" to be "running"
    Jan 18 17:26:36.525: INFO: Pod "execpodwkftx": Phase="Pending", Reason="", readiness=false. Elapsed: 7.373875ms
    Jan 18 17:26:38.536: INFO: Pod "execpodwkftx": Phase="Running", Reason="", readiness=true. Elapsed: 2.018332431s
    Jan 18 17:26:38.536: INFO: Pod "execpodwkftx" satisfied condition "running"
    Jan 18 17:26:39.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 17:26:39.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 17:26:39.755: INFO: stdout: "externalname-service-df8vd"
    Jan 18 17:26:39.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.85.122 80'
    Jan 18 17:26:39.951: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.85.122 80\nConnection to 10.96.85.122 80 port [tcp/http] succeeded!\n"
    Jan 18 17:26:39.952: INFO: stdout: ""
    Jan 18 17:26:40.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.85.122 80'
    Jan 18 17:26:41.137: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.85.122 80\nConnection to 10.96.85.122 80 port [tcp/http] succeeded!\n"
    Jan 18 17:26:41.137: INFO: stdout: "externalname-service-lrdhs"
    Jan 18 17:26:41.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 32626'
    Jan 18 17:26:41.323: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 32626\nConnection to 10.195.74.123 32626 port [tcp/*] succeeded!\n"
    Jan 18 17:26:41.323: INFO: stdout: "externalname-service-lrdhs"
    Jan 18 17:26:41.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-6727 exec execpodwkftx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 32626'
    Jan 18 17:26:41.533: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 32626\nConnection to 10.195.78.23 32626 port [tcp/*] succeeded!\n"
    Jan 18 17:26:41.533: INFO: stdout: "externalname-service-df8vd"
    Jan 18 17:26:41.533: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:26:41.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6727" for this suite. 01/18/23 17:26:41.583
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:41.599
Jan 18 17:26:41.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename events 01/18/23 17:26:41.6
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.628
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/18/23 17:26:41.633
Jan 18 17:26:41.650: INFO: created test-event-1
Jan 18 17:26:41.661: INFO: created test-event-2
Jan 18 17:26:41.668: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/18/23 17:26:41.668
STEP: delete collection of events 01/18/23 17:26:41.673
Jan 18 17:26:41.673: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/18/23 17:26:41.714
Jan 18 17:26:41.714: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 18 17:26:41.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8972" for this suite. 01/18/23 17:26:41.728
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":126,"skipped":2253,"failed":0}
------------------------------
â€¢ [0.142 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:41.599
    Jan 18 17:26:41.599: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename events 01/18/23 17:26:41.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.628
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/18/23 17:26:41.633
    Jan 18 17:26:41.650: INFO: created test-event-1
    Jan 18 17:26:41.661: INFO: created test-event-2
    Jan 18 17:26:41.668: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/18/23 17:26:41.668
    STEP: delete collection of events 01/18/23 17:26:41.673
    Jan 18 17:26:41.673: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/18/23 17:26:41.714
    Jan 18 17:26:41.714: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 18 17:26:41.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-8972" for this suite. 01/18/23 17:26:41.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:41.743
Jan 18 17:26:41.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:26:41.744
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.772
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/18/23 17:26:41.777
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 17:26:41.788
STEP: patching the secret 01/18/23 17:26:41.796
STEP: deleting the secret using a LabelSelector 01/18/23 17:26:41.815
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 17:26:41.833
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:26:41.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6296" for this suite. 01/18/23 17:26:41.85
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":127,"skipped":2262,"failed":0}
------------------------------
â€¢ [0.121 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:41.743
    Jan 18 17:26:41.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:26:41.744
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.772
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/18/23 17:26:41.777
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/18/23 17:26:41.788
    STEP: patching the secret 01/18/23 17:26:41.796
    STEP: deleting the secret using a LabelSelector 01/18/23 17:26:41.815
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/18/23 17:26:41.833
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:26:41.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6296" for this suite. 01/18/23 17:26:41.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:41.866
Jan 18 17:26:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context 01/18/23 17:26:41.867
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.897
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 17:26:41.902
Jan 18 17:26:41.915: INFO: Waiting up to 5m0s for pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a" in namespace "security-context-6343" to be "Succeeded or Failed"
Jan 18 17:26:41.921: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013581ms
Jan 18 17:26:43.929: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014337984s
Jan 18 17:26:45.931: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016246912s
STEP: Saw pod success 01/18/23 17:26:45.931
Jan 18 17:26:45.931: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a" satisfied condition "Succeeded or Failed"
Jan 18 17:26:45.939: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a container test-container: <nil>
STEP: delete the pod 01/18/23 17:26:45.954
Jan 18 17:26:45.976: INFO: Waiting for pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a to disappear
Jan 18 17:26:45.983: INFO: Pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 17:26:45.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6343" for this suite. 01/18/23 17:26:45.993
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":128,"skipped":2276,"failed":0}
------------------------------
â€¢ [4.143 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:41.866
    Jan 18 17:26:41.866: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context 01/18/23 17:26:41.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:41.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:41.897
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 17:26:41.902
    Jan 18 17:26:41.915: INFO: Waiting up to 5m0s for pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a" in namespace "security-context-6343" to be "Succeeded or Failed"
    Jan 18 17:26:41.921: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013581ms
    Jan 18 17:26:43.929: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014337984s
    Jan 18 17:26:45.931: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016246912s
    STEP: Saw pod success 01/18/23 17:26:45.931
    Jan 18 17:26:45.931: INFO: Pod "security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a" satisfied condition "Succeeded or Failed"
    Jan 18 17:26:45.939: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a container test-container: <nil>
    STEP: delete the pod 01/18/23 17:26:45.954
    Jan 18 17:26:45.976: INFO: Waiting for pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a to disappear
    Jan 18 17:26:45.983: INFO: Pod security-context-88a5ebe0-fe7c-4061-84a7-1934b9e7a53a no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 17:26:45.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6343" for this suite. 01/18/23 17:26:45.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:26:46.01
Jan 18 17:26:46.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename cronjob 01/18/23 17:26:46.011
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:46.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:46.043
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/18/23 17:26:46.047
STEP: Ensuring more than one job is running at a time 01/18/23 17:26:46.058
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 17:28:02.067
STEP: Removing cronjob 01/18/23 17:28:02.075
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 17:28:02.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1298" for this suite. 01/18/23 17:28:02.093
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":129,"skipped":2290,"failed":0}
------------------------------
â€¢ [SLOW TEST] [76.096 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:26:46.01
    Jan 18 17:26:46.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename cronjob 01/18/23 17:26:46.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:26:46.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:26:46.043
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/18/23 17:26:46.047
    STEP: Ensuring more than one job is running at a time 01/18/23 17:26:46.058
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/18/23 17:28:02.067
    STEP: Removing cronjob 01/18/23 17:28:02.075
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 17:28:02.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1298" for this suite. 01/18/23 17:28:02.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:28:02.107
Jan 18 17:28:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:28:02.108
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:28:02.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:28:02.13
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 17:28:02.159
Jan 18 17:28:02.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:28:04.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:28:20.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-926" for this suite. 01/18/23 17:28:20.933
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":130,"skipped":2311,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.838 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:28:02.107
    Jan 18 17:28:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:28:02.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:28:02.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:28:02.13
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/18/23 17:28:02.159
    Jan 18 17:28:02.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:28:04.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:28:20.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-926" for this suite. 01/18/23 17:28:20.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:28:20.946
Jan 18 17:28:20.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename init-container 01/18/23 17:28:20.947
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:28:20.97
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:28:20.975
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/18/23 17:28:20.979
Jan 18 17:28:20.980: INFO: PodSpec: initContainers in spec.initContainers
Jan 18 17:29:01.166: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bb552d2d-4cd6-44eb-9703-ed84922fc58a", GenerateName:"", Namespace:"init-container-4836", SelfLink:"", UID:"b7e45d60-4024-4f8c-a2a4-7925c4e00d77", ResourceVersion:"2631859889", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"980094088"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"737e4d75f1097af8556a569a923e14b6363d397af7d8277cfc2a925b924d8abb", "cni.projectcalico.org/podIP":"10.100.145.167/32", "cni.projectcalico.org/podIPs":"10.100.145.167/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90000), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90048), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 29, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90078), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ppc5r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000c31d40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f59878), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"scw-conformance125-default-61c39bbf4d81476a8e3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000775b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f598f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f59910)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f59918), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f5991c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0044a0550), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.195.74.123", PodIP:"10.100.145.167", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.145.167"}}, StartTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000775c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000775c70)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://1fbd9c68043118aa1286db10af0de75dbe4f0dc5eb69e29ff6bec1829a226393", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c31e40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c31e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f599b4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 17:29:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4836" for this suite. 01/18/23 17:29:01.175
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":131,"skipped":2329,"failed":0}
------------------------------
â€¢ [SLOW TEST] [40.244 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:28:20.946
    Jan 18 17:28:20.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename init-container 01/18/23 17:28:20.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:28:20.97
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:28:20.975
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/18/23 17:28:20.979
    Jan 18 17:28:20.980: INFO: PodSpec: initContainers in spec.initContainers
    Jan 18 17:29:01.166: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bb552d2d-4cd6-44eb-9703-ed84922fc58a", GenerateName:"", Namespace:"init-container-4836", SelfLink:"", UID:"b7e45d60-4024-4f8c-a2a4-7925c4e00d77", ResourceVersion:"2631859889", Generation:0, CreationTimestamp:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"980094088"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"737e4d75f1097af8556a569a923e14b6363d397af7d8277cfc2a925b924d8abb", "cni.projectcalico.org/podIP":"10.100.145.167/32", "cni.projectcalico.org/podIPs":"10.100.145.167/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90000), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90048), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 18, 17, 29, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc000a90078), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ppc5r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000c31d40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ppc5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004f59878), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"scw-conformance125-default-61c39bbf4d81476a8e3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000775b20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f598f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004f59910)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004f59918), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004f5991c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0044a0550), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 18, 17, 28, 20, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.195.74.123", PodIP:"10.100.145.167", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.100.145.167"}}, StartTime:time.Date(2023, time.January, 18, 17, 28, 21, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000775c00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000775c70)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://1fbd9c68043118aa1286db10af0de75dbe4f0dc5eb69e29ff6bec1829a226393", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c31e40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000c31e00), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc004f599b4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 17:29:01.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-4836" for this suite. 01/18/23 17:29:01.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:29:01.191
Jan 18 17:29:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:29:01.192
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:01.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:01.219
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 17:29:01.223
Jan 18 17:29:01.238: INFO: Waiting up to 5m0s for pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733" in namespace "emptydir-913" to be "Succeeded or Failed"
Jan 18 17:29:01.248: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351157ms
Jan 18 17:29:03.255: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Running", Reason="", readiness=false. Elapsed: 2.016441475s
Jan 18 17:29:05.258: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019402614s
STEP: Saw pod success 01/18/23 17:29:05.258
Jan 18 17:29:05.258: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733" satisfied condition "Succeeded or Failed"
Jan 18 17:29:05.266: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 container test-container: <nil>
STEP: delete the pod 01/18/23 17:29:05.303
Jan 18 17:29:05.325: INFO: Waiting for pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 to disappear
Jan 18 17:29:05.334: INFO: Pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:29:05.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-913" for this suite. 01/18/23 17:29:05.342
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":132,"skipped":2334,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:29:01.191
    Jan 18 17:29:01.191: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:29:01.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:01.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:01.219
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/18/23 17:29:01.223
    Jan 18 17:29:01.238: INFO: Waiting up to 5m0s for pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733" in namespace "emptydir-913" to be "Succeeded or Failed"
    Jan 18 17:29:01.248: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351157ms
    Jan 18 17:29:03.255: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Running", Reason="", readiness=false. Elapsed: 2.016441475s
    Jan 18 17:29:05.258: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019402614s
    STEP: Saw pod success 01/18/23 17:29:05.258
    Jan 18 17:29:05.258: INFO: Pod "pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733" satisfied condition "Succeeded or Failed"
    Jan 18 17:29:05.266: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:29:05.303
    Jan 18 17:29:05.325: INFO: Waiting for pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 to disappear
    Jan 18 17:29:05.334: INFO: Pod pod-b45a7ee0-bc29-4b1e-8e25-a67cbd9f4733 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:29:05.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-913" for this suite. 01/18/23 17:29:05.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:29:05.363
Jan 18 17:29:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption 01/18/23 17:29:05.364
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:05.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:05.395
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/18/23 17:29:05.413
STEP: Waiting for all pods to be running 01/18/23 17:29:07.466
Jan 18 17:29:07.472: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 17:29:09.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1582" for this suite. 01/18/23 17:29:09.499
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":133,"skipped":2409,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:29:05.363
    Jan 18 17:29:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption 01/18/23 17:29:05.364
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:05.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:05.395
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/18/23 17:29:05.413
    STEP: Waiting for all pods to be running 01/18/23 17:29:07.466
    Jan 18 17:29:07.472: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 17:29:09.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1582" for this suite. 01/18/23 17:29:09.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:29:09.513
Jan 18 17:29:09.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:29:09.514
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:09.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:09.538
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5025/secret-test-60382874-0e8b-4be4-913f-5e9d6a39ce91 01/18/23 17:29:09.542
STEP: Creating a pod to test consume secrets 01/18/23 17:29:09.552
Jan 18 17:29:09.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1" in namespace "secrets-5025" to be "Succeeded or Failed"
Jan 18 17:29:09.572: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947235ms
Jan 18 17:29:11.581: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014803755s
Jan 18 17:29:13.581: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015706351s
STEP: Saw pod success 01/18/23 17:29:13.582
Jan 18 17:29:13.582: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1" satisfied condition "Succeeded or Failed"
Jan 18 17:29:13.588: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 container env-test: <nil>
STEP: delete the pod 01/18/23 17:29:13.603
Jan 18 17:29:13.630: INFO: Waiting for pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 to disappear
Jan 18 17:29:13.636: INFO: Pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:29:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5025" for this suite. 01/18/23 17:29:13.645
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":134,"skipped":2420,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:29:09.513
    Jan 18 17:29:09.513: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:29:09.514
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:09.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:09.538
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5025/secret-test-60382874-0e8b-4be4-913f-5e9d6a39ce91 01/18/23 17:29:09.542
    STEP: Creating a pod to test consume secrets 01/18/23 17:29:09.552
    Jan 18 17:29:09.566: INFO: Waiting up to 5m0s for pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1" in namespace "secrets-5025" to be "Succeeded or Failed"
    Jan 18 17:29:09.572: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.947235ms
    Jan 18 17:29:11.581: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014803755s
    Jan 18 17:29:13.581: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015706351s
    STEP: Saw pod success 01/18/23 17:29:13.582
    Jan 18 17:29:13.582: INFO: Pod "pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1" satisfied condition "Succeeded or Failed"
    Jan 18 17:29:13.588: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 container env-test: <nil>
    STEP: delete the pod 01/18/23 17:29:13.603
    Jan 18 17:29:13.630: INFO: Waiting for pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 to disappear
    Jan 18 17:29:13.636: INFO: Pod pod-configmaps-62d8158a-b06e-45da-8538-e4ca6fac1dc1 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:29:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5025" for this suite. 01/18/23 17:29:13.645
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:29:13.663
Jan 18 17:29:13.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:29:13.664
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:13.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:13.694
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-893b0f21-e558-4981-829e-70210f60cb84 in namespace container-probe-705 01/18/23 17:29:13.699
Jan 18 17:29:13.713: INFO: Waiting up to 5m0s for pod "liveness-893b0f21-e558-4981-829e-70210f60cb84" in namespace "container-probe-705" to be "not pending"
Jan 18 17:29:13.719: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764702ms
Jan 18 17:29:15.728: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84": Phase="Running", Reason="", readiness=true. Elapsed: 2.015095374s
Jan 18 17:29:15.728: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84" satisfied condition "not pending"
Jan 18 17:29:15.728: INFO: Started pod liveness-893b0f21-e558-4981-829e-70210f60cb84 in namespace container-probe-705
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:29:15.728
Jan 18 17:29:15.735: INFO: Initial restart count of pod liveness-893b0f21-e558-4981-829e-70210f60cb84 is 0
Jan 18 17:29:35.836: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 1 (20.101106017s elapsed)
Jan 18 17:29:55.994: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 2 (40.258717896s elapsed)
Jan 18 17:30:16.091: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 3 (1m0.355702316s elapsed)
Jan 18 17:30:36.188: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 4 (1m20.452504898s elapsed)
Jan 18 17:31:50.537: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 5 (2m34.802034523s elapsed)
STEP: deleting the pod 01/18/23 17:31:50.537
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:31:50.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-705" for this suite. 01/18/23 17:31:50.565
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":135,"skipped":2454,"failed":0}
------------------------------
â€¢ [SLOW TEST] [156.914 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:29:13.663
    Jan 18 17:29:13.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:29:13.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:29:13.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:29:13.694
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-893b0f21-e558-4981-829e-70210f60cb84 in namespace container-probe-705 01/18/23 17:29:13.699
    Jan 18 17:29:13.713: INFO: Waiting up to 5m0s for pod "liveness-893b0f21-e558-4981-829e-70210f60cb84" in namespace "container-probe-705" to be "not pending"
    Jan 18 17:29:13.719: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764702ms
    Jan 18 17:29:15.728: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84": Phase="Running", Reason="", readiness=true. Elapsed: 2.015095374s
    Jan 18 17:29:15.728: INFO: Pod "liveness-893b0f21-e558-4981-829e-70210f60cb84" satisfied condition "not pending"
    Jan 18 17:29:15.728: INFO: Started pod liveness-893b0f21-e558-4981-829e-70210f60cb84 in namespace container-probe-705
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:29:15.728
    Jan 18 17:29:15.735: INFO: Initial restart count of pod liveness-893b0f21-e558-4981-829e-70210f60cb84 is 0
    Jan 18 17:29:35.836: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 1 (20.101106017s elapsed)
    Jan 18 17:29:55.994: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 2 (40.258717896s elapsed)
    Jan 18 17:30:16.091: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 3 (1m0.355702316s elapsed)
    Jan 18 17:30:36.188: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 4 (1m20.452504898s elapsed)
    Jan 18 17:31:50.537: INFO: Restart count of pod container-probe-705/liveness-893b0f21-e558-4981-829e-70210f60cb84 is now 5 (2m34.802034523s elapsed)
    STEP: deleting the pod 01/18/23 17:31:50.537
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:31:50.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-705" for this suite. 01/18/23 17:31:50.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:31:50.578
Jan 18 17:31:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:31:50.579
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:50.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:50.61
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan 18 17:31:50.629: INFO: Waiting up to 5m0s for pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155" in namespace "kubelet-test-8382" to be "running and ready"
Jan 18 17:31:50.639: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155": Phase="Pending", Reason="", readiness=false. Elapsed: 9.572861ms
Jan 18 17:31:50.639: INFO: The phase of Pod busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:31:52.666: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155": Phase="Running", Reason="", readiness=true. Elapsed: 2.036496176s
Jan 18 17:31:52.666: INFO: The phase of Pod busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155 is Running (Ready = true)
Jan 18 17:31:52.666: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 17:31:52.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8382" for this suite. 01/18/23 17:31:52.718
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":136,"skipped":2462,"failed":0}
------------------------------
â€¢ [2.155 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:31:50.578
    Jan 18 17:31:50.578: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 17:31:50.579
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:50.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:50.61
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan 18 17:31:50.629: INFO: Waiting up to 5m0s for pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155" in namespace "kubelet-test-8382" to be "running and ready"
    Jan 18 17:31:50.639: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155": Phase="Pending", Reason="", readiness=false. Elapsed: 9.572861ms
    Jan 18 17:31:50.639: INFO: The phase of Pod busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:31:52.666: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155": Phase="Running", Reason="", readiness=true. Elapsed: 2.036496176s
    Jan 18 17:31:52.666: INFO: The phase of Pod busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155 is Running (Ready = true)
    Jan 18 17:31:52.666: INFO: Pod "busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 17:31:52.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8382" for this suite. 01/18/23 17:31:52.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:31:52.739
Jan 18 17:31:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:31:52.74
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:52.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:52.771
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 18 17:31:52.806: INFO: created pod pod-service-account-defaultsa
Jan 18 17:31:52.806: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 18 17:31:52.817: INFO: created pod pod-service-account-mountsa
Jan 18 17:31:52.817: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 18 17:31:52.828: INFO: created pod pod-service-account-nomountsa
Jan 18 17:31:52.828: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 18 17:31:52.837: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 18 17:31:52.837: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 18 17:31:52.847: INFO: created pod pod-service-account-mountsa-mountspec
Jan 18 17:31:52.847: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 18 17:31:52.859: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 18 17:31:52.860: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 18 17:31:52.869: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 18 17:31:52.869: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 18 17:31:52.880: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 18 17:31:52.881: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 18 17:31:52.891: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 18 17:31:52.891: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 17:31:52.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4566" for this suite. 01/18/23 17:31:52.9
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":137,"skipped":2483,"failed":0}
------------------------------
â€¢ [0.174 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:31:52.739
    Jan 18 17:31:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:31:52.74
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:52.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:52.771
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 18 17:31:52.806: INFO: created pod pod-service-account-defaultsa
    Jan 18 17:31:52.806: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 18 17:31:52.817: INFO: created pod pod-service-account-mountsa
    Jan 18 17:31:52.817: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 18 17:31:52.828: INFO: created pod pod-service-account-nomountsa
    Jan 18 17:31:52.828: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 18 17:31:52.837: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 18 17:31:52.837: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 18 17:31:52.847: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 18 17:31:52.847: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 18 17:31:52.859: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 18 17:31:52.860: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 18 17:31:52.869: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 18 17:31:52.869: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 18 17:31:52.880: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 18 17:31:52.881: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 18 17:31:52.891: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 18 17:31:52.891: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 17:31:52.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4566" for this suite. 01/18/23 17:31:52.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:31:52.917
Jan 18 17:31:52.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:31:52.919
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:52.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:52.985
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 18 17:31:52.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 create -f -'
Jan 18 17:31:54.066: INFO: stderr: ""
Jan 18 17:31:54.066: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 18 17:31:54.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 create -f -'
Jan 18 17:31:54.350: INFO: stderr: ""
Jan 18 17:31:54.350: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 17:31:54.35
Jan 18 17:31:55.358: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:31:55.359: INFO: Found 0 / 1
Jan 18 17:31:56.359: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:31:56.359: INFO: Found 1 / 1
Jan 18 17:31:56.359: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 17:31:56.366: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:31:56.366: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 17:31:56.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe pod agnhost-primary-lsfxp'
Jan 18 17:31:56.476: INFO: stderr: ""
Jan 18 17:31:56.476: INFO: stdout: "Name:             agnhost-primary-lsfxp\nNamespace:        kubectl-8424\nPriority:         0\nService Account:  default\nNode:             scw-conformance125-default-788643c0f7cd4128a5c/10.195.78.23\nStart Time:       Wed, 18 Jan 2023 17:31:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 9cee524055e27cf076cc53aca370f9e62da780cdcc96b7a5214389b72783baaf\n                  cni.projectcalico.org/podIP: 10.100.158.19/32\n                  cni.projectcalico.org/podIPs: 10.100.158.19/32\nStatus:           Running\nIP:               10.100.158.19\nIPs:\n  IP:           10.100.158.19\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8a6d18aadf799c728b59d8e7602867a922ed37a4368f2b0c7ed0a7e064d3e09d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 17:31:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5pgs7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5pgs7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8424/agnhost-primary-lsfxp to scw-conformance125-default-788643c0f7cd4128a5c\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Jan 18 17:31:56.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe rc agnhost-primary'
Jan 18 17:31:56.599: INFO: stderr: ""
Jan 18 17:31:56.599: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8424\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lsfxp\n"
Jan 18 17:31:56.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe service agnhost-primary'
Jan 18 17:31:56.712: INFO: stderr: ""
Jan 18 17:31:56.712: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8424\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.78.186\nIPs:               10.96.78.186\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.158.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 18 17:31:56.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe node scw-conformance125-default-61c39bbf4d81476a8e3'
Jan 18 17:31:56.874: INFO: stderr: ""
Jan 18 17:31:56.874: INFO: stdout: "Name:               scw-conformance125-default-61c39bbf4d81476a8e3\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=DEV1-L\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fr-srr\n                    failure-domain.beta.kubernetes.io/zone=fr-srr-1\n                    k8s.scaleway.com/kapsule=7b01ab70-bbfa-46fe-9923-98c5e8d41008\n                    k8s.scaleway.com/managed=true\n                    k8s.scaleway.com/node=61c39bbf-4d81-476a-8e3a-1065e28f4873\n                    k8s.scaleway.com/pool=b2d82043-e6e6-47a8-9fda-5af608f4cc13\n                    k8s.scaleway.com/pool-name=default\n                    k8s.scaleway.com/runtime=containerd\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=scw-conformance125-default-61c39bbf4d81476a8e3\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=DEV1-L\n                    topology.csi.scaleway.com/zone=fr-srr-1\n                    topology.kubernetes.io/region=fr-srr\n                    topology.kubernetes.io/zone=fr-srr-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.scaleway.com\":\"fr-srr-1/ef224802-75d4-458f-a1b1-4d5781b9db7b\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.195.74.123/31\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.100.145.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 16:14:59 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  scw-conformance125-default-61c39bbf4d81476a8e3\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 17:31:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  KernelDeadlock       False   Wed, 18 Jan 2023 17:30:57 +0000   Wed, 18 Jan 2023 16:15:46 +0000   KernelHasNoDeadlock          kernel has no deadlock\n  ReadonlyFilesystem   False   Wed, 18 Jan 2023 17:30:57 +0000   Wed, 18 Jan 2023 16:15:46 +0000   FilesystemIsNotReadOnly      Filesystem is not read-only\n  NetworkUnavailable   False   Wed, 18 Jan 2023 16:15:29 +0000   Wed, 18 Jan 2023 16:15:29 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:15:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:     scw-conformance125-default-61c39bbf4d81476a8e3\n  InternalIP:   10.195.74.123\n  InternalDNS:  ef224802-75d4-458f-a1b1-4d5781b9db7b.priv.instances.scw.cloud\n  ExternalIP:   51.159.123.89\n  ExternalDNS:  ef224802-75d4-458f-a1b1-4d5781b9db7b.pub.instances.scw.cloud\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    75524576Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               8142444Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3800m\n  ephemeral-storage:    65038816Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7093868Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 44bf5810a3f14b6984ad0a8863c69dc8\n  System UUID:                ef224802-75d4-458f-a1b1-4d5781b9db7b\n  Boot ID:                    03ff148b-5083-4484-a71a-125622d5e8c9\n  Kernel Version:             5.4.0-122-generic\n  OS Image:                   Ubuntu 20.04.4 LTS c93ee8b1f5\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.100.1.0/24\nPodCIDRs:                     10.100.1.0/24\nProviderID:                   scaleway://instance/fr-srr-1/ef224802-75d4-458f-a1b1-4d5781b9db7b\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-trfh9                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 csi-node-bcj2l                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 konnectivity-agent-vglg7                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 kube-proxy-6s5vw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 node-problem-detector-pcj72                                10m (0%)      10m (0%)    80Mi (1%)        80Mi (1%)      76m\n  kubelet-test-8382           busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  svcaccounts-4566            pod-service-account-defaultsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-defaultsa-mountspec                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-mountsa                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-mountsa-mountspec                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa-mountspec                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa-nomountspec                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  260m (6%)  10m (0%)\n  memory               80Mi (1%)  80Mi (1%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
Jan 18 17:31:56.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe namespace kubectl-8424'
Jan 18 17:31:56.999: INFO: stderr: ""
Jan 18 17:31:56.999: INFO: stdout: "Name:         kubectl-8424\nLabels:       e2e-framework=kubectl\n              e2e-run=635e59ed-3adc-4183-91fa-918ac4beb641\n              kubernetes.io/metadata.name=kubectl-8424\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:31:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8424" for this suite. 01/18/23 17:31:57.009
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":138,"skipped":2524,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:31:52.917
    Jan 18 17:31:52.918: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:31:52.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:52.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:52.985
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 18 17:31:52.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 create -f -'
    Jan 18 17:31:54.066: INFO: stderr: ""
    Jan 18 17:31:54.066: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 18 17:31:54.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 create -f -'
    Jan 18 17:31:54.350: INFO: stderr: ""
    Jan 18 17:31:54.350: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 17:31:54.35
    Jan 18 17:31:55.358: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:31:55.359: INFO: Found 0 / 1
    Jan 18 17:31:56.359: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:31:56.359: INFO: Found 1 / 1
    Jan 18 17:31:56.359: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 17:31:56.366: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:31:56.366: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 17:31:56.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe pod agnhost-primary-lsfxp'
    Jan 18 17:31:56.476: INFO: stderr: ""
    Jan 18 17:31:56.476: INFO: stdout: "Name:             agnhost-primary-lsfxp\nNamespace:        kubectl-8424\nPriority:         0\nService Account:  default\nNode:             scw-conformance125-default-788643c0f7cd4128a5c/10.195.78.23\nStart Time:       Wed, 18 Jan 2023 17:31:54 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 9cee524055e27cf076cc53aca370f9e62da780cdcc96b7a5214389b72783baaf\n                  cni.projectcalico.org/podIP: 10.100.158.19/32\n                  cni.projectcalico.org/podIPs: 10.100.158.19/32\nStatus:           Running\nIP:               10.100.158.19\nIPs:\n  IP:           10.100.158.19\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://8a6d18aadf799c728b59d8e7602867a922ed37a4368f2b0c7ed0a7e064d3e09d\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 18 Jan 2023 17:31:54 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5pgs7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5pgs7:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-8424/agnhost-primary-lsfxp to scw-conformance125-default-788643c0f7cd4128a5c\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Jan 18 17:31:56.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe rc agnhost-primary'
    Jan 18 17:31:56.599: INFO: stderr: ""
    Jan 18 17:31:56.599: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8424\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-lsfxp\n"
    Jan 18 17:31:56.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe service agnhost-primary'
    Jan 18 17:31:56.712: INFO: stderr: ""
    Jan 18 17:31:56.712: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8424\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.78.186\nIPs:               10.96.78.186\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.100.158.19:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 18 17:31:56.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe node scw-conformance125-default-61c39bbf4d81476a8e3'
    Jan 18 17:31:56.874: INFO: stderr: ""
    Jan 18 17:31:56.874: INFO: stdout: "Name:               scw-conformance125-default-61c39bbf4d81476a8e3\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=DEV1-L\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=fr-srr\n                    failure-domain.beta.kubernetes.io/zone=fr-srr-1\n                    k8s.scaleway.com/kapsule=7b01ab70-bbfa-46fe-9923-98c5e8d41008\n                    k8s.scaleway.com/managed=true\n                    k8s.scaleway.com/node=61c39bbf-4d81-476a-8e3a-1065e28f4873\n                    k8s.scaleway.com/pool=b2d82043-e6e6-47a8-9fda-5af608f4cc13\n                    k8s.scaleway.com/pool-name=default\n                    k8s.scaleway.com/runtime=containerd\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=scw-conformance125-default-61c39bbf4d81476a8e3\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=DEV1-L\n                    topology.csi.scaleway.com/zone=fr-srr-1\n                    topology.kubernetes.io/region=fr-srr\n                    topology.kubernetes.io/zone=fr-srr-1\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.scaleway.com\":\"fr-srr-1/ef224802-75d4-458f-a1b1-4d5781b9db7b\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.195.74.123/31\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.100.145.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 18 Jan 2023 16:14:59 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  scw-conformance125-default-61c39bbf4d81476a8e3\n  AcquireTime:     <unset>\n  RenewTime:       Wed, 18 Jan 2023 17:31:54 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  KernelDeadlock       False   Wed, 18 Jan 2023 17:30:57 +0000   Wed, 18 Jan 2023 16:15:46 +0000   KernelHasNoDeadlock          kernel has no deadlock\n  ReadonlyFilesystem   False   Wed, 18 Jan 2023 17:30:57 +0000   Wed, 18 Jan 2023 16:15:46 +0000   FilesystemIsNotReadOnly      Filesystem is not read-only\n  NetworkUnavailable   False   Wed, 18 Jan 2023 16:15:29 +0000   Wed, 18 Jan 2023 16:15:29 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:14:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 18 Jan 2023 17:30:00 +0000   Wed, 18 Jan 2023 16:15:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  Hostname:     scw-conformance125-default-61c39bbf4d81476a8e3\n  InternalIP:   10.195.74.123\n  InternalDNS:  ef224802-75d4-458f-a1b1-4d5781b9db7b.priv.instances.scw.cloud\n  ExternalIP:   51.159.123.89\n  ExternalDNS:  ef224802-75d4-458f-a1b1-4d5781b9db7b.pub.instances.scw.cloud\nCapacity:\n  cpu:                  4\n  ephemeral-storage:    75524576Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               8142444Ki\n  pods:                 110\nAllocatable:\n  cpu:                  3800m\n  ephemeral-storage:    65038816Ki\n  example.com/fakecpu:  1k\n  hugepages-1Gi:        0\n  hugepages-2Mi:        0\n  memory:               7093868Ki\n  pods:                 110\nSystem Info:\n  Machine ID:                 44bf5810a3f14b6984ad0a8863c69dc8\n  System UUID:                ef224802-75d4-458f-a1b1-4d5781b9db7b\n  Boot ID:                    03ff148b-5083-4484-a71a-125622d5e8c9\n  Kernel Version:             5.4.0-122-generic\n  OS Image:                   Ubuntu 20.04.4 LTS c93ee8b1f5\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.15\n  Kubelet Version:            v1.25.5\n  Kube-Proxy Version:         v1.25.5\nPodCIDR:                      10.100.1.0/24\nPodCIDRs:                     10.100.1.0/24\nProviderID:                   scaleway://instance/fr-srr-1/ef224802-75d4-458f-a1b1-4d5781b9db7b\nNon-terminated Pods:          (14 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-trfh9                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 csi-node-bcj2l                                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 konnectivity-agent-vglg7                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 kube-proxy-6s5vw                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  kube-system                 node-problem-detector-pcj72                                10m (0%)      10m (0%)    80Mi (1%)        80Mi (1%)      76m\n  kubelet-test-8382           busybox-scheduling-5f4d98ca-558e-4f7f-8f58-db1cd7dee155    0 (0%)        0 (0%)      0 (0%)           0 (0%)         6s\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m    0 (0%)        0 (0%)      0 (0%)           0 (0%)         33m\n  svcaccounts-4566            pod-service-account-defaultsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-defaultsa-mountspec                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-mountsa                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-mountsa-mountspec                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa-mountspec                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  svcaccounts-4566            pod-service-account-nomountsa-nomountspec                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource             Requests   Limits\n  --------             --------   ------\n  cpu                  260m (6%)  10m (0%)\n  memory               80Mi (1%)  80Mi (1%)\n  ephemeral-storage    0 (0%)     0 (0%)\n  hugepages-1Gi        0 (0%)     0 (0%)\n  hugepages-2Mi        0 (0%)     0 (0%)\n  example.com/fakecpu  0          0\nEvents:                <none>\n"
    Jan 18 17:31:56.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-8424 describe namespace kubectl-8424'
    Jan 18 17:31:56.999: INFO: stderr: ""
    Jan 18 17:31:56.999: INFO: stdout: "Name:         kubectl-8424\nLabels:       e2e-framework=kubectl\n              e2e-run=635e59ed-3adc-4183-91fa-918ac4beb641\n              kubernetes.io/metadata.name=kubectl-8424\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:31:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8424" for this suite. 01/18/23 17:31:57.009
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:31:57.025
Jan 18 17:31:57.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 17:31:57.026
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:57.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:57.055
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3019 01/18/23 17:31:57.06
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-3019 01/18/23 17:31:57.07
Jan 18 17:31:57.088: INFO: Found 0 stateful pods, waiting for 1
Jan 18 17:32:07.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/18/23 17:32:07.113
STEP: updating a scale subresource 01/18/23 17:32:07.119
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 17:32:07.128
STEP: Patch a scale subresource 01/18/23 17:32:07.136
STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 17:32:07.147
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 17:32:07.157: INFO: Deleting all statefulset in ns statefulset-3019
Jan 18 17:32:07.164: INFO: Scaling statefulset ss to 0
Jan 18 17:32:17.234: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:32:17.241: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 17:32:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3019" for this suite. 01/18/23 17:32:17.277
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":139,"skipped":2553,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.265 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:31:57.025
    Jan 18 17:31:57.025: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 17:31:57.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:31:57.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:31:57.055
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3019 01/18/23 17:31:57.06
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-3019 01/18/23 17:31:57.07
    Jan 18 17:31:57.088: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 17:32:07.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/18/23 17:32:07.113
    STEP: updating a scale subresource 01/18/23 17:32:07.119
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 17:32:07.128
    STEP: Patch a scale subresource 01/18/23 17:32:07.136
    STEP: verifying the statefulset Spec.Replicas was modified 01/18/23 17:32:07.147
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 17:32:07.157: INFO: Deleting all statefulset in ns statefulset-3019
    Jan 18 17:32:07.164: INFO: Scaling statefulset ss to 0
    Jan 18 17:32:17.234: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:32:17.241: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 17:32:17.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3019" for this suite. 01/18/23 17:32:17.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:32:17.292
Jan 18 17:32:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:32:17.293
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:17.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:17.322
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/18/23 17:32:17.327
Jan 18 17:32:17.342: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0" in namespace "emptydir-811" to be "running"
Jan 18 17:32:17.350: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.708955ms
Jan 18 17:32:19.359: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016827847s
Jan 18 17:32:19.359: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/18/23 17:32:19.359
Jan 18 17:32:19.359: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-811 PodName:pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:32:19.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:32:19.360: INFO: ExecWithOptions: Clientset creation
Jan 18 17:32:19.360: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-811/pods/pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 18 17:32:19.463: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:32:19.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-811" for this suite. 01/18/23 17:32:19.473
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":140,"skipped":2574,"failed":0}
------------------------------
â€¢ [2.192 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:32:17.292
    Jan 18 17:32:17.292: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:32:17.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:17.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:17.322
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/18/23 17:32:17.327
    Jan 18 17:32:17.342: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0" in namespace "emptydir-811" to be "running"
    Jan 18 17:32:17.350: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.708955ms
    Jan 18 17:32:19.359: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016827847s
    Jan 18 17:32:19.359: INFO: Pod "pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/18/23 17:32:19.359
    Jan 18 17:32:19.359: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-811 PodName:pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:32:19.359: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:32:19.360: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:32:19.360: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-811/pods/pod-sharedvolume-4c66cdd2-27c1-43da-b7b1-57cf746e02d0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 18 17:32:19.463: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:32:19.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-811" for this suite. 01/18/23 17:32:19.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:32:19.488
Jan 18 17:32:19.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:32:19.489
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:19.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:19.519
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 18 17:32:19.550: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:32:19.559
Jan 18 17:32:19.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:32:19.580: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:32:20.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:32:20.596: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:32:21.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:32:21.598: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 01/18/23 17:32:21.63
STEP: Check that daemon pods images are updated. 01/18/23 17:32:21.648
Jan 18 17:32:21.656: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 17:32:21.656: INFO: Wrong image for pod: daemon-set-rf8xj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 17:32:22.677: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 17:32:23.676: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 17:32:24.679: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 18 17:32:24.679: INFO: Pod daemon-set-mclqp is not available
Jan 18 17:32:25.674: INFO: Pod daemon-set-fl9mr is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 17:32:25.68
Jan 18 17:32:25.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 17:32:25.692: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 17:32:26.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:32:26.706: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:32:26.737
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3015, will wait for the garbage collector to delete the pods 01/18/23 17:32:26.738
Jan 18 17:32:26.806: INFO: Deleting DaemonSet.extensions daemon-set took: 13.972897ms
Jan 18 17:32:26.907: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.598979ms
Jan 18 17:32:29.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:32:29.713: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 17:32:29.718: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631868168"},"items":null}

Jan 18 17:32:29.725: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631868168"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:32:29.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3015" for this suite. 01/18/23 17:32:29.754
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":141,"skipped":2652,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.278 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:32:19.488
    Jan 18 17:32:19.488: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:32:19.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:19.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:19.519
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 18 17:32:19.550: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 17:32:19.559
    Jan 18 17:32:19.580: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:32:19.580: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:32:20.596: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:32:20.596: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:32:21.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:32:21.598: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 01/18/23 17:32:21.63
    STEP: Check that daemon pods images are updated. 01/18/23 17:32:21.648
    Jan 18 17:32:21.656: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 17:32:21.656: INFO: Wrong image for pod: daemon-set-rf8xj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 17:32:22.677: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 17:32:23.676: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 17:32:24.679: INFO: Wrong image for pod: daemon-set-m8mxn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 18 17:32:24.679: INFO: Pod daemon-set-mclqp is not available
    Jan 18 17:32:25.674: INFO: Pod daemon-set-fl9mr is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/18/23 17:32:25.68
    Jan 18 17:32:25.692: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 17:32:25.692: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 17:32:26.706: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:32:26.706: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:32:26.737
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3015, will wait for the garbage collector to delete the pods 01/18/23 17:32:26.738
    Jan 18 17:32:26.806: INFO: Deleting DaemonSet.extensions daemon-set took: 13.972897ms
    Jan 18 17:32:26.907: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.598979ms
    Jan 18 17:32:29.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:32:29.713: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 17:32:29.718: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631868168"},"items":null}

    Jan 18 17:32:29.725: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631868168"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:32:29.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3015" for this suite. 01/18/23 17:32:29.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:32:29.766
Jan 18 17:32:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename watch 01/18/23 17:32:29.767
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:29.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:29.795
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/18/23 17:32:29.799
STEP: modifying the configmap once 01/18/23 17:32:29.806
STEP: modifying the configmap a second time 01/18/23 17:32:29.819
STEP: deleting the configmap 01/18/23 17:32:29.834
STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 17:32:29.844
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 17:32:29.846
Jan 18 17:32:29.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-338  5f6d4b7e-ee76-48b1-aa13-eafcae3e03c7 2631868178 0 2023-01-18 17:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 17:32:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 17:32:29.846: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-338  5f6d4b7e-ee76-48b1-aa13-eafcae3e03c7 2631868179 0 2023-01-18 17:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 17:32:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 17:32:29.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-338" for this suite. 01/18/23 17:32:29.853
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":142,"skipped":2657,"failed":0}
------------------------------
â€¢ [0.102 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:32:29.766
    Jan 18 17:32:29.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename watch 01/18/23 17:32:29.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:29.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:29.795
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/18/23 17:32:29.799
    STEP: modifying the configmap once 01/18/23 17:32:29.806
    STEP: modifying the configmap a second time 01/18/23 17:32:29.819
    STEP: deleting the configmap 01/18/23 17:32:29.834
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/18/23 17:32:29.844
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/18/23 17:32:29.846
    Jan 18 17:32:29.846: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-338  5f6d4b7e-ee76-48b1-aa13-eafcae3e03c7 2631868178 0 2023-01-18 17:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 17:32:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 17:32:29.846: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-338  5f6d4b7e-ee76-48b1-aa13-eafcae3e03c7 2631868179 0 2023-01-18 17:32:29 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-18 17:32:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 17:32:29.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-338" for this suite. 01/18/23 17:32:29.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:32:29.869
Jan 18 17:32:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:32:29.87
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:29.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:29.897
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/18/23 17:32:29.902
Jan 18 17:32:29.915: INFO: Waiting up to 2m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647" to be "running"
Jan 18 17:32:29.921: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078871ms
Jan 18 17:32:31.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013817571s
Jan 18 17:32:33.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017492076s
Jan 18 17:32:35.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01356432s
Jan 18 17:32:37.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013598654s
Jan 18 17:32:39.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014178388s
Jan 18 17:32:41.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013970748s
Jan 18 17:32:43.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014848123s
Jan 18 17:32:45.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013919547s
Jan 18 17:32:47.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013346767s
Jan 18 17:32:49.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016619455s
Jan 18 17:32:51.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016339834s
Jan 18 17:32:53.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014483827s
Jan 18 17:32:55.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01302525s
Jan 18 17:32:57.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014658866s
Jan 18 17:32:59.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015843924s
Jan 18 17:33:01.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015145442s
Jan 18 17:33:03.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014181816s
Jan 18 17:33:05.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015766698s
Jan 18 17:33:07.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014833456s
Jan 18 17:33:09.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014695618s
Jan 18 17:33:11.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 42.016858497s
Jan 18 17:33:13.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014262321s
Jan 18 17:33:15.927: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012668388s
Jan 18 17:33:17.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013423587s
Jan 18 17:33:19.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016347828s
Jan 18 17:33:21.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014364832s
Jan 18 17:33:23.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015551988s
Jan 18 17:33:25.936: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020844594s
Jan 18 17:33:27.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013650019s
Jan 18 17:33:29.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014952117s
Jan 18 17:33:31.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014985123s
Jan 18 17:33:33.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017621048s
Jan 18 17:33:35.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014634186s
Jan 18 17:33:37.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014785875s
Jan 18 17:33:39.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015898921s
Jan 18 17:33:41.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01533577s
Jan 18 17:33:43.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014485816s
Jan 18 17:33:45.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01633779s
Jan 18 17:33:47.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014094711s
Jan 18 17:33:49.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017789099s
Jan 18 17:33:51.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015380544s
Jan 18 17:33:53.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013904383s
Jan 18 17:33:55.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017409092s
Jan 18 17:33:57.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014679508s
Jan 18 17:33:59.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014890778s
Jan 18 17:34:01.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016970823s
Jan 18 17:34:03.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016102972s
Jan 18 17:34:05.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015001128s
Jan 18 17:34:07.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013554774s
Jan 18 17:34:09.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01512905s
Jan 18 17:34:11.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015585234s
Jan 18 17:34:13.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012971535s
Jan 18 17:34:15.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016413949s
Jan 18 17:34:17.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015060446s
Jan 18 17:34:19.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.016063769s
Jan 18 17:34:21.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013317469s
Jan 18 17:34:23.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015455868s
Jan 18 17:34:25.934: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018991258s
Jan 18 17:34:27.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016543468s
Jan 18 17:34:29.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013514625s
Jan 18 17:34:29.940: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.025574401s
STEP: updating the pod 01/18/23 17:34:29.94
Jan 18 17:34:30.464: INFO: Successfully updated pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360"
STEP: waiting for pod running 01/18/23 17:34:30.464
Jan 18 17:34:30.464: INFO: Waiting up to 2m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647" to be "running"
Jan 18 17:34:30.471: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728863ms
Jan 18 17:34:32.479: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Running", Reason="", readiness=true. Elapsed: 2.014947503s
Jan 18 17:34:32.479: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 17:34:32.479
Jan 18 17:34:32.479: INFO: Deleting pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647"
Jan 18 17:34:32.501: INFO: Wait up to 5m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:35:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5647" for this suite. 01/18/23 17:35:04.525
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":143,"skipped":2665,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.667 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:32:29.869
    Jan 18 17:32:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:32:29.87
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:32:29.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:32:29.897
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/18/23 17:32:29.902
    Jan 18 17:32:29.915: INFO: Waiting up to 2m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647" to be "running"
    Jan 18 17:32:29.921: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078871ms
    Jan 18 17:32:31.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013817571s
    Jan 18 17:32:33.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017492076s
    Jan 18 17:32:35.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01356432s
    Jan 18 17:32:37.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013598654s
    Jan 18 17:32:39.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 10.014178388s
    Jan 18 17:32:41.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 12.013970748s
    Jan 18 17:32:43.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 14.014848123s
    Jan 18 17:32:45.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 16.013919547s
    Jan 18 17:32:47.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 18.013346767s
    Jan 18 17:32:49.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 20.016619455s
    Jan 18 17:32:51.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 22.016339834s
    Jan 18 17:32:53.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 24.014483827s
    Jan 18 17:32:55.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 26.01302525s
    Jan 18 17:32:57.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 28.014658866s
    Jan 18 17:32:59.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 30.015843924s
    Jan 18 17:33:01.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 32.015145442s
    Jan 18 17:33:03.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 34.014181816s
    Jan 18 17:33:05.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015766698s
    Jan 18 17:33:07.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 38.014833456s
    Jan 18 17:33:09.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 40.014695618s
    Jan 18 17:33:11.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 42.016858497s
    Jan 18 17:33:13.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 44.014262321s
    Jan 18 17:33:15.927: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 46.012668388s
    Jan 18 17:33:17.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 48.013423587s
    Jan 18 17:33:19.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 50.016347828s
    Jan 18 17:33:21.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 52.014364832s
    Jan 18 17:33:23.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015551988s
    Jan 18 17:33:25.936: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 56.020844594s
    Jan 18 17:33:27.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 58.013650019s
    Jan 18 17:33:29.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.014952117s
    Jan 18 17:33:31.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.014985123s
    Jan 18 17:33:33.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.017621048s
    Jan 18 17:33:35.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.014634186s
    Jan 18 17:33:37.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.014785875s
    Jan 18 17:33:39.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.015898921s
    Jan 18 17:33:41.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01533577s
    Jan 18 17:33:43.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.014485816s
    Jan 18 17:33:45.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.01633779s
    Jan 18 17:33:47.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.014094711s
    Jan 18 17:33:49.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.017789099s
    Jan 18 17:33:51.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.015380544s
    Jan 18 17:33:53.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.013904383s
    Jan 18 17:33:55.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.017409092s
    Jan 18 17:33:57.929: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.014679508s
    Jan 18 17:33:59.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.014890778s
    Jan 18 17:34:01.932: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.016970823s
    Jan 18 17:34:03.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016102972s
    Jan 18 17:34:05.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.015001128s
    Jan 18 17:34:07.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.013554774s
    Jan 18 17:34:09.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.01512905s
    Jan 18 17:34:11.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.015585234s
    Jan 18 17:34:13.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.012971535s
    Jan 18 17:34:15.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.016413949s
    Jan 18 17:34:17.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015060446s
    Jan 18 17:34:19.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.016063769s
    Jan 18 17:34:21.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.013317469s
    Jan 18 17:34:23.930: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015455868s
    Jan 18 17:34:25.934: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.018991258s
    Jan 18 17:34:27.931: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.016543468s
    Jan 18 17:34:29.928: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.013514625s
    Jan 18 17:34:29.940: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.025574401s
    STEP: updating the pod 01/18/23 17:34:29.94
    Jan 18 17:34:30.464: INFO: Successfully updated pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360"
    STEP: waiting for pod running 01/18/23 17:34:30.464
    Jan 18 17:34:30.464: INFO: Waiting up to 2m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647" to be "running"
    Jan 18 17:34:30.471: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Pending", Reason="", readiness=false. Elapsed: 6.728863ms
    Jan 18 17:34:32.479: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360": Phase="Running", Reason="", readiness=true. Elapsed: 2.014947503s
    Jan 18 17:34:32.479: INFO: Pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 17:34:32.479
    Jan 18 17:34:32.479: INFO: Deleting pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" in namespace "var-expansion-5647"
    Jan 18 17:34:32.501: INFO: Wait up to 5m0s for pod "var-expansion-58d9dbf5-5b0a-45bd-8856-7db91785e360" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:35:04.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5647" for this suite. 01/18/23 17:35:04.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:35:04.539
Jan 18 17:35:04.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:35:04.54
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:04.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:04.569
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-20366e82-d2b6-411b-8053-7924298d221a 01/18/23 17:35:04.573
STEP: Creating a pod to test consume secrets 01/18/23 17:35:04.582
Jan 18 17:35:04.598: INFO: Waiting up to 5m0s for pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2" in namespace "secrets-16" to be "Succeeded or Failed"
Jan 18 17:35:04.603: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212561ms
Jan 18 17:35:06.612: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039325s
Jan 18 17:35:08.611: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0126412s
STEP: Saw pod success 01/18/23 17:35:08.611
Jan 18 17:35:08.611: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2" satisfied condition "Succeeded or Failed"
Jan 18 17:35:08.620: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:35:08.654
Jan 18 17:35:08.674: INFO: Waiting for pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 to disappear
Jan 18 17:35:08.679: INFO: Pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:35:08.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-16" for this suite. 01/18/23 17:35:08.694
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":144,"skipped":2702,"failed":0}
------------------------------
â€¢ [4.169 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:35:04.539
    Jan 18 17:35:04.539: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:35:04.54
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:04.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:04.569
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-20366e82-d2b6-411b-8053-7924298d221a 01/18/23 17:35:04.573
    STEP: Creating a pod to test consume secrets 01/18/23 17:35:04.582
    Jan 18 17:35:04.598: INFO: Waiting up to 5m0s for pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2" in namespace "secrets-16" to be "Succeeded or Failed"
    Jan 18 17:35:04.603: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212561ms
    Jan 18 17:35:06.612: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014039325s
    Jan 18 17:35:08.611: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0126412s
    STEP: Saw pod success 01/18/23 17:35:08.611
    Jan 18 17:35:08.611: INFO: Pod "pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2" satisfied condition "Succeeded or Failed"
    Jan 18 17:35:08.620: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:35:08.654
    Jan 18 17:35:08.674: INFO: Waiting for pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 to disappear
    Jan 18 17:35:08.679: INFO: Pod pod-secrets-fc2e9558-7db3-4f87-bcb6-0e438cf389a2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:35:08.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-16" for this suite. 01/18/23 17:35:08.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:35:08.709
Jan 18 17:35:08.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:35:08.71
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:08.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:08.736
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-747 01/18/23 17:35:08.739
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[] 01/18/23 17:35:08.762
Jan 18 17:35:08.769: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 18 17:35:09.785: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-747 01/18/23 17:35:09.785
Jan 18 17:35:09.819: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-747" to be "running and ready"
Jan 18 17:35:09.864: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.748791ms
Jan 18 17:35:09.864: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:35:11.872: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.053127404s
Jan 18 17:35:11.872: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 18 17:35:11.872: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod1:[80]] 01/18/23 17:35:11.879
Jan 18 17:35:11.901: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/18/23 17:35:11.901
Jan 18 17:35:11.901: INFO: Creating new exec pod
Jan 18 17:35:11.910: INFO: Waiting up to 5m0s for pod "execpod47slk" in namespace "services-747" to be "running"
Jan 18 17:35:11.919: INFO: Pod "execpod47slk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200314ms
Jan 18 17:35:13.926: INFO: Pod "execpod47slk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016030123s
Jan 18 17:35:13.927: INFO: Pod "execpod47slk" satisfied condition "running"
Jan 18 17:35:14.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 17:35:15.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:15.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:35:15.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
Jan 18 17:35:15.311: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:15.311: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-747 01/18/23 17:35:15.311
Jan 18 17:35:15.324: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-747" to be "running and ready"
Jan 18 17:35:15.330: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.897307ms
Jan 18 17:35:15.330: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:35:17.338: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014760716s
Jan 18 17:35:17.338: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 18 17:35:17.338: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 17:35:17.345
Jan 18 17:35:17.374: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 17:35:17.374
Jan 18 17:35:18.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 17:35:18.578: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:18.578: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:35:18.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
Jan 18 17:35:18.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:18.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-747 01/18/23 17:35:18.767
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod2:[80]] 01/18/23 17:35:18.79
Jan 18 17:35:19.824: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/18/23 17:35:19.824
Jan 18 17:35:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 18 17:35:21.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:21.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:35:21.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
Jan 18 17:35:21.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
Jan 18 17:35:21.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-747 01/18/23 17:35:21.212
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[] 01/18/23 17:35:21.235
Jan 18 17:35:21.250: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:35:21.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-747" for this suite. 01/18/23 17:35:21.289
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":145,"skipped":2727,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.591 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:35:08.709
    Jan 18 17:35:08.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:35:08.71
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:08.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:08.736
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-747 01/18/23 17:35:08.739
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[] 01/18/23 17:35:08.762
    Jan 18 17:35:08.769: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 18 17:35:09.785: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-747 01/18/23 17:35:09.785
    Jan 18 17:35:09.819: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-747" to be "running and ready"
    Jan 18 17:35:09.864: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 44.748791ms
    Jan 18 17:35:09.864: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:35:11.872: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.053127404s
    Jan 18 17:35:11.872: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 18 17:35:11.872: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod1:[80]] 01/18/23 17:35:11.879
    Jan 18 17:35:11.901: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/18/23 17:35:11.901
    Jan 18 17:35:11.901: INFO: Creating new exec pod
    Jan 18 17:35:11.910: INFO: Waiting up to 5m0s for pod "execpod47slk" in namespace "services-747" to be "running"
    Jan 18 17:35:11.919: INFO: Pod "execpod47slk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.200314ms
    Jan 18 17:35:13.926: INFO: Pod "execpod47slk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016030123s
    Jan 18 17:35:13.927: INFO: Pod "execpod47slk" satisfied condition "running"
    Jan 18 17:35:14.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 17:35:15.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:15.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:35:15.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
    Jan 18 17:35:15.311: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:15.311: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-747 01/18/23 17:35:15.311
    Jan 18 17:35:15.324: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-747" to be "running and ready"
    Jan 18 17:35:15.330: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.897307ms
    Jan 18 17:35:15.330: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:35:17.338: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.014760716s
    Jan 18 17:35:17.338: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 18 17:35:17.338: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod1:[80] pod2:[80]] 01/18/23 17:35:17.345
    Jan 18 17:35:17.374: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/18/23 17:35:17.374
    Jan 18 17:35:18.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 17:35:18.578: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:18.578: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:35:18.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
    Jan 18 17:35:18.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:18.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-747 01/18/23 17:35:18.767
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[pod2:[80]] 01/18/23 17:35:18.79
    Jan 18 17:35:19.824: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/18/23 17:35:19.824
    Jan 18 17:35:20.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 18 17:35:21.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:21.020: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:35:21.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-747 exec execpod47slk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.59.211 80'
    Jan 18 17:35:21.212: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.59.211 80\nConnection to 10.96.59.211 80 port [tcp/http] succeeded!\n"
    Jan 18 17:35:21.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-747 01/18/23 17:35:21.212
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-747 to expose endpoints map[] 01/18/23 17:35:21.235
    Jan 18 17:35:21.250: INFO: successfully validated that service endpoint-test2 in namespace services-747 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:35:21.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-747" for this suite. 01/18/23 17:35:21.289
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:35:21.3
Jan 18 17:35:21.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:35:21.301
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:21.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:21.336
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b in namespace container-probe-9108 01/18/23 17:35:21.341
Jan 18 17:35:21.357: INFO: Waiting up to 5m0s for pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b" in namespace "container-probe-9108" to be "not pending"
Jan 18 17:35:21.365: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.260917ms
Jan 18 17:35:23.373: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015723482s
Jan 18 17:35:23.373: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b" satisfied condition "not pending"
Jan 18 17:35:23.373: INFO: Started pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b in namespace container-probe-9108
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:35:23.373
Jan 18 17:35:23.380: INFO: Initial restart count of pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b is 0
Jan 18 17:35:43.481: INFO: Restart count of pod container-probe-9108/liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b is now 1 (20.100386709s elapsed)
STEP: deleting the pod 01/18/23 17:35:43.481
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:35:43.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9108" for this suite. 01/18/23 17:35:43.511
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":146,"skipped":2734,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.225 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:35:21.3
    Jan 18 17:35:21.300: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:35:21.301
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:21.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:21.336
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b in namespace container-probe-9108 01/18/23 17:35:21.341
    Jan 18 17:35:21.357: INFO: Waiting up to 5m0s for pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b" in namespace "container-probe-9108" to be "not pending"
    Jan 18 17:35:21.365: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.260917ms
    Jan 18 17:35:23.373: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b": Phase="Running", Reason="", readiness=true. Elapsed: 2.015723482s
    Jan 18 17:35:23.373: INFO: Pod "liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b" satisfied condition "not pending"
    Jan 18 17:35:23.373: INFO: Started pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b in namespace container-probe-9108
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:35:23.373
    Jan 18 17:35:23.380: INFO: Initial restart count of pod liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b is 0
    Jan 18 17:35:43.481: INFO: Restart count of pod container-probe-9108/liveness-b923c113-14cc-4819-b8bc-3c1fc7f9343b is now 1 (20.100386709s elapsed)
    STEP: deleting the pod 01/18/23 17:35:43.481
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:35:43.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9108" for this suite. 01/18/23 17:35:43.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:35:43.528
Jan 18 17:35:43.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 17:35:43.529
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:43.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:43.563
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 18 17:35:43.569: INFO: Creating deployment "webserver-deployment"
Jan 18 17:35:43.579: INFO: Waiting for observed generation 1
Jan 18 17:35:45.593: INFO: Waiting for all required pods to come up
Jan 18 17:35:45.602: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/18/23 17:35:45.602
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zqv5n" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-27bh5" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5779z" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j8w4w" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-t6pf5" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vwwgk" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sfq2z" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wdk5k" in namespace "deployment-1662" to be "running"
Jan 18 17:35:45.609: INFO: Pod "webserver-deployment-845c8977d9-27bh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335099ms
Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-wdk5k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.658932ms
Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-sfq2z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.903034ms
Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-t6pf5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.330597ms
Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-5779z": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67139ms
Jan 18 17:35:45.611: INFO: Pod "webserver-deployment-845c8977d9-vwwgk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.895231ms
Jan 18 17:35:45.612: INFO: Pod "webserver-deployment-845c8977d9-zqv5n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.855069ms
Jan 18 17:35:45.612: INFO: Pod "webserver-deployment-845c8977d9-j8w4w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.850779ms
Jan 18 17:35:47.616: INFO: Pod "webserver-deployment-845c8977d9-27bh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013258508s
Jan 18 17:35:47.616: INFO: Pod "webserver-deployment-845c8977d9-27bh5" satisfied condition "running"
Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-sfq2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.013852463s
Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-sfq2z" satisfied condition "running"
Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-zqv5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014622419s
Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-zqv5n" satisfied condition "running"
Jan 18 17:35:47.618: INFO: Pod "webserver-deployment-845c8977d9-5779z": Phase="Running", Reason="", readiness=true. Elapsed: 2.015546146s
Jan 18 17:35:47.618: INFO: Pod "webserver-deployment-845c8977d9-5779z" satisfied condition "running"
Jan 18 17:35:47.619: INFO: Pod "webserver-deployment-845c8977d9-wdk5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.015612356s
Jan 18 17:35:47.619: INFO: Pod "webserver-deployment-845c8977d9-wdk5k" satisfied condition "running"
Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-t6pf5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016732735s
Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-t6pf5" satisfied condition "running"
Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-vwwgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01735208s
Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-vwwgk" satisfied condition "running"
Jan 18 17:35:47.621: INFO: Pod "webserver-deployment-845c8977d9-j8w4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.017592432s
Jan 18 17:35:47.621: INFO: Pod "webserver-deployment-845c8977d9-j8w4w" satisfied condition "running"
Jan 18 17:35:47.621: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 18 17:35:47.634: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 18 17:35:47.653: INFO: Updating deployment webserver-deployment
Jan 18 17:35:47.653: INFO: Waiting for observed generation 2
Jan 18 17:35:49.666: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 18 17:35:49.673: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 18 17:35:49.679: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 17:35:49.697: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 18 17:35:49.697: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 18 17:35:49.702: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 18 17:35:49.715: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 18 17:35:49.715: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 18 17:35:49.733: INFO: Updating deployment webserver-deployment
Jan 18 17:35:49.733: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 18 17:35:49.773: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 18 17:35:49.786: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 17:35:49.859: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-1662  bf3794be-091b-4617-adaa-04624695868b 2631875445 3 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 17:35:47 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 17:35:49 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 18 17:35:49.867: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-1662  5f44078f-672d-4cb0-95bf-accf97339f49 2631875442 3 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bf3794be-091b-4617-adaa-04624695868b 0xc003bf0bd7 0xc003bf0bd8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf3794be-091b-4617-adaa-04624695868b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 17:35:49.867: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 18 17:35:49.867: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-1662  85b7c181-c662-45b3-be88-8ccaa837c9b4 2631875441 3 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bf3794be-091b-4617-adaa-04624695868b 0xc003bf0cd7 0xc003bf0cd8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf3794be-091b-4617-adaa-04624695868b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-2tjv5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2tjv5 webserver-deployment-69b7448995- deployment-1662  e6302d4e-11bf-4b34-a05c-b65e2ebc179b 2631875449 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1287 0xc003bf1288}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4w2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4w2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-5tfrj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5tfrj webserver-deployment-69b7448995- deployment-1662  6f44a62d-f904-4477-a9a3-fa7902b9fb3f 2631875464 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf13f7 0xc003bf13f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdr6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdr6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-dmmzx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-dmmzx webserver-deployment-69b7448995- deployment-1662  8cdb3e8f-d13c-4dc9-8b58-63a94da9b7f4 2631875390 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6639db400b317d03e7ef7a462be186b27bfb41153f68beb399bcb4542ed13406 cni.projectcalico.org/podIP:10.100.145.139/32 cni.projectcalico.org/podIPs:10.100.145.139/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1567 0xc003bf1568}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qnt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qnt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-jqtsd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-jqtsd webserver-deployment-69b7448995- deployment-1662  5fff62f8-0fdf-4e6b-8590-ae553e12771e 2631875470 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1777 0xc003bf1778}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x2l5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x2l5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-lg2nr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lg2nr webserver-deployment-69b7448995- deployment-1662  392ef150-931b-4cdb-88a7-0c4cd92c95ae 2631875467 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf18e7 0xc003bf18e8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgldd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgldd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-pvg9z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pvg9z webserver-deployment-69b7448995- deployment-1662  7b7f61ab-55d1-41f7-a61c-20f49b269273 2631875463 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1a47 0xc003bf1a48}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqkrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqkrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-tznl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tznl9 webserver-deployment-69b7448995- deployment-1662  e39d8b7d-df61-4f3a-8fa9-e57930b9da71 2631875398 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fc195b02b72b76c1889027053d0db5e1117dc4027eee6fb7769614892276225b cni.projectcalico.org/podIP:10.100.158.33/32 cni.projectcalico.org/podIPs:10.100.158.33/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1bc7 0xc003bf1bc8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nch6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nch6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.33,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-wn8n4" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wn8n4 webserver-deployment-69b7448995- deployment-1662  f1cf0fbe-7d10-420c-98e8-c5041222b5ce 2631875372 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:68c4de8d1fbfac05c783e802ed93fd5e0b1ce70304b8a2c08f8ee155ba9af821 cni.projectcalico.org/podIP:10.100.158.37/32 cni.projectcalico.org/podIPs:10.100.158.37/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1e27 0xc003bf1e28}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzjf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzjf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-wx6xm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wx6xm webserver-deployment-69b7448995- deployment-1662  8ad95fc3-ff7a-4808-9943-a54697bb18de 2631875425 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d109e39099e829ea678942f5094da37e008fb40a05b3e9b23cf5123bf35b452f cni.projectcalico.org/podIP:10.100.145.188/32 cni.projectcalico.org/podIPs:10.100.145.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a027 0xc002c9a028}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdfns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdfns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zbtj5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zbtj5 webserver-deployment-69b7448995- deployment-1662  a6ff28ed-6eb7-48a4-8237-070082a2f1b1 2631875465 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a227 0xc002c9a228}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m86wr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m86wr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zq8m6" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zq8m6 webserver-deployment-69b7448995- deployment-1662  a61e3f62-5cc5-46aa-97fb-f6f8eb00dd86 2631875407 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fe323bbb8ac4d8cdf9d7e65b8cb81424e327d8047b757cf400f6b322aba0970f cni.projectcalico.org/podIP:10.100.145.135/32 cni.projectcalico.org/podIPs:10.100.145.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a397 0xc002c9a398}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ptzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ptzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zxtkp" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zxtkp webserver-deployment-69b7448995- deployment-1662  a65a0ee8-cdfe-4062-ab74-3bc54e2f28dd 2631875462 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a5b7 0xc002c9a5b8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prwz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prwz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-845c8977d9-2pwb6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2pwb6 webserver-deployment-845c8977d9- deployment-1662  84b664d2-3366-4c89-9053-5a12712dc9b6 2631875466 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a717 0xc002c9a718}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9br4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9br4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-487nf" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-487nf webserver-deployment-845c8977d9- deployment-1662  9ba49376-5692-4fe2-8c51-64b75742a5aa 2631875471 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a877 0xc002c9a878}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59cdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59cdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-5779z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5779z webserver-deployment-845c8977d9- deployment-1662  e6c06c8b-e4b9-46bc-8483-abaa42dfc0fb 2631875297 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6aecfac2e1da42a4a002e90158e74be1e6b3b92fdd837e13d7d845c45218dc7 cni.projectcalico.org/podIP:10.100.158.32/32 cni.projectcalico.org/podIPs:10.100.158.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a9f7 0xc002c9a9f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whx52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whx52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.32,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ff578bbffd36575326b56ae59ee7562d9a5195781ff517f9ab2e39b36478362d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-6lph5" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-6lph5 webserver-deployment-845c8977d9- deployment-1662  939e0b96-0b7a-44d7-b3aa-d6774685c290 2631875453 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9ac27 0xc002c9ac28}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2tgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2tgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-99wx6" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-99wx6 webserver-deployment-845c8977d9- deployment-1662  e2276fc9-fbc4-4601-a576-e04f951c6a5f 2631875205 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:21b410e1e184921ddd0bde74f2d6f89b7cfea0170961737c9eb10a106dc5d5f7 cni.projectcalico.org/podIP:10.100.145.169/32 cni.projectcalico.org/podIPs:10.100.145.169/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9ada7 0xc002c9ada8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cgkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cgkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.169,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://31339fe481f25e15b46f3ed661951bbdcd58d951788595e59d68fd264103ff4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-j8w4w" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j8w4w webserver-deployment-845c8977d9- deployment-1662  8e67be7b-539b-4a04-b626-3f2cb63764f5 2631875258 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4035eec118135996cf7b80dc3369cbdb8cc57dfd1d3d2fdbf428226905cd63cb cni.projectcalico.org/podIP:10.100.158.31/32 cni.projectcalico.org/podIPs:10.100.158.31/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9afd7 0xc002c9afd8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f699v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f699v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.31,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://027344036412ddab6bb4d001f95026d3a350115c67ece106c4d14eddc5297735,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-jl7bs" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jl7bs webserver-deployment-845c8977d9- deployment-1662  13a37f07-4bec-470c-af8b-13f0b7044362 2631875461 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b207 0xc002c9b208}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwj8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwj8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-lblwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lblwp webserver-deployment-845c8977d9- deployment-1662  eb6c9cc8-fbe2-4955-935c-7501d8e4f7f4 2631875452 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b3d7 0xc002c9b3d8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cl25h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cl25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-nqpnt" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-nqpnt webserver-deployment-845c8977d9- deployment-1662  98acc717-caac-4586-bf0b-ac0483696687 2631875210 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e347892eac2c202655819053a8888afaf091d265a0883028b2433f4facb09c4c cni.projectcalico.org/podIP:10.100.158.28/32 cni.projectcalico.org/podIPs:10.100.158.28/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b557 0xc002c9b558}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.28,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23d9928c7b3867bc9556a21f733b66d491252d2271612216686fc0a6b18f13ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-sfq2z" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfq2z webserver-deployment-845c8977d9- deployment-1662  3b83c339-3ec2-4dfe-a34b-8695a2e161be 2631875239 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:eb2d534be41d358549ffd148f87b379046fbe63f92213ef47df713023d8dd031 cni.projectcalico.org/podIP:10.100.145.177/32 cni.projectcalico.org/podIPs:10.100.145.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b757 0xc002c9b758}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzlnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzlnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.177,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://967d4c69345402f0c9fbc717d61c9bbe17735cec809362c8473055d88b714255,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-sxd8t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-sxd8t webserver-deployment-845c8977d9- deployment-1662  4e2a5268-d362-4724-a3ee-409da1abaef9 2631875469 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b967 0xc002c9b968}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zppp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zppp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-t6pf5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t6pf5 webserver-deployment-845c8977d9- deployment-1662  3993eb47-d168-47da-b1cc-0e3aa8af7422 2631875271 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:45e127cda08c99edd132b0d570f1dfe370c7eea69513c23d3e785d4cca06259e cni.projectcalico.org/podIP:10.100.158.29/32 cni.projectcalico.org/podIPs:10.100.158.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bae7 0xc002c9bae8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvtgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvtgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.29,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b852a634361f15dba54128e048bcfd726760ba282189b10dfed918c594851d65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-vwwgk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vwwgk webserver-deployment-845c8977d9- deployment-1662  c7554d37-fb6c-4962-859a-69d67f5e35ac 2631875255 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:95c3c5ff2743def303d3ea87736cc66b1e40718aecfd03c8dc6e4e08c56868eb cni.projectcalico.org/podIP:10.100.145.183/32 cni.projectcalico.org/podIPs:10.100.145.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bcf7 0xc002c9bcf8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq5lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq5lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.183,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://db3a695398010ffc512c54a7c7d20c365c5cf25c5eecf8ce9cc717e3b6119085,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-w9rjh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-w9rjh webserver-deployment-845c8977d9- deployment-1662  a17d84e8-97b9-4389-b832-abd6a3dd147f 2631875468 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bef7 0xc002c9bef8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hb7qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hb7qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-zqv5n" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zqv5n webserver-deployment-845c8977d9- deployment-1662  89a42e44-1814-4ca3-8135-0d51cb64aecc 2631875243 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b3613452c30b36f1e367aceaf304c6954597aaa9324e819e486fe0c1ffe7a7c8 cni.projectcalico.org/podIP:10.100.158.30/32 cni.projectcalico.org/podIPs:10.100.158.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc0035262f7 0xc0035262f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8c88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8c88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.30,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://54a3f72afc31b016d50b93ff0ad454c4cd4d00d2f086d0d363db4b07d228dab9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 17:35:49.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1662" for this suite. 01/18/23 17:35:50.061
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":147,"skipped":2769,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.632 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:35:43.528
    Jan 18 17:35:43.528: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 17:35:43.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:43.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:43.563
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 18 17:35:43.569: INFO: Creating deployment "webserver-deployment"
    Jan 18 17:35:43.579: INFO: Waiting for observed generation 1
    Jan 18 17:35:45.593: INFO: Waiting for all required pods to come up
    Jan 18 17:35:45.602: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/18/23 17:35:45.602
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zqv5n" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-27bh5" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5779z" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-j8w4w" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-t6pf5" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-vwwgk" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-sfq2z" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.603: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-wdk5k" in namespace "deployment-1662" to be "running"
    Jan 18 17:35:45.609: INFO: Pod "webserver-deployment-845c8977d9-27bh5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.335099ms
    Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-wdk5k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.658932ms
    Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-sfq2z": Phase="Pending", Reason="", readiness=false. Elapsed: 6.903034ms
    Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-t6pf5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.330597ms
    Jan 18 17:35:45.610: INFO: Pod "webserver-deployment-845c8977d9-5779z": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67139ms
    Jan 18 17:35:45.611: INFO: Pod "webserver-deployment-845c8977d9-vwwgk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.895231ms
    Jan 18 17:35:45.612: INFO: Pod "webserver-deployment-845c8977d9-zqv5n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.855069ms
    Jan 18 17:35:45.612: INFO: Pod "webserver-deployment-845c8977d9-j8w4w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.850779ms
    Jan 18 17:35:47.616: INFO: Pod "webserver-deployment-845c8977d9-27bh5": Phase="Running", Reason="", readiness=true. Elapsed: 2.013258508s
    Jan 18 17:35:47.616: INFO: Pod "webserver-deployment-845c8977d9-27bh5" satisfied condition "running"
    Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-sfq2z": Phase="Running", Reason="", readiness=true. Elapsed: 2.013852463s
    Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-sfq2z" satisfied condition "running"
    Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-zqv5n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014622419s
    Jan 18 17:35:47.617: INFO: Pod "webserver-deployment-845c8977d9-zqv5n" satisfied condition "running"
    Jan 18 17:35:47.618: INFO: Pod "webserver-deployment-845c8977d9-5779z": Phase="Running", Reason="", readiness=true. Elapsed: 2.015546146s
    Jan 18 17:35:47.618: INFO: Pod "webserver-deployment-845c8977d9-5779z" satisfied condition "running"
    Jan 18 17:35:47.619: INFO: Pod "webserver-deployment-845c8977d9-wdk5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.015612356s
    Jan 18 17:35:47.619: INFO: Pod "webserver-deployment-845c8977d9-wdk5k" satisfied condition "running"
    Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-t6pf5": Phase="Running", Reason="", readiness=true. Elapsed: 2.016732735s
    Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-t6pf5" satisfied condition "running"
    Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-vwwgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.01735208s
    Jan 18 17:35:47.620: INFO: Pod "webserver-deployment-845c8977d9-vwwgk" satisfied condition "running"
    Jan 18 17:35:47.621: INFO: Pod "webserver-deployment-845c8977d9-j8w4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.017592432s
    Jan 18 17:35:47.621: INFO: Pod "webserver-deployment-845c8977d9-j8w4w" satisfied condition "running"
    Jan 18 17:35:47.621: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 18 17:35:47.634: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 18 17:35:47.653: INFO: Updating deployment webserver-deployment
    Jan 18 17:35:47.653: INFO: Waiting for observed generation 2
    Jan 18 17:35:49.666: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 18 17:35:49.673: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 18 17:35:49.679: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 17:35:49.697: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 18 17:35:49.697: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 18 17:35:49.702: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 17:35:49.715: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 18 17:35:49.715: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 18 17:35:49.733: INFO: Updating deployment webserver-deployment
    Jan 18 17:35:49.733: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 18 17:35:49.773: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 18 17:35:49.786: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 17:35:49.859: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-1662  bf3794be-091b-4617-adaa-04624695868b 2631875445 3 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0798 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-18 17:35:47 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 17:35:49 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 18 17:35:49.867: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-1662  5f44078f-672d-4cb0-95bf-accf97339f49 2631875442 3 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment bf3794be-091b-4617-adaa-04624695868b 0xc003bf0bd7 0xc003bf0bd8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf3794be-091b-4617-adaa-04624695868b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0c78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 17:35:49.867: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 18 17:35:49.867: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-1662  85b7c181-c662-45b3-be88-8ccaa837c9b4 2631875441 3 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment bf3794be-091b-4617-adaa-04624695868b 0xc003bf0cd7 0xc003bf0cd8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf3794be-091b-4617-adaa-04624695868b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003bf0d68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-2tjv5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2tjv5 webserver-deployment-69b7448995- deployment-1662  e6302d4e-11bf-4b34-a05c-b65e2ebc179b 2631875449 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1287 0xc003bf1288}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4w2c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4w2c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-5tfrj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5tfrj webserver-deployment-69b7448995- deployment-1662  6f44a62d-f904-4477-a9a3-fa7902b9fb3f 2631875464 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf13f7 0xc003bf13f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdr6t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdr6t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.964: INFO: Pod "webserver-deployment-69b7448995-dmmzx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-dmmzx webserver-deployment-69b7448995- deployment-1662  8cdb3e8f-d13c-4dc9-8b58-63a94da9b7f4 2631875390 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6639db400b317d03e7ef7a462be186b27bfb41153f68beb399bcb4542ed13406 cni.projectcalico.org/podIP:10.100.145.139/32 cni.projectcalico.org/podIPs:10.100.145.139/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1567 0xc003bf1568}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6qnt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6qnt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-jqtsd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-jqtsd webserver-deployment-69b7448995- deployment-1662  5fff62f8-0fdf-4e6b-8590-ae553e12771e 2631875470 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1777 0xc003bf1778}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x2l5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x2l5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-lg2nr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lg2nr webserver-deployment-69b7448995- deployment-1662  392ef150-931b-4cdb-88a7-0c4cd92c95ae 2631875467 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf18e7 0xc003bf18e8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgldd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgldd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-pvg9z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pvg9z webserver-deployment-69b7448995- deployment-1662  7b7f61ab-55d1-41f7-a61c-20f49b269273 2631875463 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1a47 0xc003bf1a48}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dqkrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dqkrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-tznl9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tznl9 webserver-deployment-69b7448995- deployment-1662  e39d8b7d-df61-4f3a-8fa9-e57930b9da71 2631875398 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fc195b02b72b76c1889027053d0db5e1117dc4027eee6fb7769614892276225b cni.projectcalico.org/podIP:10.100.158.33/32 cni.projectcalico.org/podIPs:10.100.158.33/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1bc7 0xc003bf1bc8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nch6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nch6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.33,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-wn8n4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wn8n4 webserver-deployment-69b7448995- deployment-1662  f1cf0fbe-7d10-420c-98e8-c5041222b5ce 2631875372 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:68c4de8d1fbfac05c783e802ed93fd5e0b1ce70304b8a2c08f8ee155ba9af821 cni.projectcalico.org/podIP:10.100.158.37/32 cni.projectcalico.org/podIPs:10.100.158.37/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc003bf1e27 0xc003bf1e28}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zzjf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zzjf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.965: INFO: Pod "webserver-deployment-69b7448995-wx6xm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wx6xm webserver-deployment-69b7448995- deployment-1662  8ad95fc3-ff7a-4808-9943-a54697bb18de 2631875425 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:d109e39099e829ea678942f5094da37e008fb40a05b3e9b23cf5123bf35b452f cni.projectcalico.org/podIP:10.100.145.188/32 cni.projectcalico.org/podIPs:10.100.145.188/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a027 0xc002c9a028}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qdfns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qdfns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zbtj5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zbtj5 webserver-deployment-69b7448995- deployment-1662  a6ff28ed-6eb7-48a4-8237-070082a2f1b1 2631875465 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a227 0xc002c9a228}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m86wr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m86wr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zq8m6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zq8m6 webserver-deployment-69b7448995- deployment-1662  a61e3f62-5cc5-46aa-97fb-f6f8eb00dd86 2631875407 0 2023-01-18 17:35:47 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fe323bbb8ac4d8cdf9d7e65b8cb81424e327d8047b757cf400f6b322aba0970f cni.projectcalico.org/podIP:10.100.145.135/32 cni.projectcalico.org/podIPs:10.100.145.135/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a397 0xc002c9a398}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:47 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ptzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ptzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:47 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:47 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-69b7448995-zxtkp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zxtkp webserver-deployment-69b7448995- deployment-1662  a65a0ee8-cdfe-4062-ab74-3bc54e2f28dd 2631875462 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 5f44078f-672d-4cb0-95bf-accf97339f49 0xc002c9a5b7 0xc002c9a5b8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f44078f-672d-4cb0-95bf-accf97339f49\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-prwz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-prwz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.966: INFO: Pod "webserver-deployment-845c8977d9-2pwb6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2pwb6 webserver-deployment-845c8977d9- deployment-1662  84b664d2-3366-4c89-9053-5a12712dc9b6 2631875466 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a717 0xc002c9a718}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9br4j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9br4j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-487nf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-487nf webserver-deployment-845c8977d9- deployment-1662  9ba49376-5692-4fe2-8c51-64b75742a5aa 2631875471 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a877 0xc002c9a878}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-59cdd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-59cdd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-5779z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5779z webserver-deployment-845c8977d9- deployment-1662  e6c06c8b-e4b9-46bc-8483-abaa42dfc0fb 2631875297 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e6aecfac2e1da42a4a002e90158e74be1e6b3b92fdd837e13d7d845c45218dc7 cni.projectcalico.org/podIP:10.100.158.32/32 cni.projectcalico.org/podIPs:10.100.158.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9a9f7 0xc002c9a9f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.32\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-whx52,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-whx52,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.32,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ff578bbffd36575326b56ae59ee7562d9a5195781ff517f9ab2e39b36478362d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.32,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-6lph5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-6lph5 webserver-deployment-845c8977d9- deployment-1662  939e0b96-0b7a-44d7-b3aa-d6774685c290 2631875453 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9ac27 0xc002c9ac28}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f2tgq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f2tgq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-99wx6" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-99wx6 webserver-deployment-845c8977d9- deployment-1662  e2276fc9-fbc4-4601-a576-e04f951c6a5f 2631875205 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:21b410e1e184921ddd0bde74f2d6f89b7cfea0170961737c9eb10a106dc5d5f7 cni.projectcalico.org/podIP:10.100.145.169/32 cni.projectcalico.org/podIPs:10.100.145.169/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9ada7 0xc002c9ada8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.169\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6cgkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6cgkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.169,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://31339fe481f25e15b46f3ed661951bbdcd58d951788595e59d68fd264103ff4f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.967: INFO: Pod "webserver-deployment-845c8977d9-j8w4w" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j8w4w webserver-deployment-845c8977d9- deployment-1662  8e67be7b-539b-4a04-b626-3f2cb63764f5 2631875258 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4035eec118135996cf7b80dc3369cbdb8cc57dfd1d3d2fdbf428226905cd63cb cni.projectcalico.org/podIP:10.100.158.31/32 cni.projectcalico.org/podIPs:10.100.158.31/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9afd7 0xc002c9afd8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.31\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f699v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f699v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.31,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://027344036412ddab6bb4d001f95026d3a350115c67ece106c4d14eddc5297735,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.31,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-jl7bs" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jl7bs webserver-deployment-845c8977d9- deployment-1662  13a37f07-4bec-470c-af8b-13f0b7044362 2631875461 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b207 0xc002c9b208}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwj8g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwj8g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 17:35:49 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-lblwp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lblwp webserver-deployment-845c8977d9- deployment-1662  eb6c9cc8-fbe2-4955-935c-7501d8e4f7f4 2631875452 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b3d7 0xc002c9b3d8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cl25h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cl25h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-nqpnt" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-nqpnt webserver-deployment-845c8977d9- deployment-1662  98acc717-caac-4586-bf0b-ac0483696687 2631875210 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e347892eac2c202655819053a8888afaf091d265a0883028b2433f4facb09c4c cni.projectcalico.org/podIP:10.100.158.28/32 cni.projectcalico.org/podIPs:10.100.158.28/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b557 0xc002c9b558}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqd7d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqd7d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.28,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://23d9928c7b3867bc9556a21f733b66d491252d2271612216686fc0a6b18f13ca,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-sfq2z" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sfq2z webserver-deployment-845c8977d9- deployment-1662  3b83c339-3ec2-4dfe-a34b-8695a2e161be 2631875239 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:eb2d534be41d358549ffd148f87b379046fbe63f92213ef47df713023d8dd031 cni.projectcalico.org/podIP:10.100.145.177/32 cni.projectcalico.org/podIPs:10.100.145.177/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b757 0xc002c9b758}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzlnx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzlnx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.177,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://967d4c69345402f0c9fbc717d61c9bbe17735cec809362c8473055d88b714255,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.968: INFO: Pod "webserver-deployment-845c8977d9-sxd8t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-sxd8t webserver-deployment-845c8977d9- deployment-1662  4e2a5268-d362-4724-a3ee-409da1abaef9 2631875469 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9b967 0xc002c9b968}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7zppp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7zppp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-t6pf5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t6pf5 webserver-deployment-845c8977d9- deployment-1662  3993eb47-d168-47da-b1cc-0e3aa8af7422 2631875271 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:45e127cda08c99edd132b0d570f1dfe370c7eea69513c23d3e785d4cca06259e cni.projectcalico.org/podIP:10.100.158.29/32 cni.projectcalico.org/podIPs:10.100.158.29/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bae7 0xc002c9bae8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.29\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mvtgc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mvtgc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.29,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b852a634361f15dba54128e048bcfd726760ba282189b10dfed918c594851d65,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.29,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-vwwgk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vwwgk webserver-deployment-845c8977d9- deployment-1662  c7554d37-fb6c-4962-859a-69d67f5e35ac 2631875255 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:95c3c5ff2743def303d3ea87736cc66b1e40718aecfd03c8dc6e4e08c56868eb cni.projectcalico.org/podIP:10.100.145.183/32 cni.projectcalico.org/podIPs:10.100.145.183/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bcf7 0xc002c9bcf8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gq5lz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gq5lz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.183,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://db3a695398010ffc512c54a7c7d20c365c5cf25c5eecf8ce9cc717e3b6119085,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-w9rjh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-w9rjh webserver-deployment-845c8977d9- deployment-1662  a17d84e8-97b9-4389-b832-abd6a3dd147f 2631875468 0 2023-01-18 17:35:49 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc002c9bef7 0xc002c9bef8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:49 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hb7qk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hb7qk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:49 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 18 17:35:49.969: INFO: Pod "webserver-deployment-845c8977d9-zqv5n" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zqv5n webserver-deployment-845c8977d9- deployment-1662  89a42e44-1814-4ca3-8135-0d51cb64aecc 2631875243 0 2023-01-18 17:35:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:b3613452c30b36f1e367aceaf304c6954597aaa9324e819e486fe0c1ffe7a7c8 cni.projectcalico.org/podIP:10.100.158.30/32 cni.projectcalico.org/podIPs:10.100.158.30/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 85b7c181-c662-45b3-be88-8ccaa837c9b4 0xc0035262f7 0xc0035262f8}] [] [{kube-controller-manager Update v1 2023-01-18 17:35:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85b7c181-c662-45b3-be88-8ccaa837c9b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:35:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:35:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.30\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f8c88,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f8c88,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:35:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.30,StartTime:2023-01-18 17:35:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:35:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://54a3f72afc31b016d50b93ff0ad454c4cd4d00d2f086d0d363db4b07d228dab9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.30,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 17:35:49.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1662" for this suite. 01/18/23 17:35:50.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:35:50.163
Jan 18 17:35:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 17:35:50.164
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:50.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:50.264
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 17:35:50.294: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 17:36:50.333: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:36:50.341
Jan 18 17:36:50.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 17:36:50.342
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:50.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:50.369
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 18 17:36:50.398: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 18 17:36:50.405: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 18 17:36:50.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-8954" for this suite. 01/18/23 17:36:50.447
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:36:50.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-434" for this suite. 01/18/23 17:36:50.55
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":148,"skipped":2806,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.471 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:35:50.163
    Jan 18 17:35:50.163: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 17:35:50.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:35:50.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:35:50.264
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 17:35:50.294: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 17:36:50.333: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:36:50.341
    Jan 18 17:36:50.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption-path 01/18/23 17:36:50.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:50.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:50.369
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 18 17:36:50.398: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 18 17:36:50.405: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 18 17:36:50.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-8954" for this suite. 01/18/23 17:36:50.447
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:36:50.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-434" for this suite. 01/18/23 17:36:50.55
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:36:50.635
Jan 18 17:36:50.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:36:50.636
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:50.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:50.667
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-93175139-0a50-494a-88ba-e0e36816758f 01/18/23 17:36:50.672
STEP: Creating secret with name secret-projected-all-test-volume-22cba2ac-4b4f-44ae-a727-5e53f39f42e9 01/18/23 17:36:50.681
STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 17:36:50.69
Jan 18 17:36:50.706: INFO: Waiting up to 5m0s for pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54" in namespace "projected-8588" to be "Succeeded or Failed"
Jan 18 17:36:50.713: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.255567ms
Jan 18 17:36:52.722: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015491355s
Jan 18 17:36:54.723: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016267299s
STEP: Saw pod success 01/18/23 17:36:54.723
Jan 18 17:36:54.723: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54" satisfied condition "Succeeded or Failed"
Jan 18 17:36:54.730: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 container projected-all-volume-test: <nil>
STEP: delete the pod 01/18/23 17:36:54.765
Jan 18 17:36:54.787: INFO: Waiting for pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 to disappear
Jan 18 17:36:54.793: INFO: Pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 18 17:36:54.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8588" for this suite. 01/18/23 17:36:54.802
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":149,"skipped":2817,"failed":0}
------------------------------
â€¢ [4.179 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:36:50.635
    Jan 18 17:36:50.635: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:36:50.636
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:50.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:50.667
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-93175139-0a50-494a-88ba-e0e36816758f 01/18/23 17:36:50.672
    STEP: Creating secret with name secret-projected-all-test-volume-22cba2ac-4b4f-44ae-a727-5e53f39f42e9 01/18/23 17:36:50.681
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/18/23 17:36:50.69
    Jan 18 17:36:50.706: INFO: Waiting up to 5m0s for pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54" in namespace "projected-8588" to be "Succeeded or Failed"
    Jan 18 17:36:50.713: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Pending", Reason="", readiness=false. Elapsed: 6.255567ms
    Jan 18 17:36:52.722: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015491355s
    Jan 18 17:36:54.723: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016267299s
    STEP: Saw pod success 01/18/23 17:36:54.723
    Jan 18 17:36:54.723: INFO: Pod "projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54" satisfied condition "Succeeded or Failed"
    Jan 18 17:36:54.730: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:36:54.765
    Jan 18 17:36:54.787: INFO: Waiting for pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 to disappear
    Jan 18 17:36:54.793: INFO: Pod projected-volume-d96660f8-8945-4a23-98e4-7c0cdac1ca54 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 18 17:36:54.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8588" for this suite. 01/18/23 17:36:54.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:36:54.819
Jan 18 17:36:54.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:36:54.82
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:54.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:54.859
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-3ddf26df-cebd-49cd-a4f2-77f4f9e6487b 01/18/23 17:36:54.863
STEP: Creating a pod to test consume configMaps 01/18/23 17:36:54.872
Jan 18 17:36:54.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af" in namespace "configmap-9254" to be "Succeeded or Failed"
Jan 18 17:36:54.897: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295208ms
Jan 18 17:36:56.907: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016086012s
Jan 18 17:36:58.905: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013615064s
STEP: Saw pod success 01/18/23 17:36:58.905
Jan 18 17:36:58.905: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af" satisfied condition "Succeeded or Failed"
Jan 18 17:36:58.911: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:36:58.926
Jan 18 17:36:58.946: INFO: Waiting for pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af to disappear
Jan 18 17:36:58.952: INFO: Pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:36:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9254" for this suite. 01/18/23 17:36:58.96
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":150,"skipped":2887,"failed":0}
------------------------------
â€¢ [4.153 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:36:54.819
    Jan 18 17:36:54.819: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:36:54.82
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:54.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:54.859
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-3ddf26df-cebd-49cd-a4f2-77f4f9e6487b 01/18/23 17:36:54.863
    STEP: Creating a pod to test consume configMaps 01/18/23 17:36:54.872
    Jan 18 17:36:54.891: INFO: Waiting up to 5m0s for pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af" in namespace "configmap-9254" to be "Succeeded or Failed"
    Jan 18 17:36:54.897: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.295208ms
    Jan 18 17:36:56.907: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016086012s
    Jan 18 17:36:58.905: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013615064s
    STEP: Saw pod success 01/18/23 17:36:58.905
    Jan 18 17:36:58.905: INFO: Pod "pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af" satisfied condition "Succeeded or Failed"
    Jan 18 17:36:58.911: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:36:58.926
    Jan 18 17:36:58.946: INFO: Waiting for pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af to disappear
    Jan 18 17:36:58.952: INFO: Pod pod-configmaps-113a228a-0343-4f09-83f6-bc94b7a2a1af no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:36:58.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9254" for this suite. 01/18/23 17:36:58.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:36:58.973
Jan 18 17:36:58.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:36:58.974
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:58.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:58.999
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 17:36:59.003
Jan 18 17:36:59.018: INFO: Waiting up to 5m0s for pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92" in namespace "emptydir-3473" to be "Succeeded or Failed"
Jan 18 17:36:59.025: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.122914ms
Jan 18 17:37:01.035: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016254769s
Jan 18 17:37:03.034: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015660025s
STEP: Saw pod success 01/18/23 17:37:03.034
Jan 18 17:37:03.034: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92" satisfied condition "Succeeded or Failed"
Jan 18 17:37:03.042: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 container test-container: <nil>
STEP: delete the pod 01/18/23 17:37:03.057
Jan 18 17:37:03.081: INFO: Waiting for pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 to disappear
Jan 18 17:37:03.087: INFO: Pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:37:03.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3473" for this suite. 01/18/23 17:37:03.094
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":151,"skipped":2900,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:36:58.973
    Jan 18 17:36:58.973: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:36:58.974
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:36:58.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:36:58.999
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/18/23 17:36:59.003
    Jan 18 17:36:59.018: INFO: Waiting up to 5m0s for pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92" in namespace "emptydir-3473" to be "Succeeded or Failed"
    Jan 18 17:36:59.025: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Pending", Reason="", readiness=false. Elapsed: 7.122914ms
    Jan 18 17:37:01.035: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016254769s
    Jan 18 17:37:03.034: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015660025s
    STEP: Saw pod success 01/18/23 17:37:03.034
    Jan 18 17:37:03.034: INFO: Pod "pod-2aedf7cc-b269-469a-96b3-52b4185e9c92" satisfied condition "Succeeded or Failed"
    Jan 18 17:37:03.042: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:37:03.057
    Jan 18 17:37:03.081: INFO: Waiting for pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 to disappear
    Jan 18 17:37:03.087: INFO: Pod pod-2aedf7cc-b269-469a-96b3-52b4185e9c92 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:37:03.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3473" for this suite. 01/18/23 17:37:03.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:37:03.109
Jan 18 17:37:03.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename conformance-tests 01/18/23 17:37:03.11
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:03.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:03.137
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/18/23 17:37:03.141
Jan 18 17:37:03.141: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 18 17:37:03.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-5451" for this suite. 01/18/23 17:37:03.16
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":152,"skipped":2914,"failed":0}
------------------------------
â€¢ [0.065 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:37:03.109
    Jan 18 17:37:03.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename conformance-tests 01/18/23 17:37:03.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:03.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:03.137
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/18/23 17:37:03.141
    Jan 18 17:37:03.141: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 18 17:37:03.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-5451" for this suite. 01/18/23 17:37:03.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:37:03.176
Jan 18 17:37:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:37:03.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:03.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:03.206
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/18/23 17:37:03.212
STEP: waiting for pod running 01/18/23 17:37:03.228
Jan 18 17:37:03.228: INFO: Waiting up to 2m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888" to be "running"
Jan 18 17:37:03.235: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Pending", Reason="", readiness=false. Elapsed: 7.222555ms
Jan 18 17:37:05.245: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Running", Reason="", readiness=true. Elapsed: 2.016808824s
Jan 18 17:37:05.245: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" satisfied condition "running"
STEP: creating a file in subpath 01/18/23 17:37:05.245
Jan 18 17:37:05.252: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1888 PodName:var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:37:05.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:37:05.253: INFO: ExecWithOptions: Clientset creation
Jan 18 17:37:05.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1888/pods/var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/18/23 17:37:05.375
Jan 18 17:37:05.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1888 PodName:var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:37:05.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:37:05.386: INFO: ExecWithOptions: Clientset creation
Jan 18 17:37:05.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1888/pods/var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/18/23 17:37:05.477
Jan 18 17:37:05.998: INFO: Successfully updated pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695"
STEP: waiting for annotated pod running 01/18/23 17:37:05.998
Jan 18 17:37:05.998: INFO: Waiting up to 2m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888" to be "running"
Jan 18 17:37:06.005: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Running", Reason="", readiness=true. Elapsed: 7.021903ms
Jan 18 17:37:06.005: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 17:37:06.005
Jan 18 17:37:06.005: INFO: Deleting pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888"
Jan 18 17:37:06.019: INFO: Wait up to 5m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:37:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1888" for this suite. 01/18/23 17:37:40.045
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":153,"skipped":2947,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.885 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:37:03.176
    Jan 18 17:37:03.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:37:03.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:03.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:03.206
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/18/23 17:37:03.212
    STEP: waiting for pod running 01/18/23 17:37:03.228
    Jan 18 17:37:03.228: INFO: Waiting up to 2m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888" to be "running"
    Jan 18 17:37:03.235: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Pending", Reason="", readiness=false. Elapsed: 7.222555ms
    Jan 18 17:37:05.245: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Running", Reason="", readiness=true. Elapsed: 2.016808824s
    Jan 18 17:37:05.245: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" satisfied condition "running"
    STEP: creating a file in subpath 01/18/23 17:37:05.245
    Jan 18 17:37:05.252: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1888 PodName:var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:37:05.252: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:37:05.253: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:37:05.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1888/pods/var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/18/23 17:37:05.375
    Jan 18 17:37:05.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1888 PodName:var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:37:05.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:37:05.386: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:37:05.386: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-1888/pods/var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/18/23 17:37:05.477
    Jan 18 17:37:05.998: INFO: Successfully updated pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695"
    STEP: waiting for annotated pod running 01/18/23 17:37:05.998
    Jan 18 17:37:05.998: INFO: Waiting up to 2m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888" to be "running"
    Jan 18 17:37:06.005: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695": Phase="Running", Reason="", readiness=true. Elapsed: 7.021903ms
    Jan 18 17:37:06.005: INFO: Pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 17:37:06.005
    Jan 18 17:37:06.005: INFO: Deleting pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" in namespace "var-expansion-1888"
    Jan 18 17:37:06.019: INFO: Wait up to 5m0s for pod "var-expansion-cf6bcec8-6310-4c7f-88e4-38e55d040695" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:37:40.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1888" for this suite. 01/18/23 17:37:40.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:37:40.061
Jan 18 17:37:40.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 17:37:40.063
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:40.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:40.093
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1010 01/18/23 17:37:40.097
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-1010 01/18/23 17:37:40.107
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1010 01/18/23 17:37:40.122
Jan 18 17:37:40.128: INFO: Found 0 stateful pods, waiting for 1
Jan 18 17:37:50.138: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 17:37:50.138
Jan 18 17:37:50.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:37:50.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:37:50.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:37:50.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:37:50.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 17:38:00.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:38:00.368: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:38:00.399: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jan 18 17:38:00.399: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
Jan 18 17:38:00.399: INFO: 
Jan 18 17:38:00.399: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 18 17:38:01.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99242207s
Jan 18 17:38:02.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983822524s
Jan 18 17:38:03.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974069961s
Jan 18 17:38:04.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96561976s
Jan 18 17:38:05.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956790038s
Jan 18 17:38:06.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947174713s
Jan 18 17:38:07.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938668286s
Jan 18 17:38:08.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929270733s
Jan 18 17:38:09.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.631559ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1010 01/18/23 17:38:10.481
Jan 18 17:38:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:38:10.686: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 17:38:10.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:38:10.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:38:10.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:38:10.880: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 17:38:10.880: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:38:10.880: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:38:10.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:38:11.045: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 18 17:38:11.045: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:38:11.045: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:38:11.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan 18 17:38:21.065: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 17:38:21.065: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 17:38:21.065: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 17:38:21.065
Jan 18 17:38:21.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:38:21.270: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:38:21.270: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:38:21.270: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:38:21.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:38:21.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:38:21.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:38:21.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:38:21.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:38:21.687: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:38:21.687: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:38:21.687: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:38:21.687: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:38:21.694: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 18 17:38:31.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:38:31.713: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:38:31.713: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:38:31.741: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jan 18 17:38:31.741: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
Jan 18 17:38:31.741: INFO: ss-1  scw-conformance125-default-788643c0f7cd4128a5c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
Jan 18 17:38:31.741: INFO: ss-2  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
Jan 18 17:38:31.741: INFO: 
Jan 18 17:38:31.741: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 18 17:38:32.751: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jan 18 17:38:32.751: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
Jan 18 17:38:32.751: INFO: ss-2  scw-conformance125-default-61c39bbf4d81476a8e3  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
Jan 18 17:38:32.751: INFO: 
Jan 18 17:38:32.751: INFO: StatefulSet ss has not reached scale 0, at 2
Jan 18 17:38:33.760: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982027909s
Jan 18 17:38:34.769: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.972802473s
Jan 18 17:38:35.778: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.96356772s
Jan 18 17:38:36.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.95431381s
Jan 18 17:38:37.796: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.945242282s
Jan 18 17:38:38.806: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935399539s
Jan 18 17:38:39.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.925488387s
Jan 18 17:38:40.824: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.110201ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1010 01/18/23 17:38:41.824
Jan 18 17:38:41.833: INFO: Scaling statefulset ss to 0
Jan 18 17:38:41.856: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 17:38:41.862: INFO: Deleting all statefulset in ns statefulset-1010
Jan 18 17:38:41.868: INFO: Scaling statefulset ss to 0
Jan 18 17:38:41.888: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:38:41.894: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 17:38:41.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1010" for this suite. 01/18/23 17:38:41.922
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":154,"skipped":2952,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.871 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:37:40.061
    Jan 18 17:37:40.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 17:37:40.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:37:40.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:37:40.093
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1010 01/18/23 17:37:40.097
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-1010 01/18/23 17:37:40.107
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1010 01/18/23 17:37:40.122
    Jan 18 17:37:40.128: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 17:37:50.138: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/18/23 17:37:50.138
    Jan 18 17:37:50.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:37:50.350: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:37:50.350: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:37:50.350: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:37:50.356: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 17:38:00.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:38:00.368: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:38:00.399: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
    Jan 18 17:38:00.399: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:50 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
    Jan 18 17:38:00.399: INFO: 
    Jan 18 17:38:00.399: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 18 17:38:01.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99242207s
    Jan 18 17:38:02.417: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983822524s
    Jan 18 17:38:03.426: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974069961s
    Jan 18 17:38:04.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.96561976s
    Jan 18 17:38:05.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.956790038s
    Jan 18 17:38:06.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.947174713s
    Jan 18 17:38:07.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938668286s
    Jan 18 17:38:08.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.929270733s
    Jan 18 17:38:09.480: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.631559ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1010 01/18/23 17:38:10.481
    Jan 18 17:38:10.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:38:10.686: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 17:38:10.687: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:38:10.687: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:38:10.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:38:10.880: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 17:38:10.880: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:38:10.880: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:38:10.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:38:11.045: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 18 17:38:11.045: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:38:11.045: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:38:11.053: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan 18 17:38:21.065: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 17:38:21.065: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 17:38:21.065: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/18/23 17:38:21.065
    Jan 18 17:38:21.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:38:21.270: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:38:21.270: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:38:21.270: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:38:21.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:38:21.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:38:21.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:38:21.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:38:21.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-1010 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:38:21.687: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:38:21.687: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:38:21.687: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:38:21.687: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:38:21.694: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 18 17:38:31.713: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:38:31.713: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:38:31.713: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:38:31.741: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
    Jan 18 17:38:31.741: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
    Jan 18 17:38:31.741: INFO: ss-1  scw-conformance125-default-788643c0f7cd4128a5c  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
    Jan 18 17:38:31.741: INFO: ss-2  scw-conformance125-default-61c39bbf4d81476a8e3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
    Jan 18 17:38:31.741: INFO: 
    Jan 18 17:38:31.741: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 18 17:38:32.751: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
    Jan 18 17:38:32.751: INFO: ss-0  scw-conformance125-default-61c39bbf4d81476a8e3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:37:40 +0000 UTC  }]
    Jan 18 17:38:32.751: INFO: ss-2  scw-conformance125-default-61c39bbf4d81476a8e3  Running  0s     [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-18 17:38:00 +0000 UTC  }]
    Jan 18 17:38:32.751: INFO: 
    Jan 18 17:38:32.751: INFO: StatefulSet ss has not reached scale 0, at 2
    Jan 18 17:38:33.760: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.982027909s
    Jan 18 17:38:34.769: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.972802473s
    Jan 18 17:38:35.778: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.96356772s
    Jan 18 17:38:36.787: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.95431381s
    Jan 18 17:38:37.796: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.945242282s
    Jan 18 17:38:38.806: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.935399539s
    Jan 18 17:38:39.815: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.925488387s
    Jan 18 17:38:40.824: INFO: Verifying statefulset ss doesn't scale past 0 for another 917.110201ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1010 01/18/23 17:38:41.824
    Jan 18 17:38:41.833: INFO: Scaling statefulset ss to 0
    Jan 18 17:38:41.856: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 17:38:41.862: INFO: Deleting all statefulset in ns statefulset-1010
    Jan 18 17:38:41.868: INFO: Scaling statefulset ss to 0
    Jan 18 17:38:41.888: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:38:41.894: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 17:38:41.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1010" for this suite. 01/18/23 17:38:41.922
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:38:41.933
Jan 18 17:38:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context-test 01/18/23 17:38:41.935
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:41.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:41.97
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 18 17:38:41.988: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552" in namespace "security-context-test-9077" to be "Succeeded or Failed"
Jan 18 17:38:41.993: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014707ms
Jan 18 17:38:44.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013490983s
Jan 18 17:38:46.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014028113s
Jan 18 17:38:46.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 17:38:46.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9077" for this suite. 01/18/23 17:38:46.012
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":155,"skipped":2960,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:38:41.933
    Jan 18 17:38:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 17:38:41.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:41.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:41.97
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 18 17:38:41.988: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552" in namespace "security-context-test-9077" to be "Succeeded or Failed"
    Jan 18 17:38:41.993: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014707ms
    Jan 18 17:38:44.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013490983s
    Jan 18 17:38:46.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014028113s
    Jan 18 17:38:46.002: INFO: Pod "busybox-readonly-false-cc3ec1bc-7497-47fc-9c62-87ed35acd552" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 17:38:46.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-9077" for this suite. 01/18/23 17:38:46.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:38:46.029
Jan 18 17:38:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 17:38:46.031
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:46.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:46.063
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/18/23 17:38:46.068
Jan 18 17:38:46.086: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4634" to be "running and ready"
Jan 18 17:38:46.094: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.568026ms
Jan 18 17:38:46.094: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:38:48.102: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016470711s
Jan 18 17:38:48.102: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 18 17:38:48.102: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/18/23 17:38:48.111
Jan 18 17:38:48.133: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4634" to be "container debugger running"
Jan 18 17:38:48.140: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.858122ms
Jan 18 17:38:50.150: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016630375s
Jan 18 17:38:52.148: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014720526s
Jan 18 17:38:52.148: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/18/23 17:38:52.148
Jan 18 17:38:52.148: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4634 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 17:38:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:38:52.149: INFO: ExecWithOptions: Clientset creation
Jan 18 17:38:52.149: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4634/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 18 17:38:52.253: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 17:38:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4634" for this suite. 01/18/23 17:38:52.29
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":156,"skipped":3020,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.277 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:38:46.029
    Jan 18 17:38:46.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/18/23 17:38:46.031
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:46.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:46.063
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/18/23 17:38:46.068
    Jan 18 17:38:46.086: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4634" to be "running and ready"
    Jan 18 17:38:46.094: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.568026ms
    Jan 18 17:38:46.094: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:38:48.102: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016470711s
    Jan 18 17:38:48.102: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 18 17:38:48.102: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/18/23 17:38:48.111
    Jan 18 17:38:48.133: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4634" to be "container debugger running"
    Jan 18 17:38:48.140: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.858122ms
    Jan 18 17:38:50.150: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016630375s
    Jan 18 17:38:52.148: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.014720526s
    Jan 18 17:38:52.148: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/18/23 17:38:52.148
    Jan 18 17:38:52.148: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4634 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 17:38:52.148: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:38:52.149: INFO: ExecWithOptions: Clientset creation
    Jan 18 17:38:52.149: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4634/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 18 17:38:52.253: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 17:38:52.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4634" for this suite. 01/18/23 17:38:52.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:38:52.309
Jan 18 17:38:52.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename endpointslice 01/18/23 17:38:52.31
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:52.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:52.343
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/18/23 17:38:57.494
STEP: referencing matching pods with named port 01/18/23 17:39:02.51
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 17:39:07.529
STEP: recreating EndpointSlices after they've been deleted 01/18/23 17:39:12.545
Jan 18 17:39:12.581: INFO: EndpointSlice for Service endpointslice-4327/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 17:39:22.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4327" for this suite. 01/18/23 17:39:22.609
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":157,"skipped":3032,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.313 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:38:52.309
    Jan 18 17:38:52.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 17:38:52.31
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:38:52.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:38:52.343
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/18/23 17:38:57.494
    STEP: referencing matching pods with named port 01/18/23 17:39:02.51
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/18/23 17:39:07.529
    STEP: recreating EndpointSlices after they've been deleted 01/18/23 17:39:12.545
    Jan 18 17:39:12.581: INFO: EndpointSlice for Service endpointslice-4327/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 17:39:22.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4327" for this suite. 01/18/23 17:39:22.609
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:39:22.623
Jan 18 17:39:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:39:22.624
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:22.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:22.655
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-9da045e8-29ca-4010-b7f0-e96fce9b0646 01/18/23 17:39:22.66
STEP: Creating a pod to test consume secrets 01/18/23 17:39:22.668
Jan 18 17:39:22.685: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4" in namespace "projected-3372" to be "Succeeded or Failed"
Jan 18 17:39:22.691: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.411677ms
Jan 18 17:39:24.700: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014558327s
Jan 18 17:39:26.701: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015776273s
STEP: Saw pod success 01/18/23 17:39:26.701
Jan 18 17:39:26.701: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4" satisfied condition "Succeeded or Failed"
Jan 18 17:39:26.709: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:39:26.732
Jan 18 17:39:26.753: INFO: Waiting for pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 to disappear
Jan 18 17:39:26.761: INFO: Pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 17:39:26.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3372" for this suite. 01/18/23 17:39:26.768
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":3033,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:39:22.623
    Jan 18 17:39:22.623: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:39:22.624
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:22.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:22.655
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-9da045e8-29ca-4010-b7f0-e96fce9b0646 01/18/23 17:39:22.66
    STEP: Creating a pod to test consume secrets 01/18/23 17:39:22.668
    Jan 18 17:39:22.685: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4" in namespace "projected-3372" to be "Succeeded or Failed"
    Jan 18 17:39:22.691: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.411677ms
    Jan 18 17:39:24.700: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014558327s
    Jan 18 17:39:26.701: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015776273s
    STEP: Saw pod success 01/18/23 17:39:26.701
    Jan 18 17:39:26.701: INFO: Pod "pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4" satisfied condition "Succeeded or Failed"
    Jan 18 17:39:26.709: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:39:26.732
    Jan 18 17:39:26.753: INFO: Waiting for pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 to disappear
    Jan 18 17:39:26.761: INFO: Pod pod-projected-secrets-6fffaaa0-c2af-4fbd-b812-cf3b7891a3a4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 17:39:26.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3372" for this suite. 01/18/23 17:39:26.768
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:39:26.782
Jan 18 17:39:26.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename proxy 01/18/23 17:39:26.783
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:26.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:26.814
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/18/23 17:39:26.842
STEP: creating replication controller proxy-service-hzk4h in namespace proxy-871 01/18/23 17:39:26.842
I0118 17:39:26.853660      21 runners.go:193] Created replication controller with name: proxy-service-hzk4h, namespace: proxy-871, replica count: 1
I0118 17:39:27.904426      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0118 17:39:28.904649      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0118 17:39:29.905173      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:39:29.915: INFO: setup took 3.096314756s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 17:39:29.915
Jan 18 17:39:29.931: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 15.305282ms)
Jan 18 17:39:29.932: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 15.724916ms)
Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 17.924812ms)
Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.066783ms)
Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 18.494616ms)
Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 18.375346ms)
Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 18.87343ms)
Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 18.360816ms)
Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 18.96522ms)
Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 19.491284ms)
Jan 18 17:39:29.936: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 19.794795ms)
Jan 18 17:39:29.942: INFO: (0) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 25.626729ms)
Jan 18 17:39:29.942: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 25.666819ms)
Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 43.118057ms)
Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 42.862826ms)
Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 43.283579ms)
Jan 18 17:39:29.967: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 7.567786ms)
Jan 18 17:39:29.968: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.540752ms)
Jan 18 17:39:29.968: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.437481ms)
Jan 18 17:39:29.969: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 9.382819ms)
Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.878457ms)
Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.599184ms)
Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.645365ms)
Jan 18 17:39:29.973: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 13.160256ms)
Jan 18 17:39:29.973: INFO: (1) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.913562ms)
Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 15.893736ms)
Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 16.462781ms)
Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 16.800634ms)
Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 16.44315ms)
Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.116293ms)
Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 18.508726ms)
Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 18.161073ms)
Jan 18 17:39:29.986: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.2497ms)
Jan 18 17:39:29.987: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 9.140687ms)
Jan 18 17:39:29.987: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.966896ms)
Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.805212ms)
Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.53766ms)
Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.003164ms)
Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.241705ms)
Jan 18 17:39:29.989: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 10.497288ms)
Jan 18 17:39:29.989: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.644088ms)
Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 11.167452ms)
Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 11.966708ms)
Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.621275ms)
Jan 18 17:39:29.991: INFO: (2) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.143067ms)
Jan 18 17:39:29.992: INFO: (2) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.859872ms)
Jan 18 17:39:29.993: INFO: (2) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.853439ms)
Jan 18 17:39:29.993: INFO: (2) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.002441ms)
Jan 18 17:39:30.066: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 72.440803ms)
Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 73.090997ms)
Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.584043ms)
Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 73.010137ms)
Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 73.642641ms)
Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 73.988333ms)
Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 73.933903ms)
Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 74.530778ms)
Jan 18 17:39:30.069: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 75.046191ms)
Jan 18 17:39:30.069: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 74.946851ms)
Jan 18 17:39:30.070: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 76.25709ms)
Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 76.886135ms)
Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 77.202037ms)
Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 77.776991ms)
Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 77.693091ms)
Jan 18 17:39:30.072: INFO: (3) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 77.941433ms)
Jan 18 17:39:30.080: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.654326ms)
Jan 18 17:39:30.081: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.781935ms)
Jan 18 17:39:30.081: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.934886ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 10.85568ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.038131ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 11.362583ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.101521ms)
Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.382703ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 11.183452ms)
Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.99542ms)
Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 12.463992ms)
Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 12.628463ms)
Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.764196ms)
Jan 18 17:39:30.085: INFO: (4) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 11.937817ms)
Jan 18 17:39:30.086: INFO: (4) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.330358ms)
Jan 18 17:39:30.087: INFO: (4) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 15.04523ms)
Jan 18 17:39:30.163: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 76.295921ms)
Jan 18 17:39:30.164: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 76.665163ms)
Jan 18 17:39:30.164: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 76.32912ms)
Jan 18 17:39:30.165: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 77.62395ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 79.156031ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 79.287441ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 79.261281ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 79.379651ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 79.515413ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 79.669505ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 79.585034ms)
Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 80.106957ms)
Jan 18 17:39:30.168: INFO: (5) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 80.845603ms)
Jan 18 17:39:30.168: INFO: (5) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 80.922823ms)
Jan 18 17:39:30.169: INFO: (5) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 81.514068ms)
Jan 18 17:39:30.170: INFO: (5) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 82.618036ms)
Jan 18 17:39:30.179: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.910565ms)
Jan 18 17:39:30.179: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.887226ms)
Jan 18 17:39:30.180: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.544092ms)
Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.024053ms)
Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.589918ms)
Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.928103ms)
Jan 18 17:39:30.182: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.803909ms)
Jan 18 17:39:30.182: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.900741ms)
Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.621366ms)
Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 12.769174ms)
Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.727117ms)
Jan 18 17:39:30.185: INFO: (6) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 13.8016ms)
Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.391825ms)
Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 15.390953ms)
Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 15.227512ms)
Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.427813ms)
Jan 18 17:39:30.195: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 8.685944ms)
Jan 18 17:39:30.195: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 9.322449ms)
Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.07116ms)
Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.512273ms)
Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 72.503562ms)
Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.29408ms)
Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 72.350341ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 74.78233ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 75.854467ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 75.737206ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 75.578255ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 75.952067ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 75.666206ms)
Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 75.878027ms)
Jan 18 17:39:30.265: INFO: (7) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 78.530516ms)
Jan 18 17:39:30.265: INFO: (7) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 78.398675ms)
Jan 18 17:39:30.279: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 13.65939ms)
Jan 18 17:39:30.279: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 13.593789ms)
Jan 18 17:39:30.280: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 14.424676ms)
Jan 18 17:39:30.280: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 14.091914ms)
Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 15.232372ms)
Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 15.314382ms)
Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 15.806136ms)
Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 16.047968ms)
Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 16.28935ms)
Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 16.497591ms)
Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 17.082295ms)
Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 16.884214ms)
Jan 18 17:39:30.284: INFO: (8) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 18.318375ms)
Jan 18 17:39:30.285: INFO: (8) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 18.931599ms)
Jan 18 17:39:30.285: INFO: (8) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 19.09204ms)
Jan 18 17:39:30.287: INFO: (8) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 21.706079ms)
Jan 18 17:39:30.294: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 6.90538ms)
Jan 18 17:39:30.295: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.619207ms)
Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 8.719365ms)
Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.578012ms)
Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 8.919575ms)
Jan 18 17:39:30.297: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.378639ms)
Jan 18 17:39:30.297: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.424519ms)
Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.978424ms)
Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 10.120814ms)
Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.412526ms)
Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 12.013968ms)
Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.31212ms)
Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.24343ms)
Jan 18 17:39:30.301: INFO: (9) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 12.893454ms)
Jan 18 17:39:30.359: INFO: (9) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 70.853979ms)
Jan 18 17:39:30.359: INFO: (9) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 70.91002ms)
Jan 18 17:39:30.368: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.907455ms)
Jan 18 17:39:30.370: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.077142ms)
Jan 18 17:39:30.370: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.352973ms)
Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.776916ms)
Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.804836ms)
Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.822446ms)
Jan 18 17:39:30.372: INFO: (10) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.785434ms)
Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 13.386629ms)
Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 14.001353ms)
Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 13.721421ms)
Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 14.033693ms)
Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 16.548552ms)
Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 16.698223ms)
Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 16.746433ms)
Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 16.845384ms)
Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 17.300987ms)
Jan 18 17:39:30.385: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.867887ms)
Jan 18 17:39:30.385: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.969398ms)
Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.230258ms)
Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.368978ms)
Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 9.828623ms)
Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.276516ms)
Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.309075ms)
Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 10.503017ms)
Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 10.643758ms)
Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.458705ms)
Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 11.136102ms)
Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 11.063671ms)
Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.253982ms)
Jan 18 17:39:30.389: INFO: (11) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.408931ms)
Jan 18 17:39:30.390: INFO: (11) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.246256ms)
Jan 18 17:39:30.391: INFO: (11) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 13.69655ms)
Jan 18 17:39:30.462: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 71.635335ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 71.436374ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 71.477814ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.002179ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 72.350101ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 72.352711ms)
Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.156619ms)
Jan 18 17:39:30.465: INFO: (12) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 74.474087ms)
Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 75.461414ms)
Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 76.157158ms)
Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 76.028547ms)
Jan 18 17:39:30.468: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 77.435317ms)
Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 77.69465ms)
Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 77.587618ms)
Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 77.848371ms)
Jan 18 17:39:30.472: INFO: (12) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 80.810092ms)
Jan 18 17:39:30.480: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 7.831208ms)
Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.897395ms)
Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.178947ms)
Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.897195ms)
Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.002886ms)
Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.58352ms)
Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.833992ms)
Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 9.783101ms)
Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.879183ms)
Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.035744ms)
Jan 18 17:39:30.483: INFO: (13) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 10.700268ms)
Jan 18 17:39:30.483: INFO: (13) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 11.180003ms)
Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.632293ms)
Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 12.899884ms)
Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.139756ms)
Jan 18 17:39:30.487: INFO: (13) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.91214ms)
Jan 18 17:39:30.494: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 6.482078ms)
Jan 18 17:39:30.494: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 7.175312ms)
Jan 18 17:39:30.495: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 7.592985ms)
Jan 18 17:39:30.495: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.938138ms)
Jan 18 17:39:30.497: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.268728ms)
Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 71.400564ms)
Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 71.325972ms)
Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 71.579055ms)
Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 71.732755ms)
Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 71.763596ms)
Jan 18 17:39:30.560: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.091908ms)
Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 74.159084ms)
Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 74.269765ms)
Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 75.09434ms)
Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 74.711847ms)
Jan 18 17:39:30.568: INFO: (14) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 80.308879ms)
Jan 18 17:39:30.575: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.267263ms)
Jan 18 17:39:30.576: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.13669ms)
Jan 18 17:39:30.577: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.788154ms)
Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.578337ms)
Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.865758ms)
Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 10.712528ms)
Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.637347ms)
Jan 18 17:39:30.580: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.426594ms)
Jan 18 17:39:30.580: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.541965ms)
Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 13.544689ms)
Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 13.875792ms)
Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 13.604691ms)
Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.767181ms)
Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 13.967052ms)
Jan 18 17:39:30.583: INFO: (15) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.572836ms)
Jan 18 17:39:30.583: INFO: (15) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.665227ms)
Jan 18 17:39:30.591: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 8.26081ms)
Jan 18 17:39:30.592: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.491742ms)
Jan 18 17:39:30.592: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.797745ms)
Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 8.810344ms)
Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.155677ms)
Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.229837ms)
Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.762743ms)
Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 10.026704ms)
Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 10.792269ms)
Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 11.013191ms)
Jan 18 17:39:30.595: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.05252ms)
Jan 18 17:39:30.595: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.185982ms)
Jan 18 17:39:30.596: INFO: (16) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.078866ms)
Jan 18 17:39:30.598: INFO: (16) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 14.748118ms)
Jan 18 17:39:30.598: INFO: (16) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.543007ms)
Jan 18 17:39:30.599: INFO: (16) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.872127ms)
Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 59.205965ms)
Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 59.491788ms)
Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 59.555597ms)
Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 59.512588ms)
Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 60.307203ms)
Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 60.353823ms)
Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 60.481264ms)
Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 60.579305ms)
Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 60.683285ms)
Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 63.94817ms)
Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 64.270133ms)
Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 63.762328ms)
Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 64.012191ms)
Jan 18 17:39:30.664: INFO: (17) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 64.282523ms)
Jan 18 17:39:30.664: INFO: (17) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 64.01829ms)
Jan 18 17:39:30.666: INFO: (17) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 66.496527ms)
Jan 18 17:39:30.676: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.351147ms)
Jan 18 17:39:30.677: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 10.990049ms)
Jan 18 17:39:30.679: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 12.639892ms)
Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 13.462788ms)
Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 13.423678ms)
Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 13.531109ms)
Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 14.199093ms)
Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 14.098293ms)
Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 14.169634ms)
Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 14.151773ms)
Jan 18 17:39:30.682: INFO: (18) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 15.550754ms)
Jan 18 17:39:30.682: INFO: (18) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 15.687054ms)
Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 17.65883ms)
Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 17.70814ms)
Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 17.76297ms)
Jan 18 17:39:30.685: INFO: (18) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.031892ms)
Jan 18 17:39:30.692: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.453875ms)
Jan 18 17:39:30.693: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.610084ms)
Jan 18 17:39:30.693: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.643173ms)
Jan 18 17:39:30.694: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.139907ms)
Jan 18 17:39:30.694: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.512361ms)
Jan 18 17:39:30.695: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.786671ms)
Jan 18 17:39:30.695: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.197405ms)
Jan 18 17:39:30.696: INFO: (19) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 11.630556ms)
Jan 18 17:39:30.696: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.563184ms)
Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 12.065788ms)
Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 12.333941ms)
Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.099828ms)
Jan 18 17:39:30.698: INFO: (19) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.730224ms)
Jan 18 17:39:30.699: INFO: (19) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 13.836182ms)
Jan 18 17:39:30.699: INFO: (19) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.884862ms)
Jan 18 17:39:30.700: INFO: (19) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 15.487473ms)
STEP: deleting ReplicationController proxy-service-hzk4h in namespace proxy-871, will wait for the garbage collector to delete the pods 01/18/23 17:39:30.7
Jan 18 17:39:30.821: INFO: Deleting ReplicationController proxy-service-hzk4h took: 11.798446ms
Jan 18 17:39:30.922: INFO: Terminating ReplicationController proxy-service-hzk4h pods took: 100.95401ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 17:39:32.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-871" for this suite. 01/18/23 17:39:32.032
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":159,"skipped":3034,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.262 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:39:26.782
    Jan 18 17:39:26.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename proxy 01/18/23 17:39:26.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:26.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:26.814
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/18/23 17:39:26.842
    STEP: creating replication controller proxy-service-hzk4h in namespace proxy-871 01/18/23 17:39:26.842
    I0118 17:39:26.853660      21 runners.go:193] Created replication controller with name: proxy-service-hzk4h, namespace: proxy-871, replica count: 1
    I0118 17:39:27.904426      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0118 17:39:28.904649      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0118 17:39:29.905173      21 runners.go:193] proxy-service-hzk4h Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:39:29.915: INFO: setup took 3.096314756s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/18/23 17:39:29.915
    Jan 18 17:39:29.931: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 15.305282ms)
    Jan 18 17:39:29.932: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 15.724916ms)
    Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 17.924812ms)
    Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.066783ms)
    Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 18.494616ms)
    Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 18.375346ms)
    Jan 18 17:39:29.934: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 18.87343ms)
    Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 18.360816ms)
    Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 18.96522ms)
    Jan 18 17:39:29.935: INFO: (0) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 19.491284ms)
    Jan 18 17:39:29.936: INFO: (0) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 19.794795ms)
    Jan 18 17:39:29.942: INFO: (0) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 25.626729ms)
    Jan 18 17:39:29.942: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 25.666819ms)
    Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 43.118057ms)
    Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 42.862826ms)
    Jan 18 17:39:29.959: INFO: (0) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 43.283579ms)
    Jan 18 17:39:29.967: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 7.567786ms)
    Jan 18 17:39:29.968: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.540752ms)
    Jan 18 17:39:29.968: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.437481ms)
    Jan 18 17:39:29.969: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 9.382819ms)
    Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.878457ms)
    Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.599184ms)
    Jan 18 17:39:29.971: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.645365ms)
    Jan 18 17:39:29.973: INFO: (1) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 13.160256ms)
    Jan 18 17:39:29.973: INFO: (1) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.913562ms)
    Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 15.893736ms)
    Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 16.462781ms)
    Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 16.800634ms)
    Jan 18 17:39:29.976: INFO: (1) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 16.44315ms)
    Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.116293ms)
    Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 18.508726ms)
    Jan 18 17:39:29.978: INFO: (1) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 18.161073ms)
    Jan 18 17:39:29.986: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.2497ms)
    Jan 18 17:39:29.987: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 9.140687ms)
    Jan 18 17:39:29.987: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.966896ms)
    Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.805212ms)
    Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.53766ms)
    Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.003164ms)
    Jan 18 17:39:29.988: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.241705ms)
    Jan 18 17:39:29.989: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 10.497288ms)
    Jan 18 17:39:29.989: INFO: (2) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.644088ms)
    Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 11.167452ms)
    Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 11.966708ms)
    Jan 18 17:39:29.990: INFO: (2) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.621275ms)
    Jan 18 17:39:29.991: INFO: (2) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.143067ms)
    Jan 18 17:39:29.992: INFO: (2) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.859872ms)
    Jan 18 17:39:29.993: INFO: (2) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.853439ms)
    Jan 18 17:39:29.993: INFO: (2) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.002441ms)
    Jan 18 17:39:30.066: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 72.440803ms)
    Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 73.090997ms)
    Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.584043ms)
    Jan 18 17:39:30.067: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 73.010137ms)
    Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 73.642641ms)
    Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 73.988333ms)
    Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 73.933903ms)
    Jan 18 17:39:30.068: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 74.530778ms)
    Jan 18 17:39:30.069: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 75.046191ms)
    Jan 18 17:39:30.069: INFO: (3) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 74.946851ms)
    Jan 18 17:39:30.070: INFO: (3) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 76.25709ms)
    Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 76.886135ms)
    Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 77.202037ms)
    Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 77.776991ms)
    Jan 18 17:39:30.071: INFO: (3) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 77.693091ms)
    Jan 18 17:39:30.072: INFO: (3) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 77.941433ms)
    Jan 18 17:39:30.080: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.654326ms)
    Jan 18 17:39:30.081: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 8.781935ms)
    Jan 18 17:39:30.081: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.934886ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 10.85568ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.038131ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 11.362583ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.101521ms)
    Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.382703ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 11.183452ms)
    Jan 18 17:39:30.083: INFO: (4) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.99542ms)
    Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 12.463992ms)
    Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 12.628463ms)
    Jan 18 17:39:30.084: INFO: (4) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.764196ms)
    Jan 18 17:39:30.085: INFO: (4) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 11.937817ms)
    Jan 18 17:39:30.086: INFO: (4) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.330358ms)
    Jan 18 17:39:30.087: INFO: (4) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 15.04523ms)
    Jan 18 17:39:30.163: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 76.295921ms)
    Jan 18 17:39:30.164: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 76.665163ms)
    Jan 18 17:39:30.164: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 76.32912ms)
    Jan 18 17:39:30.165: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 77.62395ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 79.156031ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 79.287441ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 79.261281ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 79.379651ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 79.515413ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 79.669505ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 79.585034ms)
    Jan 18 17:39:30.167: INFO: (5) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 80.106957ms)
    Jan 18 17:39:30.168: INFO: (5) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 80.845603ms)
    Jan 18 17:39:30.168: INFO: (5) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 80.922823ms)
    Jan 18 17:39:30.169: INFO: (5) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 81.514068ms)
    Jan 18 17:39:30.170: INFO: (5) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 82.618036ms)
    Jan 18 17:39:30.179: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.910565ms)
    Jan 18 17:39:30.179: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.887226ms)
    Jan 18 17:39:30.180: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.544092ms)
    Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.024053ms)
    Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.589918ms)
    Jan 18 17:39:30.181: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.928103ms)
    Jan 18 17:39:30.182: INFO: (6) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.803909ms)
    Jan 18 17:39:30.182: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.900741ms)
    Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.621366ms)
    Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 12.769174ms)
    Jan 18 17:39:30.183: INFO: (6) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.727117ms)
    Jan 18 17:39:30.185: INFO: (6) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 13.8016ms)
    Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.391825ms)
    Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 15.390953ms)
    Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 15.227512ms)
    Jan 18 17:39:30.186: INFO: (6) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.427813ms)
    Jan 18 17:39:30.195: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 8.685944ms)
    Jan 18 17:39:30.195: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 9.322449ms)
    Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.07116ms)
    Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.512273ms)
    Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 72.503562ms)
    Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.29408ms)
    Jan 18 17:39:30.259: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 72.350341ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 74.78233ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 75.854467ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 75.737206ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 75.578255ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 75.952067ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 75.666206ms)
    Jan 18 17:39:30.262: INFO: (7) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 75.878027ms)
    Jan 18 17:39:30.265: INFO: (7) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 78.530516ms)
    Jan 18 17:39:30.265: INFO: (7) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 78.398675ms)
    Jan 18 17:39:30.279: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 13.65939ms)
    Jan 18 17:39:30.279: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 13.593789ms)
    Jan 18 17:39:30.280: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 14.424676ms)
    Jan 18 17:39:30.280: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 14.091914ms)
    Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 15.232372ms)
    Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 15.314382ms)
    Jan 18 17:39:30.281: INFO: (8) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 15.806136ms)
    Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 16.047968ms)
    Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 16.28935ms)
    Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 16.497591ms)
    Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 17.082295ms)
    Jan 18 17:39:30.282: INFO: (8) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 16.884214ms)
    Jan 18 17:39:30.284: INFO: (8) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 18.318375ms)
    Jan 18 17:39:30.285: INFO: (8) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 18.931599ms)
    Jan 18 17:39:30.285: INFO: (8) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 19.09204ms)
    Jan 18 17:39:30.287: INFO: (8) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 21.706079ms)
    Jan 18 17:39:30.294: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 6.90538ms)
    Jan 18 17:39:30.295: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.619207ms)
    Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 8.719365ms)
    Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.578012ms)
    Jan 18 17:39:30.296: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 8.919575ms)
    Jan 18 17:39:30.297: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.378639ms)
    Jan 18 17:39:30.297: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.424519ms)
    Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.978424ms)
    Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 10.120814ms)
    Jan 18 17:39:30.298: INFO: (9) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.412526ms)
    Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 12.013968ms)
    Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.31212ms)
    Jan 18 17:39:30.300: INFO: (9) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.24343ms)
    Jan 18 17:39:30.301: INFO: (9) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 12.893454ms)
    Jan 18 17:39:30.359: INFO: (9) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 70.853979ms)
    Jan 18 17:39:30.359: INFO: (9) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 70.91002ms)
    Jan 18 17:39:30.368: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.907455ms)
    Jan 18 17:39:30.370: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.077142ms)
    Jan 18 17:39:30.370: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.352973ms)
    Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 11.776916ms)
    Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.804836ms)
    Jan 18 17:39:30.371: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.822446ms)
    Jan 18 17:39:30.372: INFO: (10) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.785434ms)
    Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 13.386629ms)
    Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 14.001353ms)
    Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 13.721421ms)
    Jan 18 17:39:30.373: INFO: (10) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 14.033693ms)
    Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 16.548552ms)
    Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 16.698223ms)
    Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 16.746433ms)
    Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 16.845384ms)
    Jan 18 17:39:30.376: INFO: (10) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 17.300987ms)
    Jan 18 17:39:30.385: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.867887ms)
    Jan 18 17:39:30.385: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.969398ms)
    Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.230258ms)
    Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.368978ms)
    Jan 18 17:39:30.386: INFO: (11) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 9.828623ms)
    Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.276516ms)
    Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.309075ms)
    Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 10.503017ms)
    Jan 18 17:39:30.387: INFO: (11) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 10.643758ms)
    Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 11.458705ms)
    Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 11.136102ms)
    Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 11.063671ms)
    Jan 18 17:39:30.388: INFO: (11) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.253982ms)
    Jan 18 17:39:30.389: INFO: (11) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.408931ms)
    Jan 18 17:39:30.390: INFO: (11) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.246256ms)
    Jan 18 17:39:30.391: INFO: (11) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 13.69655ms)
    Jan 18 17:39:30.462: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 71.635335ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 71.436374ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 71.477814ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.002179ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 72.350101ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 72.352711ms)
    Jan 18 17:39:30.463: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 72.156619ms)
    Jan 18 17:39:30.465: INFO: (12) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 74.474087ms)
    Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 75.461414ms)
    Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 76.157158ms)
    Jan 18 17:39:30.467: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 76.028547ms)
    Jan 18 17:39:30.468: INFO: (12) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 77.435317ms)
    Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 77.69465ms)
    Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 77.587618ms)
    Jan 18 17:39:30.469: INFO: (12) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 77.848371ms)
    Jan 18 17:39:30.472: INFO: (12) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 80.810092ms)
    Jan 18 17:39:30.480: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 7.831208ms)
    Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.897395ms)
    Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.178947ms)
    Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.897195ms)
    Jan 18 17:39:30.481: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.002886ms)
    Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.58352ms)
    Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 9.833992ms)
    Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 9.783101ms)
    Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.879183ms)
    Jan 18 17:39:30.482: INFO: (13) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.035744ms)
    Jan 18 17:39:30.483: INFO: (13) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 10.700268ms)
    Jan 18 17:39:30.483: INFO: (13) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 11.180003ms)
    Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.632293ms)
    Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 12.899884ms)
    Jan 18 17:39:30.485: INFO: (13) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 13.139756ms)
    Jan 18 17:39:30.487: INFO: (13) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.91214ms)
    Jan 18 17:39:30.494: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 6.482078ms)
    Jan 18 17:39:30.494: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 7.175312ms)
    Jan 18 17:39:30.495: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 7.592985ms)
    Jan 18 17:39:30.495: INFO: (14) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.938138ms)
    Jan 18 17:39:30.497: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.268728ms)
    Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 71.400564ms)
    Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 71.325972ms)
    Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 71.579055ms)
    Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 71.732755ms)
    Jan 18 17:39:30.559: INFO: (14) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 71.763596ms)
    Jan 18 17:39:30.560: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 72.091908ms)
    Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 74.159084ms)
    Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 74.269765ms)
    Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 75.09434ms)
    Jan 18 17:39:30.562: INFO: (14) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 74.711847ms)
    Jan 18 17:39:30.568: INFO: (14) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 80.308879ms)
    Jan 18 17:39:30.575: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 7.267263ms)
    Jan 18 17:39:30.576: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 8.13669ms)
    Jan 18 17:39:30.577: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.788154ms)
    Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 10.578337ms)
    Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.865758ms)
    Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 10.712528ms)
    Jan 18 17:39:30.579: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 10.637347ms)
    Jan 18 17:39:30.580: INFO: (15) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 11.426594ms)
    Jan 18 17:39:30.580: INFO: (15) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 11.541965ms)
    Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 13.544689ms)
    Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 13.875792ms)
    Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 13.604691ms)
    Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 13.767181ms)
    Jan 18 17:39:30.582: INFO: (15) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 13.967052ms)
    Jan 18 17:39:30.583: INFO: (15) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.572836ms)
    Jan 18 17:39:30.583: INFO: (15) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 14.665227ms)
    Jan 18 17:39:30.591: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 8.26081ms)
    Jan 18 17:39:30.592: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 8.491742ms)
    Jan 18 17:39:30.592: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.797745ms)
    Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 8.810344ms)
    Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.155677ms)
    Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.229837ms)
    Jan 18 17:39:30.593: INFO: (16) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.762743ms)
    Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 10.026704ms)
    Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 10.792269ms)
    Jan 18 17:39:30.594: INFO: (16) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 11.013191ms)
    Jan 18 17:39:30.595: INFO: (16) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 11.05252ms)
    Jan 18 17:39:30.595: INFO: (16) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.185982ms)
    Jan 18 17:39:30.596: INFO: (16) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.078866ms)
    Jan 18 17:39:30.598: INFO: (16) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 14.748118ms)
    Jan 18 17:39:30.598: INFO: (16) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 14.543007ms)
    Jan 18 17:39:30.599: INFO: (16) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 15.872127ms)
    Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 59.205965ms)
    Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 59.491788ms)
    Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 59.555597ms)
    Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 59.512588ms)
    Jan 18 17:39:30.659: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 60.307203ms)
    Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 60.353823ms)
    Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 60.481264ms)
    Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 60.579305ms)
    Jan 18 17:39:30.660: INFO: (17) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 60.683285ms)
    Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 63.94817ms)
    Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 64.270133ms)
    Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 63.762328ms)
    Jan 18 17:39:30.663: INFO: (17) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 64.012191ms)
    Jan 18 17:39:30.664: INFO: (17) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 64.282523ms)
    Jan 18 17:39:30.664: INFO: (17) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 64.01829ms)
    Jan 18 17:39:30.666: INFO: (17) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 66.496527ms)
    Jan 18 17:39:30.676: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 9.351147ms)
    Jan 18 17:39:30.677: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 10.990049ms)
    Jan 18 17:39:30.679: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 12.639892ms)
    Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 13.462788ms)
    Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 13.423678ms)
    Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 13.531109ms)
    Jan 18 17:39:30.680: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 14.199093ms)
    Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 14.098293ms)
    Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 14.169634ms)
    Jan 18 17:39:30.681: INFO: (18) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 14.151773ms)
    Jan 18 17:39:30.682: INFO: (18) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 15.550754ms)
    Jan 18 17:39:30.682: INFO: (18) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 15.687054ms)
    Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 17.65883ms)
    Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 17.70814ms)
    Jan 18 17:39:30.684: INFO: (18) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 17.76297ms)
    Jan 18 17:39:30.685: INFO: (18) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 18.031892ms)
    Jan 18 17:39:30.692: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:443/proxy/tlsrewriteme... (200; 7.453875ms)
    Jan 18 17:39:30.693: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:1080/proxy/rewriteme">t... (200; 8.610084ms)
    Jan 18 17:39:30.693: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:462/proxy/: tls qux (200; 8.643173ms)
    Jan 18 17:39:30.694: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454/proxy/rewriteme">test</a> (200; 9.139907ms)
    Jan 18 17:39:30.694: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:162/proxy/: bar (200; 9.512361ms)
    Jan 18 17:39:30.695: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:160/proxy/: foo (200; 9.786671ms)
    Jan 18 17:39:30.695: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:160/proxy/: foo (200; 10.197405ms)
    Jan 18 17:39:30.696: INFO: (19) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname1/proxy/: tls baz (200; 11.630556ms)
    Jan 18 17:39:30.696: INFO: (19) /api/v1/namespaces/proxy-871/pods/https:proxy-service-hzk4h-w7454:460/proxy/: tls baz (200; 11.563184ms)
    Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/: <a href="/api/v1/namespaces/proxy-871/pods/proxy-service-hzk4h-w7454:1080/proxy/rewriteme">test</... (200; 12.065788ms)
    Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/pods/http:proxy-service-hzk4h-w7454:162/proxy/: bar (200; 12.333941ms)
    Jan 18 17:39:30.697: INFO: (19) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname1/proxy/: foo (200; 12.099828ms)
    Jan 18 17:39:30.698: INFO: (19) /api/v1/namespaces/proxy-871/services/proxy-service-hzk4h:portname2/proxy/: bar (200; 12.730224ms)
    Jan 18 17:39:30.699: INFO: (19) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname1/proxy/: foo (200; 13.836182ms)
    Jan 18 17:39:30.699: INFO: (19) /api/v1/namespaces/proxy-871/services/https:proxy-service-hzk4h:tlsportname2/proxy/: tls qux (200; 13.884862ms)
    Jan 18 17:39:30.700: INFO: (19) /api/v1/namespaces/proxy-871/services/http:proxy-service-hzk4h:portname2/proxy/: bar (200; 15.487473ms)
    STEP: deleting ReplicationController proxy-service-hzk4h in namespace proxy-871, will wait for the garbage collector to delete the pods 01/18/23 17:39:30.7
    Jan 18 17:39:30.821: INFO: Deleting ReplicationController proxy-service-hzk4h took: 11.798446ms
    Jan 18 17:39:30.922: INFO: Terminating ReplicationController proxy-service-hzk4h pods took: 100.95401ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 17:39:32.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-871" for this suite. 01/18/23 17:39:32.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:39:32.045
Jan 18 17:39:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-watch 01/18/23 17:39:32.047
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:32.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:32.077
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 18 17:39:32.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Creating first CR  01/18/23 17:39:34.681
Jan 18 17:39:34.692: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:34Z]] name:name1 resourceVersion:2631884507 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/18/23 17:39:44.693
Jan 18 17:39:44.703: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:44Z]] name:name2 resourceVersion:2631884920 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/18/23 17:39:54.704
Jan 18 17:39:54.716: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:54Z]] name:name1 resourceVersion:2631885219 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/18/23 17:40:04.716
Jan 18 17:40:04.729: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:40:04Z]] name:name2 resourceVersion:2631885517 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/18/23 17:40:14.729
Jan 18 17:40:14.745: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:54Z]] name:name1 resourceVersion:2631885839 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/18/23 17:40:24.747
Jan 18 17:40:24.763: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:40:04Z]] name:name2 resourceVersion:2631886257 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:40:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7846" for this suite. 01/18/23 17:40:35.297
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":160,"skipped":3041,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.266 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:39:32.045
    Jan 18 17:39:32.046: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-watch 01/18/23 17:39:32.047
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:39:32.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:39:32.077
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 18 17:39:32.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Creating first CR  01/18/23 17:39:34.681
    Jan 18 17:39:34.692: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:34Z]] name:name1 resourceVersion:2631884507 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/18/23 17:39:44.693
    Jan 18 17:39:44.703: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:44Z]] name:name2 resourceVersion:2631884920 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/18/23 17:39:54.704
    Jan 18 17:39:54.716: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:54Z]] name:name1 resourceVersion:2631885219 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/18/23 17:40:04.716
    Jan 18 17:40:04.729: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:40:04Z]] name:name2 resourceVersion:2631885517 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/18/23 17:40:14.729
    Jan 18 17:40:14.745: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:34Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:39:54Z]] name:name1 resourceVersion:2631885839 uid:1574063f-cfc2-42bf-ad5c-05bfe4a1e486] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/18/23 17:40:24.747
    Jan 18 17:40:24.763: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-18T17:39:44Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-18T17:40:04Z]] name:name2 resourceVersion:2631886257 uid:2a23878f-17ac-4a90-834f-6a218539a8ef] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:40:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-7846" for this suite. 01/18/23 17:40:35.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:40:35.317
Jan 18 17:40:35.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename endpointslice 01/18/23 17:40:35.318
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:35.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:35.349
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/18/23 17:40:35.354
STEP: getting /apis/discovery.k8s.io 01/18/23 17:40:35.359
STEP: getting /apis/discovery.k8s.iov1 01/18/23 17:40:35.361
STEP: creating 01/18/23 17:40:35.363
STEP: getting 01/18/23 17:40:35.389
STEP: listing 01/18/23 17:40:35.394
STEP: watching 01/18/23 17:40:35.401
Jan 18 17:40:35.401: INFO: starting watch
STEP: cluster-wide listing 01/18/23 17:40:35.403
STEP: cluster-wide watching 01/18/23 17:40:35.41
Jan 18 17:40:35.410: INFO: starting watch
STEP: patching 01/18/23 17:40:35.412
STEP: updating 01/18/23 17:40:35.421
Jan 18 17:40:35.437: INFO: waiting for watch events with expected annotations
Jan 18 17:40:35.437: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 17:40:35.438
STEP: deleting a collection 01/18/23 17:40:35.463
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 17:40:35.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-340" for this suite. 01/18/23 17:40:35.502
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":161,"skipped":3087,"failed":0}
------------------------------
â€¢ [0.198 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:40:35.317
    Jan 18 17:40:35.317: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 17:40:35.318
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:35.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:35.349
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/18/23 17:40:35.354
    STEP: getting /apis/discovery.k8s.io 01/18/23 17:40:35.359
    STEP: getting /apis/discovery.k8s.iov1 01/18/23 17:40:35.361
    STEP: creating 01/18/23 17:40:35.363
    STEP: getting 01/18/23 17:40:35.389
    STEP: listing 01/18/23 17:40:35.394
    STEP: watching 01/18/23 17:40:35.401
    Jan 18 17:40:35.401: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 17:40:35.403
    STEP: cluster-wide watching 01/18/23 17:40:35.41
    Jan 18 17:40:35.410: INFO: starting watch
    STEP: patching 01/18/23 17:40:35.412
    STEP: updating 01/18/23 17:40:35.421
    Jan 18 17:40:35.437: INFO: waiting for watch events with expected annotations
    Jan 18 17:40:35.437: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 17:40:35.438
    STEP: deleting a collection 01/18/23 17:40:35.463
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 17:40:35.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-340" for this suite. 01/18/23 17:40:35.502
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:40:35.515
Jan 18 17:40:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:40:35.517
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:35.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:35.542
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/18/23 17:40:35.546
STEP: Creating a ResourceQuota 01/18/23 17:40:40.553
STEP: Ensuring resource quota status is calculated 01/18/23 17:40:40.57
STEP: Creating a Pod that fits quota 01/18/23 17:40:42.578
STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 17:40:42.602
STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 17:40:44.611
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 17:40:44.615
STEP: Ensuring a pod cannot update its resource requirements 01/18/23 17:40:44.618
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 17:40:44.627
STEP: Deleting the pod 01/18/23 17:40:46.636
STEP: Ensuring resource quota status released the pod usage 01/18/23 17:40:46.657
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:40:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4502" for this suite. 01/18/23 17:40:48.674
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":162,"skipped":3089,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:40:35.515
    Jan 18 17:40:35.516: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:40:35.517
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:35.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:35.542
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/18/23 17:40:35.546
    STEP: Creating a ResourceQuota 01/18/23 17:40:40.553
    STEP: Ensuring resource quota status is calculated 01/18/23 17:40:40.57
    STEP: Creating a Pod that fits quota 01/18/23 17:40:42.578
    STEP: Ensuring ResourceQuota status captures the pod usage 01/18/23 17:40:42.602
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/18/23 17:40:44.611
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/18/23 17:40:44.615
    STEP: Ensuring a pod cannot update its resource requirements 01/18/23 17:40:44.618
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/18/23 17:40:44.627
    STEP: Deleting the pod 01/18/23 17:40:46.636
    STEP: Ensuring resource quota status released the pod usage 01/18/23 17:40:46.657
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:40:48.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4502" for this suite. 01/18/23 17:40:48.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:40:48.69
Jan 18 17:40:48.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 17:40:48.691
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:48.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:48.717
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/18/23 17:40:48.721
STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:40:49.263
Jan 18 17:40:49.284: INFO: Pod name wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54: Found 0 pods out of 5
Jan 18 17:40:54.369: INFO: Pod name wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 17:40:54.369
Jan 18 17:40:54.369: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:40:54.376: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.10571ms
Jan 18 17:40:56.387: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038617s
Jan 18 17:40:58.386: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017500387s
Jan 18 17:41:00.387: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018065583s
Jan 18 17:41:02.385: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Running", Reason="", readiness=true. Elapsed: 8.016743283s
Jan 18 17:41:02.385: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk" satisfied condition "running"
Jan 18 17:41:02.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:02.393: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg": Phase="Pending", Reason="", readiness=false. Elapsed: 7.337463ms
Jan 18 17:41:04.402: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.0161401s
Jan 18 17:41:04.402: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg" satisfied condition "running"
Jan 18 17:41:04.402: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:04.418: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd": Phase="Running", Reason="", readiness=true. Elapsed: 16.043016ms
Jan 18 17:41:04.418: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd" satisfied condition "running"
Jan 18 17:41:04.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:04.425: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8": Phase="Running", Reason="", readiness=true. Elapsed: 7.363223ms
Jan 18 17:41:04.425: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8" satisfied condition "running"
Jan 18 17:41:04.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:04.432: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.220424ms
Jan 18 17:41:04.432: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:04.432
Jan 18 17:41:04.502: INFO: Deleting ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 took: 12.296549ms
Jan 18 17:41:04.603: INFO: Terminating ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 pods took: 100.605695ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:41:07.614
Jan 18 17:41:07.636: INFO: Pod name wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce: Found 0 pods out of 5
Jan 18 17:41:12.652: INFO: Pod name wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 17:41:12.652
Jan 18 17:41:12.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:12.659: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.745699ms
Jan 18 17:41:14.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017306743s
Jan 18 17:41:16.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01796064s
Jan 18 17:41:18.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017865245s
Jan 18 17:41:20.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017459024s
Jan 18 17:41:22.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Running", Reason="", readiness=true. Elapsed: 10.01872982s
Jan 18 17:41:22.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4" satisfied condition "running"
Jan 18 17:41:22.671: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:22.679: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419354ms
Jan 18 17:41:24.688: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.016449871s
Jan 18 17:41:24.688: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv" satisfied condition "running"
Jan 18 17:41:24.688: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:24.696: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q": Phase="Running", Reason="", readiness=true. Elapsed: 8.612762ms
Jan 18 17:41:24.697: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q" satisfied condition "running"
Jan 18 17:41:24.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:24.704: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh": Phase="Running", Reason="", readiness=true. Elapsed: 7.633444ms
Jan 18 17:41:24.704: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh" satisfied condition "running"
Jan 18 17:41:24.704: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:24.712: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885": Phase="Running", Reason="", readiness=true. Elapsed: 7.478165ms
Jan 18 17:41:24.712: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:24.712
Jan 18 17:41:24.785: INFO: Deleting ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce took: 16.337847ms
Jan 18 17:41:24.886: INFO: Terminating ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce pods took: 100.830173ms
STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:41:27.995
Jan 18 17:41:28.022: INFO: Pod name wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e: Found 0 pods out of 5
Jan 18 17:41:33.036: INFO: Pod name wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/18/23 17:41:33.036
Jan 18 17:41:33.036: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:33.043: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.441223ms
Jan 18 17:41:35.052: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016312554s
Jan 18 17:41:37.056: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020193047s
Jan 18 17:41:39.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017057461s
Jan 18 17:41:41.054: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018089601s
Jan 18 17:41:43.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Running", Reason="", readiness=true. Elapsed: 10.017427205s
Jan 18 17:41:43.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp" satisfied condition "running"
Jan 18 17:41:43.053: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:43.061: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.772046ms
Jan 18 17:41:45.073: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019302581s
Jan 18 17:41:45.073: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc" satisfied condition "running"
Jan 18 17:41:45.073: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:45.080: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd": Phase="Running", Reason="", readiness=true. Elapsed: 6.861149ms
Jan 18 17:41:45.080: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd" satisfied condition "running"
Jan 18 17:41:45.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:45.089: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c": Phase="Running", Reason="", readiness=true. Elapsed: 9.089945ms
Jan 18 17:41:45.089: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c" satisfied condition "running"
Jan 18 17:41:45.089: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh" in namespace "emptydir-wrapper-3577" to be "running"
Jan 18 17:41:45.098: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh": Phase="Running", Reason="", readiness=true. Elapsed: 8.793493ms
Jan 18 17:41:45.098: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:45.098
Jan 18 17:41:45.170: INFO: Deleting ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e took: 13.688168ms
Jan 18 17:41:45.370: INFO: Terminating ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e pods took: 200.222392ms
STEP: Cleaning up the configMaps 01/18/23 17:41:48.371
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 17:41:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3577" for this suite. 01/18/23 17:41:48.999
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":163,"skipped":3140,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.322 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:40:48.69
    Jan 18 17:40:48.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 17:40:48.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:40:48.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:40:48.717
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/18/23 17:40:48.721
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:40:49.263
    Jan 18 17:40:49.284: INFO: Pod name wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54: Found 0 pods out of 5
    Jan 18 17:40:54.369: INFO: Pod name wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 17:40:54.369
    Jan 18 17:40:54.369: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:40:54.376: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.10571ms
    Jan 18 17:40:56.387: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018038617s
    Jan 18 17:40:58.386: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017500387s
    Jan 18 17:41:00.387: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018065583s
    Jan 18 17:41:02.385: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk": Phase="Running", Reason="", readiness=true. Elapsed: 8.016743283s
    Jan 18 17:41:02.385: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-hgwxk" satisfied condition "running"
    Jan 18 17:41:02.386: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:02.393: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg": Phase="Pending", Reason="", readiness=false. Elapsed: 7.337463ms
    Jan 18 17:41:04.402: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.0161401s
    Jan 18 17:41:04.402: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-q5hgg" satisfied condition "running"
    Jan 18 17:41:04.402: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:04.418: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd": Phase="Running", Reason="", readiness=true. Elapsed: 16.043016ms
    Jan 18 17:41:04.418: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-rhwpd" satisfied condition "running"
    Jan 18 17:41:04.418: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:04.425: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8": Phase="Running", Reason="", readiness=true. Elapsed: 7.363223ms
    Jan 18 17:41:04.425: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-tgqp8" satisfied condition "running"
    Jan 18 17:41:04.425: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:04.432: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj": Phase="Running", Reason="", readiness=true. Elapsed: 6.220424ms
    Jan 18 17:41:04.432: INFO: Pod "wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54-w8vxj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:04.432
    Jan 18 17:41:04.502: INFO: Deleting ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 took: 12.296549ms
    Jan 18 17:41:04.603: INFO: Terminating ReplicationController wrapped-volume-race-d22e9c79-d391-40fe-ae55-031362cf0d54 pods took: 100.605695ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:41:07.614
    Jan 18 17:41:07.636: INFO: Pod name wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce: Found 0 pods out of 5
    Jan 18 17:41:12.652: INFO: Pod name wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 17:41:12.652
    Jan 18 17:41:12.653: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:12.659: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.745699ms
    Jan 18 17:41:14.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017306743s
    Jan 18 17:41:16.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01796064s
    Jan 18 17:41:18.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017865245s
    Jan 18 17:41:20.670: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017459024s
    Jan 18 17:41:22.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4": Phase="Running", Reason="", readiness=true. Elapsed: 10.01872982s
    Jan 18 17:41:22.671: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-74td4" satisfied condition "running"
    Jan 18 17:41:22.671: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:22.679: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419354ms
    Jan 18 17:41:24.688: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv": Phase="Running", Reason="", readiness=true. Elapsed: 2.016449871s
    Jan 18 17:41:24.688: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-dklfv" satisfied condition "running"
    Jan 18 17:41:24.688: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:24.696: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q": Phase="Running", Reason="", readiness=true. Elapsed: 8.612762ms
    Jan 18 17:41:24.697: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jd48q" satisfied condition "running"
    Jan 18 17:41:24.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:24.704: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh": Phase="Running", Reason="", readiness=true. Elapsed: 7.633444ms
    Jan 18 17:41:24.704: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-jq9sh" satisfied condition "running"
    Jan 18 17:41:24.704: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:24.712: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885": Phase="Running", Reason="", readiness=true. Elapsed: 7.478165ms
    Jan 18 17:41:24.712: INFO: Pod "wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce-w6885" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:24.712
    Jan 18 17:41:24.785: INFO: Deleting ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce took: 16.337847ms
    Jan 18 17:41:24.886: INFO: Terminating ReplicationController wrapped-volume-race-8b04b3bd-badc-4d5d-9afe-24a5d423e0ce pods took: 100.830173ms
    STEP: Creating RC which spawns configmap-volume pods 01/18/23 17:41:27.995
    Jan 18 17:41:28.022: INFO: Pod name wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e: Found 0 pods out of 5
    Jan 18 17:41:33.036: INFO: Pod name wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/18/23 17:41:33.036
    Jan 18 17:41:33.036: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:33.043: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 7.441223ms
    Jan 18 17:41:35.052: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016312554s
    Jan 18 17:41:37.056: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020193047s
    Jan 18 17:41:39.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017057461s
    Jan 18 17:41:41.054: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018089601s
    Jan 18 17:41:43.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp": Phase="Running", Reason="", readiness=true. Elapsed: 10.017427205s
    Jan 18 17:41:43.053: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-ccckp" satisfied condition "running"
    Jan 18 17:41:43.053: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:43.061: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.772046ms
    Jan 18 17:41:45.073: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc": Phase="Running", Reason="", readiness=true. Elapsed: 2.019302581s
    Jan 18 17:41:45.073: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-hqxzc" satisfied condition "running"
    Jan 18 17:41:45.073: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:45.080: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd": Phase="Running", Reason="", readiness=true. Elapsed: 6.861149ms
    Jan 18 17:41:45.080: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-jhldd" satisfied condition "running"
    Jan 18 17:41:45.080: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:45.089: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c": Phase="Running", Reason="", readiness=true. Elapsed: 9.089945ms
    Jan 18 17:41:45.089: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-qrp2c" satisfied condition "running"
    Jan 18 17:41:45.089: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh" in namespace "emptydir-wrapper-3577" to be "running"
    Jan 18 17:41:45.098: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh": Phase="Running", Reason="", readiness=true. Elapsed: 8.793493ms
    Jan 18 17:41:45.098: INFO: Pod "wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e-s8sgh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e in namespace emptydir-wrapper-3577, will wait for the garbage collector to delete the pods 01/18/23 17:41:45.098
    Jan 18 17:41:45.170: INFO: Deleting ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e took: 13.688168ms
    Jan 18 17:41:45.370: INFO: Terminating ReplicationController wrapped-volume-race-85b56728-264c-4fb2-8cf2-94309ede009e pods took: 200.222392ms
    STEP: Cleaning up the configMaps 01/18/23 17:41:48.371
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:41:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-3577" for this suite. 01/18/23 17:41:48.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:41:49.015
Jan 18 17:41:49.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename subpath 01/18/23 17:41:49.016
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:41:49.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:41:49.046
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 17:41:49.051
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-f752 01/18/23 17:41:49.067
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:41:49.068
Jan 18 17:41:49.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f752" in namespace "subpath-4044" to be "Succeeded or Failed"
Jan 18 17:41:49.092: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886023ms
Jan 18 17:41:51.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 2.014762464s
Jan 18 17:41:53.103: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 4.016325812s
Jan 18 17:41:55.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 6.015368987s
Jan 18 17:41:57.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 8.015558372s
Jan 18 17:41:59.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 10.01492982s
Jan 18 17:42:01.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 12.015926904s
Jan 18 17:42:03.100: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 14.013995612s
Jan 18 17:42:05.103: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 16.017142221s
Jan 18 17:42:07.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 18.014796659s
Jan 18 17:42:09.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 20.014588934s
Jan 18 17:42:11.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=false. Elapsed: 22.015549677s
Jan 18 17:42:13.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014923403s
STEP: Saw pod success 01/18/23 17:42:13.101
Jan 18 17:42:13.101: INFO: Pod "pod-subpath-test-configmap-f752" satisfied condition "Succeeded or Failed"
Jan 18 17:42:13.110: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-configmap-f752 container test-container-subpath-configmap-f752: <nil>
STEP: delete the pod 01/18/23 17:42:13.146
Jan 18 17:42:13.168: INFO: Waiting for pod pod-subpath-test-configmap-f752 to disappear
Jan 18 17:42:13.174: INFO: Pod pod-subpath-test-configmap-f752 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f752 01/18/23 17:42:13.174
Jan 18 17:42:13.174: INFO: Deleting pod "pod-subpath-test-configmap-f752" in namespace "subpath-4044"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 17:42:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4044" for this suite. 01/18/23 17:42:13.19
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":164,"skipped":3157,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.189 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:41:49.015
    Jan 18 17:41:49.015: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename subpath 01/18/23 17:41:49.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:41:49.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:41:49.046
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 17:41:49.051
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-f752 01/18/23 17:41:49.067
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:41:49.068
    Jan 18 17:41:49.086: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f752" in namespace "subpath-4044" to be "Succeeded or Failed"
    Jan 18 17:41:49.092: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Pending", Reason="", readiness=false. Elapsed: 5.886023ms
    Jan 18 17:41:51.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 2.014762464s
    Jan 18 17:41:53.103: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 4.016325812s
    Jan 18 17:41:55.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 6.015368987s
    Jan 18 17:41:57.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 8.015558372s
    Jan 18 17:41:59.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 10.01492982s
    Jan 18 17:42:01.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 12.015926904s
    Jan 18 17:42:03.100: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 14.013995612s
    Jan 18 17:42:05.103: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 16.017142221s
    Jan 18 17:42:07.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 18.014796659s
    Jan 18 17:42:09.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=true. Elapsed: 20.014588934s
    Jan 18 17:42:11.102: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Running", Reason="", readiness=false. Elapsed: 22.015549677s
    Jan 18 17:42:13.101: INFO: Pod "pod-subpath-test-configmap-f752": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.014923403s
    STEP: Saw pod success 01/18/23 17:42:13.101
    Jan 18 17:42:13.101: INFO: Pod "pod-subpath-test-configmap-f752" satisfied condition "Succeeded or Failed"
    Jan 18 17:42:13.110: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-configmap-f752 container test-container-subpath-configmap-f752: <nil>
    STEP: delete the pod 01/18/23 17:42:13.146
    Jan 18 17:42:13.168: INFO: Waiting for pod pod-subpath-test-configmap-f752 to disappear
    Jan 18 17:42:13.174: INFO: Pod pod-subpath-test-configmap-f752 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-f752 01/18/23 17:42:13.174
    Jan 18 17:42:13.174: INFO: Deleting pod "pod-subpath-test-configmap-f752" in namespace "subpath-4044"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 17:42:13.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4044" for this suite. 01/18/23 17:42:13.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:42:13.205
Jan 18 17:42:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replication-controller 01/18/23 17:42:13.206
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:13.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:13.238
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 17:42:13.244
Jan 18 17:42:13.261: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8237" to be "running and ready"
Jan 18 17:42:13.267: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882032ms
Jan 18 17:42:13.267: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:42:15.277: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.015803596s
Jan 18 17:42:15.277: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 18 17:42:15.277: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/18/23 17:42:15.285
STEP: Then the orphan pod is adopted 01/18/23 17:42:15.293
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 17:42:16.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8237" for this suite. 01/18/23 17:42:16.37
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":165,"skipped":3163,"failed":0}
------------------------------
â€¢ [3.265 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:42:13.205
    Jan 18 17:42:13.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 17:42:13.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:13.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:13.238
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/18/23 17:42:13.244
    Jan 18 17:42:13.261: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8237" to be "running and ready"
    Jan 18 17:42:13.267: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.882032ms
    Jan 18 17:42:13.267: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:42:15.277: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.015803596s
    Jan 18 17:42:15.277: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 18 17:42:15.277: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/18/23 17:42:15.285
    STEP: Then the orphan pod is adopted 01/18/23 17:42:15.293
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 17:42:16.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8237" for this suite. 01/18/23 17:42:16.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:42:16.471
Jan 18 17:42:16.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:42:16.472
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:16.672
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:16.677
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-213d4047-2e7c-4ef4-9421-07739928110a 01/18/23 17:42:16.682
STEP: Creating a pod to test consume configMaps 01/18/23 17:42:16.691
Jan 18 17:42:16.704: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f" in namespace "projected-779" to be "Succeeded or Failed"
Jan 18 17:42:16.709: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367789ms
Jan 18 17:42:18.717: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013382064s
Jan 18 17:42:20.720: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016369379s
STEP: Saw pod success 01/18/23 17:42:20.72
Jan 18 17:42:20.721: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f" satisfied condition "Succeeded or Failed"
Jan 18 17:42:20.727: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:42:20.743
Jan 18 17:42:20.767: INFO: Waiting for pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f to disappear
Jan 18 17:42:20.774: INFO: Pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:42:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-779" for this suite. 01/18/23 17:42:20.785
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":166,"skipped":3170,"failed":0}
------------------------------
â€¢ [4.327 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:42:16.471
    Jan 18 17:42:16.471: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:42:16.472
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:16.672
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:16.677
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-213d4047-2e7c-4ef4-9421-07739928110a 01/18/23 17:42:16.682
    STEP: Creating a pod to test consume configMaps 01/18/23 17:42:16.691
    Jan 18 17:42:16.704: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f" in namespace "projected-779" to be "Succeeded or Failed"
    Jan 18 17:42:16.709: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.367789ms
    Jan 18 17:42:18.717: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013382064s
    Jan 18 17:42:20.720: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016369379s
    STEP: Saw pod success 01/18/23 17:42:20.72
    Jan 18 17:42:20.721: INFO: Pod "pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f" satisfied condition "Succeeded or Failed"
    Jan 18 17:42:20.727: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:42:20.743
    Jan 18 17:42:20.767: INFO: Waiting for pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f to disappear
    Jan 18 17:42:20.774: INFO: Pod pod-projected-configmaps-97682e5b-b2ab-43d3-a635-41ed921a848f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:42:20.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-779" for this suite. 01/18/23 17:42:20.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:42:20.799
Jan 18 17:42:20.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:42:20.8
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:20.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:20.829
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 18 17:42:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 17:42:23.711
Jan 18 17:42:23.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 create -f -'
Jan 18 17:42:24.461: INFO: stderr: ""
Jan 18 17:42:24.461: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 17:42:24.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 delete e2e-test-crd-publish-openapi-9770-crds test-cr'
Jan 18 17:42:24.564: INFO: stderr: ""
Jan 18 17:42:24.564: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 18 17:42:24.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 apply -f -'
Jan 18 17:42:24.849: INFO: stderr: ""
Jan 18 17:42:24.849: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 18 17:42:24.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 delete e2e-test-crd-publish-openapi-9770-crds test-cr'
Jan 18 17:42:24.953: INFO: stderr: ""
Jan 18 17:42:24.953: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/18/23 17:42:24.953
Jan 18 17:42:24.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 explain e2e-test-crd-publish-openapi-9770-crds'
Jan 18 17:42:25.207: INFO: stderr: ""
Jan 18 17:42:25.207: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9770-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:42:29.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-651" for this suite. 01/18/23 17:42:29.132
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":167,"skipped":3183,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.348 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:42:20.799
    Jan 18 17:42:20.799: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:42:20.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:20.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:20.829
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 18 17:42:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 17:42:23.711
    Jan 18 17:42:23.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 create -f -'
    Jan 18 17:42:24.461: INFO: stderr: ""
    Jan 18 17:42:24.461: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 17:42:24.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 delete e2e-test-crd-publish-openapi-9770-crds test-cr'
    Jan 18 17:42:24.564: INFO: stderr: ""
    Jan 18 17:42:24.564: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 18 17:42:24.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 apply -f -'
    Jan 18 17:42:24.849: INFO: stderr: ""
    Jan 18 17:42:24.849: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 18 17:42:24.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 --namespace=crd-publish-openapi-651 delete e2e-test-crd-publish-openapi-9770-crds test-cr'
    Jan 18 17:42:24.953: INFO: stderr: ""
    Jan 18 17:42:24.953: INFO: stdout: "e2e-test-crd-publish-openapi-9770-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/18/23 17:42:24.953
    Jan 18 17:42:24.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-651 explain e2e-test-crd-publish-openapi-9770-crds'
    Jan 18 17:42:25.207: INFO: stderr: ""
    Jan 18 17:42:25.207: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9770-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:42:29.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-651" for this suite. 01/18/23 17:42:29.132
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:42:29.149
Jan 18 17:42:29.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 17:42:29.15
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:29.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:29.177
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/18/23 17:42:29.181
STEP: Ensuring active pods == parallelism 01/18/23 17:42:29.189
STEP: delete a job 01/18/23 17:42:31.199
STEP: deleting Job.batch foo in namespace job-2360, will wait for the garbage collector to delete the pods 01/18/23 17:42:31.199
Jan 18 17:42:31.269: INFO: Deleting Job.batch foo took: 13.509596ms
Jan 18 17:42:31.370: INFO: Terminating Job.batch foo pods took: 100.418253ms
STEP: Ensuring job was deleted 01/18/23 17:43:03.57
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 17:43:03.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2360" for this suite. 01/18/23 17:43:03.587
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":168,"skipped":3185,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.452 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:42:29.149
    Jan 18 17:42:29.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 17:42:29.15
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:42:29.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:42:29.177
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/18/23 17:42:29.181
    STEP: Ensuring active pods == parallelism 01/18/23 17:42:29.189
    STEP: delete a job 01/18/23 17:42:31.199
    STEP: deleting Job.batch foo in namespace job-2360, will wait for the garbage collector to delete the pods 01/18/23 17:42:31.199
    Jan 18 17:42:31.269: INFO: Deleting Job.batch foo took: 13.509596ms
    Jan 18 17:42:31.370: INFO: Terminating Job.batch foo pods took: 100.418253ms
    STEP: Ensuring job was deleted 01/18/23 17:43:03.57
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 17:43:03.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2360" for this suite. 01/18/23 17:43:03.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:03.603
Jan 18 17:43:03.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:43:03.604
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:03.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:03.632
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/18/23 17:43:03.636
Jan 18 17:43:03.650: INFO: Waiting up to 5m0s for pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52" in namespace "downward-api-7657" to be "Succeeded or Failed"
Jan 18 17:43:03.659: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139375ms
Jan 18 17:43:05.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019162287s
Jan 18 17:43:07.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019316618s
STEP: Saw pod success 01/18/23 17:43:07.67
Jan 18 17:43:07.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52" satisfied condition "Succeeded or Failed"
Jan 18 17:43:07.676: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:43:07.693
Jan 18 17:43:07.719: INFO: Waiting for pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 to disappear
Jan 18 17:43:07.725: INFO: Pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 17:43:07.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7657" for this suite. 01/18/23 17:43:07.734
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":169,"skipped":3197,"failed":0}
------------------------------
â€¢ [4.145 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:03.603
    Jan 18 17:43:03.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:43:03.604
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:03.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:03.632
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/18/23 17:43:03.636
    Jan 18 17:43:03.650: INFO: Waiting up to 5m0s for pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52" in namespace "downward-api-7657" to be "Succeeded or Failed"
    Jan 18 17:43:03.659: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Pending", Reason="", readiness=false. Elapsed: 9.139375ms
    Jan 18 17:43:05.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019162287s
    Jan 18 17:43:07.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019316618s
    STEP: Saw pod success 01/18/23 17:43:07.67
    Jan 18 17:43:07.670: INFO: Pod "downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52" satisfied condition "Succeeded or Failed"
    Jan 18 17:43:07.676: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:43:07.693
    Jan 18 17:43:07.719: INFO: Waiting for pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 to disappear
    Jan 18 17:43:07.725: INFO: Pod downward-api-f6f099b5-a8c7-4cb7-bb3c-65746e840f52 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 17:43:07.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7657" for this suite. 01/18/23 17:43:07.734
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:07.748
Jan 18 17:43:07.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:43:07.749
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:07.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:07.777
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-761-delete-me 01/18/23 17:43:07.791
STEP: Waiting for the RuntimeClass to disappear 01/18/23 17:43:07.805
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 17:43:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-761" for this suite. 01/18/23 17:43:07.833
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":170,"skipped":3197,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:07.748
    Jan 18 17:43:07.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 17:43:07.749
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:07.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:07.777
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-761-delete-me 01/18/23 17:43:07.791
    STEP: Waiting for the RuntimeClass to disappear 01/18/23 17:43:07.805
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 17:43:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-761" for this suite. 01/18/23 17:43:07.833
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:07.849
Jan 18 17:43:07.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:43:07.851
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:07.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:07.879
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/18/23 17:43:07.883
STEP: submitting the pod to kubernetes 01/18/23 17:43:07.883
Jan 18 17:43:07.900: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" in namespace "pods-3129" to be "running and ready"
Jan 18 17:43:07.905: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.460589ms
Jan 18 17:43:07.905: INFO: The phase of Pod pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:43:09.913: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013279217s
Jan 18 17:43:09.913: INFO: The phase of Pod pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f is Running (Ready = true)
Jan 18 17:43:09.913: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/18/23 17:43:09.921
STEP: updating the pod 01/18/23 17:43:09.929
Jan 18 17:43:10.452: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f"
Jan 18 17:43:10.452: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" in namespace "pods-3129" to be "terminated with reason DeadlineExceeded"
Jan 18 17:43:10.458: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 6.384645ms
Jan 18 17:43:12.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014372149s
Jan 18 17:43:14.465: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=false. Elapsed: 4.013216612s
Jan 18 17:43:16.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.013972208s
Jan 18 17:43:16.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:43:16.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3129" for this suite. 01/18/23 17:43:16.477
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":171,"skipped":3199,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.640 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:07.849
    Jan 18 17:43:07.849: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:43:07.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:07.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:07.879
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/18/23 17:43:07.883
    STEP: submitting the pod to kubernetes 01/18/23 17:43:07.883
    Jan 18 17:43:07.900: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" in namespace "pods-3129" to be "running and ready"
    Jan 18 17:43:07.905: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.460589ms
    Jan 18 17:43:07.905: INFO: The phase of Pod pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:43:09.913: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 2.013279217s
    Jan 18 17:43:09.913: INFO: The phase of Pod pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f is Running (Ready = true)
    Jan 18 17:43:09.913: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/18/23 17:43:09.921
    STEP: updating the pod 01/18/23 17:43:09.929
    Jan 18 17:43:10.452: INFO: Successfully updated pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f"
    Jan 18 17:43:10.452: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" in namespace "pods-3129" to be "terminated with reason DeadlineExceeded"
    Jan 18 17:43:10.458: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 6.384645ms
    Jan 18 17:43:12.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=true. Elapsed: 2.014372149s
    Jan 18 17:43:14.465: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Running", Reason="", readiness=false. Elapsed: 4.013216612s
    Jan 18 17:43:16.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.013972208s
    Jan 18 17:43:16.466: INFO: Pod "pod-update-activedeadlineseconds-3783a05a-f0c5-4b9e-867b-020d27f3300f" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:43:16.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3129" for this suite. 01/18/23 17:43:16.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:16.494
Jan 18 17:43:16.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:43:16.495
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:16.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:16.524
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 17:43:16.529
Jan 18 17:43:16.541: INFO: Waiting up to 5m0s for pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676" in namespace "emptydir-9347" to be "Succeeded or Failed"
Jan 18 17:43:16.547: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199214ms
Jan 18 17:43:18.555: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Running", Reason="", readiness=false. Elapsed: 2.013692512s
Jan 18 17:43:20.556: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014795571s
STEP: Saw pod success 01/18/23 17:43:20.556
Jan 18 17:43:20.556: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676" satisfied condition "Succeeded or Failed"
Jan 18 17:43:20.561: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 container test-container: <nil>
STEP: delete the pod 01/18/23 17:43:20.578
Jan 18 17:43:20.601: INFO: Waiting for pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 to disappear
Jan 18 17:43:20.606: INFO: Pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:43:20.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9347" for this suite. 01/18/23 17:43:20.614
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":172,"skipped":3262,"failed":0}
------------------------------
â€¢ [4.133 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:16.494
    Jan 18 17:43:16.494: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:43:16.495
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:16.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:16.524
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 17:43:16.529
    Jan 18 17:43:16.541: INFO: Waiting up to 5m0s for pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676" in namespace "emptydir-9347" to be "Succeeded or Failed"
    Jan 18 17:43:16.547: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199214ms
    Jan 18 17:43:18.555: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Running", Reason="", readiness=false. Elapsed: 2.013692512s
    Jan 18 17:43:20.556: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014795571s
    STEP: Saw pod success 01/18/23 17:43:20.556
    Jan 18 17:43:20.556: INFO: Pod "pod-355d5387-5245-4e15-8c7d-7d5f83a04676" satisfied condition "Succeeded or Failed"
    Jan 18 17:43:20.561: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:43:20.578
    Jan 18 17:43:20.601: INFO: Waiting for pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 to disappear
    Jan 18 17:43:20.606: INFO: Pod pod-355d5387-5245-4e15-8c7d-7d5f83a04676 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:43:20.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9347" for this suite. 01/18/23 17:43:20.614
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:20.628
Jan 18 17:43:20.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename subpath 01/18/23 17:43:20.629
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:20.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:20.662
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 17:43:20.666
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-t47r 01/18/23 17:43:20.684
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:43:20.684
Jan 18 17:43:20.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t47r" in namespace "subpath-3312" to be "Succeeded or Failed"
Jan 18 17:43:20.707: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305968ms
Jan 18 17:43:22.714: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015705996s
Jan 18 17:43:24.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 4.020008041s
Jan 18 17:43:26.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 6.019248364s
Jan 18 17:43:28.714: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 8.015450541s
Jan 18 17:43:30.715: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 10.016033456s
Jan 18 17:43:32.716: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 12.017229401s
Jan 18 17:43:34.716: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 14.01785941s
Jan 18 17:43:36.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 16.018347971s
Jan 18 17:43:38.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 18.018734959s
Jan 18 17:43:40.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 20.018273657s
Jan 18 17:43:42.715: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=false. Elapsed: 22.016172772s
Jan 18 17:43:44.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019694621s
STEP: Saw pod success 01/18/23 17:43:44.718
Jan 18 17:43:44.718: INFO: Pod "pod-subpath-test-secret-t47r" satisfied condition "Succeeded or Failed"
Jan 18 17:43:44.725: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-secret-t47r container test-container-subpath-secret-t47r: <nil>
STEP: delete the pod 01/18/23 17:43:44.741
Jan 18 17:43:44.759: INFO: Waiting for pod pod-subpath-test-secret-t47r to disappear
Jan 18 17:43:44.765: INFO: Pod pod-subpath-test-secret-t47r no longer exists
STEP: Deleting pod pod-subpath-test-secret-t47r 01/18/23 17:43:44.765
Jan 18 17:43:44.765: INFO: Deleting pod "pod-subpath-test-secret-t47r" in namespace "subpath-3312"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 17:43:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3312" for this suite. 01/18/23 17:43:44.778
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":173,"skipped":3274,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.163 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:20.628
    Jan 18 17:43:20.628: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename subpath 01/18/23 17:43:20.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:20.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:20.662
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 17:43:20.666
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-t47r 01/18/23 17:43:20.684
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:43:20.684
    Jan 18 17:43:20.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-t47r" in namespace "subpath-3312" to be "Succeeded or Failed"
    Jan 18 17:43:20.707: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305968ms
    Jan 18 17:43:22.714: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 2.015705996s
    Jan 18 17:43:24.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 4.020008041s
    Jan 18 17:43:26.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 6.019248364s
    Jan 18 17:43:28.714: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 8.015450541s
    Jan 18 17:43:30.715: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 10.016033456s
    Jan 18 17:43:32.716: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 12.017229401s
    Jan 18 17:43:34.716: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 14.01785941s
    Jan 18 17:43:36.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 16.018347971s
    Jan 18 17:43:38.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 18.018734959s
    Jan 18 17:43:40.717: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=true. Elapsed: 20.018273657s
    Jan 18 17:43:42.715: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Running", Reason="", readiness=false. Elapsed: 22.016172772s
    Jan 18 17:43:44.718: INFO: Pod "pod-subpath-test-secret-t47r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.019694621s
    STEP: Saw pod success 01/18/23 17:43:44.718
    Jan 18 17:43:44.718: INFO: Pod "pod-subpath-test-secret-t47r" satisfied condition "Succeeded or Failed"
    Jan 18 17:43:44.725: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-secret-t47r container test-container-subpath-secret-t47r: <nil>
    STEP: delete the pod 01/18/23 17:43:44.741
    Jan 18 17:43:44.759: INFO: Waiting for pod pod-subpath-test-secret-t47r to disappear
    Jan 18 17:43:44.765: INFO: Pod pod-subpath-test-secret-t47r no longer exists
    STEP: Deleting pod pod-subpath-test-secret-t47r 01/18/23 17:43:44.765
    Jan 18 17:43:44.765: INFO: Deleting pod "pod-subpath-test-secret-t47r" in namespace "subpath-3312"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 17:43:44.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3312" for this suite. 01/18/23 17:43:44.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:44.792
Jan 18 17:43:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename endpointslice 01/18/23 17:43:44.793
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:44.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:44.82
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 18 17:43:44.842: INFO: Endpoints addresses: [195.154.97.20] , ports: [6443]
Jan 18 17:43:44.842: INFO: EndpointSlices addresses: [195.154.97.20] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 18 17:43:44.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-9294" for this suite. 01/18/23 17:43:44.849
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":174,"skipped":3290,"failed":0}
------------------------------
â€¢ [0.069 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:44.792
    Jan 18 17:43:44.792: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename endpointslice 01/18/23 17:43:44.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:44.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:44.82
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 18 17:43:44.842: INFO: Endpoints addresses: [195.154.97.20] , ports: [6443]
    Jan 18 17:43:44.842: INFO: EndpointSlices addresses: [195.154.97.20] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 18 17:43:44.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-9294" for this suite. 01/18/23 17:43:44.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:44.862
Jan 18 17:43:44.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 17:43:44.863
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:44.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:44.893
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/18/23 17:43:44.897
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:44.905
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:44.906
STEP: creating a pod to probe DNS 01/18/23 17:43:44.906
STEP: submitting the pod to kubernetes 01/18/23 17:43:44.906
Jan 18 17:43:44.925: INFO: Waiting up to 15m0s for pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1" in namespace "dns-2584" to be "running"
Jan 18 17:43:44.932: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.626043ms
Jan 18 17:43:46.941: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016657256s
Jan 18 17:43:46.942: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1" satisfied condition "running"
STEP: retrieving the pod 01/18/23 17:43:46.942
STEP: looking for the results for each expected name from probers 01/18/23 17:43:46.949
Jan 18 17:43:46.973: INFO: DNS probes using dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1 succeeded

STEP: deleting the pod 01/18/23 17:43:46.973
STEP: changing the externalName to bar.example.com 01/18/23 17:43:46.995
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:47.011
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:47.011
STEP: creating a second pod to probe DNS 01/18/23 17:43:47.011
STEP: submitting the pod to kubernetes 01/18/23 17:43:47.011
Jan 18 17:43:47.021: INFO: Waiting up to 15m0s for pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7" in namespace "dns-2584" to be "running"
Jan 18 17:43:47.027: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099162ms
Jan 18 17:43:49.037: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.015868478s
Jan 18 17:43:49.037: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7" satisfied condition "running"
STEP: retrieving the pod 01/18/23 17:43:49.037
STEP: looking for the results for each expected name from probers 01/18/23 17:43:49.045
Jan 18 17:43:49.060: INFO: File wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local from pod  dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 contains '' instead of 'bar.example.com.'
Jan 18 17:43:49.069: INFO: File jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local from pod  dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 18 17:43:49.069: INFO: Lookups using dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 failed for: [wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local]

Jan 18 17:43:54.094: INFO: DNS probes using dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 succeeded

STEP: deleting the pod 01/18/23 17:43:54.094
STEP: changing the service to type=ClusterIP 01/18/23 17:43:54.115
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:54.14
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
 01/18/23 17:43:54.141
STEP: creating a third pod to probe DNS 01/18/23 17:43:54.141
STEP: submitting the pod to kubernetes 01/18/23 17:43:54.146
Jan 18 17:43:54.155: INFO: Waiting up to 15m0s for pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14" in namespace "dns-2584" to be "running"
Jan 18 17:43:54.166: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.296783ms
Jan 18 17:43:56.175: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14": Phase="Running", Reason="", readiness=true. Elapsed: 2.01952153s
Jan 18 17:43:56.175: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14" satisfied condition "running"
STEP: retrieving the pod 01/18/23 17:43:56.175
STEP: looking for the results for each expected name from probers 01/18/23 17:43:56.182
Jan 18 17:43:56.204: INFO: DNS probes using dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14 succeeded

STEP: deleting the pod 01/18/23 17:43:56.204
STEP: deleting the test externalName service 01/18/23 17:43:56.23
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 17:43:56.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2584" for this suite. 01/18/23 17:43:56.262
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":175,"skipped":3297,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.411 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:44.862
    Jan 18 17:43:44.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 17:43:44.863
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:44.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:44.893
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/18/23 17:43:44.897
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:44.905
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:44.906
    STEP: creating a pod to probe DNS 01/18/23 17:43:44.906
    STEP: submitting the pod to kubernetes 01/18/23 17:43:44.906
    Jan 18 17:43:44.925: INFO: Waiting up to 15m0s for pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1" in namespace "dns-2584" to be "running"
    Jan 18 17:43:44.932: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.626043ms
    Jan 18 17:43:46.941: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1": Phase="Running", Reason="", readiness=true. Elapsed: 2.016657256s
    Jan 18 17:43:46.942: INFO: Pod "dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 17:43:46.942
    STEP: looking for the results for each expected name from probers 01/18/23 17:43:46.949
    Jan 18 17:43:46.973: INFO: DNS probes using dns-test-ac33c9d4-2c95-45f7-828c-9b55365b66a1 succeeded

    STEP: deleting the pod 01/18/23 17:43:46.973
    STEP: changing the externalName to bar.example.com 01/18/23 17:43:46.995
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:47.011
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:47.011
    STEP: creating a second pod to probe DNS 01/18/23 17:43:47.011
    STEP: submitting the pod to kubernetes 01/18/23 17:43:47.011
    Jan 18 17:43:47.021: INFO: Waiting up to 15m0s for pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7" in namespace "dns-2584" to be "running"
    Jan 18 17:43:47.027: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099162ms
    Jan 18 17:43:49.037: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7": Phase="Running", Reason="", readiness=true. Elapsed: 2.015868478s
    Jan 18 17:43:49.037: INFO: Pod "dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 17:43:49.037
    STEP: looking for the results for each expected name from probers 01/18/23 17:43:49.045
    Jan 18 17:43:49.060: INFO: File wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local from pod  dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 contains '' instead of 'bar.example.com.'
    Jan 18 17:43:49.069: INFO: File jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local from pod  dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 18 17:43:49.069: INFO: Lookups using dns-2584/dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 failed for: [wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local]

    Jan 18 17:43:54.094: INFO: DNS probes using dns-test-c5f73a7c-e0ad-404c-8e73-665c54b504d7 succeeded

    STEP: deleting the pod 01/18/23 17:43:54.094
    STEP: changing the service to type=ClusterIP 01/18/23 17:43:54.115
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:54.14
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2584.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2584.svc.cluster.local; sleep 1; done
     01/18/23 17:43:54.141
    STEP: creating a third pod to probe DNS 01/18/23 17:43:54.141
    STEP: submitting the pod to kubernetes 01/18/23 17:43:54.146
    Jan 18 17:43:54.155: INFO: Waiting up to 15m0s for pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14" in namespace "dns-2584" to be "running"
    Jan 18 17:43:54.166: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.296783ms
    Jan 18 17:43:56.175: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14": Phase="Running", Reason="", readiness=true. Elapsed: 2.01952153s
    Jan 18 17:43:56.175: INFO: Pod "dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 17:43:56.175
    STEP: looking for the results for each expected name from probers 01/18/23 17:43:56.182
    Jan 18 17:43:56.204: INFO: DNS probes using dns-test-1e594578-8ec9-4308-9c1d-953bbf926b14 succeeded

    STEP: deleting the pod 01/18/23 17:43:56.204
    STEP: deleting the test externalName service 01/18/23 17:43:56.23
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 17:43:56.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2584" for this suite. 01/18/23 17:43:56.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:43:56.279
Jan 18 17:43:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:43:56.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:56.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:56.314
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:43:56.34
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:43:56.858
STEP: Deploying the webhook pod 01/18/23 17:43:56.873
STEP: Wait for the deployment to be ready 01/18/23 17:43:56.893
Jan 18 17:43:56.905: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 17:43:58.926
STEP: Verifying the service has paired with the endpoint 01/18/23 17:43:58.946
Jan 18 17:43:59.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 17:43:59.956
STEP: create a pod that should be denied by the webhook 01/18/23 17:43:59.989
STEP: create a pod that causes the webhook to hang 01/18/23 17:44:00.012
STEP: create a configmap that should be denied by the webhook 01/18/23 17:44:10.025
STEP: create a configmap that should be admitted by the webhook 01/18/23 17:44:10.046
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 17:44:10.069
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 17:44:10.091
STEP: create a namespace that bypass the webhook 01/18/23 17:44:10.107
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 17:44:10.123
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:44:10.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-30" for this suite. 01/18/23 17:44:10.188
STEP: Destroying namespace "webhook-30-markers" for this suite. 01/18/23 17:44:10.2
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":176,"skipped":3336,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.997 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:43:56.279
    Jan 18 17:43:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:43:56.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:43:56.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:43:56.314
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:43:56.34
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:43:56.858
    STEP: Deploying the webhook pod 01/18/23 17:43:56.873
    STEP: Wait for the deployment to be ready 01/18/23 17:43:56.893
    Jan 18 17:43:56.905: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 17:43:58.926
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:43:58.946
    Jan 18 17:43:59.947: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/18/23 17:43:59.956
    STEP: create a pod that should be denied by the webhook 01/18/23 17:43:59.989
    STEP: create a pod that causes the webhook to hang 01/18/23 17:44:00.012
    STEP: create a configmap that should be denied by the webhook 01/18/23 17:44:10.025
    STEP: create a configmap that should be admitted by the webhook 01/18/23 17:44:10.046
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 17:44:10.069
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/18/23 17:44:10.091
    STEP: create a namespace that bypass the webhook 01/18/23 17:44:10.107
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/18/23 17:44:10.123
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:44:10.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-30" for this suite. 01/18/23 17:44:10.188
    STEP: Destroying namespace "webhook-30-markers" for this suite. 01/18/23 17:44:10.2
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:10.279
Jan 18 17:44:10.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 17:44:10.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:10.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:10.304
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/18/23 17:44:10.309
STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 17:44:10.317
STEP: patching /status 01/18/23 17:44:12.326
STEP: updating /status 01/18/23 17:44:12.341
STEP: get /status 01/18/23 17:44:12.382
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 17:44:12.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8609" for this suite. 01/18/23 17:44:12.397
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":177,"skipped":3350,"failed":0}
------------------------------
â€¢ [2.286 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:10.279
    Jan 18 17:44:10.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 17:44:10.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:10.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:10.304
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/18/23 17:44:10.309
    STEP: Ensure pods equal to paralellism count is attached to the job 01/18/23 17:44:10.317
    STEP: patching /status 01/18/23 17:44:12.326
    STEP: updating /status 01/18/23 17:44:12.341
    STEP: get /status 01/18/23 17:44:12.382
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 17:44:12.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8609" for this suite. 01/18/23 17:44:12.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:12.57
Jan 18 17:44:12.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:44:12.571
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:12.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:12.771
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-668bc6b1-c5de-4784-beb0-44a31880d129 01/18/23 17:44:12.776
STEP: Creating a pod to test consume secrets 01/18/23 17:44:12.784
Jan 18 17:44:12.799: INFO: Waiting up to 5m0s for pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9" in namespace "secrets-4215" to be "Succeeded or Failed"
Jan 18 17:44:12.805: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73456ms
Jan 18 17:44:14.813: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013929377s
Jan 18 17:44:16.811: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012351842s
STEP: Saw pod success 01/18/23 17:44:16.811
Jan 18 17:44:16.811: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9" satisfied condition "Succeeded or Failed"
Jan 18 17:44:16.818: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:44:16.832
Jan 18 17:44:16.853: INFO: Waiting for pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 to disappear
Jan 18 17:44:16.860: INFO: Pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:44:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4215" for this suite. 01/18/23 17:44:16.867
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":178,"skipped":3413,"failed":0}
------------------------------
â€¢ [4.310 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:12.57
    Jan 18 17:44:12.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:44:12.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:12.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:12.771
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-668bc6b1-c5de-4784-beb0-44a31880d129 01/18/23 17:44:12.776
    STEP: Creating a pod to test consume secrets 01/18/23 17:44:12.784
    Jan 18 17:44:12.799: INFO: Waiting up to 5m0s for pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9" in namespace "secrets-4215" to be "Succeeded or Failed"
    Jan 18 17:44:12.805: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73456ms
    Jan 18 17:44:14.813: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013929377s
    Jan 18 17:44:16.811: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012351842s
    STEP: Saw pod success 01/18/23 17:44:16.811
    Jan 18 17:44:16.811: INFO: Pod "pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:16.818: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:44:16.832
    Jan 18 17:44:16.853: INFO: Waiting for pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 to disappear
    Jan 18 17:44:16.860: INFO: Pod pod-secrets-048268a3-a69d-4abf-81f0-701bfa5504b9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:44:16.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4215" for this suite. 01/18/23 17:44:16.867
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:16.882
Jan 18 17:44:16.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:44:16.883
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:16.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:16.912
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 17:44:16.917
Jan 18 17:44:16.933: INFO: Waiting up to 5m0s for pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4" in namespace "emptydir-8659" to be "Succeeded or Failed"
Jan 18 17:44:16.942: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.291215ms
Jan 18 17:44:18.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01848159s
Jan 18 17:44:20.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017789385s
STEP: Saw pod success 01/18/23 17:44:20.951
Jan 18 17:44:20.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4" satisfied condition "Succeeded or Failed"
Jan 18 17:44:20.959: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 container test-container: <nil>
STEP: delete the pod 01/18/23 17:44:20.974
Jan 18 17:44:20.994: INFO: Waiting for pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 to disappear
Jan 18 17:44:21.003: INFO: Pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:44:21.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8659" for this suite. 01/18/23 17:44:21.012
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":179,"skipped":3429,"failed":0}
------------------------------
â€¢ [4.141 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:16.882
    Jan 18 17:44:16.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:44:16.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:16.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:16.912
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 17:44:16.917
    Jan 18 17:44:16.933: INFO: Waiting up to 5m0s for pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4" in namespace "emptydir-8659" to be "Succeeded or Failed"
    Jan 18 17:44:16.942: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.291215ms
    Jan 18 17:44:18.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01848159s
    Jan 18 17:44:20.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017789385s
    STEP: Saw pod success 01/18/23 17:44:20.951
    Jan 18 17:44:20.951: INFO: Pod "pod-3b18ee59-f692-45c5-97a8-af1466832ae4" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:20.959: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:44:20.974
    Jan 18 17:44:20.994: INFO: Waiting for pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 to disappear
    Jan 18 17:44:21.003: INFO: Pod pod-3b18ee59-f692-45c5-97a8-af1466832ae4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:44:21.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8659" for this suite. 01/18/23 17:44:21.012
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:21.023
Jan 18 17:44:21.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:44:21.024
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:21.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:21.049
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:44:21.073
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:44:21.242
STEP: Deploying the webhook pod 01/18/23 17:44:21.252
STEP: Wait for the deployment to be ready 01/18/23 17:44:21.27
Jan 18 17:44:21.281: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:44:23.3
STEP: Verifying the service has paired with the endpoint 01/18/23 17:44:23.316
Jan 18 17:44:24.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 17:44:24.327
STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 17:44:24.36
Jan 18 17:44:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:44:24.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2843" for this suite. 01/18/23 17:44:24.402
STEP: Destroying namespace "webhook-2843-markers" for this suite. 01/18/23 17:44:24.415
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":180,"skipped":3430,"failed":0}
------------------------------
â€¢ [3.474 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:21.023
    Jan 18 17:44:21.023: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:44:21.024
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:21.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:21.049
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:44:21.073
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:44:21.242
    STEP: Deploying the webhook pod 01/18/23 17:44:21.252
    STEP: Wait for the deployment to be ready 01/18/23 17:44:21.27
    Jan 18 17:44:21.281: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:44:23.3
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:44:23.316
    Jan 18 17:44:24.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/18/23 17:44:24.327
    STEP: Creating a custom resource definition that should be denied by the webhook 01/18/23 17:44:24.36
    Jan 18 17:44:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:44:24.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2843" for this suite. 01/18/23 17:44:24.402
    STEP: Destroying namespace "webhook-2843-markers" for this suite. 01/18/23 17:44:24.415
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:24.499
Jan 18 17:44:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:44:24.5
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:24.522
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:24.527
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/18/23 17:44:24.532
STEP: Getting a ResourceQuota 01/18/23 17:44:24.54
STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 17:44:24.547
STEP: Patching the ResourceQuota 01/18/23 17:44:24.551
STEP: Deleting a Collection of ResourceQuotas 01/18/23 17:44:24.565
STEP: Verifying the deleted ResourceQuota 01/18/23 17:44:24.582
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:44:24.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-194" for this suite. 01/18/23 17:44:24.593
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":181,"skipped":3440,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:24.499
    Jan 18 17:44:24.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:44:24.5
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:24.522
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:24.527
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/18/23 17:44:24.532
    STEP: Getting a ResourceQuota 01/18/23 17:44:24.54
    STEP: Listing all ResourceQuotas with LabelSelector 01/18/23 17:44:24.547
    STEP: Patching the ResourceQuota 01/18/23 17:44:24.551
    STEP: Deleting a Collection of ResourceQuotas 01/18/23 17:44:24.565
    STEP: Verifying the deleted ResourceQuota 01/18/23 17:44:24.582
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:44:24.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-194" for this suite. 01/18/23 17:44:24.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:24.606
Jan 18 17:44:24.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:44:24.607
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:24.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:24.634
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/18/23 17:44:24.639
Jan 18 17:44:24.658: INFO: Waiting up to 5m0s for pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c" in namespace "projected-9251" to be "running and ready"
Jan 18 17:44:24.666: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.286328ms
Jan 18 17:44:24.666: INFO: The phase of Pod labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:44:26.674: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c": Phase="Running", Reason="", readiness=true. Elapsed: 2.016315672s
Jan 18 17:44:26.674: INFO: The phase of Pod labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c is Running (Ready = true)
Jan 18 17:44:26.674: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c" satisfied condition "running and ready"
Jan 18 17:44:27.225: INFO: Successfully updated pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:44:31.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9251" for this suite. 01/18/23 17:44:31.283
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":182,"skipped":3450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.691 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:24.606
    Jan 18 17:44:24.606: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:44:24.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:24.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:24.634
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/18/23 17:44:24.639
    Jan 18 17:44:24.658: INFO: Waiting up to 5m0s for pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c" in namespace "projected-9251" to be "running and ready"
    Jan 18 17:44:24.666: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.286328ms
    Jan 18 17:44:24.666: INFO: The phase of Pod labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:44:26.674: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c": Phase="Running", Reason="", readiness=true. Elapsed: 2.016315672s
    Jan 18 17:44:26.674: INFO: The phase of Pod labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c is Running (Ready = true)
    Jan 18 17:44:26.674: INFO: Pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c" satisfied condition "running and ready"
    Jan 18 17:44:27.225: INFO: Successfully updated pod "labelsupdatedb210d77-7937-4ede-be8e-fac02ce6307c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:44:31.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9251" for this suite. 01/18/23 17:44:31.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:31.298
Jan 18 17:44:31.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:44:31.299
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:31.326
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:31.331
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-f162311a-8a8d-404c-abc7-318a8e0609b0 01/18/23 17:44:31.335
STEP: Creating a pod to test consume secrets 01/18/23 17:44:31.343
Jan 18 17:44:31.359: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8" in namespace "projected-8395" to be "Succeeded or Failed"
Jan 18 17:44:31.366: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649826ms
Jan 18 17:44:33.375: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015512103s
Jan 18 17:44:35.374: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014630491s
STEP: Saw pod success 01/18/23 17:44:35.374
Jan 18 17:44:35.374: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8" satisfied condition "Succeeded or Failed"
Jan 18 17:44:35.380: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:44:35.397
Jan 18 17:44:35.421: INFO: Waiting for pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 to disappear
Jan 18 17:44:35.426: INFO: Pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 17:44:35.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8395" for this suite. 01/18/23 17:44:35.435
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":183,"skipped":3473,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:31.298
    Jan 18 17:44:31.298: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:44:31.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:31.326
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:31.331
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-f162311a-8a8d-404c-abc7-318a8e0609b0 01/18/23 17:44:31.335
    STEP: Creating a pod to test consume secrets 01/18/23 17:44:31.343
    Jan 18 17:44:31.359: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8" in namespace "projected-8395" to be "Succeeded or Failed"
    Jan 18 17:44:31.366: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.649826ms
    Jan 18 17:44:33.375: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015512103s
    Jan 18 17:44:35.374: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014630491s
    STEP: Saw pod success 01/18/23 17:44:35.374
    Jan 18 17:44:35.374: INFO: Pod "pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:35.380: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:44:35.397
    Jan 18 17:44:35.421: INFO: Waiting for pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 to disappear
    Jan 18 17:44:35.426: INFO: Pod pod-projected-secrets-8093cf42-ddf3-489c-9463-0b52330334d8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 17:44:35.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8395" for this suite. 01/18/23 17:44:35.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:35.45
Jan 18 17:44:35.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename containers 01/18/23 17:44:35.451
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:35.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:35.481
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/18/23 17:44:35.487
Jan 18 17:44:35.501: INFO: Waiting up to 5m0s for pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903" in namespace "containers-8160" to be "Succeeded or Failed"
Jan 18 17:44:35.507: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480558ms
Jan 18 17:44:37.514: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012502014s
Jan 18 17:44:39.515: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013891709s
STEP: Saw pod success 01/18/23 17:44:39.515
Jan 18 17:44:39.515: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903" satisfied condition "Succeeded or Failed"
Jan 18 17:44:39.522: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:44:39.538
Jan 18 17:44:39.562: INFO: Waiting for pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 to disappear
Jan 18 17:44:39.568: INFO: Pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 18 17:44:39.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8160" for this suite. 01/18/23 17:44:39.579
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":184,"skipped":3546,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:35.45
    Jan 18 17:44:35.450: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename containers 01/18/23 17:44:35.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:35.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:35.481
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/18/23 17:44:35.487
    Jan 18 17:44:35.501: INFO: Waiting up to 5m0s for pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903" in namespace "containers-8160" to be "Succeeded or Failed"
    Jan 18 17:44:35.507: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Pending", Reason="", readiness=false. Elapsed: 5.480558ms
    Jan 18 17:44:37.514: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012502014s
    Jan 18 17:44:39.515: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013891709s
    STEP: Saw pod success 01/18/23 17:44:39.515
    Jan 18 17:44:39.515: INFO: Pod "client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:39.522: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:44:39.538
    Jan 18 17:44:39.562: INFO: Waiting for pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 to disappear
    Jan 18 17:44:39.568: INFO: Pod client-containers-66e21e3b-397f-4ecd-a05f-722d6e294903 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 18 17:44:39.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8160" for this suite. 01/18/23 17:44:39.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:39.594
Jan 18 17:44:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context 01/18/23 17:44:39.595
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:39.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:39.622
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 17:44:39.627
Jan 18 17:44:39.641: INFO: Waiting up to 5m0s for pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d" in namespace "security-context-5787" to be "Succeeded or Failed"
Jan 18 17:44:39.650: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.642657ms
Jan 18 17:44:41.659: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018204869s
Jan 18 17:44:43.660: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018919391s
STEP: Saw pod success 01/18/23 17:44:43.66
Jan 18 17:44:43.660: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d" satisfied condition "Succeeded or Failed"
Jan 18 17:44:43.667: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d container test-container: <nil>
STEP: delete the pod 01/18/23 17:44:43.683
Jan 18 17:44:43.709: INFO: Waiting for pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d to disappear
Jan 18 17:44:43.715: INFO: Pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 17:44:43.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-5787" for this suite. 01/18/23 17:44:43.723
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":185,"skipped":3560,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:39.594
    Jan 18 17:44:39.594: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context 01/18/23 17:44:39.595
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:39.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:39.622
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/18/23 17:44:39.627
    Jan 18 17:44:39.641: INFO: Waiting up to 5m0s for pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d" in namespace "security-context-5787" to be "Succeeded or Failed"
    Jan 18 17:44:39.650: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.642657ms
    Jan 18 17:44:41.659: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018204869s
    Jan 18 17:44:43.660: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018919391s
    STEP: Saw pod success 01/18/23 17:44:43.66
    Jan 18 17:44:43.660: INFO: Pod "security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:43.667: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d container test-container: <nil>
    STEP: delete the pod 01/18/23 17:44:43.683
    Jan 18 17:44:43.709: INFO: Waiting for pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d to disappear
    Jan 18 17:44:43.715: INFO: Pod security-context-4a5fe61a-0009-4bb3-9ad6-4ff888fb7a5d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 17:44:43.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-5787" for this suite. 01/18/23 17:44:43.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:43.741
Jan 18 17:44:43.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:44:43.742
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:43.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:43.771
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/18/23 17:44:43.776
Jan 18 17:44:43.791: INFO: Waiting up to 5m0s for pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960" in namespace "var-expansion-7070" to be "Succeeded or Failed"
Jan 18 17:44:43.800: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Pending", Reason="", readiness=false. Elapsed: 9.182394ms
Jan 18 17:44:45.806: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015304813s
Jan 18 17:44:47.811: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019718105s
STEP: Saw pod success 01/18/23 17:44:47.811
Jan 18 17:44:47.811: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960" satisfied condition "Succeeded or Failed"
Jan 18 17:44:47.816: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:44:47.831
Jan 18 17:44:47.849: INFO: Waiting for pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 to disappear
Jan 18 17:44:47.856: INFO: Pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:44:47.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7070" for this suite. 01/18/23 17:44:47.862
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":186,"skipped":3589,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:43.741
    Jan 18 17:44:43.741: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:44:43.742
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:43.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:43.771
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/18/23 17:44:43.776
    Jan 18 17:44:43.791: INFO: Waiting up to 5m0s for pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960" in namespace "var-expansion-7070" to be "Succeeded or Failed"
    Jan 18 17:44:43.800: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Pending", Reason="", readiness=false. Elapsed: 9.182394ms
    Jan 18 17:44:45.806: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015304813s
    Jan 18 17:44:47.811: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019718105s
    STEP: Saw pod success 01/18/23 17:44:47.811
    Jan 18 17:44:47.811: INFO: Pod "var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960" satisfied condition "Succeeded or Failed"
    Jan 18 17:44:47.816: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:44:47.831
    Jan 18 17:44:47.849: INFO: Waiting for pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 to disappear
    Jan 18 17:44:47.856: INFO: Pod var-expansion-c2990d90-93e9-439f-b7c7-651087d4f960 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:44:47.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7070" for this suite. 01/18/23 17:44:47.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:47.878
Jan 18 17:44:47.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 17:44:47.879
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:47.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:47.904
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-4dd0863e-81e3-410e-b2cb-b92d57da8892 01/18/23 17:44:47.909
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 17:44:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8086" for this suite. 01/18/23 17:44:47.918
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":187,"skipped":3616,"failed":0}
------------------------------
â€¢ [0.053 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:47.878
    Jan 18 17:44:47.878: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 17:44:47.879
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:47.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:47.904
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-4dd0863e-81e3-410e-b2cb-b92d57da8892 01/18/23 17:44:47.909
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 17:44:47.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8086" for this suite. 01/18/23 17:44:47.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:47.932
Jan 18 17:44:47.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:44:47.933
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:47.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:47.966
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:44:47.977
Jan 18 17:44:47.995: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9128" to be "running and ready"
Jan 18 17:44:47.999: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37883ms
Jan 18 17:44:47.999: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:44:50.007: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012625519s
Jan 18 17:44:50.007: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 17:44:50.007: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/18/23 17:44:50.014
Jan 18 17:44:50.024: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9128" to be "running and ready"
Jan 18 17:44:50.031: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782277ms
Jan 18 17:44:50.031: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:44:52.040: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015599318s
Jan 18 17:44:52.040: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 18 17:44:52.040: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 17:44:52.047
Jan 18 17:44:52.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 17:44:52.072: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 17:44:54.073: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 17:44:54.082: INFO: Pod pod-with-prestop-http-hook still exists
Jan 18 17:44:56.075: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 18 17:44:56.083: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/18/23 17:44:56.083
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 17:44:56.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9128" for this suite. 01/18/23 17:44:56.127
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":188,"skipped":3625,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.206 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:47.932
    Jan 18 17:44:47.933: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:44:47.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:47.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:47.966
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:44:47.977
    Jan 18 17:44:47.995: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9128" to be "running and ready"
    Jan 18 17:44:47.999: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.37883ms
    Jan 18 17:44:47.999: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:44:50.007: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.012625519s
    Jan 18 17:44:50.007: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 17:44:50.007: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/18/23 17:44:50.014
    Jan 18 17:44:50.024: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9128" to be "running and ready"
    Jan 18 17:44:50.031: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.782277ms
    Jan 18 17:44:50.031: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:44:52.040: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.015599318s
    Jan 18 17:44:52.040: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 18 17:44:52.040: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 17:44:52.047
    Jan 18 17:44:52.064: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 17:44:52.072: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 17:44:54.073: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 17:44:54.082: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 18 17:44:56.075: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 18 17:44:56.083: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/18/23 17:44:56.083
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 17:44:56.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-9128" for this suite. 01/18/23 17:44:56.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:44:56.14
Jan 18 17:44:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replication-controller 01/18/23 17:44:56.142
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:56.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:56.172
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824 01/18/23 17:44:56.176
Jan 18 17:44:56.192: INFO: Pod name my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Found 0 pods out of 1
Jan 18 17:45:01.201: INFO: Pod name my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Found 1 pods out of 1
Jan 18 17:45:01.201: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824" are running
Jan 18 17:45:01.201: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" in namespace "replication-controller-61" to be "running"
Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv": Phase="Running", Reason="", readiness=true. Elapsed: 5.86279ms
Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" satisfied condition "running"
Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:56 +0000 UTC Reason: Message:}])
Jan 18 17:45:01.207: INFO: Trying to dial the pod
Jan 18 17:45:06.242: INFO: Controller my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Got expected result from replica 1 [my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv]: "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 17:45:06.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-61" for this suite. 01/18/23 17:45:06.254
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":189,"skipped":3647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.126 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:44:56.14
    Jan 18 17:44:56.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 17:44:56.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:44:56.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:44:56.172
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824 01/18/23 17:44:56.176
    Jan 18 17:44:56.192: INFO: Pod name my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Found 0 pods out of 1
    Jan 18 17:45:01.201: INFO: Pod name my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Found 1 pods out of 1
    Jan 18 17:45:01.201: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824" are running
    Jan 18 17:45:01.201: INFO: Waiting up to 5m0s for pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" in namespace "replication-controller-61" to be "running"
    Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv": Phase="Running", Reason="", readiness=true. Elapsed: 5.86279ms
    Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" satisfied condition "running"
    Jan 18 17:45:01.207: INFO: Pod "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:57 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:57 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-18 17:44:56 +0000 UTC Reason: Message:}])
    Jan 18 17:45:01.207: INFO: Trying to dial the pod
    Jan 18 17:45:06.242: INFO: Controller my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824: Got expected result from replica 1 [my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv]: "my-hostname-basic-cb21ceea-fa5d-4110-888f-50f787a02824-hf4bv", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 17:45:06.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-61" for this suite. 01/18/23 17:45:06.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:45:06.27
Jan 18 17:45:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:45:06.272
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:06.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:06.299
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/18/23 17:45:06.304
Jan 18 17:45:06.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-9118 api-versions'
Jan 18 17:45:06.386: INFO: stderr: ""
Jan 18 17:45:06.386: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:45:06.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9118" for this suite. 01/18/23 17:45:06.393
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":190,"skipped":3686,"failed":0}
------------------------------
â€¢ [0.137 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:45:06.27
    Jan 18 17:45:06.270: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:45:06.272
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:06.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:06.299
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/18/23 17:45:06.304
    Jan 18 17:45:06.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-9118 api-versions'
    Jan 18 17:45:06.386: INFO: stderr: ""
    Jan 18 17:45:06.386: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.apiserver.k8s.io/v1alpha1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1alpha1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:45:06.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9118" for this suite. 01/18/23 17:45:06.393
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:45:06.408
Jan 18 17:45:06.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 17:45:06.41
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:06.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:06.436
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 18 17:45:06.472: INFO: Waiting up to 5m0s for pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e" in namespace "emptydir-wrapper-747" to be "running and ready"
Jan 18 17:45:06.477: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640639ms
Jan 18 17:45:06.477: INFO: The phase of Pod pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:45:08.491: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.018898042s
Jan 18 17:45:08.491: INFO: The phase of Pod pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e is Running (Ready = true)
Jan 18 17:45:08.491: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/18/23 17:45:08.498
STEP: Cleaning up the configmap 01/18/23 17:45:08.508
STEP: Cleaning up the pod 01/18/23 17:45:08.518
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 18 17:45:08.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-747" for this suite. 01/18/23 17:45:08.543
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":191,"skipped":3698,"failed":0}
------------------------------
â€¢ [2.147 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:45:06.408
    Jan 18 17:45:06.408: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir-wrapper 01/18/23 17:45:06.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:06.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:06.436
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 18 17:45:06.472: INFO: Waiting up to 5m0s for pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e" in namespace "emptydir-wrapper-747" to be "running and ready"
    Jan 18 17:45:06.477: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640639ms
    Jan 18 17:45:06.477: INFO: The phase of Pod pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:45:08.491: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e": Phase="Running", Reason="", readiness=true. Elapsed: 2.018898042s
    Jan 18 17:45:08.491: INFO: The phase of Pod pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e is Running (Ready = true)
    Jan 18 17:45:08.491: INFO: Pod "pod-secrets-c17cd9c8-a26c-4dc3-a32d-83b3b5ebfc3e" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/18/23 17:45:08.498
    STEP: Cleaning up the configmap 01/18/23 17:45:08.508
    STEP: Cleaning up the pod 01/18/23 17:45:08.518
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:45:08.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-747" for this suite. 01/18/23 17:45:08.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:45:08.556
Jan 18 17:45:08.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename proxy 01/18/23 17:45:08.557
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:08.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:08.59
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 18 17:45:08.595: INFO: Creating pod...
Jan 18 17:45:08.610: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1331" to be "running"
Jan 18 17:45:08.617: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.637136ms
Jan 18 17:45:10.625: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014856957s
Jan 18 17:45:10.625: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 17:45:10.625: INFO: Creating service...
Jan 18 17:45:10.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=DELETE
Jan 18 17:45:10.659: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 17:45:10.659: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=OPTIONS
Jan 18 17:45:10.668: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 17:45:10.668: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=PATCH
Jan 18 17:45:10.678: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 17:45:10.678: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=POST
Jan 18 17:45:10.687: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 17:45:10.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=PUT
Jan 18 17:45:10.696: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 17:45:10.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 18 17:45:10.708: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 17:45:10.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 18 17:45:10.721: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 17:45:10.721: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 18 17:45:10.738: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 17:45:10.738: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=POST
Jan 18 17:45:10.750: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 17:45:10.750: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=PUT
Jan 18 17:45:10.762: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 17:45:10.762: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=GET
Jan 18 17:45:10.769: INFO: http.Client request:GET StatusCode:301
Jan 18 17:45:10.769: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=GET
Jan 18 17:45:10.780: INFO: http.Client request:GET StatusCode:301
Jan 18 17:45:10.780: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=HEAD
Jan 18 17:45:10.786: INFO: http.Client request:HEAD StatusCode:301
Jan 18 17:45:10.786: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 18 17:45:10.794: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 17:45:10.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1331" for this suite. 01/18/23 17:45:10.802
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":192,"skipped":3706,"failed":0}
------------------------------
â€¢ [2.258 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:45:08.556
    Jan 18 17:45:08.556: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename proxy 01/18/23 17:45:08.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:08.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:08.59
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 18 17:45:08.595: INFO: Creating pod...
    Jan 18 17:45:08.610: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1331" to be "running"
    Jan 18 17:45:08.617: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.637136ms
    Jan 18 17:45:10.625: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.014856957s
    Jan 18 17:45:10.625: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 17:45:10.625: INFO: Creating service...
    Jan 18 17:45:10.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=DELETE
    Jan 18 17:45:10.659: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 17:45:10.659: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=OPTIONS
    Jan 18 17:45:10.668: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 17:45:10.668: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=PATCH
    Jan 18 17:45:10.678: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 17:45:10.678: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=POST
    Jan 18 17:45:10.687: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 17:45:10.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=PUT
    Jan 18 17:45:10.696: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 17:45:10.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 18 17:45:10.708: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 17:45:10.708: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 18 17:45:10.721: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 17:45:10.721: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 18 17:45:10.738: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 17:45:10.738: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=POST
    Jan 18 17:45:10.750: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 17:45:10.750: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 18 17:45:10.762: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 17:45:10.762: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=GET
    Jan 18 17:45:10.769: INFO: http.Client request:GET StatusCode:301
    Jan 18 17:45:10.769: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=GET
    Jan 18 17:45:10.780: INFO: http.Client request:GET StatusCode:301
    Jan 18 17:45:10.780: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/pods/agnhost/proxy?method=HEAD
    Jan 18 17:45:10.786: INFO: http.Client request:HEAD StatusCode:301
    Jan 18 17:45:10.786: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-1331/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 18 17:45:10.794: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 17:45:10.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1331" for this suite. 01/18/23 17:45:10.802
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:45:10.815
Jan 18 17:45:10.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename cronjob 01/18/23 17:45:10.816
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:10.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:10.851
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/18/23 17:45:10.855
STEP: Ensuring a job is scheduled 01/18/23 17:45:10.865
STEP: Ensuring exactly one is scheduled 01/18/23 17:46:00.872
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 17:46:00.878
STEP: Ensuring the job is replaced with a new one 01/18/23 17:46:00.886
STEP: Removing cronjob 01/18/23 17:47:00.895
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 17:47:00.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5435" for this suite. 01/18/23 17:47:00.919
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":193,"skipped":3714,"failed":0}
------------------------------
â€¢ [SLOW TEST] [110.119 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:45:10.815
    Jan 18 17:45:10.815: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename cronjob 01/18/23 17:45:10.816
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:45:10.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:45:10.851
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/18/23 17:45:10.855
    STEP: Ensuring a job is scheduled 01/18/23 17:45:10.865
    STEP: Ensuring exactly one is scheduled 01/18/23 17:46:00.872
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 17:46:00.878
    STEP: Ensuring the job is replaced with a new one 01/18/23 17:46:00.886
    STEP: Removing cronjob 01/18/23 17:47:00.895
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 17:47:00.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5435" for this suite. 01/18/23 17:47:00.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:00.939
Jan 18 17:47:00.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:47:00.94
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:00.963
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:00.968
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/18/23 17:47:00.972
Jan 18 17:47:00.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-1477 create -f -'
Jan 18 17:47:02.080: INFO: stderr: ""
Jan 18 17:47:02.080: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 17:47:02.08
Jan 18 17:47:03.089: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:47:03.089: INFO: Found 0 / 1
Jan 18 17:47:04.089: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:47:04.089: INFO: Found 1 / 1
Jan 18 17:47:04.089: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/18/23 17:47:04.089
Jan 18 17:47:04.095: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:47:04.095: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 17:47:04.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-1477 patch pod agnhost-primary-l4b8v -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 18 17:47:04.204: INFO: stderr: ""
Jan 18 17:47:04.204: INFO: stdout: "pod/agnhost-primary-l4b8v patched\n"
STEP: checking annotations 01/18/23 17:47:04.204
Jan 18 17:47:04.211: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 17:47:04.211: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:47:04.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1477" for this suite. 01/18/23 17:47:04.22
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":194,"skipped":3768,"failed":0}
------------------------------
â€¢ [3.293 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:00.939
    Jan 18 17:47:00.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:47:00.94
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:00.963
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:00.968
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/18/23 17:47:00.972
    Jan 18 17:47:00.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-1477 create -f -'
    Jan 18 17:47:02.080: INFO: stderr: ""
    Jan 18 17:47:02.080: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 17:47:02.08
    Jan 18 17:47:03.089: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:47:03.089: INFO: Found 0 / 1
    Jan 18 17:47:04.089: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:47:04.089: INFO: Found 1 / 1
    Jan 18 17:47:04.089: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/18/23 17:47:04.089
    Jan 18 17:47:04.095: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:47:04.095: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 17:47:04.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-1477 patch pod agnhost-primary-l4b8v -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 18 17:47:04.204: INFO: stderr: ""
    Jan 18 17:47:04.204: INFO: stdout: "pod/agnhost-primary-l4b8v patched\n"
    STEP: checking annotations 01/18/23 17:47:04.204
    Jan 18 17:47:04.211: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 17:47:04.211: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:47:04.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1477" for this suite. 01/18/23 17:47:04.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:04.232
Jan 18 17:47:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:47:04.233
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:04.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:04.281
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/18/23 17:47:04.288
Jan 18 17:47:04.289: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-5715 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/18/23 17:47:04.364
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:47:04.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5715" for this suite. 01/18/23 17:47:04.387
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":195,"skipped":3775,"failed":0}
------------------------------
â€¢ [0.167 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:04.232
    Jan 18 17:47:04.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:47:04.233
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:04.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:04.281
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/18/23 17:47:04.288
    Jan 18 17:47:04.289: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-5715 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/18/23 17:47:04.364
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:47:04.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5715" for this suite. 01/18/23 17:47:04.387
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:04.401
Jan 18 17:47:04.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:47:04.402
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:04.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:04.43
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 18 17:47:04.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:47:04.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1922" for this suite. 01/18/23 17:47:05.013
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":196,"skipped":3779,"failed":0}
------------------------------
â€¢ [0.626 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:04.401
    Jan 18 17:47:04.401: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:47:04.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:04.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:04.43
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 18 17:47:04.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:47:04.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1922" for this suite. 01/18/23 17:47:05.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:05.027
Jan 18 17:47:05.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:47:05.028
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:05.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:05.056
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:47:05.084
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:05.37
STEP: Deploying the webhook pod 01/18/23 17:47:05.385
STEP: Wait for the deployment to be ready 01/18/23 17:47:05.408
Jan 18 17:47:05.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:47:07.446
STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:07.471
Jan 18 17:47:08.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/18/23 17:47:08.597
STEP: Creating a configMap that should be mutated 01/18/23 17:47:08.625
STEP: Deleting the collection of validation webhooks 01/18/23 17:47:08.686
STEP: Creating a configMap that should not be mutated 01/18/23 17:47:08.794
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:47:08.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3195" for this suite. 01/18/23 17:47:08.828
STEP: Destroying namespace "webhook-3195-markers" for this suite. 01/18/23 17:47:08.842
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":197,"skipped":3789,"failed":0}
------------------------------
â€¢ [3.895 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:05.027
    Jan 18 17:47:05.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:47:05.028
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:05.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:05.056
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:47:05.084
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:05.37
    STEP: Deploying the webhook pod 01/18/23 17:47:05.385
    STEP: Wait for the deployment to be ready 01/18/23 17:47:05.408
    Jan 18 17:47:05.422: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:47:07.446
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:07.471
    Jan 18 17:47:08.471: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/18/23 17:47:08.597
    STEP: Creating a configMap that should be mutated 01/18/23 17:47:08.625
    STEP: Deleting the collection of validation webhooks 01/18/23 17:47:08.686
    STEP: Creating a configMap that should not be mutated 01/18/23 17:47:08.794
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:47:08.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3195" for this suite. 01/18/23 17:47:08.828
    STEP: Destroying namespace "webhook-3195-markers" for this suite. 01/18/23 17:47:08.842
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:08.922
Jan 18 17:47:08.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 17:47:08.924
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:08.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:08.952
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:47:08.956
Jan 18 17:47:08.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf" in namespace "downward-api-8872" to be "Succeeded or Failed"
Jan 18 17:47:08.979: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972241ms
Jan 18 17:47:10.986: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01337132s
Jan 18 17:47:12.988: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015761754s
STEP: Saw pod success 01/18/23 17:47:12.988
Jan 18 17:47:12.989: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf" satisfied condition "Succeeded or Failed"
Jan 18 17:47:12.996: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf container client-container: <nil>
STEP: delete the pod 01/18/23 17:47:13.025
Jan 18 17:47:13.044: INFO: Waiting for pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf to disappear
Jan 18 17:47:13.052: INFO: Pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 17:47:13.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8872" for this suite. 01/18/23 17:47:13.06
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":198,"skipped":3797,"failed":0}
------------------------------
â€¢ [4.150 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:08.922
    Jan 18 17:47:08.923: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 17:47:08.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:08.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:08.952
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:47:08.956
    Jan 18 17:47:08.973: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf" in namespace "downward-api-8872" to be "Succeeded or Failed"
    Jan 18 17:47:08.979: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.972241ms
    Jan 18 17:47:10.986: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01337132s
    Jan 18 17:47:12.988: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015761754s
    STEP: Saw pod success 01/18/23 17:47:12.988
    Jan 18 17:47:12.989: INFO: Pod "downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf" satisfied condition "Succeeded or Failed"
    Jan 18 17:47:12.996: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf container client-container: <nil>
    STEP: delete the pod 01/18/23 17:47:13.025
    Jan 18 17:47:13.044: INFO: Waiting for pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf to disappear
    Jan 18 17:47:13.052: INFO: Pod downwardapi-volume-2b34c0b7-bad2-4d01-b428-061364379cbf no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 17:47:13.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8872" for this suite. 01/18/23 17:47:13.06
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:13.074
Jan 18 17:47:13.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption 01/18/23 17:47:13.075
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:13.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:13.105
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:13.11
Jan 18 17:47:13.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption-2 01/18/23 17:47:13.111
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:13.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:13.141
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/18/23 17:47:13.152
STEP: Waiting for the pdb to be processed 01/18/23 17:47:15.173
STEP: Waiting for the pdb to be processed 01/18/23 17:47:17.201
STEP: listing a collection of PDBs across all namespaces 01/18/23 17:47:17.212
STEP: listing a collection of PDBs in namespace disruption-3714 01/18/23 17:47:17.219
STEP: deleting a collection of PDBs 01/18/23 17:47:17.225
STEP: Waiting for the PDB collection to be deleted 01/18/23 17:47:17.252
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 18 17:47:17.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-2486" for this suite. 01/18/23 17:47:17.264
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 17:47:17.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3714" for this suite. 01/18/23 17:47:17.283
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":199,"skipped":3798,"failed":0}
------------------------------
â€¢ [4.223 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:13.074
    Jan 18 17:47:13.074: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption 01/18/23 17:47:13.075
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:13.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:13.105
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:13.11
    Jan 18 17:47:13.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption-2 01/18/23 17:47:13.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:13.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:13.141
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/18/23 17:47:13.152
    STEP: Waiting for the pdb to be processed 01/18/23 17:47:15.173
    STEP: Waiting for the pdb to be processed 01/18/23 17:47:17.201
    STEP: listing a collection of PDBs across all namespaces 01/18/23 17:47:17.212
    STEP: listing a collection of PDBs in namespace disruption-3714 01/18/23 17:47:17.219
    STEP: deleting a collection of PDBs 01/18/23 17:47:17.225
    STEP: Waiting for the PDB collection to be deleted 01/18/23 17:47:17.252
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 18 17:47:17.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-2486" for this suite. 01/18/23 17:47:17.264
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 17:47:17.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3714" for this suite. 01/18/23 17:47:17.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:17.299
Jan 18 17:47:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:47:17.3
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:17.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:17.333
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-aaf30339-5d19-4cf3-907f-73ba73555e3b 01/18/23 17:47:17.338
STEP: Creating a pod to test consume configMaps 01/18/23 17:47:17.346
Jan 18 17:47:17.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460" in namespace "projected-9148" to be "Succeeded or Failed"
Jan 18 17:47:17.371: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Pending", Reason="", readiness=false. Elapsed: 8.859161ms
Jan 18 17:47:19.380: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01791192s
Jan 18 17:47:21.382: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020591622s
STEP: Saw pod success 01/18/23 17:47:21.383
Jan 18 17:47:21.383: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460" satisfied condition "Succeeded or Failed"
Jan 18 17:47:21.391: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:47:21.408
Jan 18 17:47:21.432: INFO: Waiting for pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 to disappear
Jan 18 17:47:21.438: INFO: Pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:47:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9148" for this suite. 01/18/23 17:47:21.447
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":200,"skipped":3821,"failed":0}
------------------------------
â€¢ [4.160 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:17.299
    Jan 18 17:47:17.299: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:47:17.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:17.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:17.333
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-aaf30339-5d19-4cf3-907f-73ba73555e3b 01/18/23 17:47:17.338
    STEP: Creating a pod to test consume configMaps 01/18/23 17:47:17.346
    Jan 18 17:47:17.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460" in namespace "projected-9148" to be "Succeeded or Failed"
    Jan 18 17:47:17.371: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Pending", Reason="", readiness=false. Elapsed: 8.859161ms
    Jan 18 17:47:19.380: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01791192s
    Jan 18 17:47:21.382: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020591622s
    STEP: Saw pod success 01/18/23 17:47:21.383
    Jan 18 17:47:21.383: INFO: Pod "pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460" satisfied condition "Succeeded or Failed"
    Jan 18 17:47:21.391: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:47:21.408
    Jan 18 17:47:21.432: INFO: Waiting for pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 to disappear
    Jan 18 17:47:21.438: INFO: Pod pod-projected-configmaps-bd87031a-41d9-4f2a-aef1-c45ff3403460 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:47:21.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9148" for this suite. 01/18/23 17:47:21.447
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:21.461
Jan 18 17:47:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:47:21.463
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:21.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:21.489
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:47:21.514
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:21.713
STEP: Deploying the webhook pod 01/18/23 17:47:21.722
STEP: Wait for the deployment to be ready 01/18/23 17:47:21.744
Jan 18 17:47:21.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:47:23.783
STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:23.803
Jan 18 17:47:24.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 18 17:47:24.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 17:47:25.36
STEP: Creating a custom resource that should be denied by the webhook 01/18/23 17:47:25.481
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 17:47:27.625
STEP: Updating the custom resource with disallowed data should be denied 01/18/23 17:47:27.646
STEP: Deleting the custom resource should be denied 01/18/23 17:47:27.666
STEP: Remove the offending key and value from the custom resource data 01/18/23 17:47:27.682
STEP: Deleting the updated custom resource should be successful 01/18/23 17:47:27.704
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:47:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-934" for this suite. 01/18/23 17:47:28.261
STEP: Destroying namespace "webhook-934-markers" for this suite. 01/18/23 17:47:28.274
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":201,"skipped":3848,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.905 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:21.461
    Jan 18 17:47:21.462: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:47:21.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:21.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:21.489
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:47:21.514
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:21.713
    STEP: Deploying the webhook pod 01/18/23 17:47:21.722
    STEP: Wait for the deployment to be ready 01/18/23 17:47:21.744
    Jan 18 17:47:21.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:47:23.783
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:23.803
    Jan 18 17:47:24.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 18 17:47:24.811: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/18/23 17:47:25.36
    STEP: Creating a custom resource that should be denied by the webhook 01/18/23 17:47:25.481
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/18/23 17:47:27.625
    STEP: Updating the custom resource with disallowed data should be denied 01/18/23 17:47:27.646
    STEP: Deleting the custom resource should be denied 01/18/23 17:47:27.666
    STEP: Remove the offending key and value from the custom resource data 01/18/23 17:47:27.682
    STEP: Deleting the updated custom resource should be successful 01/18/23 17:47:27.704
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:47:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-934" for this suite. 01/18/23 17:47:28.261
    STEP: Destroying namespace "webhook-934-markers" for this suite. 01/18/23 17:47:28.274
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:28.367
Jan 18 17:47:28.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:47:28.369
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:28.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:28.397
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:47:28.408
Jan 18 17:47:28.421: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5402" to be "running and ready"
Jan 18 17:47:28.428: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.537465ms
Jan 18 17:47:28.429: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:47:30.436: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014148206s
Jan 18 17:47:30.436: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 17:47:30.436: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/18/23 17:47:30.443
Jan 18 17:47:30.455: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5402" to be "running and ready"
Jan 18 17:47:30.462: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716916ms
Jan 18 17:47:30.462: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:47:32.473: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.017164407s
Jan 18 17:47:32.473: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 18 17:47:32.473: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/18/23 17:47:32.486
Jan 18 17:47:32.505: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 17:47:32.513: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 17:47:34.514: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 17:47:34.523: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 18 17:47:36.514: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 18 17:47:36.523: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/18/23 17:47:36.523
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 17:47:36.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5402" for this suite. 01/18/23 17:47:36.563
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":202,"skipped":3851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.210 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:28.367
    Jan 18 17:47:28.367: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 17:47:28.369
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:28.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:28.397
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 17:47:28.408
    Jan 18 17:47:28.421: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5402" to be "running and ready"
    Jan 18 17:47:28.428: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.537465ms
    Jan 18 17:47:28.429: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:47:30.436: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014148206s
    Jan 18 17:47:30.436: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 17:47:30.436: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/18/23 17:47:30.443
    Jan 18 17:47:30.455: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-5402" to be "running and ready"
    Jan 18 17:47:30.462: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.716916ms
    Jan 18 17:47:30.462: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:47:32.473: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.017164407s
    Jan 18 17:47:32.473: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 18 17:47:32.473: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/18/23 17:47:32.486
    Jan 18 17:47:32.505: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 17:47:32.513: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 17:47:34.514: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 17:47:34.523: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 18 17:47:36.514: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 18 17:47:36.523: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/18/23 17:47:36.523
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 17:47:36.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5402" for this suite. 01/18/23 17:47:36.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:36.582
Jan 18 17:47:36.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:47:36.583
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:36.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:36.616
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:47:36.661
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:36.943
STEP: Deploying the webhook pod 01/18/23 17:47:36.959
STEP: Wait for the deployment to be ready 01/18/23 17:47:36.981
Jan 18 17:47:36.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 17:47:39.023
STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:39.042
Jan 18 17:47:40.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 17:47:40.051
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 17:47:40.084
STEP: Creating a dummy validating-webhook-configuration object 01/18/23 17:47:40.115
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 17:47:40.13
STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 17:47:40.141
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 17:47:40.159
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:47:40.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5517" for this suite. 01/18/23 17:47:40.202
STEP: Destroying namespace "webhook-5517-markers" for this suite. 01/18/23 17:47:40.213
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":203,"skipped":3898,"failed":0}
------------------------------
â€¢ [3.705 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:36.582
    Jan 18 17:47:36.582: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:47:36.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:36.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:36.616
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:47:36.661
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:47:36.943
    STEP: Deploying the webhook pod 01/18/23 17:47:36.959
    STEP: Wait for the deployment to be ready 01/18/23 17:47:36.981
    Jan 18 17:47:36.993: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 17:47:39.023
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:47:39.042
    Jan 18 17:47:40.044: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 17:47:40.051
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/18/23 17:47:40.084
    STEP: Creating a dummy validating-webhook-configuration object 01/18/23 17:47:40.115
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/18/23 17:47:40.13
    STEP: Creating a dummy mutating-webhook-configuration object 01/18/23 17:47:40.141
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/18/23 17:47:40.159
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:47:40.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5517" for this suite. 01/18/23 17:47:40.202
    STEP: Destroying namespace "webhook-5517-markers" for this suite. 01/18/23 17:47:40.213
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:47:40.293
Jan 18 17:47:40.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 17:47:40.294
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:40.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:40.321
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf in namespace container-probe-2837 01/18/23 17:47:40.326
Jan 18 17:47:40.340: INFO: Waiting up to 5m0s for pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf" in namespace "container-probe-2837" to be "not pending"
Jan 18 17:47:40.346: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.275933ms
Jan 18 17:47:42.353: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013439043s
Jan 18 17:47:42.353: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf" satisfied condition "not pending"
Jan 18 17:47:42.353: INFO: Started pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf in namespace container-probe-2837
STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:47:42.353
Jan 18 17:47:42.360: INFO: Initial restart count of pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf is 0
STEP: deleting the pod 01/18/23 17:51:43.476
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 17:51:43.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2837" for this suite. 01/18/23 17:51:43.51
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":204,"skipped":3961,"failed":0}
------------------------------
â€¢ [SLOW TEST] [243.228 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:47:40.293
    Jan 18 17:47:40.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 17:47:40.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:47:40.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:47:40.321
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf in namespace container-probe-2837 01/18/23 17:47:40.326
    Jan 18 17:47:40.340: INFO: Waiting up to 5m0s for pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf" in namespace "container-probe-2837" to be "not pending"
    Jan 18 17:47:40.346: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.275933ms
    Jan 18 17:47:42.353: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013439043s
    Jan 18 17:47:42.353: INFO: Pod "liveness-b037cbcd-685b-49ad-86b3-24416d6569cf" satisfied condition "not pending"
    Jan 18 17:47:42.353: INFO: Started pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf in namespace container-probe-2837
    STEP: checking the pod's current state and verifying that restartCount is present 01/18/23 17:47:42.353
    Jan 18 17:47:42.360: INFO: Initial restart count of pod liveness-b037cbcd-685b-49ad-86b3-24416d6569cf is 0
    STEP: deleting the pod 01/18/23 17:51:43.476
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 17:51:43.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2837" for this suite. 01/18/23 17:51:43.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:51:43.523
Jan 18 17:51:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename prestop 01/18/23 17:51:43.524
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:43.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:43.57
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2590 01/18/23 17:51:43.576
STEP: Waiting for pods to come up. 01/18/23 17:51:43.594
Jan 18 17:51:43.594: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2590" to be "running"
Jan 18 17:51:43.601: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314499ms
Jan 18 17:51:45.610: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.015930488s
Jan 18 17:51:45.610: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2590 01/18/23 17:51:45.617
Jan 18 17:51:45.627: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2590" to be "running"
Jan 18 17:51:45.634: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962317ms
Jan 18 17:51:47.643: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.015447047s
Jan 18 17:51:47.643: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/18/23 17:51:47.643
Jan 18 17:51:52.671: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/18/23 17:51:52.671
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 18 17:51:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2590" for this suite. 01/18/23 17:51:52.71
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":205,"skipped":3970,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.200 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:51:43.523
    Jan 18 17:51:43.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename prestop 01/18/23 17:51:43.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:43.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:43.57
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2590 01/18/23 17:51:43.576
    STEP: Waiting for pods to come up. 01/18/23 17:51:43.594
    Jan 18 17:51:43.594: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2590" to be "running"
    Jan 18 17:51:43.601: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314499ms
    Jan 18 17:51:45.610: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.015930488s
    Jan 18 17:51:45.610: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2590 01/18/23 17:51:45.617
    Jan 18 17:51:45.627: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2590" to be "running"
    Jan 18 17:51:45.634: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962317ms
    Jan 18 17:51:47.643: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.015447047s
    Jan 18 17:51:47.643: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/18/23 17:51:47.643
    Jan 18 17:51:52.671: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/18/23 17:51:52.671
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 18 17:51:52.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-2590" for this suite. 01/18/23 17:51:52.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:51:52.724
Jan 18 17:51:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:51:52.726
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:52.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:52.754
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/18/23 17:51:52.759
Jan 18 17:51:52.777: INFO: Waiting up to 5m0s for pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c" in namespace "var-expansion-5507" to be "Succeeded or Failed"
Jan 18 17:51:52.785: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598441ms
Jan 18 17:51:54.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016084648s
Jan 18 17:51:56.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016143433s
STEP: Saw pod success 01/18/23 17:51:56.794
Jan 18 17:51:56.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c" satisfied condition "Succeeded or Failed"
Jan 18 17:51:56.801: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:51:56.83
Jan 18 17:51:56.850: INFO: Waiting for pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c to disappear
Jan 18 17:51:56.856: INFO: Pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:51:56.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5507" for this suite. 01/18/23 17:51:56.864
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":206,"skipped":3978,"failed":0}
------------------------------
â€¢ [4.154 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:51:52.724
    Jan 18 17:51:52.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:51:52.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:52.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:52.754
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/18/23 17:51:52.759
    Jan 18 17:51:52.777: INFO: Waiting up to 5m0s for pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c" in namespace "var-expansion-5507" to be "Succeeded or Failed"
    Jan 18 17:51:52.785: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.598441ms
    Jan 18 17:51:54.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016084648s
    Jan 18 17:51:56.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016143433s
    STEP: Saw pod success 01/18/23 17:51:56.794
    Jan 18 17:51:56.794: INFO: Pod "var-expansion-05ca213d-de5e-4122-8562-e98691aba14c" satisfied condition "Succeeded or Failed"
    Jan 18 17:51:56.801: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:51:56.83
    Jan 18 17:51:56.850: INFO: Waiting for pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c to disappear
    Jan 18 17:51:56.856: INFO: Pod var-expansion-05ca213d-de5e-4122-8562-e98691aba14c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:51:56.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5507" for this suite. 01/18/23 17:51:56.864
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:51:56.879
Jan 18 17:51:56.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename watch 01/18/23 17:51:56.88
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:56.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:56.909
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/18/23 17:51:56.913
STEP: creating a new configmap 01/18/23 17:51:56.915
STEP: modifying the configmap once 01/18/23 17:51:56.922
STEP: changing the label value of the configmap 01/18/23 17:51:56.937
STEP: Expecting to observe a delete notification for the watched object 01/18/23 17:51:56.951
Jan 18 17:51:56.951: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911421 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 17:51:56.952: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911422 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 17:51:56.952: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911423 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/18/23 17:51:56.952
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 17:51:56.968
STEP: changing the label value of the configmap back 01/18/23 17:52:06.969
STEP: modifying the configmap a third time 01/18/23 17:52:06.987
STEP: deleting the configmap 01/18/23 17:52:07.008
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 17:52:07.02
Jan 18 17:52:07.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911777 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 17:52:07.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911782 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 17:52:07.020: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911787 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 17:52:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5304" for this suite. 01/18/23 17:52:07.03
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":207,"skipped":3981,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.164 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:51:56.879
    Jan 18 17:51:56.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename watch 01/18/23 17:51:56.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:51:56.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:51:56.909
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/18/23 17:51:56.913
    STEP: creating a new configmap 01/18/23 17:51:56.915
    STEP: modifying the configmap once 01/18/23 17:51:56.922
    STEP: changing the label value of the configmap 01/18/23 17:51:56.937
    STEP: Expecting to observe a delete notification for the watched object 01/18/23 17:51:56.951
    Jan 18 17:51:56.951: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911421 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 17:51:56.952: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911422 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 17:51:56.952: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911423 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:51:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/18/23 17:51:56.952
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/18/23 17:51:56.968
    STEP: changing the label value of the configmap back 01/18/23 17:52:06.969
    STEP: modifying the configmap a third time 01/18/23 17:52:06.987
    STEP: deleting the configmap 01/18/23 17:52:07.008
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/18/23 17:52:07.02
    Jan 18 17:52:07.020: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911777 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 17:52:07.020: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911782 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 17:52:07.020: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5304  3be119f9-9f45-4768-b4ed-ec6d42148e40 2631911787 0 2023-01-18 17:51:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-18 17:52:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 17:52:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5304" for this suite. 01/18/23 17:52:07.03
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:07.05
Jan 18 17:52:07.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:52:07.051
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:07.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:07.083
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/18/23 17:52:07.088
STEP: watching for the ServiceAccount to be added 01/18/23 17:52:07.102
STEP: patching the ServiceAccount 01/18/23 17:52:07.105
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 17:52:07.116
STEP: deleting the ServiceAccount 01/18/23 17:52:07.124
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 17:52:07.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4861" for this suite. 01/18/23 17:52:07.158
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":208,"skipped":4062,"failed":0}
------------------------------
â€¢ [0.123 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:07.05
    Jan 18 17:52:07.050: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:52:07.051
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:07.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:07.083
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/18/23 17:52:07.088
    STEP: watching for the ServiceAccount to be added 01/18/23 17:52:07.102
    STEP: patching the ServiceAccount 01/18/23 17:52:07.105
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/18/23 17:52:07.116
    STEP: deleting the ServiceAccount 01/18/23 17:52:07.124
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 17:52:07.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4861" for this suite. 01/18/23 17:52:07.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:07.176
Jan 18 17:52:07.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 17:52:07.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:07.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:07.205
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan 18 17:52:07.274: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"788766f6-c2fc-463c-be77-39fa9bb5f65d", Controller:(*bool)(0xc000d46632), BlockOwnerDeletion:(*bool)(0xc000d46633)}}
Jan 18 17:52:07.293: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"faae373b-153c-4fd8-bbc2-5cf669b1985b", Controller:(*bool)(0xc00152f426), BlockOwnerDeletion:(*bool)(0xc00152f427)}}
Jan 18 17:52:07.310: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"66c6fae9-dcf5-4a6c-99b9-3721e27fec32", Controller:(*bool)(0xc00152f696), BlockOwnerDeletion:(*bool)(0xc00152f697)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 17:52:12.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3672" for this suite. 01/18/23 17:52:12.336
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":209,"skipped":4093,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.173 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:07.176
    Jan 18 17:52:07.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 17:52:07.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:07.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:07.205
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan 18 17:52:07.274: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"788766f6-c2fc-463c-be77-39fa9bb5f65d", Controller:(*bool)(0xc000d46632), BlockOwnerDeletion:(*bool)(0xc000d46633)}}
    Jan 18 17:52:07.293: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"faae373b-153c-4fd8-bbc2-5cf669b1985b", Controller:(*bool)(0xc00152f426), BlockOwnerDeletion:(*bool)(0xc00152f427)}}
    Jan 18 17:52:07.310: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"66c6fae9-dcf5-4a6c-99b9-3721e27fec32", Controller:(*bool)(0xc00152f696), BlockOwnerDeletion:(*bool)(0xc00152f697)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 17:52:12.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3672" for this suite. 01/18/23 17:52:12.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:12.35
Jan 18 17:52:12.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-pred 01/18/23 17:52:12.351
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:12.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:12.378
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 17:52:12.383: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 17:52:12.397: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 17:52:12.401: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
Jan 18 17:52:12.412: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 17:52:12.412: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 17:52:12.412: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 17:52:12.412: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 17:52:12.412: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 17:52:12.412: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 17:52:12.412: INFO: tester from prestop-2590 started at 2023-01-18 17:51:45 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container tester ready: true, restart count 0
Jan 18 17:52:12.412: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
Jan 18 17:52:12.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:52:12.412: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 17:52:12.412: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
Jan 18 17:52:12.423: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 17:52:12.423: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 17:52:12.423: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container coredns ready: true, restart count 0
Jan 18 17:52:12.423: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 17:52:12.423: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 17:52:12.423: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 17:52:12.423: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 17:52:12.423: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 17:52:12.423: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 17:52:12.423: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 17:52:12.423: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container e2e ready: true, restart count 0
Jan 18 17:52:12.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:52:12.423: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 17:52:12.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:52:12.423: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 17:52:12.423
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173b78a40d86399c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 01/18/23 17:52:12.514
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:52:13.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7422" for this suite. 01/18/23 17:52:13.506
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":210,"skipped":4102,"failed":0}
------------------------------
â€¢ [1.170 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:12.35
    Jan 18 17:52:12.350: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 17:52:12.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:12.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:12.378
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 17:52:12.383: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 17:52:12.397: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 17:52:12.401: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
    Jan 18 17:52:12.412: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: tester from prestop-2590 started at 2023-01-18 17:51:45 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container tester ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
    Jan 18 17:52:12.412: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 17:52:12.412: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
    Jan 18 17:52:12.423: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 17:52:12.423: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:52:12.423: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/18/23 17:52:12.423
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173b78a40d86399c], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling.] 01/18/23 17:52:12.514
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:52:13.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7422" for this suite. 01/18/23 17:52:13.506
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:13.524
Jan 18 17:52:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption 01/18/23 17:52:13.525
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:13.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:13.556
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/18/23 17:52:13.571
STEP: Updating PodDisruptionBudget status 01/18/23 17:52:15.586
STEP: Waiting for all pods to be running 01/18/23 17:52:15.602
Jan 18 17:52:15.609: INFO: running pods: 0 < 1
STEP: locating a running pod 01/18/23 17:52:17.616
STEP: Waiting for the pdb to be processed 01/18/23 17:52:17.641
STEP: Patching PodDisruptionBudget status 01/18/23 17:52:17.653
STEP: Waiting for the pdb to be processed 01/18/23 17:52:17.683
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 17:52:17.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6575" for this suite. 01/18/23 17:52:17.696
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":211,"skipped":4145,"failed":0}
------------------------------
â€¢ [4.184 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:13.524
    Jan 18 17:52:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption 01/18/23 17:52:13.525
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:13.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:13.556
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/18/23 17:52:13.571
    STEP: Updating PodDisruptionBudget status 01/18/23 17:52:15.586
    STEP: Waiting for all pods to be running 01/18/23 17:52:15.602
    Jan 18 17:52:15.609: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/18/23 17:52:17.616
    STEP: Waiting for the pdb to be processed 01/18/23 17:52:17.641
    STEP: Patching PodDisruptionBudget status 01/18/23 17:52:17.653
    STEP: Waiting for the pdb to be processed 01/18/23 17:52:17.683
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 17:52:17.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6575" for this suite. 01/18/23 17:52:17.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:17.711
Jan 18 17:52:17.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename namespaces 01/18/23 17:52:17.712
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:17.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:17.739
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/18/23 17:52:17.743
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:17.764
STEP: Creating a service in the namespace 01/18/23 17:52:17.768
STEP: Deleting the namespace 01/18/23 17:52:17.786
STEP: Waiting for the namespace to be removed. 01/18/23 17:52:17.798
STEP: Recreating the namespace 01/18/23 17:52:23.81
STEP: Verifying there is no service in the namespace 01/18/23 17:52:23.834
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:52:23.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-860" for this suite. 01/18/23 17:52:23.85
STEP: Destroying namespace "nsdeletetest-4196" for this suite. 01/18/23 17:52:23.864
Jan 18 17:52:23.873: INFO: Namespace nsdeletetest-4196 was already deleted
STEP: Destroying namespace "nsdeletetest-9268" for this suite. 01/18/23 17:52:23.873
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":212,"skipped":4178,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.179 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:17.711
    Jan 18 17:52:17.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename namespaces 01/18/23 17:52:17.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:17.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:17.739
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/18/23 17:52:17.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:17.764
    STEP: Creating a service in the namespace 01/18/23 17:52:17.768
    STEP: Deleting the namespace 01/18/23 17:52:17.786
    STEP: Waiting for the namespace to be removed. 01/18/23 17:52:17.798
    STEP: Recreating the namespace 01/18/23 17:52:23.81
    STEP: Verifying there is no service in the namespace 01/18/23 17:52:23.834
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:52:23.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-860" for this suite. 01/18/23 17:52:23.85
    STEP: Destroying namespace "nsdeletetest-4196" for this suite. 01/18/23 17:52:23.864
    Jan 18 17:52:23.873: INFO: Namespace nsdeletetest-4196 was already deleted
    STEP: Destroying namespace "nsdeletetest-9268" for this suite. 01/18/23 17:52:23.873
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:23.89
Jan 18 17:52:23.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:52:23.891
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:23.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:23.918
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-3e237155-4516-4527-9632-08d4a22336c2 01/18/23 17:52:23.922
STEP: Creating a pod to test consume configMaps 01/18/23 17:52:23.931
Jan 18 17:52:23.948: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4" in namespace "projected-2690" to be "Succeeded or Failed"
Jan 18 17:52:23.955: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57298ms
Jan 18 17:52:25.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015913875s
Jan 18 17:52:27.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015849845s
STEP: Saw pod success 01/18/23 17:52:27.964
Jan 18 17:52:27.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4" satisfied condition "Succeeded or Failed"
Jan 18 17:52:27.971: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 17:52:27.99
Jan 18 17:52:28.011: INFO: Waiting for pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 to disappear
Jan 18 17:52:28.017: INFO: Pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:52:28.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2690" for this suite. 01/18/23 17:52:28.025
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":213,"skipped":4180,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:23.89
    Jan 18 17:52:23.890: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:52:23.891
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:23.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:23.918
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-3e237155-4516-4527-9632-08d4a22336c2 01/18/23 17:52:23.922
    STEP: Creating a pod to test consume configMaps 01/18/23 17:52:23.931
    Jan 18 17:52:23.948: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4" in namespace "projected-2690" to be "Succeeded or Failed"
    Jan 18 17:52:23.955: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.57298ms
    Jan 18 17:52:25.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015913875s
    Jan 18 17:52:27.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015849845s
    STEP: Saw pod success 01/18/23 17:52:27.964
    Jan 18 17:52:27.964: INFO: Pod "pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4" satisfied condition "Succeeded or Failed"
    Jan 18 17:52:27.971: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:52:27.99
    Jan 18 17:52:28.011: INFO: Waiting for pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 to disappear
    Jan 18 17:52:28.017: INFO: Pod pod-projected-configmaps-1097175e-ffc4-4ae7-ad74-b3c7ce23e6c4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:52:28.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2690" for this suite. 01/18/23 17:52:28.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:28.042
Jan 18 17:52:28.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 17:52:28.043
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:28.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:28.075
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 18 17:52:28.115: INFO: Create a RollingUpdate DaemonSet
Jan 18 17:52:28.126: INFO: Check that daemon pods launch on every node of the cluster
Jan 18 17:52:28.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:52:28.142: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:52:29.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:52:29.168: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 17:52:30.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 17:52:30.156: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Jan 18 17:52:30.156: INFO: Update the DaemonSet to trigger a rollout
Jan 18 17:52:30.173: INFO: Updating DaemonSet daemon-set
Jan 18 17:52:33.203: INFO: Roll back the DaemonSet before rollout is complete
Jan 18 17:52:33.224: INFO: Updating DaemonSet daemon-set
Jan 18 17:52:33.225: INFO: Make sure DaemonSet rollback is complete
Jan 18 17:52:33.231: INFO: Wrong image for pod: daemon-set-5wrlx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 18 17:52:33.231: INFO: Pod daemon-set-5wrlx is not available
Jan 18 17:52:36.245: INFO: Pod daemon-set-4pxvq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:52:36.266
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7348, will wait for the garbage collector to delete the pods 01/18/23 17:52:36.266
Jan 18 17:52:36.335: INFO: Deleting DaemonSet.extensions daemon-set took: 13.084049ms
Jan 18 17:52:36.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.833328ms
Jan 18 17:52:37.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 17:52:37.843: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 17:52:37.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631913292"},"items":null}

Jan 18 17:52:37.857: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631913292"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:52:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7348" for this suite. 01/18/23 17:52:37.884
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":214,"skipped":4223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.856 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:28.042
    Jan 18 17:52:28.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 17:52:28.043
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:28.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:28.075
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 18 17:52:28.115: INFO: Create a RollingUpdate DaemonSet
    Jan 18 17:52:28.126: INFO: Check that daemon pods launch on every node of the cluster
    Jan 18 17:52:28.142: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:52:28.142: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:52:29.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:52:29.168: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 17:52:30.156: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 17:52:30.156: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Jan 18 17:52:30.156: INFO: Update the DaemonSet to trigger a rollout
    Jan 18 17:52:30.173: INFO: Updating DaemonSet daemon-set
    Jan 18 17:52:33.203: INFO: Roll back the DaemonSet before rollout is complete
    Jan 18 17:52:33.224: INFO: Updating DaemonSet daemon-set
    Jan 18 17:52:33.225: INFO: Make sure DaemonSet rollback is complete
    Jan 18 17:52:33.231: INFO: Wrong image for pod: daemon-set-5wrlx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 18 17:52:33.231: INFO: Pod daemon-set-5wrlx is not available
    Jan 18 17:52:36.245: INFO: Pod daemon-set-4pxvq is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 17:52:36.266
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7348, will wait for the garbage collector to delete the pods 01/18/23 17:52:36.266
    Jan 18 17:52:36.335: INFO: Deleting DaemonSet.extensions daemon-set took: 13.084049ms
    Jan 18 17:52:36.436: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.833328ms
    Jan 18 17:52:37.843: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 17:52:37.843: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 17:52:37.850: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631913292"},"items":null}

    Jan 18 17:52:37.857: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631913292"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:52:37.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7348" for this suite. 01/18/23 17:52:37.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:37.898
Jan 18 17:52:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 17:52:37.899
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:37.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:37.926
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/18/23 17:52:37.931
Jan 18 17:52:37.946: INFO: Waiting up to 5m0s for pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6" in namespace "var-expansion-2211" to be "Succeeded or Failed"
Jan 18 17:52:37.956: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091967ms
Jan 18 17:52:39.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017635962s
Jan 18 17:52:41.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0177123s
STEP: Saw pod success 01/18/23 17:52:41.964
Jan 18 17:52:41.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6" satisfied condition "Succeeded or Failed"
Jan 18 17:52:41.972: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 container dapi-container: <nil>
STEP: delete the pod 01/18/23 17:52:41.988
Jan 18 17:52:42.009: INFO: Waiting for pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 to disappear
Jan 18 17:52:42.015: INFO: Pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 17:52:42.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2211" for this suite. 01/18/23 17:52:42.025
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":215,"skipped":4231,"failed":0}
------------------------------
â€¢ [4.140 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:37.898
    Jan 18 17:52:37.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 17:52:37.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:37.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:37.926
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/18/23 17:52:37.931
    Jan 18 17:52:37.946: INFO: Waiting up to 5m0s for pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6" in namespace "var-expansion-2211" to be "Succeeded or Failed"
    Jan 18 17:52:37.956: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091967ms
    Jan 18 17:52:39.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017635962s
    Jan 18 17:52:41.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0177123s
    STEP: Saw pod success 01/18/23 17:52:41.964
    Jan 18 17:52:41.964: INFO: Pod "var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6" satisfied condition "Succeeded or Failed"
    Jan 18 17:52:41.972: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 container dapi-container: <nil>
    STEP: delete the pod 01/18/23 17:52:41.988
    Jan 18 17:52:42.009: INFO: Waiting for pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 to disappear
    Jan 18 17:52:42.015: INFO: Pod var-expansion-2b3e206a-034b-435b-a910-adedf456fcd6 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 17:52:42.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2211" for this suite. 01/18/23 17:52:42.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:52:42.043
Jan 18 17:52:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-runtime 01/18/23 17:52:42.044
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:42.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:42.074
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 17:52:42.094
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 17:52:59.239
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 17:52:59.247
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 17:52:59.26
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 17:52:59.261
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 17:52:59.3
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 17:53:02.332
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 17:53:04.357
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 17:53:04.396
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 17:53:04.396
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 17:53:04.428
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 17:53:05.442
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 17:53:08.469
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 17:53:08.486
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 17:53:08.486
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 17:53:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7670" for this suite. 01/18/23 17:53:08.542
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":216,"skipped":4274,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.512 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:52:42.043
    Jan 18 17:52:42.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 17:52:42.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:52:42.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:52:42.074
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/18/23 17:52:42.094
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/18/23 17:52:59.239
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/18/23 17:52:59.247
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/18/23 17:52:59.26
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/18/23 17:52:59.261
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/18/23 17:52:59.3
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/18/23 17:53:02.332
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/18/23 17:53:04.357
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/18/23 17:53:04.396
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/18/23 17:53:04.396
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/18/23 17:53:04.428
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/18/23 17:53:05.442
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/18/23 17:53:08.469
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/18/23 17:53:08.486
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/18/23 17:53:08.486
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 17:53:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7670" for this suite. 01/18/23 17:53:08.542
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:08.555
Jan 18 17:53:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:53:08.557
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:08.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:08.59
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 18 17:53:08.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-310 version'
Jan 18 17:53:08.676: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 18 17:53:08.676: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:53:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-310" for this suite. 01/18/23 17:53:08.684
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":217,"skipped":4276,"failed":0}
------------------------------
â€¢ [0.143 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:08.555
    Jan 18 17:53:08.555: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:53:08.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:08.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:08.59
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 18 17:53:08.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-310 version'
    Jan 18 17:53:08.676: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 18 17:53:08.676: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:15:02Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.5\", GitCommit:\"804d6167111f6858541cef440ccc53887fbbc96a\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T10:08:09Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:53:08.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-310" for this suite. 01/18/23 17:53:08.684
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:08.699
Jan 18 17:53:08.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:53:08.7
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:08.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:08.731
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 17:53:08.735
Jan 18 17:53:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 17:53:23.038
Jan 18 17:53:23.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 17:53:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:53:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4606" for this suite. 01/18/23 17:53:36.126
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":218,"skipped":4280,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.440 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:08.699
    Jan 18 17:53:08.699: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 17:53:08.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:08.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:08.731
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/18/23 17:53:08.735
    Jan 18 17:53:08.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/18/23 17:53:23.038
    Jan 18 17:53:23.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 17:53:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:53:36.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4606" for this suite. 01/18/23 17:53:36.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:36.14
Jan 18 17:53:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:53:36.142
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:36.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:36.17
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 18 17:53:36.180: INFO: Got root ca configmap in namespace "svcaccounts-5534"
Jan 18 17:53:36.194: INFO: Deleted root ca configmap in namespace "svcaccounts-5534"
STEP: waiting for a new root ca configmap created 01/18/23 17:53:36.694
Jan 18 17:53:36.702: INFO: Recreated root ca configmap in namespace "svcaccounts-5534"
Jan 18 17:53:36.712: INFO: Updated root ca configmap in namespace "svcaccounts-5534"
STEP: waiting for the root ca configmap reconciled 01/18/23 17:53:37.213
Jan 18 17:53:37.220: INFO: Reconciled root ca configmap in namespace "svcaccounts-5534"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 17:53:37.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5534" for this suite. 01/18/23 17:53:37.228
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":219,"skipped":4289,"failed":0}
------------------------------
â€¢ [1.101 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:36.14
    Jan 18 17:53:36.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:53:36.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:36.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:36.17
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 18 17:53:36.180: INFO: Got root ca configmap in namespace "svcaccounts-5534"
    Jan 18 17:53:36.194: INFO: Deleted root ca configmap in namespace "svcaccounts-5534"
    STEP: waiting for a new root ca configmap created 01/18/23 17:53:36.694
    Jan 18 17:53:36.702: INFO: Recreated root ca configmap in namespace "svcaccounts-5534"
    Jan 18 17:53:36.712: INFO: Updated root ca configmap in namespace "svcaccounts-5534"
    STEP: waiting for the root ca configmap reconciled 01/18/23 17:53:37.213
    Jan 18 17:53:37.220: INFO: Reconciled root ca configmap in namespace "svcaccounts-5534"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 17:53:37.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5534" for this suite. 01/18/23 17:53:37.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:37.243
Jan 18 17:53:37.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:53:37.244
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:37.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:37.274
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 18 17:53:37.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:53:40.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-304" for this suite. 01/18/23 17:53:40.519
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":220,"skipped":4307,"failed":0}
------------------------------
â€¢ [3.290 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:37.243
    Jan 18 17:53:37.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 17:53:37.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:37.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:37.274
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 18 17:53:37.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:53:40.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-304" for this suite. 01/18/23 17:53:40.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:40.535
Jan 18 17:53:40.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:53:40.536
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:40.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:40.563
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 17:53:40.567
Jan 18 17:53:40.581: INFO: Waiting up to 5m0s for pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb" in namespace "emptydir-6445" to be "Succeeded or Failed"
Jan 18 17:53:40.587: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141671ms
Jan 18 17:53:42.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015252922s
Jan 18 17:53:44.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015449023s
STEP: Saw pod success 01/18/23 17:53:44.596
Jan 18 17:53:44.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb" satisfied condition "Succeeded or Failed"
Jan 18 17:53:44.605: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb container test-container: <nil>
STEP: delete the pod 01/18/23 17:53:44.621
Jan 18 17:53:44.641: INFO: Waiting for pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb to disappear
Jan 18 17:53:44.648: INFO: Pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:53:44.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6445" for this suite. 01/18/23 17:53:44.656
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":221,"skipped":4322,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:40.535
    Jan 18 17:53:40.535: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:53:40.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:40.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:40.563
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/18/23 17:53:40.567
    Jan 18 17:53:40.581: INFO: Waiting up to 5m0s for pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb" in namespace "emptydir-6445" to be "Succeeded or Failed"
    Jan 18 17:53:40.587: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.141671ms
    Jan 18 17:53:42.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015252922s
    Jan 18 17:53:44.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015449023s
    STEP: Saw pod success 01/18/23 17:53:44.596
    Jan 18 17:53:44.596: INFO: Pod "pod-f9b41237-050a-48f2-8dc4-3212de9a42bb" satisfied condition "Succeeded or Failed"
    Jan 18 17:53:44.605: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb container test-container: <nil>
    STEP: delete the pod 01/18/23 17:53:44.621
    Jan 18 17:53:44.641: INFO: Waiting for pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb to disappear
    Jan 18 17:53:44.648: INFO: Pod pod-f9b41237-050a-48f2-8dc4-3212de9a42bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:53:44.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6445" for this suite. 01/18/23 17:53:44.656
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:44.668
Jan 18 17:53:44.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:53:44.669
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:44.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:44.697
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-3475 01/18/23 17:53:44.702
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 17:53:44.763
STEP: creating service externalsvc in namespace services-3475 01/18/23 17:53:44.763
STEP: creating replication controller externalsvc in namespace services-3475 01/18/23 17:53:44.789
I0118 17:53:44.801842      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3475, replica count: 2
I0118 17:53:47.858596      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/18/23 17:53:47.869
Jan 18 17:53:47.906: INFO: Creating new exec pod
Jan 18 17:53:47.915: INFO: Waiting up to 5m0s for pod "execpodx8gzq" in namespace "services-3475" to be "running"
Jan 18 17:53:47.922: INFO: Pod "execpodx8gzq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555325ms
Jan 18 17:53:49.929: INFO: Pod "execpodx8gzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.01393831s
Jan 18 17:53:49.929: INFO: Pod "execpodx8gzq" satisfied condition "running"
Jan 18 17:53:49.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3475 exec execpodx8gzq -- /bin/sh -x -c nslookup nodeport-service.services-3475.svc.cluster.local'
Jan 18 17:53:50.148: INFO: stderr: "+ nslookup nodeport-service.services-3475.svc.cluster.local\n"
Jan 18 17:53:50.148: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3475.svc.cluster.local\tcanonical name = externalsvc.services-3475.svc.cluster.local.\nName:\texternalsvc.services-3475.svc.cluster.local\nAddress: 10.96.132.157\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3475, will wait for the garbage collector to delete the pods 01/18/23 17:53:50.148
Jan 18 17:53:50.216: INFO: Deleting ReplicationController externalsvc took: 11.269965ms
Jan 18 17:53:50.317: INFO: Terminating ReplicationController externalsvc pods took: 100.957336ms
Jan 18 17:53:52.347: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:53:52.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3475" for this suite. 01/18/23 17:53:52.373
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":222,"skipped":4352,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.715 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:44.668
    Jan 18 17:53:44.668: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:53:44.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:44.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:44.697
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-3475 01/18/23 17:53:44.702
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 17:53:44.763
    STEP: creating service externalsvc in namespace services-3475 01/18/23 17:53:44.763
    STEP: creating replication controller externalsvc in namespace services-3475 01/18/23 17:53:44.789
    I0118 17:53:44.801842      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-3475, replica count: 2
    I0118 17:53:47.858596      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/18/23 17:53:47.869
    Jan 18 17:53:47.906: INFO: Creating new exec pod
    Jan 18 17:53:47.915: INFO: Waiting up to 5m0s for pod "execpodx8gzq" in namespace "services-3475" to be "running"
    Jan 18 17:53:47.922: INFO: Pod "execpodx8gzq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.555325ms
    Jan 18 17:53:49.929: INFO: Pod "execpodx8gzq": Phase="Running", Reason="", readiness=true. Elapsed: 2.01393831s
    Jan 18 17:53:49.929: INFO: Pod "execpodx8gzq" satisfied condition "running"
    Jan 18 17:53:49.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3475 exec execpodx8gzq -- /bin/sh -x -c nslookup nodeport-service.services-3475.svc.cluster.local'
    Jan 18 17:53:50.148: INFO: stderr: "+ nslookup nodeport-service.services-3475.svc.cluster.local\n"
    Jan 18 17:53:50.148: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-3475.svc.cluster.local\tcanonical name = externalsvc.services-3475.svc.cluster.local.\nName:\texternalsvc.services-3475.svc.cluster.local\nAddress: 10.96.132.157\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-3475, will wait for the garbage collector to delete the pods 01/18/23 17:53:50.148
    Jan 18 17:53:50.216: INFO: Deleting ReplicationController externalsvc took: 11.269965ms
    Jan 18 17:53:50.317: INFO: Terminating ReplicationController externalsvc pods took: 100.957336ms
    Jan 18 17:53:52.347: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:53:52.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3475" for this suite. 01/18/23 17:53:52.373
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:53:52.384
Jan 18 17:53:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 17:53:52.387
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:52.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:52.42
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3509 01/18/23 17:53:52.431
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 17:53:52.446
STEP: Creating stateful set ss in namespace statefulset-3509 01/18/23 17:53:52.452
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3509 01/18/23 17:53:52.464
Jan 18 17:53:52.470: INFO: Found 0 stateful pods, waiting for 1
Jan 18 17:54:02.482: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 17:54:02.482
Jan 18 17:54:02.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:54:02.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:54:02.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:54:02.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:54:02.699: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 18 17:54:12.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:54:12.711: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:54:12.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999966s
Jan 18 17:54:13.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991336926s
Jan 18 17:54:14.760: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982607381s
Jan 18 17:54:15.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973320262s
Jan 18 17:54:16.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964059163s
Jan 18 17:54:17.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.953180853s
Jan 18 17:54:18.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.944295257s
Jan 18 17:54:19.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.935739536s
Jan 18 17:54:20.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.928640035s
Jan 18 17:54:21.820: INFO: Verifying statefulset ss doesn't scale past 1 for another 920.8426ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3509 01/18/23 17:54:22.82
Jan 18 17:54:22.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:54:23.019: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 17:54:23.019: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:54:23.019: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:54:23.026: INFO: Found 1 stateful pods, waiting for 3
Jan 18 17:54:33.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 17:54:33.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 17:54:33.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/18/23 17:54:33.034
STEP: Scale down will halt with unhealthy stateful pod 01/18/23 17:54:33.035
Jan 18 17:54:33.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:54:33.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:54:33.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:54:33.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:54:33.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:54:33.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:54:33.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:54:33.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:54:33.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 17:54:33.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 17:54:33.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 17:54:33.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 17:54:33.655: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:54:33.661: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 18 17:54:43.681: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:54:43.681: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:54:43.681: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 18 17:54:43.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
Jan 18 17:54:44.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994254304s
Jan 18 17:54:45.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985021712s
Jan 18 17:54:46.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97551223s
Jan 18 17:54:47.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965116322s
Jan 18 17:54:48.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95409218s
Jan 18 17:54:49.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943761805s
Jan 18 17:54:50.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933574317s
Jan 18 17:54:51.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925417126s
Jan 18 17:54:52.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.309162ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3509 01/18/23 17:54:53.792
Jan 18 17:54:53.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:54:53.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 17:54:53.993: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:54:53.993: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:54:53.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:54:54.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 17:54:54.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:54:54.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:54:54.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 17:54:54.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 17:54:54.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 17:54:54.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 18 17:54:54.384: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 17:55:04.414
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 17:55:04.414: INFO: Deleting all statefulset in ns statefulset-3509
Jan 18 17:55:04.419: INFO: Scaling statefulset ss to 0
Jan 18 17:55:04.438: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 17:55:04.444: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 17:55:04.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3509" for this suite. 01/18/23 17:55:04.472
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":223,"skipped":4360,"failed":0}
------------------------------
â€¢ [SLOW TEST] [72.105 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:53:52.384
    Jan 18 17:53:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 17:53:52.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:53:52.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:53:52.42
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3509 01/18/23 17:53:52.431
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/18/23 17:53:52.446
    STEP: Creating stateful set ss in namespace statefulset-3509 01/18/23 17:53:52.452
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3509 01/18/23 17:53:52.464
    Jan 18 17:53:52.470: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 17:54:02.482: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/18/23 17:54:02.482
    Jan 18 17:54:02.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:54:02.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:54:02.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:54:02.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:54:02.699: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 18 17:54:12.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:54:12.711: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:54:12.742: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999966s
    Jan 18 17:54:13.751: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991336926s
    Jan 18 17:54:14.760: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982607381s
    Jan 18 17:54:15.769: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973320262s
    Jan 18 17:54:16.780: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.964059163s
    Jan 18 17:54:17.789: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.953180853s
    Jan 18 17:54:18.798: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.944295257s
    Jan 18 17:54:19.805: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.935739536s
    Jan 18 17:54:20.813: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.928640035s
    Jan 18 17:54:21.820: INFO: Verifying statefulset ss doesn't scale past 1 for another 920.8426ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3509 01/18/23 17:54:22.82
    Jan 18 17:54:22.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:54:23.019: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 17:54:23.019: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:54:23.019: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:54:23.026: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 17:54:33.034: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 17:54:33.034: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 17:54:33.034: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/18/23 17:54:33.034
    STEP: Scale down will halt with unhealthy stateful pod 01/18/23 17:54:33.035
    Jan 18 17:54:33.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:54:33.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:54:33.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:54:33.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:54:33.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:54:33.450: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:54:33.450: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:54:33.450: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:54:33.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 17:54:33.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 17:54:33.655: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 17:54:33.655: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 17:54:33.655: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:54:33.661: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 18 17:54:43.681: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:54:43.681: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:54:43.681: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 18 17:54:43.704: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
    Jan 18 17:54:44.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994254304s
    Jan 18 17:54:45.722: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985021712s
    Jan 18 17:54:46.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97551223s
    Jan 18 17:54:47.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965116322s
    Jan 18 17:54:48.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95409218s
    Jan 18 17:54:49.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943761805s
    Jan 18 17:54:50.773: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933574317s
    Jan 18 17:54:51.782: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.925417126s
    Jan 18 17:54:52.791: INFO: Verifying statefulset ss doesn't scale past 3 for another 915.309162ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3509 01/18/23 17:54:53.792
    Jan 18 17:54:53.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:54:53.993: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 17:54:53.993: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:54:53.993: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:54:53.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:54:54.184: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 17:54:54.184: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:54:54.184: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:54:54.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-3509 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 17:54:54.383: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 17:54:54.384: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 17:54:54.384: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 18 17:54:54.384: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/18/23 17:55:04.414
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 17:55:04.414: INFO: Deleting all statefulset in ns statefulset-3509
    Jan 18 17:55:04.419: INFO: Scaling statefulset ss to 0
    Jan 18 17:55:04.438: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 17:55:04.444: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 17:55:04.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3509" for this suite. 01/18/23 17:55:04.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:04.49
Jan 18 17:55:04.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:55:04.492
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:04.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:04.519
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:55:04.522
Jan 18 17:55:04.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64" in namespace "projected-6524" to be "Succeeded or Failed"
Jan 18 17:55:04.543: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.086096ms
Jan 18 17:55:06.557: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Running", Reason="", readiness=false. Elapsed: 2.021431706s
Jan 18 17:55:08.550: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014070024s
STEP: Saw pod success 01/18/23 17:55:08.55
Jan 18 17:55:08.550: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64" satisfied condition "Succeeded or Failed"
Jan 18 17:55:08.557: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 container client-container: <nil>
STEP: delete the pod 01/18/23 17:55:08.576
Jan 18 17:55:08.595: INFO: Waiting for pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 to disappear
Jan 18 17:55:08.602: INFO: Pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:55:08.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6524" for this suite. 01/18/23 17:55:08.609
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":224,"skipped":4385,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:04.49
    Jan 18 17:55:04.491: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:55:04.492
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:04.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:04.519
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:55:04.522
    Jan 18 17:55:04.535: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64" in namespace "projected-6524" to be "Succeeded or Failed"
    Jan 18 17:55:04.543: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.086096ms
    Jan 18 17:55:06.557: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Running", Reason="", readiness=false. Elapsed: 2.021431706s
    Jan 18 17:55:08.550: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014070024s
    STEP: Saw pod success 01/18/23 17:55:08.55
    Jan 18 17:55:08.550: INFO: Pod "downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64" satisfied condition "Succeeded or Failed"
    Jan 18 17:55:08.557: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:55:08.576
    Jan 18 17:55:08.595: INFO: Waiting for pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 to disappear
    Jan 18 17:55:08.602: INFO: Pod downwardapi-volume-b27c31d4-1fe2-48cf-afa3-47d39ccfff64 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:55:08.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6524" for this suite. 01/18/23 17:55:08.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:08.622
Jan 18 17:55:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 17:55:08.623
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:08.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:08.65
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/18/23 17:55:08.654
STEP: Ensuring job reaches completions 01/18/23 17:55:08.665
STEP: Ensuring pods with index for job exist 01/18/23 17:55:16.676
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 17:55:16.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4084" for this suite. 01/18/23 17:55:16.689
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":225,"skipped":4395,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.079 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:08.622
    Jan 18 17:55:08.622: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 17:55:08.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:08.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:08.65
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/18/23 17:55:08.654
    STEP: Ensuring job reaches completions 01/18/23 17:55:08.665
    STEP: Ensuring pods with index for job exist 01/18/23 17:55:16.676
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 17:55:16.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4084" for this suite. 01/18/23 17:55:16.689
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:16.701
Jan 18 17:55:16.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:55:16.703
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:16.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:16.733
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/18/23 17:55:16.738
STEP: Creating a ResourceQuota 01/18/23 17:55:21.745
STEP: Ensuring resource quota status is calculated 01/18/23 17:55:21.756
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:55:23.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2254" for this suite. 01/18/23 17:55:23.773
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":226,"skipped":4398,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:16.701
    Jan 18 17:55:16.702: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:55:16.703
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:16.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:16.733
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/18/23 17:55:16.738
    STEP: Creating a ResourceQuota 01/18/23 17:55:21.745
    STEP: Ensuring resource quota status is calculated 01/18/23 17:55:21.756
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:55:23.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2254" for this suite. 01/18/23 17:55:23.773
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:23.788
Jan 18 17:55:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename server-version 01/18/23 17:55:23.789
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:23.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:23.82
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/18/23 17:55:23.826
STEP: Confirm major version 01/18/23 17:55:23.828
Jan 18 17:55:23.828: INFO: Major version: 1
STEP: Confirm minor version 01/18/23 17:55:23.828
Jan 18 17:55:23.828: INFO: cleanMinorVersion: 25
Jan 18 17:55:23.828: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 18 17:55:23.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-9261" for this suite. 01/18/23 17:55:23.835
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":227,"skipped":4401,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:23.788
    Jan 18 17:55:23.788: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename server-version 01/18/23 17:55:23.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:23.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:23.82
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/18/23 17:55:23.826
    STEP: Confirm major version 01/18/23 17:55:23.828
    Jan 18 17:55:23.828: INFO: Major version: 1
    STEP: Confirm minor version 01/18/23 17:55:23.828
    Jan 18 17:55:23.828: INFO: cleanMinorVersion: 25
    Jan 18 17:55:23.828: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 18 17:55:23.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-9261" for this suite. 01/18/23 17:55:23.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:23.85
Jan 18 17:55:23.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 17:55:23.851
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:23.874
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:23.878
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-dae8b386-cb7b-4e47-9d82-7a663d477ac0 01/18/23 17:55:23.882
STEP: Creating a pod to test consume secrets 01/18/23 17:55:23.891
Jan 18 17:55:23.905: INFO: Waiting up to 5m0s for pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597" in namespace "secrets-4196" to be "Succeeded or Failed"
Jan 18 17:55:23.911: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756359ms
Jan 18 17:55:25.922: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017171412s
Jan 18 17:55:27.918: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0131316s
STEP: Saw pod success 01/18/23 17:55:27.918
Jan 18 17:55:27.918: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597" satisfied condition "Succeeded or Failed"
Jan 18 17:55:27.926: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 17:55:27.941
Jan 18 17:55:27.962: INFO: Waiting for pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 to disappear
Jan 18 17:55:27.967: INFO: Pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 17:55:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4196" for this suite. 01/18/23 17:55:27.976
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":228,"skipped":4454,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:23.85
    Jan 18 17:55:23.851: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 17:55:23.851
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:23.874
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:23.878
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-dae8b386-cb7b-4e47-9d82-7a663d477ac0 01/18/23 17:55:23.882
    STEP: Creating a pod to test consume secrets 01/18/23 17:55:23.891
    Jan 18 17:55:23.905: INFO: Waiting up to 5m0s for pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597" in namespace "secrets-4196" to be "Succeeded or Failed"
    Jan 18 17:55:23.911: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Pending", Reason="", readiness=false. Elapsed: 5.756359ms
    Jan 18 17:55:25.922: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017171412s
    Jan 18 17:55:27.918: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0131316s
    STEP: Saw pod success 01/18/23 17:55:27.918
    Jan 18 17:55:27.918: INFO: Pod "pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597" satisfied condition "Succeeded or Failed"
    Jan 18 17:55:27.926: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 17:55:27.941
    Jan 18 17:55:27.962: INFO: Waiting for pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 to disappear
    Jan 18 17:55:27.967: INFO: Pod pod-secrets-98b23cb0-add2-4af9-b412-bfbec2521597 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 17:55:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4196" for this suite. 01/18/23 17:55:27.976
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:27.989
Jan 18 17:55:27.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 17:55:27.99
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:28.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:28.017
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/18/23 17:55:28.021
STEP: Ensuring ResourceQuota status is calculated 01/18/23 17:55:28.031
STEP: Creating a ResourceQuota with not best effort scope 01/18/23 17:55:30.039
STEP: Ensuring ResourceQuota status is calculated 01/18/23 17:55:30.05
STEP: Creating a best-effort pod 01/18/23 17:55:32.057
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 17:55:32.083
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 17:55:34.092
STEP: Deleting the pod 01/18/23 17:55:36.102
STEP: Ensuring resource quota status released the pod usage 01/18/23 17:55:36.122
STEP: Creating a not best-effort pod 01/18/23 17:55:38.131
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 17:55:38.151
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 17:55:40.161
STEP: Deleting the pod 01/18/23 17:55:42.173
STEP: Ensuring resource quota status released the pod usage 01/18/23 17:55:42.197
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 17:55:44.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4565" for this suite. 01/18/23 17:55:44.214
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":229,"skipped":4462,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.239 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:27.989
    Jan 18 17:55:27.989: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 17:55:27.99
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:28.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:28.017
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/18/23 17:55:28.021
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 17:55:28.031
    STEP: Creating a ResourceQuota with not best effort scope 01/18/23 17:55:30.039
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 17:55:30.05
    STEP: Creating a best-effort pod 01/18/23 17:55:32.057
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/18/23 17:55:32.083
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/18/23 17:55:34.092
    STEP: Deleting the pod 01/18/23 17:55:36.102
    STEP: Ensuring resource quota status released the pod usage 01/18/23 17:55:36.122
    STEP: Creating a not best-effort pod 01/18/23 17:55:38.131
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/18/23 17:55:38.151
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/18/23 17:55:40.161
    STEP: Deleting the pod 01/18/23 17:55:42.173
    STEP: Ensuring resource quota status released the pod usage 01/18/23 17:55:42.197
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 17:55:44.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4565" for this suite. 01/18/23 17:55:44.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:44.229
Jan 18 17:55:44.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename namespaces 01/18/23 17:55:44.23
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:44.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:44.258
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/18/23 17:55:44.262
STEP: patching the Namespace 01/18/23 17:55:44.286
STEP: get the Namespace and ensuring it has the label 01/18/23 17:55:44.295
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:55:44.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9646" for this suite. 01/18/23 17:55:44.307
STEP: Destroying namespace "nspatchtest-3b043bd7-1184-42b3-9d1b-fab14857e818-2020" for this suite. 01/18/23 17:55:44.324
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":230,"skipped":4469,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:44.229
    Jan 18 17:55:44.229: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename namespaces 01/18/23 17:55:44.23
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:44.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:44.258
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/18/23 17:55:44.262
    STEP: patching the Namespace 01/18/23 17:55:44.286
    STEP: get the Namespace and ensuring it has the label 01/18/23 17:55:44.295
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:55:44.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9646" for this suite. 01/18/23 17:55:44.307
    STEP: Destroying namespace "nspatchtest-3b043bd7-1184-42b3-9d1b-fab14857e818-2020" for this suite. 01/18/23 17:55:44.324
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:44.342
Jan 18 17:55:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-pred 01/18/23 17:55:44.343
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:44.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:44.368
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 17:55:44.372: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 17:55:44.388: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 17:55:44.398: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
Jan 18 17:55:44.409: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 17:55:44.409: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 17:55:44.409: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 17:55:44.409: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 17:55:44.409: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 17:55:44.409: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 17:55:44.409: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
Jan 18 17:55:44.409: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:55:44.409: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 17:55:44.409: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
Jan 18 17:55:44.420: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 17:55:44.420: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 17:55:44.420: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container coredns ready: true, restart count 0
Jan 18 17:55:44.420: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 17:55:44.420: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 17:55:44.420: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 17:55:44.420: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 17:55:44.420: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 17:55:44.420: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 17:55:44.420: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 17:55:44.420: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container e2e ready: true, restart count 0
Jan 18 17:55:44.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:55:44.420: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 17:55:44.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 17:55:44.420: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 17:55:44.42
Jan 18 17:55:44.434: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8379" to be "running"
Jan 18 17:55:44.440: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004571ms
Jan 18 17:55:46.450: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016400427s
Jan 18 17:55:46.451: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 17:55:46.458
STEP: Trying to apply a random label on the found node. 01/18/23 17:55:46.477
STEP: verifying the node has the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 42 01/18/23 17:55:46.5
STEP: Trying to relaunch the pod, now with labels. 01/18/23 17:55:46.508
Jan 18 17:55:46.518: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8379" to be "not pending"
Jan 18 17:55:46.525: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352293ms
Jan 18 17:55:48.532: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014241692s
Jan 18 17:55:48.532: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 17:55:48.539
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 01/18/23 17:55:48.565
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 17:55:48.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8379" for this suite. 01/18/23 17:55:48.582
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":231,"skipped":4471,"failed":0}
------------------------------
â€¢ [4.252 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:44.342
    Jan 18 17:55:44.342: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 17:55:44.343
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:44.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:44.368
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 17:55:44.372: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 17:55:44.388: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 17:55:44.398: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
    Jan 18 17:55:44.409: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
    Jan 18 17:55:44.409: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 17:55:44.409: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
    Jan 18 17:55:44.420: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 17:55:44.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 17:55:44.420: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 17:55:44.42
    Jan 18 17:55:44.434: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8379" to be "running"
    Jan 18 17:55:44.440: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004571ms
    Jan 18 17:55:46.450: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.016400427s
    Jan 18 17:55:46.451: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 17:55:46.458
    STEP: Trying to apply a random label on the found node. 01/18/23 17:55:46.477
    STEP: verifying the node has the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 42 01/18/23 17:55:46.5
    STEP: Trying to relaunch the pod, now with labels. 01/18/23 17:55:46.508
    Jan 18 17:55:46.518: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8379" to be "not pending"
    Jan 18 17:55:46.525: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352293ms
    Jan 18 17:55:48.532: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.014241692s
    Jan 18 17:55:48.532: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 17:55:48.539
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-a8b79f37-d663-45c5-8cb9-9b6b6cf6c2e9 01/18/23 17:55:48.565
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 17:55:48.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8379" for this suite. 01/18/23 17:55:48.582
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:48.595
Jan 18 17:55:48.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename disruption 01/18/23 17:55:48.596
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:48.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:48.622
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 17:55:48.626
STEP: Waiting for the pdb to be processed 01/18/23 17:55:48.635
STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 17:55:50.662
STEP: Waiting for all pods to be running 01/18/23 17:55:50.662
Jan 18 17:55:50.670: INFO: pods: 0 < 3
STEP: locating a running pod 01/18/23 17:55:52.679
STEP: Updating the pdb to allow a pod to be evicted 01/18/23 17:55:52.697
STEP: Waiting for the pdb to be processed 01/18/23 17:55:52.711
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 17:55:54.728
STEP: Waiting for all pods to be running 01/18/23 17:55:54.728
STEP: Waiting for the pdb to observed all healthy pods 01/18/23 17:55:54.736
STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 17:55:54.78
STEP: Waiting for the pdb to be processed 01/18/23 17:55:54.797
STEP: Waiting for all pods to be running 01/18/23 17:55:56.811
STEP: locating a running pod 01/18/23 17:55:56.818
STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 17:55:56.837
STEP: Waiting for the pdb to be deleted 01/18/23 17:55:56.848
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 17:55:56.855
STEP: Waiting for all pods to be running 01/18/23 17:55:56.855
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 18 17:55:56.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2133" for this suite. 01/18/23 17:55:56.89
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":232,"skipped":4471,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.310 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:48.595
    Jan 18 17:55:48.595: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename disruption 01/18/23 17:55:48.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:48.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:48.622
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/18/23 17:55:48.626
    STEP: Waiting for the pdb to be processed 01/18/23 17:55:48.635
    STEP: First trying to evict a pod which shouldn't be evictable 01/18/23 17:55:50.662
    STEP: Waiting for all pods to be running 01/18/23 17:55:50.662
    Jan 18 17:55:50.670: INFO: pods: 0 < 3
    STEP: locating a running pod 01/18/23 17:55:52.679
    STEP: Updating the pdb to allow a pod to be evicted 01/18/23 17:55:52.697
    STEP: Waiting for the pdb to be processed 01/18/23 17:55:52.711
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 17:55:54.728
    STEP: Waiting for all pods to be running 01/18/23 17:55:54.728
    STEP: Waiting for the pdb to observed all healthy pods 01/18/23 17:55:54.736
    STEP: Patching the pdb to disallow a pod to be evicted 01/18/23 17:55:54.78
    STEP: Waiting for the pdb to be processed 01/18/23 17:55:54.797
    STEP: Waiting for all pods to be running 01/18/23 17:55:56.811
    STEP: locating a running pod 01/18/23 17:55:56.818
    STEP: Deleting the pdb to allow a pod to be evicted 01/18/23 17:55:56.837
    STEP: Waiting for the pdb to be deleted 01/18/23 17:55:56.848
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/18/23 17:55:56.855
    STEP: Waiting for all pods to be running 01/18/23 17:55:56.855
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 18 17:55:56.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2133" for this suite. 01/18/23 17:55:56.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:56.913
Jan 18 17:55:56.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:55:56.915
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:56.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:56.94
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/18/23 17:55:56.959
STEP: waiting for available Endpoint 01/18/23 17:55:56.967
STEP: listing all Endpoints 01/18/23 17:55:56.97
STEP: updating the Endpoint 01/18/23 17:55:56.978
STEP: fetching the Endpoint 01/18/23 17:55:56.989
STEP: patching the Endpoint 01/18/23 17:55:56.995
STEP: fetching the Endpoint 01/18/23 17:55:57.011
STEP: deleting the Endpoint by Collection 01/18/23 17:55:57.016
STEP: waiting for Endpoint deletion 01/18/23 17:55:57.031
STEP: fetching the Endpoint 01/18/23 17:55:57.033
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:55:57.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-441" for this suite. 01/18/23 17:55:57.047
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":233,"skipped":4546,"failed":0}
------------------------------
â€¢ [0.146 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:56.913
    Jan 18 17:55:56.914: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:55:56.915
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:56.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:56.94
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/18/23 17:55:56.959
    STEP: waiting for available Endpoint 01/18/23 17:55:56.967
    STEP: listing all Endpoints 01/18/23 17:55:56.97
    STEP: updating the Endpoint 01/18/23 17:55:56.978
    STEP: fetching the Endpoint 01/18/23 17:55:56.989
    STEP: patching the Endpoint 01/18/23 17:55:56.995
    STEP: fetching the Endpoint 01/18/23 17:55:57.011
    STEP: deleting the Endpoint by Collection 01/18/23 17:55:57.016
    STEP: waiting for Endpoint deletion 01/18/23 17:55:57.031
    STEP: fetching the Endpoint 01/18/23 17:55:57.033
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:55:57.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-441" for this suite. 01/18/23 17:55:57.047
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:55:57.062
Jan 18 17:55:57.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replication-controller 01/18/23 17:55:57.063
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:57.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:57.092
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/18/23 17:55:57.105
STEP: waiting for RC to be added 01/18/23 17:55:57.114
STEP: waiting for available Replicas 01/18/23 17:55:57.114
STEP: patching ReplicationController 01/18/23 17:55:58.678
STEP: waiting for RC to be modified 01/18/23 17:55:58.692
STEP: patching ReplicationController status 01/18/23 17:55:58.692
STEP: waiting for RC to be modified 01/18/23 17:55:58.705
STEP: waiting for available Replicas 01/18/23 17:55:58.705
STEP: fetching ReplicationController status 01/18/23 17:55:58.716
STEP: patching ReplicationController scale 01/18/23 17:55:58.724
STEP: waiting for RC to be modified 01/18/23 17:55:58.734
STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 17:55:58.735
STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 17:56:00.283
STEP: updating ReplicationController status 01/18/23 17:56:00.289
STEP: waiting for RC to be modified 01/18/23 17:56:00.302
STEP: listing all ReplicationControllers 01/18/23 17:56:00.302
STEP: checking that ReplicationController has expected values 01/18/23 17:56:00.309
STEP: deleting ReplicationControllers by collection 01/18/23 17:56:00.309
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 17:56:00.327
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 17:56:00.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8937" for this suite. 01/18/23 17:56:00.419
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":234,"skipped":4547,"failed":0}
------------------------------
â€¢ [3.369 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:55:57.062
    Jan 18 17:55:57.062: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 17:55:57.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:55:57.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:55:57.092
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/18/23 17:55:57.105
    STEP: waiting for RC to be added 01/18/23 17:55:57.114
    STEP: waiting for available Replicas 01/18/23 17:55:57.114
    STEP: patching ReplicationController 01/18/23 17:55:58.678
    STEP: waiting for RC to be modified 01/18/23 17:55:58.692
    STEP: patching ReplicationController status 01/18/23 17:55:58.692
    STEP: waiting for RC to be modified 01/18/23 17:55:58.705
    STEP: waiting for available Replicas 01/18/23 17:55:58.705
    STEP: fetching ReplicationController status 01/18/23 17:55:58.716
    STEP: patching ReplicationController scale 01/18/23 17:55:58.724
    STEP: waiting for RC to be modified 01/18/23 17:55:58.734
    STEP: waiting for ReplicationController's scale to be the max amount 01/18/23 17:55:58.735
    STEP: fetching ReplicationController; ensuring that it's patched 01/18/23 17:56:00.283
    STEP: updating ReplicationController status 01/18/23 17:56:00.289
    STEP: waiting for RC to be modified 01/18/23 17:56:00.302
    STEP: listing all ReplicationControllers 01/18/23 17:56:00.302
    STEP: checking that ReplicationController has expected values 01/18/23 17:56:00.309
    STEP: deleting ReplicationControllers by collection 01/18/23 17:56:00.309
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/18/23 17:56:00.327
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 17:56:00.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8937" for this suite. 01/18/23 17:56:00.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:00.433
Jan 18 17:56:00.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 17:56:00.434
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:00.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:00.462
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/18/23 17:56:00.473
STEP: waiting for Deployment to be created 01/18/23 17:56:00.483
STEP: waiting for all Replicas to be Ready 01/18/23 17:56:00.485
Jan 18 17:56:00.488: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.488: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.520: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.520: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.541: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.541: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.590: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:00.590: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 18 17:56:01.705: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 17:56:01.705: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 18 17:56:02.306: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/18/23 17:56:02.306
W0118 17:56:02.318255      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 17:56:02.320: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/18/23 17:56:02.32
Jan 18 17:56:02.323: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.323: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.359: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.359: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.387: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.387: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:02.473: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:02.473: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:02.569: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:02.569: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:03.775: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:03.775: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:03.810: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
STEP: listing Deployments 01/18/23 17:56:03.81
Jan 18 17:56:03.818: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/18/23 17:56:03.818
Jan 18 17:56:03.838: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/18/23 17:56:03.838
Jan 18 17:56:03.850: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:03.860: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:03.883: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:03.903: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:03.969: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:05.363: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:05.730: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:05.860: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:05.884: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 18 17:56:07.341: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/18/23 17:56:07.376
STEP: fetching the DeploymentStatus 01/18/23 17:56:07.389
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3
Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3
STEP: deleting the Deployment 01/18/23 17:56:07.399
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.421: INFO: observed event type MODIFIED
Jan 18 17:56:07.422: INFO: observed event type MODIFIED
Jan 18 17:56:07.422: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 17:56:07.429: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 18 17:56:07.438: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1642  56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 2631921981 4 2023-01-18 17:56:02 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 61ea8e07-03a7-4180-a13c-adab4073f377 0xc000d478c7 0xc000d478c8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ea8e07-03a7-4180-a13c-adab4073f377\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d47950 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan 18 17:56:07.446: INFO: pod: "test-deployment-54cc775c4b-dzgnf":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-dzgnf test-deployment-54cc775c4b- deployment-1642  2edb3e5e-2ede-4c74-acc4-d5d943703d36 2631921953 0 2023-01-18 17:56:02 +0000 UTC 2023-01-18 17:56:06 +0000 UTC 0xc005282058 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:bcd689375ff3eb8c8d71376784ddc3846213ed487105ad135f3c04b908d419fe cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 0xc0052820b7 0xc0052820b8}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4tzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4tzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.130,StartTime:2023-01-18 17:56:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://18d74e76cff0acb2ec768731f0b7365cfa8485bd4904ba8ec1e821da9ea0534d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 17:56:07.446: INFO: pod: "test-deployment-54cc775c4b-mtxsn":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-mtxsn test-deployment-54cc775c4b- deployment-1642  5b8224cd-7715-4064-8ac0-a31ea5a89253 2631921978 0 2023-01-18 17:56:03 +0000 UTC 2023-01-18 17:56:08 +0000 UTC 0xc005282500 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:61e4f6264cb65d808896fc1d0237f3fcced4e354aa4cd52ae87af61337e62f1f cni.projectcalico.org/podIP:10.100.158.8/32 cni.projectcalico.org/podIPs:10.100.158.8/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 0xc005282557 0xc005282558}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbtzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbtzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.8,StartTime:2023-01-18 17:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://f8ac723f68c684d7b4c6adaa704ad91791fa389097ac548681d3314d1717162f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 17:56:07.446: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1642  ef4187b8-6850-4c94-9653-bff0367f45ad 2631921975 2 2023-01-18 17:56:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 61ea8e07-03a7-4180-a13c-adab4073f377 0xc000d479c7 0xc000d479c8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ea8e07-03a7-4180-a13c-adab4073f377\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d47a50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 18 17:56:07.453: INFO: pod: "test-deployment-7c7d8d58c8-2xcjm":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2xcjm test-deployment-7c7d8d58c8- deployment-1642  d4ccdd43-2754-454d-bf7b-d34fa8602141 2631921972 0 2023-01-18 17:56:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:9a86a76a9bd91e04bdacb6efae87f6f44ac3783ff6fd82136c2f56849fb85235 cni.projectcalico.org/podIP:10.100.158.9/32 cni.projectcalico.org/podIPs:10.100.158.9/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 ef4187b8-6850-4c94-9653-bff0367f45ad 0xc000d47d17 0xc000d47d18}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4187b8-6850-4c94-9653-bff0367f45ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf2cg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf2cg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.9,StartTime:2023-01-18 17:56:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9837009b9deb8bde74868ea6e0ecd050186e117d38943b4e92d6c024e457f778,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 18 17:56:07.454: INFO: pod: "test-deployment-7c7d8d58c8-v42gw":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-v42gw test-deployment-7c7d8d58c8- deployment-1642  84ed8363-42f9-452f-9dcb-c71ca0f1e88a 2631921903 0 2023-01-18 17:56:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:a5064d62fae80de8e08e078df74bdf3943a17c11f36e8fcc659e5aaf08ef27e7 cni.projectcalico.org/podIP:10.100.145.131/32 cni.projectcalico.org/podIPs:10.100.145.131/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 ef4187b8-6850-4c94-9653-bff0367f45ad 0xc000d47f37 0xc000d47f38}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4187b8-6850-4c94-9653-bff0367f45ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4kjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4kjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.131,StartTime:2023-01-18 17:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7f461fce0e6b29da8232fb48dffe6eba419f08274f0a536dae19b52dd33b6c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 17:56:07.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1642" for this suite. 01/18/23 17:56:07.463
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":235,"skipped":4579,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.041 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:00.433
    Jan 18 17:56:00.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 17:56:00.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:00.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:00.462
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/18/23 17:56:00.473
    STEP: waiting for Deployment to be created 01/18/23 17:56:00.483
    STEP: waiting for all Replicas to be Ready 01/18/23 17:56:00.485
    Jan 18 17:56:00.488: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.488: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.520: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.520: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.541: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.541: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.590: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:00.590: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 18 17:56:01.705: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 17:56:01.705: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 18 17:56:02.306: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/18/23 17:56:02.306
    W0118 17:56:02.318255      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 17:56:02.320: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/18/23 17:56:02.32
    Jan 18 17:56:02.323: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.323: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 0
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.324: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.359: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.359: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.387: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.387: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:02.473: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:02.473: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:02.569: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:02.569: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:03.775: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:03.775: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:03.810: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    STEP: listing Deployments 01/18/23 17:56:03.81
    Jan 18 17:56:03.818: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/18/23 17:56:03.818
    Jan 18 17:56:03.838: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/18/23 17:56:03.838
    Jan 18 17:56:03.850: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:03.860: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:03.883: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:03.903: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:03.969: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:05.363: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:05.730: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:05.860: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:05.884: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 18 17:56:07.341: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/18/23 17:56:07.376
    STEP: fetching the DeploymentStatus 01/18/23 17:56:07.389
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 1
    Jan 18 17:56:07.398: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3
    Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 2
    Jan 18 17:56:07.399: INFO: observed Deployment test-deployment in namespace deployment-1642 with ReadyReplicas 3
    STEP: deleting the Deployment 01/18/23 17:56:07.399
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.421: INFO: observed event type MODIFIED
    Jan 18 17:56:07.422: INFO: observed event type MODIFIED
    Jan 18 17:56:07.422: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 17:56:07.429: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 18 17:56:07.438: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-1642  56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 2631921981 4 2023-01-18 17:56:02 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 61ea8e07-03a7-4180-a13c-adab4073f377 0xc000d478c7 0xc000d478c8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ea8e07-03a7-4180-a13c-adab4073f377\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d47950 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 17:56:07.446: INFO: pod: "test-deployment-54cc775c4b-dzgnf":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-dzgnf test-deployment-54cc775c4b- deployment-1642  2edb3e5e-2ede-4c74-acc4-d5d943703d36 2631921953 0 2023-01-18 17:56:02 +0000 UTC 2023-01-18 17:56:06 +0000 UTC 0xc005282058 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:bcd689375ff3eb8c8d71376784ddc3846213ed487105ad135f3c04b908d419fe cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 0xc0052820b7 0xc0052820b8}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-01-18 17:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h4tzf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h4tzf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.130,StartTime:2023-01-18 17:56:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://18d74e76cff0acb2ec768731f0b7365cfa8485bd4904ba8ec1e821da9ea0534d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.130,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 17:56:07.446: INFO: pod: "test-deployment-54cc775c4b-mtxsn":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-mtxsn test-deployment-54cc775c4b- deployment-1642  5b8224cd-7715-4064-8ac0-a31ea5a89253 2631921978 0 2023-01-18 17:56:03 +0000 UTC 2023-01-18 17:56:08 +0000 UTC 0xc005282500 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:61e4f6264cb65d808896fc1d0237f3fcced4e354aa4cd52ae87af61337e62f1f cni.projectcalico.org/podIP:10.100.158.8/32 cni.projectcalico.org/podIPs:10.100.158.8/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f 0xc005282557 0xc005282558}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56c27b89-4e9e-4b0a-8cd6-1bd97da4e93f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wbtzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wbtzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.8,StartTime:2023-01-18 17:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://f8ac723f68c684d7b4c6adaa704ad91791fa389097ac548681d3314d1717162f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 17:56:07.446: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-1642  ef4187b8-6850-4c94-9653-bff0367f45ad 2631921975 2 2023-01-18 17:56:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 61ea8e07-03a7-4180-a13c-adab4073f377 0xc000d479c7 0xc000d479c8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61ea8e07-03a7-4180-a13c-adab4073f377\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d47a50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 18 17:56:07.453: INFO: pod: "test-deployment-7c7d8d58c8-2xcjm":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-2xcjm test-deployment-7c7d8d58c8- deployment-1642  d4ccdd43-2754-454d-bf7b-d34fa8602141 2631921972 0 2023-01-18 17:56:05 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:9a86a76a9bd91e04bdacb6efae87f6f44ac3783ff6fd82136c2f56849fb85235 cni.projectcalico.org/podIP:10.100.158.9/32 cni.projectcalico.org/podIPs:10.100.158.9/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 ef4187b8-6850-4c94-9653-bff0367f45ad 0xc000d47d17 0xc000d47d18}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4187b8-6850-4c94-9653-bff0367f45ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.158.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kf2cg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kf2cg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-788643c0f7cd4128a5c,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.78.23,PodIP:10.100.158.9,StartTime:2023-01-18 17:56:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://9837009b9deb8bde74868ea6e0ecd050186e117d38943b4e92d6c024e457f778,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.158.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 18 17:56:07.454: INFO: pod: "test-deployment-7c7d8d58c8-v42gw":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-v42gw test-deployment-7c7d8d58c8- deployment-1642  84ed8363-42f9-452f-9dcb-c71ca0f1e88a 2631921903 0 2023-01-18 17:56:03 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:a5064d62fae80de8e08e078df74bdf3943a17c11f36e8fcc659e5aaf08ef27e7 cni.projectcalico.org/podIP:10.100.145.131/32 cni.projectcalico.org/podIPs:10.100.145.131/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 ef4187b8-6850-4c94-9653-bff0367f45ad 0xc000d47f37 0xc000d47f38}] [] [{kube-controller-manager Update v1 2023-01-18 17:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ef4187b8-6850-4c94-9653-bff0367f45ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 17:56:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 17:56:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r4kjh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r4kjh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 17:56:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.131,StartTime:2023-01-18 17:56:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 17:56:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://a7f461fce0e6b29da8232fb48dffe6eba419f08274f0a536dae19b52dd33b6c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.131,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 17:56:07.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1642" for this suite. 01/18/23 17:56:07.463
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:07.476
Jan 18 17:56:07.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 17:56:07.477
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:07.5
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:07.505
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 17:56:07.533
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:56:08.009
STEP: Deploying the webhook pod 01/18/23 17:56:08.025
STEP: Wait for the deployment to be ready 01/18/23 17:56:08.047
Jan 18 17:56:08.058: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/18/23 17:56:10.098
STEP: Verifying the service has paired with the endpoint 01/18/23 17:56:10.156
Jan 18 17:56:11.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 17:56:11.165
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:11.165
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 17:56:11.195
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 17:56:12.266
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:12.266
STEP: Having no error when timeout is longer than webhook latency 01/18/23 17:56:13.408
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:13.408
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 17:56:18.481
STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:18.481
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 17:56:23.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2771" for this suite. 01/18/23 17:56:23.553
STEP: Destroying namespace "webhook-2771-markers" for this suite. 01/18/23 17:56:23.566
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":236,"skipped":4581,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.177 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:07.476
    Jan 18 17:56:07.476: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 17:56:07.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:07.5
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:07.505
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 17:56:07.533
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 17:56:08.009
    STEP: Deploying the webhook pod 01/18/23 17:56:08.025
    STEP: Wait for the deployment to be ready 01/18/23 17:56:08.047
    Jan 18 17:56:08.058: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/18/23 17:56:10.098
    STEP: Verifying the service has paired with the endpoint 01/18/23 17:56:10.156
    Jan 18 17:56:11.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/18/23 17:56:11.165
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:11.165
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/18/23 17:56:11.195
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/18/23 17:56:12.266
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:12.266
    STEP: Having no error when timeout is longer than webhook latency 01/18/23 17:56:13.408
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:13.408
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/18/23 17:56:18.481
    STEP: Registering slow webhook via the AdmissionRegistration API 01/18/23 17:56:18.481
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 17:56:23.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2771" for this suite. 01/18/23 17:56:23.553
    STEP: Destroying namespace "webhook-2771-markers" for this suite. 01/18/23 17:56:23.566
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:23.654
Jan 18 17:56:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:56:23.655
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:23.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:23.686
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/18/23 17:56:23.69
STEP: setting up watch 01/18/23 17:56:23.69
STEP: submitting the pod to kubernetes 01/18/23 17:56:23.797
STEP: verifying the pod is in kubernetes 01/18/23 17:56:23.818
STEP: verifying pod creation was observed 01/18/23 17:56:23.823
Jan 18 17:56:23.823: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c" in namespace "pods-4816" to be "running"
Jan 18 17:56:23.829: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84439ms
Jan 18 17:56:25.839: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015580914s
Jan 18 17:56:25.839: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c" satisfied condition "running"
STEP: deleting the pod gracefully 01/18/23 17:56:25.846
STEP: verifying pod deletion was observed 01/18/23 17:56:25.861
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:56:28.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4816" for this suite. 01/18/23 17:56:28.825
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":237,"skipped":4589,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.182 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:23.654
    Jan 18 17:56:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:56:23.655
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:23.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:23.686
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/18/23 17:56:23.69
    STEP: setting up watch 01/18/23 17:56:23.69
    STEP: submitting the pod to kubernetes 01/18/23 17:56:23.797
    STEP: verifying the pod is in kubernetes 01/18/23 17:56:23.818
    STEP: verifying pod creation was observed 01/18/23 17:56:23.823
    Jan 18 17:56:23.823: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c" in namespace "pods-4816" to be "running"
    Jan 18 17:56:23.829: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.84439ms
    Jan 18 17:56:25.839: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c": Phase="Running", Reason="", readiness=true. Elapsed: 2.015580914s
    Jan 18 17:56:25.839: INFO: Pod "pod-submit-remove-e28eacc6-fc9c-4cd8-95d0-b554cadb219c" satisfied condition "running"
    STEP: deleting the pod gracefully 01/18/23 17:56:25.846
    STEP: verifying pod deletion was observed 01/18/23 17:56:25.861
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:56:28.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4816" for this suite. 01/18/23 17:56:28.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:28.837
Jan 18 17:56:28.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:56:28.838
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:28.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:28.877
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:56:28.881
Jan 18 17:56:28.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141" in namespace "projected-7750" to be "Succeeded or Failed"
Jan 18 17:56:28.902: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741762ms
Jan 18 17:56:30.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Running", Reason="", readiness=false. Elapsed: 2.011119313s
Jan 18 17:56:32.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011418281s
STEP: Saw pod success 01/18/23 17:56:32.908
Jan 18 17:56:32.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141" satisfied condition "Succeeded or Failed"
Jan 18 17:56:32.916: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 container client-container: <nil>
STEP: delete the pod 01/18/23 17:56:32.931
Jan 18 17:56:32.952: INFO: Waiting for pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 to disappear
Jan 18 17:56:32.958: INFO: Pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:56:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7750" for this suite. 01/18/23 17:56:32.966
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":238,"skipped":4609,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:28.837
    Jan 18 17:56:28.837: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:56:28.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:28.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:28.877
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:56:28.881
    Jan 18 17:56:28.897: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141" in namespace "projected-7750" to be "Succeeded or Failed"
    Jan 18 17:56:28.902: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741762ms
    Jan 18 17:56:30.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Running", Reason="", readiness=false. Elapsed: 2.011119313s
    Jan 18 17:56:32.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011418281s
    STEP: Saw pod success 01/18/23 17:56:32.908
    Jan 18 17:56:32.908: INFO: Pod "downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141" satisfied condition "Succeeded or Failed"
    Jan 18 17:56:32.916: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:56:32.931
    Jan 18 17:56:32.952: INFO: Waiting for pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 to disappear
    Jan 18 17:56:32.958: INFO: Pod downwardapi-volume-4ce85657-9976-4711-a253-1e9df383b141 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:56:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7750" for this suite. 01/18/23 17:56:32.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:32.98
Jan 18 17:56:32.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 17:56:32.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:33.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:33.008
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/18/23 17:56:33.031
Jan 18 17:56:33.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/18/23 17:56:35.049
Jan 18 17:56:35.064: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/18/23 17:56:37.074
Jan 18 17:56:37.096: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 18 17:56:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-400" for this suite. 01/18/23 17:56:39.113
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":239,"skipped":4625,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.148 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:32.98
    Jan 18 17:56:32.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename endpointslicemirroring 01/18/23 17:56:32.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:33.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:33.008
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/18/23 17:56:33.031
    Jan 18 17:56:33.043: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/18/23 17:56:35.049
    Jan 18 17:56:35.064: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/18/23 17:56:37.074
    Jan 18 17:56:37.096: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 18 17:56:39.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-400" for this suite. 01/18/23 17:56:39.113
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:39.128
Jan 18 17:56:39.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:56:39.129
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:39.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:39.161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:56:39.166
Jan 18 17:56:39.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595" in namespace "projected-4008" to be "Succeeded or Failed"
Jan 18 17:56:39.191: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Pending", Reason="", readiness=false. Elapsed: 7.232369ms
Jan 18 17:56:41.205: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0217707s
Jan 18 17:56:43.200: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016325647s
STEP: Saw pod success 01/18/23 17:56:43.2
Jan 18 17:56:43.200: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595" satisfied condition "Succeeded or Failed"
Jan 18 17:56:43.209: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 container client-container: <nil>
STEP: delete the pod 01/18/23 17:56:43.222
Jan 18 17:56:43.245: INFO: Waiting for pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 to disappear
Jan 18 17:56:43.253: INFO: Pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:56:43.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4008" for this suite. 01/18/23 17:56:43.262
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":240,"skipped":4625,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:39.128
    Jan 18 17:56:39.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:56:39.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:39.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:39.161
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:56:39.166
    Jan 18 17:56:39.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595" in namespace "projected-4008" to be "Succeeded or Failed"
    Jan 18 17:56:39.191: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Pending", Reason="", readiness=false. Elapsed: 7.232369ms
    Jan 18 17:56:41.205: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0217707s
    Jan 18 17:56:43.200: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016325647s
    STEP: Saw pod success 01/18/23 17:56:43.2
    Jan 18 17:56:43.200: INFO: Pod "downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595" satisfied condition "Succeeded or Failed"
    Jan 18 17:56:43.209: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:56:43.222
    Jan 18 17:56:43.245: INFO: Waiting for pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 to disappear
    Jan 18 17:56:43.253: INFO: Pod downwardapi-volume-27d5f503-c068-4a3e-86ac-5a3866213595 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:56:43.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4008" for this suite. 01/18/23 17:56:43.262
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:43.279
Jan 18 17:56:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename discovery 01/18/23 17:56:43.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:43.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:43.304
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/18/23 17:56:43.31
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 18 17:56:43.502: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 18 17:56:43.505: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 18 17:56:43.505: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 18 17:56:43.505: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 18 17:56:43.505: INFO: Checking APIGroup: apps
Jan 18 17:56:43.506: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 18 17:56:43.506: INFO: Versions found [{apps/v1 v1}]
Jan 18 17:56:43.506: INFO: apps/v1 matches apps/v1
Jan 18 17:56:43.506: INFO: Checking APIGroup: events.k8s.io
Jan 18 17:56:43.508: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 18 17:56:43.508: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 18 17:56:43.508: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 18 17:56:43.508: INFO: Checking APIGroup: authentication.k8s.io
Jan 18 17:56:43.511: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 18 17:56:43.511: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 18 17:56:43.511: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 18 17:56:43.511: INFO: Checking APIGroup: authorization.k8s.io
Jan 18 17:56:43.513: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 18 17:56:43.513: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 18 17:56:43.513: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 18 17:56:43.513: INFO: Checking APIGroup: autoscaling
Jan 18 17:56:43.514: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 18 17:56:43.514: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 18 17:56:43.514: INFO: autoscaling/v2 matches autoscaling/v2
Jan 18 17:56:43.514: INFO: Checking APIGroup: batch
Jan 18 17:56:43.516: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 18 17:56:43.516: INFO: Versions found [{batch/v1 v1}]
Jan 18 17:56:43.516: INFO: batch/v1 matches batch/v1
Jan 18 17:56:43.516: INFO: Checking APIGroup: certificates.k8s.io
Jan 18 17:56:43.518: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 18 17:56:43.518: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 18 17:56:43.518: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 18 17:56:43.518: INFO: Checking APIGroup: networking.k8s.io
Jan 18 17:56:43.520: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 18 17:56:43.520: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
Jan 18 17:56:43.520: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 18 17:56:43.520: INFO: Checking APIGroup: policy
Jan 18 17:56:43.522: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 18 17:56:43.522: INFO: Versions found [{policy/v1 v1}]
Jan 18 17:56:43.522: INFO: policy/v1 matches policy/v1
Jan 18 17:56:43.522: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 18 17:56:43.524: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 18 17:56:43.524: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 18 17:56:43.524: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 18 17:56:43.524: INFO: Checking APIGroup: storage.k8s.io
Jan 18 17:56:43.526: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 18 17:56:43.526: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 18 17:56:43.526: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 18 17:56:43.526: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 18 17:56:43.527: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 18 17:56:43.527: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 18 17:56:43.527: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 18 17:56:43.527: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 18 17:56:43.529: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 18 17:56:43.529: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 18 17:56:43.529: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 18 17:56:43.529: INFO: Checking APIGroup: scheduling.k8s.io
Jan 18 17:56:43.531: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 18 17:56:43.531: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 18 17:56:43.531: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 18 17:56:43.531: INFO: Checking APIGroup: coordination.k8s.io
Jan 18 17:56:43.532: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 18 17:56:43.532: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 18 17:56:43.532: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 18 17:56:43.532: INFO: Checking APIGroup: node.k8s.io
Jan 18 17:56:43.534: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 18 17:56:43.534: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 18 17:56:43.534: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 18 17:56:43.534: INFO: Checking APIGroup: discovery.k8s.io
Jan 18 17:56:43.536: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 18 17:56:43.536: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 18 17:56:43.536: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 18 17:56:43.536: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 18 17:56:43.538: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 17:56:43.538: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 18 17:56:43.538: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 18 17:56:43.538: INFO: Checking APIGroup: internal.apiserver.k8s.io
Jan 18 17:56:43.540: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
Jan 18 17:56:43.540: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
Jan 18 17:56:43.540: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
Jan 18 17:56:43.540: INFO: Checking APIGroup: crd.projectcalico.org
Jan 18 17:56:43.542: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jan 18 17:56:43.542: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jan 18 17:56:43.542: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jan 18 17:56:43.542: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 18 17:56:43.544: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 18 17:56:43.544: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan 18 17:56:43.544: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 18 17:56:43.544: INFO: Checking APIGroup: metrics.k8s.io
Jan 18 17:56:43.546: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 18 17:56:43.546: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 18 17:56:43.546: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 18 17:56:43.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-8915" for this suite. 01/18/23 17:56:43.553
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":241,"skipped":4625,"failed":0}
------------------------------
â€¢ [0.288 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:43.279
    Jan 18 17:56:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename discovery 01/18/23 17:56:43.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:43.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:43.304
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/18/23 17:56:43.31
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 18 17:56:43.502: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 18 17:56:43.505: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 18 17:56:43.505: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 18 17:56:43.505: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 18 17:56:43.505: INFO: Checking APIGroup: apps
    Jan 18 17:56:43.506: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 18 17:56:43.506: INFO: Versions found [{apps/v1 v1}]
    Jan 18 17:56:43.506: INFO: apps/v1 matches apps/v1
    Jan 18 17:56:43.506: INFO: Checking APIGroup: events.k8s.io
    Jan 18 17:56:43.508: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 18 17:56:43.508: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 18 17:56:43.508: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 18 17:56:43.508: INFO: Checking APIGroup: authentication.k8s.io
    Jan 18 17:56:43.511: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 18 17:56:43.511: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 18 17:56:43.511: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 18 17:56:43.511: INFO: Checking APIGroup: authorization.k8s.io
    Jan 18 17:56:43.513: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 18 17:56:43.513: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 18 17:56:43.513: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 18 17:56:43.513: INFO: Checking APIGroup: autoscaling
    Jan 18 17:56:43.514: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 18 17:56:43.514: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 18 17:56:43.514: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 18 17:56:43.514: INFO: Checking APIGroup: batch
    Jan 18 17:56:43.516: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 18 17:56:43.516: INFO: Versions found [{batch/v1 v1}]
    Jan 18 17:56:43.516: INFO: batch/v1 matches batch/v1
    Jan 18 17:56:43.516: INFO: Checking APIGroup: certificates.k8s.io
    Jan 18 17:56:43.518: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 18 17:56:43.518: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 18 17:56:43.518: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 18 17:56:43.518: INFO: Checking APIGroup: networking.k8s.io
    Jan 18 17:56:43.520: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 18 17:56:43.520: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1alpha1 v1alpha1}]
    Jan 18 17:56:43.520: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 18 17:56:43.520: INFO: Checking APIGroup: policy
    Jan 18 17:56:43.522: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 18 17:56:43.522: INFO: Versions found [{policy/v1 v1}]
    Jan 18 17:56:43.522: INFO: policy/v1 matches policy/v1
    Jan 18 17:56:43.522: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 18 17:56:43.524: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 18 17:56:43.524: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 18 17:56:43.524: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 18 17:56:43.524: INFO: Checking APIGroup: storage.k8s.io
    Jan 18 17:56:43.526: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 18 17:56:43.526: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 17:56:43.526: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 18 17:56:43.526: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 18 17:56:43.527: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 18 17:56:43.527: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 18 17:56:43.527: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 18 17:56:43.527: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 18 17:56:43.529: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 18 17:56:43.529: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 18 17:56:43.529: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 18 17:56:43.529: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 18 17:56:43.531: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 18 17:56:43.531: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 18 17:56:43.531: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 18 17:56:43.531: INFO: Checking APIGroup: coordination.k8s.io
    Jan 18 17:56:43.532: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 18 17:56:43.532: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 18 17:56:43.532: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 18 17:56:43.532: INFO: Checking APIGroup: node.k8s.io
    Jan 18 17:56:43.534: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 18 17:56:43.534: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 18 17:56:43.534: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 18 17:56:43.534: INFO: Checking APIGroup: discovery.k8s.io
    Jan 18 17:56:43.536: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 18 17:56:43.536: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 18 17:56:43.536: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 18 17:56:43.536: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 18 17:56:43.538: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 17:56:43.538: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 18 17:56:43.538: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 18 17:56:43.538: INFO: Checking APIGroup: internal.apiserver.k8s.io
    Jan 18 17:56:43.540: INFO: PreferredVersion.GroupVersion: internal.apiserver.k8s.io/v1alpha1
    Jan 18 17:56:43.540: INFO: Versions found [{internal.apiserver.k8s.io/v1alpha1 v1alpha1}]
    Jan 18 17:56:43.540: INFO: internal.apiserver.k8s.io/v1alpha1 matches internal.apiserver.k8s.io/v1alpha1
    Jan 18 17:56:43.540: INFO: Checking APIGroup: crd.projectcalico.org
    Jan 18 17:56:43.542: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jan 18 17:56:43.542: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jan 18 17:56:43.542: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jan 18 17:56:43.542: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 18 17:56:43.544: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 18 17:56:43.544: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan 18 17:56:43.544: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 18 17:56:43.544: INFO: Checking APIGroup: metrics.k8s.io
    Jan 18 17:56:43.546: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 18 17:56:43.546: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 18 17:56:43.546: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 18 17:56:43.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-8915" for this suite. 01/18/23 17:56:43.553
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:43.57
Jan 18 17:56:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 17:56:43.571
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:43.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:43.603
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 17:56:43.607
Jan 18 17:56:43.626: INFO: Waiting up to 5m0s for pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0" in namespace "emptydir-655" to be "Succeeded or Failed"
Jan 18 17:56:43.631: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140894ms
Jan 18 17:56:45.639: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013223787s
Jan 18 17:56:47.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013810602s
Jan 18 17:56:49.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013987923s
STEP: Saw pod success 01/18/23 17:56:49.64
Jan 18 17:56:49.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0" satisfied condition "Succeeded or Failed"
Jan 18 17:56:49.647: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 container test-container: <nil>
STEP: delete the pod 01/18/23 17:56:49.666
Jan 18 17:56:49.688: INFO: Waiting for pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 to disappear
Jan 18 17:56:49.694: INFO: Pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 17:56:49.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-655" for this suite. 01/18/23 17:56:49.701
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":242,"skipped":4639,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.145 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:43.57
    Jan 18 17:56:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 17:56:43.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:43.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:43.603
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 17:56:43.607
    Jan 18 17:56:43.626: INFO: Waiting up to 5m0s for pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0" in namespace "emptydir-655" to be "Succeeded or Failed"
    Jan 18 17:56:43.631: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.140894ms
    Jan 18 17:56:45.639: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Running", Reason="", readiness=true. Elapsed: 2.013223787s
    Jan 18 17:56:47.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013810602s
    Jan 18 17:56:49.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013987923s
    STEP: Saw pod success 01/18/23 17:56:49.64
    Jan 18 17:56:49.640: INFO: Pod "pod-f4c340c1-54b0-4f5b-9757-7594e47367b0" satisfied condition "Succeeded or Failed"
    Jan 18 17:56:49.647: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 container test-container: <nil>
    STEP: delete the pod 01/18/23 17:56:49.666
    Jan 18 17:56:49.688: INFO: Waiting for pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 to disappear
    Jan 18 17:56:49.694: INFO: Pod pod-f4c340c1-54b0-4f5b-9757-7594e47367b0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 17:56:49.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-655" for this suite. 01/18/23 17:56:49.701
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:56:49.716
Jan 18 17:56:49.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename subpath 01/18/23 17:56:49.717
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:49.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:49.747
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 17:56:49.753
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-s5fk 01/18/23 17:56:49.774
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:56:49.774
Jan 18 17:56:49.791: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s5fk" in namespace "subpath-8344" to be "Succeeded or Failed"
Jan 18 17:56:49.797: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.331932ms
Jan 18 17:56:51.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014358855s
Jan 18 17:56:53.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 4.014492282s
Jan 18 17:56:55.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 6.01568974s
Jan 18 17:56:57.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014583729s
Jan 18 17:56:59.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 10.014407664s
Jan 18 17:57:01.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 12.015706539s
Jan 18 17:57:03.804: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 14.013186646s
Jan 18 17:57:05.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 16.015928409s
Jan 18 17:57:07.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 18.015942542s
Jan 18 17:57:09.824: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 20.032918707s
Jan 18 17:57:11.808: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=false. Elapsed: 22.017244555s
Jan 18 17:57:13.806: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015016155s
STEP: Saw pod success 01/18/23 17:57:13.806
Jan 18 17:57:13.806: INFO: Pod "pod-subpath-test-projected-s5fk" satisfied condition "Succeeded or Failed"
Jan 18 17:57:13.813: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-projected-s5fk container test-container-subpath-projected-s5fk: <nil>
STEP: delete the pod 01/18/23 17:57:13.828
Jan 18 17:57:13.852: INFO: Waiting for pod pod-subpath-test-projected-s5fk to disappear
Jan 18 17:57:13.859: INFO: Pod pod-subpath-test-projected-s5fk no longer exists
STEP: Deleting pod pod-subpath-test-projected-s5fk 01/18/23 17:57:13.859
Jan 18 17:57:13.859: INFO: Deleting pod "pod-subpath-test-projected-s5fk" in namespace "subpath-8344"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 17:57:13.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8344" for this suite. 01/18/23 17:57:13.875
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":243,"skipped":4641,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.173 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:56:49.716
    Jan 18 17:56:49.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename subpath 01/18/23 17:56:49.717
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:56:49.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:56:49.747
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 17:56:49.753
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-s5fk 01/18/23 17:56:49.774
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 17:56:49.774
    Jan 18 17:56:49.791: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-s5fk" in namespace "subpath-8344" to be "Succeeded or Failed"
    Jan 18 17:56:49.797: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.331932ms
    Jan 18 17:56:51.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 2.014358855s
    Jan 18 17:56:53.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 4.014492282s
    Jan 18 17:56:55.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 6.01568974s
    Jan 18 17:56:57.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014583729s
    Jan 18 17:56:59.805: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 10.014407664s
    Jan 18 17:57:01.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 12.015706539s
    Jan 18 17:57:03.804: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 14.013186646s
    Jan 18 17:57:05.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 16.015928409s
    Jan 18 17:57:07.807: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 18.015942542s
    Jan 18 17:57:09.824: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=true. Elapsed: 20.032918707s
    Jan 18 17:57:11.808: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Running", Reason="", readiness=false. Elapsed: 22.017244555s
    Jan 18 17:57:13.806: INFO: Pod "pod-subpath-test-projected-s5fk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015016155s
    STEP: Saw pod success 01/18/23 17:57:13.806
    Jan 18 17:57:13.806: INFO: Pod "pod-subpath-test-projected-s5fk" satisfied condition "Succeeded or Failed"
    Jan 18 17:57:13.813: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-projected-s5fk container test-container-subpath-projected-s5fk: <nil>
    STEP: delete the pod 01/18/23 17:57:13.828
    Jan 18 17:57:13.852: INFO: Waiting for pod pod-subpath-test-projected-s5fk to disappear
    Jan 18 17:57:13.859: INFO: Pod pod-subpath-test-projected-s5fk no longer exists
    STEP: Deleting pod pod-subpath-test-projected-s5fk 01/18/23 17:57:13.859
    Jan 18 17:57:13.859: INFO: Deleting pod "pod-subpath-test-projected-s5fk" in namespace "subpath-8344"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 17:57:13.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8344" for this suite. 01/18/23 17:57:13.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:13.891
Jan 18 17:57:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:57:13.892
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:13.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:13.922
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/18/23 17:57:13.926
Jan 18 17:57:13.941: INFO: Waiting up to 5m0s for pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a" in namespace "svcaccounts-6009" to be "Succeeded or Failed"
Jan 18 17:57:13.951: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.717326ms
Jan 18 17:57:15.962: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Running", Reason="", readiness=false. Elapsed: 2.020591321s
Jan 18 17:57:17.962: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Running", Reason="", readiness=false. Elapsed: 4.021100155s
Jan 18 17:57:19.961: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01920022s
STEP: Saw pod success 01/18/23 17:57:19.961
Jan 18 17:57:19.961: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a" satisfied condition "Succeeded or Failed"
Jan 18 17:57:19.968: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:57:19.982
Jan 18 17:57:20.003: INFO: Waiting for pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a to disappear
Jan 18 17:57:20.009: INFO: Pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 17:57:20.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6009" for this suite. 01/18/23 17:57:20.017
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":244,"skipped":4667,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.139 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:13.891
    Jan 18 17:57:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 17:57:13.892
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:13.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:13.922
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/18/23 17:57:13.926
    Jan 18 17:57:13.941: INFO: Waiting up to 5m0s for pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a" in namespace "svcaccounts-6009" to be "Succeeded or Failed"
    Jan 18 17:57:13.951: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.717326ms
    Jan 18 17:57:15.962: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Running", Reason="", readiness=false. Elapsed: 2.020591321s
    Jan 18 17:57:17.962: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Running", Reason="", readiness=false. Elapsed: 4.021100155s
    Jan 18 17:57:19.961: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01920022s
    STEP: Saw pod success 01/18/23 17:57:19.961
    Jan 18 17:57:19.961: INFO: Pod "test-pod-a46fbda4-1a50-4791-823a-13d1318b482a" satisfied condition "Succeeded or Failed"
    Jan 18 17:57:19.968: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:57:19.982
    Jan 18 17:57:20.003: INFO: Waiting for pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a to disappear
    Jan 18 17:57:20.009: INFO: Pod test-pod-a46fbda4-1a50-4791-823a-13d1318b482a no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 17:57:20.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6009" for this suite. 01/18/23 17:57:20.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:20.033
Jan 18 17:57:20.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:57:20.035
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:20.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:20.073
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/18/23 17:57:20.078
Jan 18 17:57:20.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739" in namespace "projected-224" to be "Succeeded or Failed"
Jan 18 17:57:20.097: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986579ms
Jan 18 17:57:22.107: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01451565s
Jan 18 17:57:24.105: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012279243s
STEP: Saw pod success 01/18/23 17:57:24.105
Jan 18 17:57:24.105: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739" satisfied condition "Succeeded or Failed"
Jan 18 17:57:24.113: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 container client-container: <nil>
STEP: delete the pod 01/18/23 17:57:24.126
Jan 18 17:57:24.147: INFO: Waiting for pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 to disappear
Jan 18 17:57:24.153: INFO: Pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 17:57:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-224" for this suite. 01/18/23 17:57:24.161
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":245,"skipped":4713,"failed":0}
------------------------------
â€¢ [4.144 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:20.033
    Jan 18 17:57:20.034: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:57:20.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:20.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:20.073
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/18/23 17:57:20.078
    Jan 18 17:57:20.092: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739" in namespace "projected-224" to be "Succeeded or Failed"
    Jan 18 17:57:20.097: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Pending", Reason="", readiness=false. Elapsed: 4.986579ms
    Jan 18 17:57:22.107: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01451565s
    Jan 18 17:57:24.105: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012279243s
    STEP: Saw pod success 01/18/23 17:57:24.105
    Jan 18 17:57:24.105: INFO: Pod "downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739" satisfied condition "Succeeded or Failed"
    Jan 18 17:57:24.113: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 container client-container: <nil>
    STEP: delete the pod 01/18/23 17:57:24.126
    Jan 18 17:57:24.147: INFO: Waiting for pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 to disappear
    Jan 18 17:57:24.153: INFO: Pod downwardapi-volume-6629ae10-96d4-4715-a6cf-3ab1b621f739 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 17:57:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-224" for this suite. 01/18/23 17:57:24.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:24.18
Jan 18 17:57:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 17:57:24.181
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:24.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:24.208
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/18/23 17:57:24.213
Jan 18 17:57:24.226: INFO: Waiting up to 5m0s for pod "pod-nml96" in namespace "pods-4760" to be "running"
Jan 18 17:57:24.234: INFO: Pod "pod-nml96": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026868ms
Jan 18 17:57:26.246: INFO: Pod "pod-nml96": Phase="Running", Reason="", readiness=true. Elapsed: 2.019750922s
Jan 18 17:57:26.246: INFO: Pod "pod-nml96" satisfied condition "running"
STEP: patching /status 01/18/23 17:57:26.246
Jan 18 17:57:26.260: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 17:57:26.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4760" for this suite. 01/18/23 17:57:26.267
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":246,"skipped":4754,"failed":0}
------------------------------
â€¢ [2.098 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:24.18
    Jan 18 17:57:24.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 17:57:24.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:24.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:24.208
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/18/23 17:57:24.213
    Jan 18 17:57:24.226: INFO: Waiting up to 5m0s for pod "pod-nml96" in namespace "pods-4760" to be "running"
    Jan 18 17:57:24.234: INFO: Pod "pod-nml96": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026868ms
    Jan 18 17:57:26.246: INFO: Pod "pod-nml96": Phase="Running", Reason="", readiness=true. Elapsed: 2.019750922s
    Jan 18 17:57:26.246: INFO: Pod "pod-nml96" satisfied condition "running"
    STEP: patching /status 01/18/23 17:57:26.246
    Jan 18 17:57:26.260: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 17:57:26.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4760" for this suite. 01/18/23 17:57:26.267
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:26.278
Jan 18 17:57:26.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 17:57:26.28
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:26.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:26.309
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-52eb0ceb-35eb-4e8c-96b0-956f651d2204 01/18/23 17:57:26.314
STEP: Creating a pod to test consume configMaps 01/18/23 17:57:26.324
Jan 18 17:57:26.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae" in namespace "projected-4423" to be "Succeeded or Failed"
Jan 18 17:57:26.354: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.720606ms
Jan 18 17:57:28.362: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015586001s
Jan 18 17:57:30.362: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016340702s
STEP: Saw pod success 01/18/23 17:57:30.362
Jan 18 17:57:30.363: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae" satisfied condition "Succeeded or Failed"
Jan 18 17:57:30.369: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae container agnhost-container: <nil>
STEP: delete the pod 01/18/23 17:57:30.387
Jan 18 17:57:30.411: INFO: Waiting for pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae to disappear
Jan 18 17:57:30.417: INFO: Pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 17:57:30.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4423" for this suite. 01/18/23 17:57:30.424
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":247,"skipped":4754,"failed":0}
------------------------------
â€¢ [4.158 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:26.278
    Jan 18 17:57:26.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 17:57:26.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:26.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:26.309
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-52eb0ceb-35eb-4e8c-96b0-956f651d2204 01/18/23 17:57:26.314
    STEP: Creating a pod to test consume configMaps 01/18/23 17:57:26.324
    Jan 18 17:57:26.346: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae" in namespace "projected-4423" to be "Succeeded or Failed"
    Jan 18 17:57:26.354: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.720606ms
    Jan 18 17:57:28.362: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015586001s
    Jan 18 17:57:30.362: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016340702s
    STEP: Saw pod success 01/18/23 17:57:30.362
    Jan 18 17:57:30.363: INFO: Pod "pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae" satisfied condition "Succeeded or Failed"
    Jan 18 17:57:30.369: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 17:57:30.387
    Jan 18 17:57:30.411: INFO: Waiting for pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae to disappear
    Jan 18 17:57:30.417: INFO: Pod pod-projected-configmaps-c3fd12f5-a3e4-4108-8458-e3330582d2ae no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 17:57:30.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4423" for this suite. 01/18/23 17:57:30.424
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:30.437
Jan 18 17:57:30.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 17:57:30.439
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:30.46
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:30.465
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/18/23 17:57:30.47
STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 17:57:30.48
STEP: delete the deployment 01/18/23 17:57:30.993
STEP: wait for all rs to be garbage collected 01/18/23 17:57:31.004
STEP: expected 0 pods, got 2 pods 01/18/23 17:57:31.01
STEP: expected 0 rs, got 1 rs 01/18/23 17:57:31.022
STEP: Gathering metrics 01/18/23 17:57:31.544
W0118 17:57:31.555410      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 17:57:31.555: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 17:57:31.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3353" for this suite. 01/18/23 17:57:31.562
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":248,"skipped":4766,"failed":0}
------------------------------
â€¢ [1.137 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:30.437
    Jan 18 17:57:30.438: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 17:57:30.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:30.46
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:30.465
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/18/23 17:57:30.47
    STEP: Wait for the Deployment to create new ReplicaSet 01/18/23 17:57:30.48
    STEP: delete the deployment 01/18/23 17:57:30.993
    STEP: wait for all rs to be garbage collected 01/18/23 17:57:31.004
    STEP: expected 0 pods, got 2 pods 01/18/23 17:57:31.01
    STEP: expected 0 rs, got 1 rs 01/18/23 17:57:31.022
    STEP: Gathering metrics 01/18/23 17:57:31.544
    W0118 17:57:31.555410      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 17:57:31.555: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 17:57:31.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3353" for this suite. 01/18/23 17:57:31.562
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:31.575
Jan 18 17:57:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 17:57:31.576
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:31.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:31.604
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/18/23 17:57:31.608
Jan 18 17:57:31.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 create -f -'
Jan 18 17:57:33.669: INFO: stderr: ""
Jan 18 17:57:33.669: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 17:57:33.669
Jan 18 17:57:33.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 17:57:33.783: INFO: stderr: ""
Jan 18 17:57:33.783: INFO: stdout: "update-demo-nautilus-25brt update-demo-nautilus-pqr2s "
Jan 18 17:57:33.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 17:57:33.877: INFO: stderr: ""
Jan 18 17:57:33.877: INFO: stdout: ""
Jan 18 17:57:33.877: INFO: update-demo-nautilus-25brt is created but not running
Jan 18 17:57:38.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 17:57:38.973: INFO: stderr: ""
Jan 18 17:57:38.973: INFO: stdout: "update-demo-nautilus-25brt update-demo-nautilus-pqr2s "
Jan 18 17:57:38.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 17:57:39.067: INFO: stderr: ""
Jan 18 17:57:39.067: INFO: stdout: "true"
Jan 18 17:57:39.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 17:57:39.161: INFO: stderr: ""
Jan 18 17:57:39.161: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 17:57:39.161: INFO: validating pod update-demo-nautilus-25brt
Jan 18 17:57:39.174: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 17:57:39.174: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 17:57:39.174: INFO: update-demo-nautilus-25brt is verified up and running
Jan 18 17:57:39.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-pqr2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 17:57:39.269: INFO: stderr: ""
Jan 18 17:57:39.269: INFO: stdout: "true"
Jan 18 17:57:39.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-pqr2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 17:57:39.372: INFO: stderr: ""
Jan 18 17:57:39.372: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 17:57:39.372: INFO: validating pod update-demo-nautilus-pqr2s
Jan 18 17:57:39.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 17:57:39.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 17:57:39.388: INFO: update-demo-nautilus-pqr2s is verified up and running
STEP: using delete to clean up resources 01/18/23 17:57:39.388
Jan 18 17:57:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 delete --grace-period=0 --force -f -'
Jan 18 17:57:39.489: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 17:57:39.489: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 17:57:39.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get rc,svc -l name=update-demo --no-headers'
Jan 18 17:57:39.609: INFO: stderr: "No resources found in kubectl-2350 namespace.\n"
Jan 18 17:57:39.609: INFO: stdout: ""
Jan 18 17:57:39.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 17:57:39.700: INFO: stderr: ""
Jan 18 17:57:39.700: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 17:57:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2350" for this suite. 01/18/23 17:57:39.709
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":249,"skipped":4766,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.148 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:31.575
    Jan 18 17:57:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 17:57:31.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:31.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:31.604
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/18/23 17:57:31.608
    Jan 18 17:57:31.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 create -f -'
    Jan 18 17:57:33.669: INFO: stderr: ""
    Jan 18 17:57:33.669: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 17:57:33.669
    Jan 18 17:57:33.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 17:57:33.783: INFO: stderr: ""
    Jan 18 17:57:33.783: INFO: stdout: "update-demo-nautilus-25brt update-demo-nautilus-pqr2s "
    Jan 18 17:57:33.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 17:57:33.877: INFO: stderr: ""
    Jan 18 17:57:33.877: INFO: stdout: ""
    Jan 18 17:57:33.877: INFO: update-demo-nautilus-25brt is created but not running
    Jan 18 17:57:38.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 17:57:38.973: INFO: stderr: ""
    Jan 18 17:57:38.973: INFO: stdout: "update-demo-nautilus-25brt update-demo-nautilus-pqr2s "
    Jan 18 17:57:38.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 17:57:39.067: INFO: stderr: ""
    Jan 18 17:57:39.067: INFO: stdout: "true"
    Jan 18 17:57:39.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-25brt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 17:57:39.161: INFO: stderr: ""
    Jan 18 17:57:39.161: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 17:57:39.161: INFO: validating pod update-demo-nautilus-25brt
    Jan 18 17:57:39.174: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 17:57:39.174: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 17:57:39.174: INFO: update-demo-nautilus-25brt is verified up and running
    Jan 18 17:57:39.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-pqr2s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 17:57:39.269: INFO: stderr: ""
    Jan 18 17:57:39.269: INFO: stdout: "true"
    Jan 18 17:57:39.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods update-demo-nautilus-pqr2s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 17:57:39.372: INFO: stderr: ""
    Jan 18 17:57:39.372: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 17:57:39.372: INFO: validating pod update-demo-nautilus-pqr2s
    Jan 18 17:57:39.388: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 17:57:39.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 17:57:39.388: INFO: update-demo-nautilus-pqr2s is verified up and running
    STEP: using delete to clean up resources 01/18/23 17:57:39.388
    Jan 18 17:57:39.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 delete --grace-period=0 --force -f -'
    Jan 18 17:57:39.489: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 17:57:39.489: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 17:57:39.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get rc,svc -l name=update-demo --no-headers'
    Jan 18 17:57:39.609: INFO: stderr: "No resources found in kubectl-2350 namespace.\n"
    Jan 18 17:57:39.609: INFO: stdout: ""
    Jan 18 17:57:39.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2350 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 17:57:39.700: INFO: stderr: ""
    Jan 18 17:57:39.700: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 17:57:39.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2350" for this suite. 01/18/23 17:57:39.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:39.723
Jan 18 17:57:39.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 17:57:39.725
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:39.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:39.753
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 17:57:39.758
Jan 18 17:57:39.773: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4601" to be "running and ready"
Jan 18 17:57:39.779: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.21085ms
Jan 18 17:57:39.779: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:57:41.787: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01420447s
Jan 18 17:57:41.787: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 18 17:57:43.787: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.013974898s
Jan 18 17:57:43.787: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 18 17:57:43.787: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/18/23 17:57:43.795
STEP: Then the orphan pod is adopted 01/18/23 17:57:43.806
STEP: When the matched label of one of its pods change 01/18/23 17:57:44.821
Jan 18 17:57:44.827: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 17:57:44.845
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 17:57:45.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4601" for this suite. 01/18/23 17:57:45.868
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":250,"skipped":4776,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.156 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:39.723
    Jan 18 17:57:39.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 17:57:39.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:39.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:39.753
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/18/23 17:57:39.758
    Jan 18 17:57:39.773: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4601" to be "running and ready"
    Jan 18 17:57:39.779: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 6.21085ms
    Jan 18 17:57:39.779: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:57:41.787: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01420447s
    Jan 18 17:57:41.787: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 17:57:43.787: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 4.013974898s
    Jan 18 17:57:43.787: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 18 17:57:43.787: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/18/23 17:57:43.795
    STEP: Then the orphan pod is adopted 01/18/23 17:57:43.806
    STEP: When the matched label of one of its pods change 01/18/23 17:57:44.821
    Jan 18 17:57:44.827: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 17:57:44.845
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 17:57:45.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4601" for this suite. 01/18/23 17:57:45.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:45.882
Jan 18 17:57:45.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 17:57:45.883
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:45.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:45.912
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-3700 01/18/23 17:57:45.917
STEP: creating service affinity-clusterip in namespace services-3700 01/18/23 17:57:45.917
STEP: creating replication controller affinity-clusterip in namespace services-3700 01/18/23 17:57:45.938
I0118 17:57:45.950581      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3700, replica count: 3
I0118 17:57:49.002739      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 17:57:49.018: INFO: Creating new exec pod
Jan 18 17:57:49.028: INFO: Waiting up to 5m0s for pod "execpod-affinitylz7m7" in namespace "services-3700" to be "running"
Jan 18 17:57:49.035: INFO: Pod "execpod-affinitylz7m7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.29789ms
Jan 18 17:57:51.042: INFO: Pod "execpod-affinitylz7m7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013968436s
Jan 18 17:57:51.042: INFO: Pod "execpod-affinitylz7m7" satisfied condition "running"
Jan 18 17:57:52.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 18 17:57:52.251: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 18 17:57:52.251: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:57:52.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.146.98 80'
Jan 18 17:57:52.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.146.98 80\nConnection to 10.96.146.98 80 port [tcp/http] succeeded!\n"
Jan 18 17:57:52.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 17:57:52.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.146.98:80/ ; done'
Jan 18 17:57:52.723: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n"
Jan 18 17:57:52.723: INFO: stdout: "\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp"
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
Jan 18 17:57:52.723: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3700, will wait for the garbage collector to delete the pods 01/18/23 17:57:52.746
Jan 18 17:57:52.816: INFO: Deleting ReplicationController affinity-clusterip took: 12.763223ms
Jan 18 17:57:52.917: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.137983ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 17:57:55.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3700" for this suite. 01/18/23 17:57:55.257
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":251,"skipped":4818,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.387 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:45.882
    Jan 18 17:57:45.882: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 17:57:45.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:45.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:45.912
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-3700 01/18/23 17:57:45.917
    STEP: creating service affinity-clusterip in namespace services-3700 01/18/23 17:57:45.917
    STEP: creating replication controller affinity-clusterip in namespace services-3700 01/18/23 17:57:45.938
    I0118 17:57:45.950581      21 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-3700, replica count: 3
    I0118 17:57:49.002739      21 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 17:57:49.018: INFO: Creating new exec pod
    Jan 18 17:57:49.028: INFO: Waiting up to 5m0s for pod "execpod-affinitylz7m7" in namespace "services-3700" to be "running"
    Jan 18 17:57:49.035: INFO: Pod "execpod-affinitylz7m7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.29789ms
    Jan 18 17:57:51.042: INFO: Pod "execpod-affinitylz7m7": Phase="Running", Reason="", readiness=true. Elapsed: 2.013968436s
    Jan 18 17:57:51.042: INFO: Pod "execpod-affinitylz7m7" satisfied condition "running"
    Jan 18 17:57:52.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 18 17:57:52.251: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 18 17:57:52.251: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:57:52.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.146.98 80'
    Jan 18 17:57:52.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.146.98 80\nConnection to 10.96.146.98 80 port [tcp/http] succeeded!\n"
    Jan 18 17:57:52.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 17:57:52.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-3700 exec execpod-affinitylz7m7 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.146.98:80/ ; done'
    Jan 18 17:57:52.723: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.146.98:80/\n"
    Jan 18 17:57:52.723: INFO: stdout: "\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp\naffinity-clusterip-nnhzp"
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Received response from host: affinity-clusterip-nnhzp
    Jan 18 17:57:52.723: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-3700, will wait for the garbage collector to delete the pods 01/18/23 17:57:52.746
    Jan 18 17:57:52.816: INFO: Deleting ReplicationController affinity-clusterip took: 12.763223ms
    Jan 18 17:57:52.917: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.137983ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 17:57:55.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3700" for this suite. 01/18/23 17:57:55.257
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 17:57:55.269
Jan 18 17:57:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename cronjob 01/18/23 17:57:55.27
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:55.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:55.304
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/18/23 17:57:55.309
STEP: Ensuring no jobs are scheduled 01/18/23 17:57:55.318
STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 18:02:55.333
STEP: Removing cronjob 01/18/23 18:02:55.34
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 18:02:55.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5114" for this suite. 01/18/23 18:02:55.359
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":252,"skipped":4819,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.102 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 17:57:55.269
    Jan 18 17:57:55.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename cronjob 01/18/23 17:57:55.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 17:57:55.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 17:57:55.304
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/18/23 17:57:55.309
    STEP: Ensuring no jobs are scheduled 01/18/23 17:57:55.318
    STEP: Ensuring no job exists by listing jobs explicitly 01/18/23 18:02:55.333
    STEP: Removing cronjob 01/18/23 18:02:55.34
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 18:02:55.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5114" for this suite. 01/18/23 18:02:55.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:02:55.373
Jan 18 18:02:55.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 18:02:55.374
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:02:55.399
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:02:55.404
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 18:02:55.409
Jan 18 18:02:55.424: INFO: Waiting up to 5m0s for pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7" in namespace "emptydir-1391" to be "Succeeded or Failed"
Jan 18 18:02:55.430: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084071ms
Jan 18 18:02:57.439: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014691935s
Jan 18 18:02:59.438: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013555064s
STEP: Saw pod success 01/18/23 18:02:59.438
Jan 18 18:02:59.438: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7" satisfied condition "Succeeded or Failed"
Jan 18 18:02:59.445: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 container test-container: <nil>
STEP: delete the pod 01/18/23 18:02:59.479
Jan 18 18:02:59.497: INFO: Waiting for pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 to disappear
Jan 18 18:02:59.502: INFO: Pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 18:02:59.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1391" for this suite. 01/18/23 18:02:59.511
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":253,"skipped":4834,"failed":0}
------------------------------
â€¢ [4.152 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:02:55.373
    Jan 18 18:02:55.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 18:02:55.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:02:55.399
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:02:55.404
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/18/23 18:02:55.409
    Jan 18 18:02:55.424: INFO: Waiting up to 5m0s for pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7" in namespace "emptydir-1391" to be "Succeeded or Failed"
    Jan 18 18:02:55.430: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084071ms
    Jan 18 18:02:57.439: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014691935s
    Jan 18 18:02:59.438: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013555064s
    STEP: Saw pod success 01/18/23 18:02:59.438
    Jan 18 18:02:59.438: INFO: Pod "pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7" satisfied condition "Succeeded or Failed"
    Jan 18 18:02:59.445: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 container test-container: <nil>
    STEP: delete the pod 01/18/23 18:02:59.479
    Jan 18 18:02:59.497: INFO: Waiting for pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 to disappear
    Jan 18 18:02:59.502: INFO: Pod pod-fc863a21-ce74-4088-8af1-9b8d3aa5c0c7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 18:02:59.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1391" for this suite. 01/18/23 18:02:59.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:02:59.526
Jan 18 18:02:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 18:02:59.527
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:02:59.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:02:59.554
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 18:02:59.558
Jan 18 18:02:59.573: INFO: Waiting up to 5m0s for pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a" in namespace "emptydir-910" to be "Succeeded or Failed"
Jan 18 18:02:59.580: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.336693ms
Jan 18 18:03:01.590: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016796556s
Jan 18 18:03:03.587: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014076224s
STEP: Saw pod success 01/18/23 18:03:03.588
Jan 18 18:03:03.588: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a" satisfied condition "Succeeded or Failed"
Jan 18 18:03:03.596: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a container test-container: <nil>
STEP: delete the pod 01/18/23 18:03:03.608
Jan 18 18:03:03.627: INFO: Waiting for pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a to disappear
Jan 18 18:03:03.632: INFO: Pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 18:03:03.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-910" for this suite. 01/18/23 18:03:03.64
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4841,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:02:59.526
    Jan 18 18:02:59.526: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 18:02:59.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:02:59.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:02:59.554
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/18/23 18:02:59.558
    Jan 18 18:02:59.573: INFO: Waiting up to 5m0s for pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a" in namespace "emptydir-910" to be "Succeeded or Failed"
    Jan 18 18:02:59.580: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.336693ms
    Jan 18 18:03:01.590: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016796556s
    Jan 18 18:03:03.587: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014076224s
    STEP: Saw pod success 01/18/23 18:03:03.588
    Jan 18 18:03:03.588: INFO: Pod "pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a" satisfied condition "Succeeded or Failed"
    Jan 18 18:03:03.596: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a container test-container: <nil>
    STEP: delete the pod 01/18/23 18:03:03.608
    Jan 18 18:03:03.627: INFO: Waiting for pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a to disappear
    Jan 18 18:03:03.632: INFO: Pod pod-30d84f93-7aeb-4e97-b85e-fa3d36a1bc6a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 18:03:03.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-910" for this suite. 01/18/23 18:03:03.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:03.657
Jan 18 18:03:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:03:03.659
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:03.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:03.683
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2121 01/18/23 18:03:03.687
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 18:03:03.706
STEP: creating service externalsvc in namespace services-2121 01/18/23 18:03:03.707
STEP: creating replication controller externalsvc in namespace services-2121 01/18/23 18:03:03.726
I0118 18:03:03.737054      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
I0118 18:03:06.789020      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/18/23 18:03:06.797
Jan 18 18:03:06.828: INFO: Creating new exec pod
Jan 18 18:03:06.837: INFO: Waiting up to 5m0s for pod "execpodh9tmz" in namespace "services-2121" to be "running"
Jan 18 18:03:06.844: INFO: Pod "execpodh9tmz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.878848ms
Jan 18 18:03:08.851: INFO: Pod "execpodh9tmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013812251s
Jan 18 18:03:08.851: INFO: Pod "execpodh9tmz" satisfied condition "running"
Jan 18 18:03:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-2121 exec execpodh9tmz -- /bin/sh -x -c nslookup clusterip-service.services-2121.svc.cluster.local'
Jan 18 18:03:09.067: INFO: stderr: "+ nslookup clusterip-service.services-2121.svc.cluster.local\n"
Jan 18 18:03:09.067: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 10.96.196.27\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 01/18/23 18:03:09.067
Jan 18 18:03:09.141: INFO: Deleting ReplicationController externalsvc took: 18.209613ms
Jan 18 18:03:09.242: INFO: Terminating ReplicationController externalsvc pods took: 100.186078ms
Jan 18 18:03:11.477: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:03:11.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2121" for this suite. 01/18/23 18:03:11.505
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":255,"skipped":4850,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.860 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:03.657
    Jan 18 18:03:03.657: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:03:03.659
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:03.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:03.683
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2121 01/18/23 18:03:03.687
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/18/23 18:03:03.706
    STEP: creating service externalsvc in namespace services-2121 01/18/23 18:03:03.707
    STEP: creating replication controller externalsvc in namespace services-2121 01/18/23 18:03:03.726
    I0118 18:03:03.737054      21 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2121, replica count: 2
    I0118 18:03:06.789020      21 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/18/23 18:03:06.797
    Jan 18 18:03:06.828: INFO: Creating new exec pod
    Jan 18 18:03:06.837: INFO: Waiting up to 5m0s for pod "execpodh9tmz" in namespace "services-2121" to be "running"
    Jan 18 18:03:06.844: INFO: Pod "execpodh9tmz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.878848ms
    Jan 18 18:03:08.851: INFO: Pod "execpodh9tmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.013812251s
    Jan 18 18:03:08.851: INFO: Pod "execpodh9tmz" satisfied condition "running"
    Jan 18 18:03:08.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-2121 exec execpodh9tmz -- /bin/sh -x -c nslookup clusterip-service.services-2121.svc.cluster.local'
    Jan 18 18:03:09.067: INFO: stderr: "+ nslookup clusterip-service.services-2121.svc.cluster.local\n"
    Jan 18 18:03:09.067: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-2121.svc.cluster.local\tcanonical name = externalsvc.services-2121.svc.cluster.local.\nName:\texternalsvc.services-2121.svc.cluster.local\nAddress: 10.96.196.27\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2121, will wait for the garbage collector to delete the pods 01/18/23 18:03:09.067
    Jan 18 18:03:09.141: INFO: Deleting ReplicationController externalsvc took: 18.209613ms
    Jan 18 18:03:09.242: INFO: Terminating ReplicationController externalsvc pods took: 100.186078ms
    Jan 18 18:03:11.477: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:03:11.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2121" for this suite. 01/18/23 18:03:11.505
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:11.519
Jan 18 18:03:11.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:03:11.52
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:11.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:11.548
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9788 01/18/23 18:03:11.552
STEP: changing the ExternalName service to type=ClusterIP 01/18/23 18:03:11.56
STEP: creating replication controller externalname-service in namespace services-9788 01/18/23 18:03:11.586
I0118 18:03:11.594275      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9788, replica count: 2
I0118 18:03:14.646259      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 18:03:14.646: INFO: Creating new exec pod
Jan 18 18:03:14.660: INFO: Waiting up to 5m0s for pod "execpodx7cv9" in namespace "services-9788" to be "running"
Jan 18 18:03:14.666: INFO: Pod "execpodx7cv9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065181ms
Jan 18 18:03:16.674: INFO: Pod "execpodx7cv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013550517s
Jan 18 18:03:16.674: INFO: Pod "execpodx7cv9" satisfied condition "running"
Jan 18 18:03:17.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 18:03:17.880: INFO: stderr: "+ + echonc hostName -v -t -w\n 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 18:03:17.880: INFO: stdout: ""
Jan 18 18:03:18.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 18 18:03:19.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 18 18:03:19.075: INFO: stdout: "externalname-service-tjz5w"
Jan 18 18:03:19.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.150.208 80'
Jan 18 18:03:19.279: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.150.208 80\nConnection to 10.96.150.208 80 port [tcp/http] succeeded!\n"
Jan 18 18:03:19.279: INFO: stdout: "externalname-service-tjz5w"
Jan 18 18:03:19.279: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:03:19.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9788" for this suite. 01/18/23 18:03:19.325
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":256,"skipped":4891,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.821 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:11.519
    Jan 18 18:03:11.519: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:03:11.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:11.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:11.548
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9788 01/18/23 18:03:11.552
    STEP: changing the ExternalName service to type=ClusterIP 01/18/23 18:03:11.56
    STEP: creating replication controller externalname-service in namespace services-9788 01/18/23 18:03:11.586
    I0118 18:03:11.594275      21 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9788, replica count: 2
    I0118 18:03:14.646259      21 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 18:03:14.646: INFO: Creating new exec pod
    Jan 18 18:03:14.660: INFO: Waiting up to 5m0s for pod "execpodx7cv9" in namespace "services-9788" to be "running"
    Jan 18 18:03:14.666: INFO: Pod "execpodx7cv9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065181ms
    Jan 18 18:03:16.674: INFO: Pod "execpodx7cv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.013550517s
    Jan 18 18:03:16.674: INFO: Pod "execpodx7cv9" satisfied condition "running"
    Jan 18 18:03:17.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 18:03:17.880: INFO: stderr: "+ + echonc hostName -v -t -w\n 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 18:03:17.880: INFO: stdout: ""
    Jan 18 18:03:18.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 18 18:03:19.075: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 18 18:03:19.075: INFO: stdout: "externalname-service-tjz5w"
    Jan 18 18:03:19.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-9788 exec execpodx7cv9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.150.208 80'
    Jan 18 18:03:19.279: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.150.208 80\nConnection to 10.96.150.208 80 port [tcp/http] succeeded!\n"
    Jan 18 18:03:19.279: INFO: stdout: "externalname-service-tjz5w"
    Jan 18 18:03:19.279: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:03:19.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9788" for this suite. 01/18/23 18:03:19.325
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:19.341
Jan 18 18:03:19.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 18:03:19.342
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:19.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:19.372
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-3d90c25a-3871-4b68-b60e-79485875d859 01/18/23 18:03:19.377
STEP: Creating a pod to test consume secrets 01/18/23 18:03:19.389
Jan 18 18:03:19.403: INFO: Waiting up to 5m0s for pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd" in namespace "secrets-898" to be "Succeeded or Failed"
Jan 18 18:03:19.410: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533605ms
Jan 18 18:03:21.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015738107s
Jan 18 18:03:23.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015606121s
STEP: Saw pod success 01/18/23 18:03:23.419
Jan 18 18:03:23.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd" satisfied condition "Succeeded or Failed"
Jan 18 18:03:23.426: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 18:03:23.443
Jan 18 18:03:23.472: INFO: Waiting for pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd to disappear
Jan 18 18:03:23.479: INFO: Pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 18:03:23.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-898" for this suite. 01/18/23 18:03:23.488
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":257,"skipped":4902,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:19.341
    Jan 18 18:03:19.341: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 18:03:19.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:19.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:19.372
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-3d90c25a-3871-4b68-b60e-79485875d859 01/18/23 18:03:19.377
    STEP: Creating a pod to test consume secrets 01/18/23 18:03:19.389
    Jan 18 18:03:19.403: INFO: Waiting up to 5m0s for pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd" in namespace "secrets-898" to be "Succeeded or Failed"
    Jan 18 18:03:19.410: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.533605ms
    Jan 18 18:03:21.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015738107s
    Jan 18 18:03:23.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015606121s
    STEP: Saw pod success 01/18/23 18:03:23.419
    Jan 18 18:03:23.419: INFO: Pod "pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd" satisfied condition "Succeeded or Failed"
    Jan 18 18:03:23.426: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 18:03:23.443
    Jan 18 18:03:23.472: INFO: Waiting for pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd to disappear
    Jan 18 18:03:23.479: INFO: Pod pod-secrets-d6eeb94a-1edc-4b4e-994a-d32acf1dbebd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 18:03:23.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-898" for this suite. 01/18/23 18:03:23.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:23.507
Jan 18 18:03:23.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context-test 01/18/23 18:03:23.508
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:23.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:23.538
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 18 18:03:23.559: INFO: Waiting up to 5m0s for pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60" in namespace "security-context-test-8975" to be "Succeeded or Failed"
Jan 18 18:03:23.566: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Pending", Reason="", readiness=false. Elapsed: 7.244171ms
Jan 18 18:03:25.574: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014730398s
Jan 18 18:03:27.579: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020378825s
Jan 18 18:03:27.580: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 18:03:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8975" for this suite. 01/18/23 18:03:27.587
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":258,"skipped":4930,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:23.507
    Jan 18 18:03:23.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 18:03:23.508
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:23.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:23.538
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 18 18:03:23.559: INFO: Waiting up to 5m0s for pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60" in namespace "security-context-test-8975" to be "Succeeded or Failed"
    Jan 18 18:03:23.566: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Pending", Reason="", readiness=false. Elapsed: 7.244171ms
    Jan 18 18:03:25.574: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014730398s
    Jan 18 18:03:27.579: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020378825s
    Jan 18 18:03:27.580: INFO: Pod "busybox-user-65534-96b3eb6f-6e26-4e1d-a5be-5ac1b2597e60" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 18:03:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8975" for this suite. 01/18/23 18:03:27.587
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:27.602
Jan 18 18:03:27.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 18:03:27.604
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:27.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:27.629
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 18:03:27.633
Jan 18 18:03:27.648: INFO: Waiting up to 5m0s for pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e" in namespace "emptydir-1532" to be "Succeeded or Failed"
Jan 18 18:03:27.654: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800398ms
Jan 18 18:03:29.661: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012652067s
Jan 18 18:03:31.663: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014612564s
STEP: Saw pod success 01/18/23 18:03:31.663
Jan 18 18:03:31.663: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e" satisfied condition "Succeeded or Failed"
Jan 18 18:03:31.671: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e container test-container: <nil>
STEP: delete the pod 01/18/23 18:03:31.687
Jan 18 18:03:31.709: INFO: Waiting for pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e to disappear
Jan 18 18:03:31.715: INFO: Pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 18:03:31.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1532" for this suite. 01/18/23 18:03:31.722
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":259,"skipped":4930,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:27.602
    Jan 18 18:03:27.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 18:03:27.604
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:27.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:27.629
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/18/23 18:03:27.633
    Jan 18 18:03:27.648: INFO: Waiting up to 5m0s for pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e" in namespace "emptydir-1532" to be "Succeeded or Failed"
    Jan 18 18:03:27.654: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.800398ms
    Jan 18 18:03:29.661: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012652067s
    Jan 18 18:03:31.663: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014612564s
    STEP: Saw pod success 01/18/23 18:03:31.663
    Jan 18 18:03:31.663: INFO: Pod "pod-36cb3d2c-2954-4640-8b9e-080ada57191e" satisfied condition "Succeeded or Failed"
    Jan 18 18:03:31.671: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e container test-container: <nil>
    STEP: delete the pod 01/18/23 18:03:31.687
    Jan 18 18:03:31.709: INFO: Waiting for pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e to disappear
    Jan 18 18:03:31.715: INFO: Pod pod-36cb3d2c-2954-4640-8b9e-080ada57191e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 18:03:31.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1532" for this suite. 01/18/23 18:03:31.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:31.738
Jan 18 18:03:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename ingressclass 01/18/23 18:03:31.739
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:31.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:31.767
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/18/23 18:03:31.772
STEP: getting /apis/networking.k8s.io 01/18/23 18:03:31.775
STEP: getting /apis/networking.k8s.iov1 01/18/23 18:03:31.777
STEP: creating 01/18/23 18:03:31.778
STEP: getting 01/18/23 18:03:31.806
STEP: listing 01/18/23 18:03:31.812
STEP: watching 01/18/23 18:03:31.819
Jan 18 18:03:31.819: INFO: starting watch
STEP: patching 01/18/23 18:03:31.821
STEP: updating 01/18/23 18:03:31.832
Jan 18 18:03:31.840: INFO: waiting for watch events with expected annotations
Jan 18 18:03:31.840: INFO: saw patched and updated annotations
STEP: deleting 01/18/23 18:03:31.84
STEP: deleting a collection 01/18/23 18:03:31.864
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 18 18:03:31.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-678" for this suite. 01/18/23 18:03:31.899
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":260,"skipped":4962,"failed":0}
------------------------------
â€¢ [0.173 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:31.738
    Jan 18 18:03:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename ingressclass 01/18/23 18:03:31.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:31.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:31.767
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/18/23 18:03:31.772
    STEP: getting /apis/networking.k8s.io 01/18/23 18:03:31.775
    STEP: getting /apis/networking.k8s.iov1 01/18/23 18:03:31.777
    STEP: creating 01/18/23 18:03:31.778
    STEP: getting 01/18/23 18:03:31.806
    STEP: listing 01/18/23 18:03:31.812
    STEP: watching 01/18/23 18:03:31.819
    Jan 18 18:03:31.819: INFO: starting watch
    STEP: patching 01/18/23 18:03:31.821
    STEP: updating 01/18/23 18:03:31.832
    Jan 18 18:03:31.840: INFO: waiting for watch events with expected annotations
    Jan 18 18:03:31.840: INFO: saw patched and updated annotations
    STEP: deleting 01/18/23 18:03:31.84
    STEP: deleting a collection 01/18/23 18:03:31.864
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 18 18:03:31.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-678" for this suite. 01/18/23 18:03:31.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:03:31.912
Jan 18 18:03:31.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 18:03:31.913
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:31.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:31.941
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/18/23 18:03:31.953
STEP: delete the rc 01/18/23 18:03:36.978
STEP: wait for the rc to be deleted 01/18/23 18:03:37.06
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 18:03:42.067
STEP: Gathering metrics 01/18/23 18:04:12.092
W0118 18:04:12.105181      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 18:04:12.105: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 18:04:12.105: INFO: Deleting pod "simpletest.rc-26nq5" in namespace "gc-8035"
Jan 18 18:04:12.128: INFO: Deleting pod "simpletest.rc-2bq5l" in namespace "gc-8035"
Jan 18 18:04:12.152: INFO: Deleting pod "simpletest.rc-2f6gb" in namespace "gc-8035"
Jan 18 18:04:12.171: INFO: Deleting pod "simpletest.rc-2gdrt" in namespace "gc-8035"
Jan 18 18:04:12.190: INFO: Deleting pod "simpletest.rc-2kjz6" in namespace "gc-8035"
Jan 18 18:04:12.208: INFO: Deleting pod "simpletest.rc-2t7sm" in namespace "gc-8035"
Jan 18 18:04:12.235: INFO: Deleting pod "simpletest.rc-45t5w" in namespace "gc-8035"
Jan 18 18:04:12.256: INFO: Deleting pod "simpletest.rc-48s7q" in namespace "gc-8035"
Jan 18 18:04:12.280: INFO: Deleting pod "simpletest.rc-4cqnw" in namespace "gc-8035"
Jan 18 18:04:12.296: INFO: Deleting pod "simpletest.rc-4nd24" in namespace "gc-8035"
Jan 18 18:04:12.347: INFO: Deleting pod "simpletest.rc-559th" in namespace "gc-8035"
Jan 18 18:04:12.365: INFO: Deleting pod "simpletest.rc-5nwfx" in namespace "gc-8035"
Jan 18 18:04:12.382: INFO: Deleting pod "simpletest.rc-5q2nz" in namespace "gc-8035"
Jan 18 18:04:12.397: INFO: Deleting pod "simpletest.rc-5xl9b" in namespace "gc-8035"
Jan 18 18:04:12.417: INFO: Deleting pod "simpletest.rc-68wbx" in namespace "gc-8035"
Jan 18 18:04:12.439: INFO: Deleting pod "simpletest.rc-6cvts" in namespace "gc-8035"
Jan 18 18:04:12.460: INFO: Deleting pod "simpletest.rc-6v8vk" in namespace "gc-8035"
Jan 18 18:04:12.483: INFO: Deleting pod "simpletest.rc-777xz" in namespace "gc-8035"
Jan 18 18:04:12.505: INFO: Deleting pod "simpletest.rc-78bnf" in namespace "gc-8035"
Jan 18 18:04:12.526: INFO: Deleting pod "simpletest.rc-7ffpm" in namespace "gc-8035"
Jan 18 18:04:12.547: INFO: Deleting pod "simpletest.rc-7l7sx" in namespace "gc-8035"
Jan 18 18:04:12.570: INFO: Deleting pod "simpletest.rc-85wpl" in namespace "gc-8035"
Jan 18 18:04:12.587: INFO: Deleting pod "simpletest.rc-8jcjp" in namespace "gc-8035"
Jan 18 18:04:12.606: INFO: Deleting pod "simpletest.rc-8rkpq" in namespace "gc-8035"
Jan 18 18:04:12.624: INFO: Deleting pod "simpletest.rc-8tnsq" in namespace "gc-8035"
Jan 18 18:04:12.640: INFO: Deleting pod "simpletest.rc-8w5vq" in namespace "gc-8035"
Jan 18 18:04:12.665: INFO: Deleting pod "simpletest.rc-8zcj4" in namespace "gc-8035"
Jan 18 18:04:12.692: INFO: Deleting pod "simpletest.rc-9kk8q" in namespace "gc-8035"
Jan 18 18:04:12.708: INFO: Deleting pod "simpletest.rc-b9hxn" in namespace "gc-8035"
Jan 18 18:04:12.769: INFO: Deleting pod "simpletest.rc-bg48j" in namespace "gc-8035"
Jan 18 18:04:12.784: INFO: Deleting pod "simpletest.rc-bkdst" in namespace "gc-8035"
Jan 18 18:04:12.859: INFO: Deleting pod "simpletest.rc-bmkll" in namespace "gc-8035"
Jan 18 18:04:12.879: INFO: Deleting pod "simpletest.rc-bsz29" in namespace "gc-8035"
Jan 18 18:04:12.896: INFO: Deleting pod "simpletest.rc-bvs6c" in namespace "gc-8035"
Jan 18 18:04:12.962: INFO: Deleting pod "simpletest.rc-bvvgc" in namespace "gc-8035"
Jan 18 18:04:12.985: INFO: Deleting pod "simpletest.rc-c4m54" in namespace "gc-8035"
Jan 18 18:04:13.007: INFO: Deleting pod "simpletest.rc-c9j24" in namespace "gc-8035"
Jan 18 18:04:13.084: INFO: Deleting pod "simpletest.rc-cd2vm" in namespace "gc-8035"
Jan 18 18:04:13.165: INFO: Deleting pod "simpletest.rc-cfxlw" in namespace "gc-8035"
Jan 18 18:04:13.183: INFO: Deleting pod "simpletest.rc-cgb27" in namespace "gc-8035"
Jan 18 18:04:13.260: INFO: Deleting pod "simpletest.rc-cmdjl" in namespace "gc-8035"
Jan 18 18:04:13.279: INFO: Deleting pod "simpletest.rc-cxkh6" in namespace "gc-8035"
Jan 18 18:04:13.361: INFO: Deleting pod "simpletest.rc-ddlzw" in namespace "gc-8035"
Jan 18 18:04:13.384: INFO: Deleting pod "simpletest.rc-dngbk" in namespace "gc-8035"
Jan 18 18:04:13.399: INFO: Deleting pod "simpletest.rc-dppl9" in namespace "gc-8035"
Jan 18 18:04:13.473: INFO: Deleting pod "simpletest.rc-dpwkp" in namespace "gc-8035"
Jan 18 18:04:13.494: INFO: Deleting pod "simpletest.rc-fgn8c" in namespace "gc-8035"
Jan 18 18:04:13.659: INFO: Deleting pod "simpletest.rc-fhvhw" in namespace "gc-8035"
Jan 18 18:04:13.682: INFO: Deleting pod "simpletest.rc-fnmwj" in namespace "gc-8035"
Jan 18 18:04:13.710: INFO: Deleting pod "simpletest.rc-fq764" in namespace "gc-8035"
Jan 18 18:04:13.782: INFO: Deleting pod "simpletest.rc-g7rrc" in namespace "gc-8035"
Jan 18 18:04:13.870: INFO: Deleting pod "simpletest.rc-gk2dq" in namespace "gc-8035"
Jan 18 18:04:13.892: INFO: Deleting pod "simpletest.rc-gnqb2" in namespace "gc-8035"
Jan 18 18:04:13.973: INFO: Deleting pod "simpletest.rc-gtvl7" in namespace "gc-8035"
Jan 18 18:04:13.996: INFO: Deleting pod "simpletest.rc-hnh77" in namespace "gc-8035"
Jan 18 18:04:14.073: INFO: Deleting pod "simpletest.rc-hw2vt" in namespace "gc-8035"
Jan 18 18:04:14.088: INFO: Deleting pod "simpletest.rc-j2nvc" in namespace "gc-8035"
Jan 18 18:04:14.166: INFO: Deleting pod "simpletest.rc-j8kb2" in namespace "gc-8035"
Jan 18 18:04:14.183: INFO: Deleting pod "simpletest.rc-jdcz7" in namespace "gc-8035"
Jan 18 18:04:14.206: INFO: Deleting pod "simpletest.rc-jvhrz" in namespace "gc-8035"
Jan 18 18:04:14.279: INFO: Deleting pod "simpletest.rc-jztm7" in namespace "gc-8035"
Jan 18 18:04:14.295: INFO: Deleting pod "simpletest.rc-kbc69" in namespace "gc-8035"
Jan 18 18:04:14.371: INFO: Deleting pod "simpletest.rc-kfvss" in namespace "gc-8035"
Jan 18 18:04:14.388: INFO: Deleting pod "simpletest.rc-kgkg7" in namespace "gc-8035"
Jan 18 18:04:14.408: INFO: Deleting pod "simpletest.rc-kmsjq" in namespace "gc-8035"
Jan 18 18:04:14.477: INFO: Deleting pod "simpletest.rc-kz7rn" in namespace "gc-8035"
Jan 18 18:04:14.494: INFO: Deleting pod "simpletest.rc-l7sgl" in namespace "gc-8035"
Jan 18 18:04:14.571: INFO: Deleting pod "simpletest.rc-l8jms" in namespace "gc-8035"
Jan 18 18:04:14.590: INFO: Deleting pod "simpletest.rc-l99n4" in namespace "gc-8035"
Jan 18 18:04:14.671: INFO: Deleting pod "simpletest.rc-lbqdn" in namespace "gc-8035"
Jan 18 18:04:14.688: INFO: Deleting pod "simpletest.rc-lcgtz" in namespace "gc-8035"
Jan 18 18:04:14.760: INFO: Deleting pod "simpletest.rc-lxb8z" in namespace "gc-8035"
Jan 18 18:04:14.782: INFO: Deleting pod "simpletest.rc-nb88d" in namespace "gc-8035"
Jan 18 18:04:14.861: INFO: Deleting pod "simpletest.rc-p8dvg" in namespace "gc-8035"
Jan 18 18:04:14.884: INFO: Deleting pod "simpletest.rc-psv7g" in namespace "gc-8035"
Jan 18 18:04:14.903: INFO: Deleting pod "simpletest.rc-q9wm9" in namespace "gc-8035"
Jan 18 18:04:14.973: INFO: Deleting pod "simpletest.rc-qhmb5" in namespace "gc-8035"
Jan 18 18:04:14.994: INFO: Deleting pod "simpletest.rc-qhskb" in namespace "gc-8035"
Jan 18 18:04:15.260: INFO: Deleting pod "simpletest.rc-qj9nz" in namespace "gc-8035"
Jan 18 18:04:15.366: INFO: Deleting pod "simpletest.rc-qpmgf" in namespace "gc-8035"
Jan 18 18:04:15.567: INFO: Deleting pod "simpletest.rc-r57pz" in namespace "gc-8035"
Jan 18 18:04:15.673: INFO: Deleting pod "simpletest.rc-rgv8p" in namespace "gc-8035"
Jan 18 18:04:15.860: INFO: Deleting pod "simpletest.rc-rhp8s" in namespace "gc-8035"
Jan 18 18:04:15.887: INFO: Deleting pod "simpletest.rc-rplw9" in namespace "gc-8035"
Jan 18 18:04:15.960: INFO: Deleting pod "simpletest.rc-rs77q" in namespace "gc-8035"
Jan 18 18:04:15.978: INFO: Deleting pod "simpletest.rc-s4bzp" in namespace "gc-8035"
Jan 18 18:04:16.001: INFO: Deleting pod "simpletest.rc-s62w9" in namespace "gc-8035"
Jan 18 18:04:16.067: INFO: Deleting pod "simpletest.rc-shkm9" in namespace "gc-8035"
Jan 18 18:04:16.091: INFO: Deleting pod "simpletest.rc-srk8d" in namespace "gc-8035"
Jan 18 18:04:16.166: INFO: Deleting pod "simpletest.rc-t9sct" in namespace "gc-8035"
Jan 18 18:04:16.205: INFO: Deleting pod "simpletest.rc-tcdvd" in namespace "gc-8035"
Jan 18 18:04:16.262: INFO: Deleting pod "simpletest.rc-tl8nj" in namespace "gc-8035"
Jan 18 18:04:16.278: INFO: Deleting pod "simpletest.rc-v2ttb" in namespace "gc-8035"
Jan 18 18:04:16.360: INFO: Deleting pod "simpletest.rc-wtnct" in namespace "gc-8035"
Jan 18 18:04:16.380: INFO: Deleting pod "simpletest.rc-xj8fx" in namespace "gc-8035"
Jan 18 18:04:16.460: INFO: Deleting pod "simpletest.rc-xks44" in namespace "gc-8035"
Jan 18 18:04:16.478: INFO: Deleting pod "simpletest.rc-z6qnm" in namespace "gc-8035"
Jan 18 18:04:16.566: INFO: Deleting pod "simpletest.rc-zjcwb" in namespace "gc-8035"
Jan 18 18:04:16.584: INFO: Deleting pod "simpletest.rc-zsg5s" in namespace "gc-8035"
Jan 18 18:04:16.665: INFO: Deleting pod "simpletest.rc-zvkgb" in namespace "gc-8035"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 18:04:16.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8035" for this suite. 01/18/23 18:04:16.694
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":261,"skipped":4980,"failed":0}
------------------------------
â€¢ [SLOW TEST] [44.855 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:03:31.912
    Jan 18 18:03:31.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 18:03:31.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:03:31.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:03:31.941
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/18/23 18:03:31.953
    STEP: delete the rc 01/18/23 18:03:36.978
    STEP: wait for the rc to be deleted 01/18/23 18:03:37.06
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/18/23 18:03:42.067
    STEP: Gathering metrics 01/18/23 18:04:12.092
    W0118 18:04:12.105181      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 18:04:12.105: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 18:04:12.105: INFO: Deleting pod "simpletest.rc-26nq5" in namespace "gc-8035"
    Jan 18 18:04:12.128: INFO: Deleting pod "simpletest.rc-2bq5l" in namespace "gc-8035"
    Jan 18 18:04:12.152: INFO: Deleting pod "simpletest.rc-2f6gb" in namespace "gc-8035"
    Jan 18 18:04:12.171: INFO: Deleting pod "simpletest.rc-2gdrt" in namespace "gc-8035"
    Jan 18 18:04:12.190: INFO: Deleting pod "simpletest.rc-2kjz6" in namespace "gc-8035"
    Jan 18 18:04:12.208: INFO: Deleting pod "simpletest.rc-2t7sm" in namespace "gc-8035"
    Jan 18 18:04:12.235: INFO: Deleting pod "simpletest.rc-45t5w" in namespace "gc-8035"
    Jan 18 18:04:12.256: INFO: Deleting pod "simpletest.rc-48s7q" in namespace "gc-8035"
    Jan 18 18:04:12.280: INFO: Deleting pod "simpletest.rc-4cqnw" in namespace "gc-8035"
    Jan 18 18:04:12.296: INFO: Deleting pod "simpletest.rc-4nd24" in namespace "gc-8035"
    Jan 18 18:04:12.347: INFO: Deleting pod "simpletest.rc-559th" in namespace "gc-8035"
    Jan 18 18:04:12.365: INFO: Deleting pod "simpletest.rc-5nwfx" in namespace "gc-8035"
    Jan 18 18:04:12.382: INFO: Deleting pod "simpletest.rc-5q2nz" in namespace "gc-8035"
    Jan 18 18:04:12.397: INFO: Deleting pod "simpletest.rc-5xl9b" in namespace "gc-8035"
    Jan 18 18:04:12.417: INFO: Deleting pod "simpletest.rc-68wbx" in namespace "gc-8035"
    Jan 18 18:04:12.439: INFO: Deleting pod "simpletest.rc-6cvts" in namespace "gc-8035"
    Jan 18 18:04:12.460: INFO: Deleting pod "simpletest.rc-6v8vk" in namespace "gc-8035"
    Jan 18 18:04:12.483: INFO: Deleting pod "simpletest.rc-777xz" in namespace "gc-8035"
    Jan 18 18:04:12.505: INFO: Deleting pod "simpletest.rc-78bnf" in namespace "gc-8035"
    Jan 18 18:04:12.526: INFO: Deleting pod "simpletest.rc-7ffpm" in namespace "gc-8035"
    Jan 18 18:04:12.547: INFO: Deleting pod "simpletest.rc-7l7sx" in namespace "gc-8035"
    Jan 18 18:04:12.570: INFO: Deleting pod "simpletest.rc-85wpl" in namespace "gc-8035"
    Jan 18 18:04:12.587: INFO: Deleting pod "simpletest.rc-8jcjp" in namespace "gc-8035"
    Jan 18 18:04:12.606: INFO: Deleting pod "simpletest.rc-8rkpq" in namespace "gc-8035"
    Jan 18 18:04:12.624: INFO: Deleting pod "simpletest.rc-8tnsq" in namespace "gc-8035"
    Jan 18 18:04:12.640: INFO: Deleting pod "simpletest.rc-8w5vq" in namespace "gc-8035"
    Jan 18 18:04:12.665: INFO: Deleting pod "simpletest.rc-8zcj4" in namespace "gc-8035"
    Jan 18 18:04:12.692: INFO: Deleting pod "simpletest.rc-9kk8q" in namespace "gc-8035"
    Jan 18 18:04:12.708: INFO: Deleting pod "simpletest.rc-b9hxn" in namespace "gc-8035"
    Jan 18 18:04:12.769: INFO: Deleting pod "simpletest.rc-bg48j" in namespace "gc-8035"
    Jan 18 18:04:12.784: INFO: Deleting pod "simpletest.rc-bkdst" in namespace "gc-8035"
    Jan 18 18:04:12.859: INFO: Deleting pod "simpletest.rc-bmkll" in namespace "gc-8035"
    Jan 18 18:04:12.879: INFO: Deleting pod "simpletest.rc-bsz29" in namespace "gc-8035"
    Jan 18 18:04:12.896: INFO: Deleting pod "simpletest.rc-bvs6c" in namespace "gc-8035"
    Jan 18 18:04:12.962: INFO: Deleting pod "simpletest.rc-bvvgc" in namespace "gc-8035"
    Jan 18 18:04:12.985: INFO: Deleting pod "simpletest.rc-c4m54" in namespace "gc-8035"
    Jan 18 18:04:13.007: INFO: Deleting pod "simpletest.rc-c9j24" in namespace "gc-8035"
    Jan 18 18:04:13.084: INFO: Deleting pod "simpletest.rc-cd2vm" in namespace "gc-8035"
    Jan 18 18:04:13.165: INFO: Deleting pod "simpletest.rc-cfxlw" in namespace "gc-8035"
    Jan 18 18:04:13.183: INFO: Deleting pod "simpletest.rc-cgb27" in namespace "gc-8035"
    Jan 18 18:04:13.260: INFO: Deleting pod "simpletest.rc-cmdjl" in namespace "gc-8035"
    Jan 18 18:04:13.279: INFO: Deleting pod "simpletest.rc-cxkh6" in namespace "gc-8035"
    Jan 18 18:04:13.361: INFO: Deleting pod "simpletest.rc-ddlzw" in namespace "gc-8035"
    Jan 18 18:04:13.384: INFO: Deleting pod "simpletest.rc-dngbk" in namespace "gc-8035"
    Jan 18 18:04:13.399: INFO: Deleting pod "simpletest.rc-dppl9" in namespace "gc-8035"
    Jan 18 18:04:13.473: INFO: Deleting pod "simpletest.rc-dpwkp" in namespace "gc-8035"
    Jan 18 18:04:13.494: INFO: Deleting pod "simpletest.rc-fgn8c" in namespace "gc-8035"
    Jan 18 18:04:13.659: INFO: Deleting pod "simpletest.rc-fhvhw" in namespace "gc-8035"
    Jan 18 18:04:13.682: INFO: Deleting pod "simpletest.rc-fnmwj" in namespace "gc-8035"
    Jan 18 18:04:13.710: INFO: Deleting pod "simpletest.rc-fq764" in namespace "gc-8035"
    Jan 18 18:04:13.782: INFO: Deleting pod "simpletest.rc-g7rrc" in namespace "gc-8035"
    Jan 18 18:04:13.870: INFO: Deleting pod "simpletest.rc-gk2dq" in namespace "gc-8035"
    Jan 18 18:04:13.892: INFO: Deleting pod "simpletest.rc-gnqb2" in namespace "gc-8035"
    Jan 18 18:04:13.973: INFO: Deleting pod "simpletest.rc-gtvl7" in namespace "gc-8035"
    Jan 18 18:04:13.996: INFO: Deleting pod "simpletest.rc-hnh77" in namespace "gc-8035"
    Jan 18 18:04:14.073: INFO: Deleting pod "simpletest.rc-hw2vt" in namespace "gc-8035"
    Jan 18 18:04:14.088: INFO: Deleting pod "simpletest.rc-j2nvc" in namespace "gc-8035"
    Jan 18 18:04:14.166: INFO: Deleting pod "simpletest.rc-j8kb2" in namespace "gc-8035"
    Jan 18 18:04:14.183: INFO: Deleting pod "simpletest.rc-jdcz7" in namespace "gc-8035"
    Jan 18 18:04:14.206: INFO: Deleting pod "simpletest.rc-jvhrz" in namespace "gc-8035"
    Jan 18 18:04:14.279: INFO: Deleting pod "simpletest.rc-jztm7" in namespace "gc-8035"
    Jan 18 18:04:14.295: INFO: Deleting pod "simpletest.rc-kbc69" in namespace "gc-8035"
    Jan 18 18:04:14.371: INFO: Deleting pod "simpletest.rc-kfvss" in namespace "gc-8035"
    Jan 18 18:04:14.388: INFO: Deleting pod "simpletest.rc-kgkg7" in namespace "gc-8035"
    Jan 18 18:04:14.408: INFO: Deleting pod "simpletest.rc-kmsjq" in namespace "gc-8035"
    Jan 18 18:04:14.477: INFO: Deleting pod "simpletest.rc-kz7rn" in namespace "gc-8035"
    Jan 18 18:04:14.494: INFO: Deleting pod "simpletest.rc-l7sgl" in namespace "gc-8035"
    Jan 18 18:04:14.571: INFO: Deleting pod "simpletest.rc-l8jms" in namespace "gc-8035"
    Jan 18 18:04:14.590: INFO: Deleting pod "simpletest.rc-l99n4" in namespace "gc-8035"
    Jan 18 18:04:14.671: INFO: Deleting pod "simpletest.rc-lbqdn" in namespace "gc-8035"
    Jan 18 18:04:14.688: INFO: Deleting pod "simpletest.rc-lcgtz" in namespace "gc-8035"
    Jan 18 18:04:14.760: INFO: Deleting pod "simpletest.rc-lxb8z" in namespace "gc-8035"
    Jan 18 18:04:14.782: INFO: Deleting pod "simpletest.rc-nb88d" in namespace "gc-8035"
    Jan 18 18:04:14.861: INFO: Deleting pod "simpletest.rc-p8dvg" in namespace "gc-8035"
    Jan 18 18:04:14.884: INFO: Deleting pod "simpletest.rc-psv7g" in namespace "gc-8035"
    Jan 18 18:04:14.903: INFO: Deleting pod "simpletest.rc-q9wm9" in namespace "gc-8035"
    Jan 18 18:04:14.973: INFO: Deleting pod "simpletest.rc-qhmb5" in namespace "gc-8035"
    Jan 18 18:04:14.994: INFO: Deleting pod "simpletest.rc-qhskb" in namespace "gc-8035"
    Jan 18 18:04:15.260: INFO: Deleting pod "simpletest.rc-qj9nz" in namespace "gc-8035"
    Jan 18 18:04:15.366: INFO: Deleting pod "simpletest.rc-qpmgf" in namespace "gc-8035"
    Jan 18 18:04:15.567: INFO: Deleting pod "simpletest.rc-r57pz" in namespace "gc-8035"
    Jan 18 18:04:15.673: INFO: Deleting pod "simpletest.rc-rgv8p" in namespace "gc-8035"
    Jan 18 18:04:15.860: INFO: Deleting pod "simpletest.rc-rhp8s" in namespace "gc-8035"
    Jan 18 18:04:15.887: INFO: Deleting pod "simpletest.rc-rplw9" in namespace "gc-8035"
    Jan 18 18:04:15.960: INFO: Deleting pod "simpletest.rc-rs77q" in namespace "gc-8035"
    Jan 18 18:04:15.978: INFO: Deleting pod "simpletest.rc-s4bzp" in namespace "gc-8035"
    Jan 18 18:04:16.001: INFO: Deleting pod "simpletest.rc-s62w9" in namespace "gc-8035"
    Jan 18 18:04:16.067: INFO: Deleting pod "simpletest.rc-shkm9" in namespace "gc-8035"
    Jan 18 18:04:16.091: INFO: Deleting pod "simpletest.rc-srk8d" in namespace "gc-8035"
    Jan 18 18:04:16.166: INFO: Deleting pod "simpletest.rc-t9sct" in namespace "gc-8035"
    Jan 18 18:04:16.205: INFO: Deleting pod "simpletest.rc-tcdvd" in namespace "gc-8035"
    Jan 18 18:04:16.262: INFO: Deleting pod "simpletest.rc-tl8nj" in namespace "gc-8035"
    Jan 18 18:04:16.278: INFO: Deleting pod "simpletest.rc-v2ttb" in namespace "gc-8035"
    Jan 18 18:04:16.360: INFO: Deleting pod "simpletest.rc-wtnct" in namespace "gc-8035"
    Jan 18 18:04:16.380: INFO: Deleting pod "simpletest.rc-xj8fx" in namespace "gc-8035"
    Jan 18 18:04:16.460: INFO: Deleting pod "simpletest.rc-xks44" in namespace "gc-8035"
    Jan 18 18:04:16.478: INFO: Deleting pod "simpletest.rc-z6qnm" in namespace "gc-8035"
    Jan 18 18:04:16.566: INFO: Deleting pod "simpletest.rc-zjcwb" in namespace "gc-8035"
    Jan 18 18:04:16.584: INFO: Deleting pod "simpletest.rc-zsg5s" in namespace "gc-8035"
    Jan 18 18:04:16.665: INFO: Deleting pod "simpletest.rc-zvkgb" in namespace "gc-8035"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 18:04:16.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8035" for this suite. 01/18/23 18:04:16.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:04:16.774
Jan 18 18:04:16.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 18:04:16.776
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:04:16.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:04:16.864
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4060 01/18/23 18:04:16.868
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/18/23 18:04:16.876
Jan 18 18:04:16.891: INFO: Found 0 stateful pods, waiting for 3
Jan 18 18:04:26.898: INFO: Found 1 stateful pods, waiting for 3
Jan 18 18:04:36.901: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:04:36.901: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:04:36.901: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:04:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 18:04:37.163: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 18:04:37.163: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 18:04:37.163: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 18:04:47.199
Jan 18 18:04:47.227: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 18:04:47.227
STEP: Updating Pods in reverse ordinal order 01/18/23 18:04:57.257
Jan 18 18:04:57.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 18:04:57.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 18:04:57.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 18:04:57.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/18/23 18:05:17.506
Jan 18 18:05:17.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 18 18:05:17.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 18 18:05:17.701: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 18 18:05:17.701: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 18 18:05:27.760: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/18/23 18:05:37.794
Jan 18 18:05:37.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 18 18:05:37.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 18 18:05:37.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 18 18:05:37.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 18:05:48.044: INFO: Deleting all statefulset in ns statefulset-4060
Jan 18 18:05:48.051: INFO: Scaling statefulset ss2 to 0
Jan 18 18:05:58.087: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 18:05:58.093: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 18:05:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4060" for this suite. 01/18/23 18:05:58.128
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":262,"skipped":5012,"failed":0}
------------------------------
â€¢ [SLOW TEST] [101.366 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:04:16.774
    Jan 18 18:04:16.775: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 18:04:16.776
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:04:16.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:04:16.864
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4060 01/18/23 18:04:16.868
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/18/23 18:04:16.876
    Jan 18 18:04:16.891: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 18:04:26.898: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 18:04:36.901: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:04:36.901: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:04:36.901: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:04:36.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 18:04:37.163: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 18:04:37.163: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 18:04:37.163: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 18:04:47.199
    Jan 18 18:04:47.227: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 18:04:47.227
    STEP: Updating Pods in reverse ordinal order 01/18/23 18:04:57.257
    Jan 18 18:04:57.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 18:04:57.458: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 18:04:57.458: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 18:04:57.458: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/18/23 18:05:17.506
    Jan 18 18:05:17.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 18 18:05:17.700: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 18 18:05:17.701: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 18 18:05:17.701: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 18 18:05:27.760: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/18/23 18:05:37.794
    Jan 18 18:05:37.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=statefulset-4060 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 18 18:05:37.998: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 18 18:05:37.998: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 18 18:05:37.998: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 18:05:48.044: INFO: Deleting all statefulset in ns statefulset-4060
    Jan 18 18:05:48.051: INFO: Scaling statefulset ss2 to 0
    Jan 18 18:05:58.087: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 18:05:58.093: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 18:05:58.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4060" for this suite. 01/18/23 18:05:58.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:05:58.141
Jan 18 18:05:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:05:58.142
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:05:58.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:05:58.174
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/18/23 18:05:58.178
Jan 18 18:05:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: rename a version 01/18/23 18:06:05.743
STEP: check the new version name is served 01/18/23 18:06:05.766
STEP: check the old version name is removed 01/18/23 18:06:09.081
STEP: check the other version is not changed 01/18/23 18:06:10.61
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:06:18.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5337" for this suite. 01/18/23 18:06:18.395
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":263,"skipped":5030,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.267 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:05:58.141
    Jan 18 18:05:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:05:58.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:05:58.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:05:58.174
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/18/23 18:05:58.178
    Jan 18 18:05:58.179: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: rename a version 01/18/23 18:06:05.743
    STEP: check the new version name is served 01/18/23 18:06:05.766
    STEP: check the old version name is removed 01/18/23 18:06:09.081
    STEP: check the other version is not changed 01/18/23 18:06:10.61
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:06:18.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5337" for this suite. 01/18/23 18:06:18.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:18.408
Jan 18 18:06:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:06:18.41
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:18.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:18.442
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-c89bbff0-c78e-418c-b5e2-c630c83a84b8 01/18/23 18:06:18.446
STEP: Creating a pod to test consume configMaps 01/18/23 18:06:18.455
Jan 18 18:06:18.467: INFO: Waiting up to 5m0s for pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef" in namespace "configmap-3910" to be "Succeeded or Failed"
Jan 18 18:06:18.480: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.5734ms
Jan 18 18:06:20.488: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020867323s
Jan 18 18:06:22.489: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021195867s
STEP: Saw pod success 01/18/23 18:06:22.489
Jan 18 18:06:22.489: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef" satisfied condition "Succeeded or Failed"
Jan 18 18:06:22.499: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef container agnhost-container: <nil>
STEP: delete the pod 01/18/23 18:06:22.529
Jan 18 18:06:22.552: INFO: Waiting for pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef to disappear
Jan 18 18:06:22.557: INFO: Pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:06:22.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3910" for this suite. 01/18/23 18:06:22.564
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":264,"skipped":5035,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:18.408
    Jan 18 18:06:18.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:06:18.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:18.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:18.442
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-c89bbff0-c78e-418c-b5e2-c630c83a84b8 01/18/23 18:06:18.446
    STEP: Creating a pod to test consume configMaps 01/18/23 18:06:18.455
    Jan 18 18:06:18.467: INFO: Waiting up to 5m0s for pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef" in namespace "configmap-3910" to be "Succeeded or Failed"
    Jan 18 18:06:18.480: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Pending", Reason="", readiness=false. Elapsed: 12.5734ms
    Jan 18 18:06:20.488: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020867323s
    Jan 18 18:06:22.489: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021195867s
    STEP: Saw pod success 01/18/23 18:06:22.489
    Jan 18 18:06:22.489: INFO: Pod "pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef" satisfied condition "Succeeded or Failed"
    Jan 18 18:06:22.499: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 18:06:22.529
    Jan 18 18:06:22.552: INFO: Waiting for pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef to disappear
    Jan 18 18:06:22.557: INFO: Pod pod-configmaps-81b2ff90-afc6-40ee-8c88-61287de348ef no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:06:22.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3910" for this suite. 01/18/23 18:06:22.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:22.577
Jan 18 18:06:22.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 18:06:22.578
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:22.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:22.605
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 18:06:22.61
Jan 18 18:06:22.625: INFO: Waiting up to 5m0s for pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e" in namespace "emptydir-4506" to be "Succeeded or Failed"
Jan 18 18:06:22.629: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626317ms
Jan 18 18:06:24.639: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014359968s
Jan 18 18:06:26.638: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012853042s
STEP: Saw pod success 01/18/23 18:06:26.638
Jan 18 18:06:26.638: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e" satisfied condition "Succeeded or Failed"
Jan 18 18:06:26.644: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-8392db9f-b496-48a9-a053-c3c419f9144e container test-container: <nil>
STEP: delete the pod 01/18/23 18:06:26.658
Jan 18 18:06:26.676: INFO: Waiting for pod pod-8392db9f-b496-48a9-a053-c3c419f9144e to disappear
Jan 18 18:06:26.684: INFO: Pod pod-8392db9f-b496-48a9-a053-c3c419f9144e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 18:06:26.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4506" for this suite. 01/18/23 18:06:26.693
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":5046,"failed":0}
------------------------------
â€¢ [4.130 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:22.577
    Jan 18 18:06:22.577: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 18:06:22.578
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:22.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:22.605
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/18/23 18:06:22.61
    Jan 18 18:06:22.625: INFO: Waiting up to 5m0s for pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e" in namespace "emptydir-4506" to be "Succeeded or Failed"
    Jan 18 18:06:22.629: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626317ms
    Jan 18 18:06:24.639: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014359968s
    Jan 18 18:06:26.638: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012853042s
    STEP: Saw pod success 01/18/23 18:06:26.638
    Jan 18 18:06:26.638: INFO: Pod "pod-8392db9f-b496-48a9-a053-c3c419f9144e" satisfied condition "Succeeded or Failed"
    Jan 18 18:06:26.644: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-8392db9f-b496-48a9-a053-c3c419f9144e container test-container: <nil>
    STEP: delete the pod 01/18/23 18:06:26.658
    Jan 18 18:06:26.676: INFO: Waiting for pod pod-8392db9f-b496-48a9-a053-c3c419f9144e to disappear
    Jan 18 18:06:26.684: INFO: Pod pod-8392db9f-b496-48a9-a053-c3c419f9144e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 18:06:26.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4506" for this suite. 01/18/23 18:06:26.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:26.709
Jan 18 18:06:26.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pod-network-test 01/18/23 18:06:26.711
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:26.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:26.742
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-6881 01/18/23 18:06:26.747
STEP: creating a selector 01/18/23 18:06:26.747
STEP: Creating the service pods in kubernetes 01/18/23 18:06:26.747
Jan 18 18:06:26.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 18 18:06:26.780: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6881" to be "running and ready"
Jan 18 18:06:26.785: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485894ms
Jan 18 18:06:26.785: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:06:28.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013960157s
Jan 18 18:06:28.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 18:06:30.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014694461s
Jan 18 18:06:30.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 18:06:32.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014875386s
Jan 18 18:06:32.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 18:06:34.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014330006s
Jan 18 18:06:34.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 18:06:36.793: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013236023s
Jan 18 18:06:36.793: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 18 18:06:38.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012818639s
Jan 18 18:06:38.792: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 18 18:06:38.792: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 18 18:06:38.799: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6881" to be "running and ready"
Jan 18 18:06:38.806: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.038975ms
Jan 18 18:06:38.807: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 18 18:06:38.807: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 01/18/23 18:06:38.813
Jan 18 18:06:38.825: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6881" to be "running"
Jan 18 18:06:38.834: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.155693ms
Jan 18 18:06:40.845: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019504863s
Jan 18 18:06:40.845: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 18 18:06:40.852: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Jan 18 18:06:40.852: INFO: Breadth first check of 10.100.145.170 on host 10.195.74.123...
Jan 18 18:06:40.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.167:9080/dial?request=hostname&protocol=udp&host=10.100.145.170&port=8081&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 18:06:40.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 18:06:40.859: INFO: ExecWithOptions: Clientset creation
Jan 18 18:06:40.859: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.145.170%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 18:06:40.969: INFO: Waiting for responses: map[]
Jan 18 18:06:40.969: INFO: reached 10.100.145.170 after 0/1 tries
Jan 18 18:06:40.969: INFO: Breadth first check of 10.100.158.4 on host 10.195.78.23...
Jan 18 18:06:40.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.167:9080/dial?request=hostname&protocol=udp&host=10.100.158.4&port=8081&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 18 18:06:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 18:06:40.979: INFO: ExecWithOptions: Clientset creation
Jan 18 18:06:40.979: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.158.4%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 18 18:06:41.085: INFO: Waiting for responses: map[]
Jan 18 18:06:41.085: INFO: reached 10.100.158.4 after 0/1 tries
Jan 18 18:06:41.085: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 18 18:06:41.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6881" for this suite. 01/18/23 18:06:41.094
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":266,"skipped":5066,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.403 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:26.709
    Jan 18 18:06:26.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pod-network-test 01/18/23 18:06:26.711
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:26.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:26.742
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-6881 01/18/23 18:06:26.747
    STEP: creating a selector 01/18/23 18:06:26.747
    STEP: Creating the service pods in kubernetes 01/18/23 18:06:26.747
    Jan 18 18:06:26.747: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 18 18:06:26.780: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-6881" to be "running and ready"
    Jan 18 18:06:26.785: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.485894ms
    Jan 18 18:06:26.785: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:06:28.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013960157s
    Jan 18 18:06:28.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 18:06:30.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014694461s
    Jan 18 18:06:30.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 18:06:32.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014875386s
    Jan 18 18:06:32.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 18:06:34.794: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014330006s
    Jan 18 18:06:34.794: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 18:06:36.793: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013236023s
    Jan 18 18:06:36.793: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 18 18:06:38.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.012818639s
    Jan 18 18:06:38.792: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 18 18:06:38.792: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 18 18:06:38.799: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-6881" to be "running and ready"
    Jan 18 18:06:38.806: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.038975ms
    Jan 18 18:06:38.807: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 18 18:06:38.807: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 01/18/23 18:06:38.813
    Jan 18 18:06:38.825: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-6881" to be "running"
    Jan 18 18:06:38.834: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.155693ms
    Jan 18 18:06:40.845: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.019504863s
    Jan 18 18:06:40.845: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 18 18:06:40.852: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Jan 18 18:06:40.852: INFO: Breadth first check of 10.100.145.170 on host 10.195.74.123...
    Jan 18 18:06:40.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.167:9080/dial?request=hostname&protocol=udp&host=10.100.145.170&port=8081&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 18:06:40.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 18:06:40.859: INFO: ExecWithOptions: Clientset creation
    Jan 18 18:06:40.859: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.145.170%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 18:06:40.969: INFO: Waiting for responses: map[]
    Jan 18 18:06:40.969: INFO: reached 10.100.145.170 after 0/1 tries
    Jan 18 18:06:40.969: INFO: Breadth first check of 10.100.158.4 on host 10.195.78.23...
    Jan 18 18:06:40.978: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.100.145.167:9080/dial?request=hostname&protocol=udp&host=10.100.158.4&port=8081&tries=1'] Namespace:pod-network-test-6881 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 18 18:06:40.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 18:06:40.979: INFO: ExecWithOptions: Clientset creation
    Jan 18 18:06:40.979: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6881/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.100.145.167%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.100.158.4%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 18 18:06:41.085: INFO: Waiting for responses: map[]
    Jan 18 18:06:41.085: INFO: reached 10.100.158.4 after 0/1 tries
    Jan 18 18:06:41.085: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 18 18:06:41.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-6881" for this suite. 01/18/23 18:06:41.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:41.114
Jan 18 18:06:41.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename tables 01/18/23 18:06:41.115
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.142
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 18 18:06:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6635" for this suite. 01/18/23 18:06:41.156
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":267,"skipped":5085,"failed":0}
------------------------------
â€¢ [0.055 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:41.114
    Jan 18 18:06:41.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename tables 01/18/23 18:06:41.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.142
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 18 18:06:41.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-6635" for this suite. 01/18/23 18:06:41.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:41.171
Jan 18 18:06:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename events 01/18/23 18:06:41.172
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.202
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/18/23 18:06:41.206
STEP: listing events in all namespaces 01/18/23 18:06:41.228
STEP: listing events in test namespace 01/18/23 18:06:41.236
STEP: listing events with field selection filtering on source 01/18/23 18:06:41.24
STEP: listing events with field selection filtering on reportingController 01/18/23 18:06:41.245
STEP: getting the test event 01/18/23 18:06:41.314
STEP: patching the test event 01/18/23 18:06:41.318
STEP: getting the test event 01/18/23 18:06:41.335
STEP: updating the test event 01/18/23 18:06:41.339
STEP: getting the test event 01/18/23 18:06:41.351
STEP: deleting the test event 01/18/23 18:06:41.355
STEP: listing events in all namespaces 01/18/23 18:06:41.365
STEP: listing events in test namespace 01/18/23 18:06:41.371
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 18 18:06:41.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5140" for this suite. 01/18/23 18:06:41.423
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":268,"skipped":5111,"failed":0}
------------------------------
â€¢ [0.269 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:41.171
    Jan 18 18:06:41.171: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename events 01/18/23 18:06:41.172
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.202
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/18/23 18:06:41.206
    STEP: listing events in all namespaces 01/18/23 18:06:41.228
    STEP: listing events in test namespace 01/18/23 18:06:41.236
    STEP: listing events with field selection filtering on source 01/18/23 18:06:41.24
    STEP: listing events with field selection filtering on reportingController 01/18/23 18:06:41.245
    STEP: getting the test event 01/18/23 18:06:41.314
    STEP: patching the test event 01/18/23 18:06:41.318
    STEP: getting the test event 01/18/23 18:06:41.335
    STEP: updating the test event 01/18/23 18:06:41.339
    STEP: getting the test event 01/18/23 18:06:41.351
    STEP: deleting the test event 01/18/23 18:06:41.355
    STEP: listing events in all namespaces 01/18/23 18:06:41.365
    STEP: listing events in test namespace 01/18/23 18:06:41.371
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 18 18:06:41.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5140" for this suite. 01/18/23 18:06:41.423
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:41.441
Jan 18 18:06:41.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:06:41.443
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.473
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-2749/configmap-test-09baacfc-d25f-4112-82a6-5c876132dea4 01/18/23 18:06:41.477
STEP: Creating a pod to test consume configMaps 01/18/23 18:06:41.487
Jan 18 18:06:41.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6" in namespace "configmap-2749" to be "Succeeded or Failed"
Jan 18 18:06:41.507: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626826ms
Jan 18 18:06:43.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014073169s
Jan 18 18:06:45.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01378854s
STEP: Saw pod success 01/18/23 18:06:45.517
Jan 18 18:06:45.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6" satisfied condition "Succeeded or Failed"
Jan 18 18:06:45.525: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 container env-test: <nil>
STEP: delete the pod 01/18/23 18:06:45.541
Jan 18 18:06:45.563: INFO: Waiting for pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 to disappear
Jan 18 18:06:45.569: INFO: Pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:06:45.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2749" for this suite. 01/18/23 18:06:45.577
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":269,"skipped":5113,"failed":0}
------------------------------
â€¢ [4.148 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:41.441
    Jan 18 18:06:41.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:06:41.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:41.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:41.473
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-2749/configmap-test-09baacfc-d25f-4112-82a6-5c876132dea4 01/18/23 18:06:41.477
    STEP: Creating a pod to test consume configMaps 01/18/23 18:06:41.487
    Jan 18 18:06:41.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6" in namespace "configmap-2749" to be "Succeeded or Failed"
    Jan 18 18:06:41.507: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.626826ms
    Jan 18 18:06:43.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014073169s
    Jan 18 18:06:45.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01378854s
    STEP: Saw pod success 01/18/23 18:06:45.517
    Jan 18 18:06:45.517: INFO: Pod "pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6" satisfied condition "Succeeded or Failed"
    Jan 18 18:06:45.525: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 container env-test: <nil>
    STEP: delete the pod 01/18/23 18:06:45.541
    Jan 18 18:06:45.563: INFO: Waiting for pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 to disappear
    Jan 18 18:06:45.569: INFO: Pod pod-configmaps-ce3a6549-7c5d-49ad-89ca-fd2d1caabbc6 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:06:45.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2749" for this suite. 01/18/23 18:06:45.577
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:45.589
Jan 18 18:06:45.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:06:45.591
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:45.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:45.619
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/18/23 18:06:45.624
Jan 18 18:06:45.639: INFO: Waiting up to 5m0s for pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7" in namespace "downward-api-9735" to be "running and ready"
Jan 18 18:06:45.644: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523932ms
Jan 18 18:06:45.645: INFO: The phase of Pod labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:06:47.652: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012722054s
Jan 18 18:06:47.652: INFO: The phase of Pod labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7 is Running (Ready = true)
Jan 18 18:06:47.652: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7" satisfied condition "running and ready"
Jan 18 18:06:48.199: INFO: Successfully updated pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 18:06:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9735" for this suite. 01/18/23 18:06:52.257
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":270,"skipped":5116,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.680 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:45.589
    Jan 18 18:06:45.589: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:06:45.591
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:45.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:45.619
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/18/23 18:06:45.624
    Jan 18 18:06:45.639: INFO: Waiting up to 5m0s for pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7" in namespace "downward-api-9735" to be "running and ready"
    Jan 18 18:06:45.644: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.523932ms
    Jan 18 18:06:45.645: INFO: The phase of Pod labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:06:47.652: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7": Phase="Running", Reason="", readiness=true. Elapsed: 2.012722054s
    Jan 18 18:06:47.652: INFO: The phase of Pod labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7 is Running (Ready = true)
    Jan 18 18:06:47.652: INFO: Pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7" satisfied condition "running and ready"
    Jan 18 18:06:48.199: INFO: Successfully updated pod "labelsupdate5f70c3d6-abd8-4923-bd16-aa2baddc05e7"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 18:06:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9735" for this suite. 01/18/23 18:06:52.257
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:52.27
Jan 18 18:06:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:06:52.271
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:52.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:52.302
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-4ebec106-1a4d-4e8d-bb37-73565537b60a 01/18/23 18:06:52.307
STEP: Creating a pod to test consume configMaps 01/18/23 18:06:52.315
Jan 18 18:06:52.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869" in namespace "projected-6385" to be "Succeeded or Failed"
Jan 18 18:06:52.360: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Pending", Reason="", readiness=false. Elapsed: 25.204058ms
Jan 18 18:06:54.370: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035312174s
Jan 18 18:06:56.368: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033804813s
STEP: Saw pod success 01/18/23 18:06:56.368
Jan 18 18:06:56.369: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869" satisfied condition "Succeeded or Failed"
Jan 18 18:06:56.375: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 18:06:56.39
Jan 18 18:06:56.412: INFO: Waiting for pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 to disappear
Jan 18 18:06:56.419: INFO: Pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 18:06:56.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6385" for this suite. 01/18/23 18:06:56.427
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":271,"skipped":5117,"failed":0}
------------------------------
â€¢ [4.175 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:52.27
    Jan 18 18:06:52.271: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:06:52.271
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:52.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:52.302
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-4ebec106-1a4d-4e8d-bb37-73565537b60a 01/18/23 18:06:52.307
    STEP: Creating a pod to test consume configMaps 01/18/23 18:06:52.315
    Jan 18 18:06:52.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869" in namespace "projected-6385" to be "Succeeded or Failed"
    Jan 18 18:06:52.360: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Pending", Reason="", readiness=false. Elapsed: 25.204058ms
    Jan 18 18:06:54.370: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035312174s
    Jan 18 18:06:56.368: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033804813s
    STEP: Saw pod success 01/18/23 18:06:56.368
    Jan 18 18:06:56.369: INFO: Pod "pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869" satisfied condition "Succeeded or Failed"
    Jan 18 18:06:56.375: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 18:06:56.39
    Jan 18 18:06:56.412: INFO: Waiting for pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 to disappear
    Jan 18 18:06:56.419: INFO: Pod pod-projected-configmaps-f6c0b886-05c6-4f35-95f6-0153ee4e0869 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 18:06:56.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6385" for this suite. 01/18/23 18:06:56.427
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:56.445
Jan 18 18:06:56.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:06:56.447
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:56.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:56.478
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/18/23 18:06:56.489
STEP: watching for the Service to be added 01/18/23 18:06:56.507
Jan 18 18:06:56.511: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 18 18:06:56.511: INFO: Service test-service-552xt created
STEP: Getting /status 01/18/23 18:06:56.511
Jan 18 18:06:56.518: INFO: Service test-service-552xt has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/18/23 18:06:56.518
STEP: watching for the Service to be patched 01/18/23 18:06:56.532
Jan 18 18:06:56.534: INFO: observed Service test-service-552xt in namespace services-93 with annotations: map[] & LoadBalancer: {[]}
Jan 18 18:06:56.534: INFO: Found Service test-service-552xt in namespace services-93 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 18 18:06:56.535: INFO: Service test-service-552xt has service status patched
STEP: updating the ServiceStatus 01/18/23 18:06:56.535
Jan 18 18:06:56.558: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/18/23 18:06:56.559
Jan 18 18:06:56.562: INFO: Observed Service test-service-552xt in namespace services-93 with annotations: map[] & Conditions: {[]}
Jan 18 18:06:56.562: INFO: Observed event: &Service{ObjectMeta:{test-service-552xt  services-93  00ac6c38-6174-4931-aac8-94bc4e57e984 2631948014 0 2023-01-18 18:06:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 18:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 18:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.75.8,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.75.8],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 18 18:06:56.562: INFO: Found Service test-service-552xt in namespace services-93 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 18 18:06:56.563: INFO: Service test-service-552xt has service status updated
STEP: patching the service 01/18/23 18:06:56.563
STEP: watching for the Service to be patched 01/18/23 18:06:56.582
Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
Jan 18 18:06:56.585: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service:patched test-service-static:true]
Jan 18 18:06:56.586: INFO: Service test-service-552xt patched
STEP: deleting the service 01/18/23 18:06:56.586
STEP: watching for the Service to be deleted 01/18/23 18:06:56.615
Jan 18 18:06:56.617: INFO: Observed event: ADDED
Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
Jan 18 18:06:56.618: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 18 18:06:56.618: INFO: Service test-service-552xt deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:06:56.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-93" for this suite. 01/18/23 18:06:56.626
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":272,"skipped":5120,"failed":0}
------------------------------
â€¢ [0.193 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:56.445
    Jan 18 18:06:56.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:06:56.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:56.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:56.478
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/18/23 18:06:56.489
    STEP: watching for the Service to be added 01/18/23 18:06:56.507
    Jan 18 18:06:56.511: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 18 18:06:56.511: INFO: Service test-service-552xt created
    STEP: Getting /status 01/18/23 18:06:56.511
    Jan 18 18:06:56.518: INFO: Service test-service-552xt has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/18/23 18:06:56.518
    STEP: watching for the Service to be patched 01/18/23 18:06:56.532
    Jan 18 18:06:56.534: INFO: observed Service test-service-552xt in namespace services-93 with annotations: map[] & LoadBalancer: {[]}
    Jan 18 18:06:56.534: INFO: Found Service test-service-552xt in namespace services-93 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 18 18:06:56.535: INFO: Service test-service-552xt has service status patched
    STEP: updating the ServiceStatus 01/18/23 18:06:56.535
    Jan 18 18:06:56.558: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/18/23 18:06:56.559
    Jan 18 18:06:56.562: INFO: Observed Service test-service-552xt in namespace services-93 with annotations: map[] & Conditions: {[]}
    Jan 18 18:06:56.562: INFO: Observed event: &Service{ObjectMeta:{test-service-552xt  services-93  00ac6c38-6174-4931-aac8-94bc4e57e984 2631948014 0 2023-01-18 18:06:56 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-18 18:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-18 18:06:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.75.8,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.75.8],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 18 18:06:56.562: INFO: Found Service test-service-552xt in namespace services-93 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 18 18:06:56.563: INFO: Service test-service-552xt has service status updated
    STEP: patching the service 01/18/23 18:06:56.563
    STEP: watching for the Service to be patched 01/18/23 18:06:56.582
    Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
    Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
    Jan 18 18:06:56.585: INFO: observed Service test-service-552xt in namespace services-93 with labels: map[test-service-static:true]
    Jan 18 18:06:56.585: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service:patched test-service-static:true]
    Jan 18 18:06:56.586: INFO: Service test-service-552xt patched
    STEP: deleting the service 01/18/23 18:06:56.586
    STEP: watching for the Service to be deleted 01/18/23 18:06:56.615
    Jan 18 18:06:56.617: INFO: Observed event: ADDED
    Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
    Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
    Jan 18 18:06:56.618: INFO: Observed event: MODIFIED
    Jan 18 18:06:56.618: INFO: Found Service test-service-552xt in namespace services-93 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 18 18:06:56.618: INFO: Service test-service-552xt deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:06:56.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-93" for this suite. 01/18/23 18:06:56.626
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:06:56.64
Jan 18 18:06:56.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:06:56.64
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:56.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:56.666
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/18/23 18:06:56.67
Jan 18 18:06:56.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab" in namespace "downward-api-7810" to be "Succeeded or Failed"
Jan 18 18:06:56.707: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.784215ms
Jan 18 18:06:58.716: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033607016s
Jan 18 18:07:00.717: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034268586s
STEP: Saw pod success 01/18/23 18:07:00.717
Jan 18 18:07:00.717: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab" satisfied condition "Succeeded or Failed"
Jan 18 18:07:00.724: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab container client-container: <nil>
STEP: delete the pod 01/18/23 18:07:00.741
Jan 18 18:07:00.762: INFO: Waiting for pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab to disappear
Jan 18 18:07:00.768: INFO: Pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 18:07:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7810" for this suite. 01/18/23 18:07:00.776
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":273,"skipped":5133,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:06:56.64
    Jan 18 18:06:56.640: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:06:56.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:06:56.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:06:56.666
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/18/23 18:06:56.67
    Jan 18 18:06:56.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab" in namespace "downward-api-7810" to be "Succeeded or Failed"
    Jan 18 18:06:56.707: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.784215ms
    Jan 18 18:06:58.716: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033607016s
    Jan 18 18:07:00.717: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034268586s
    STEP: Saw pod success 01/18/23 18:07:00.717
    Jan 18 18:07:00.717: INFO: Pod "downwardapi-volume-ae377485-f652-4a84-995c-571811509bab" satisfied condition "Succeeded or Failed"
    Jan 18 18:07:00.724: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab container client-container: <nil>
    STEP: delete the pod 01/18/23 18:07:00.741
    Jan 18 18:07:00.762: INFO: Waiting for pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab to disappear
    Jan 18 18:07:00.768: INFO: Pod downwardapi-volume-ae377485-f652-4a84-995c-571811509bab no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 18:07:00.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7810" for this suite. 01/18/23 18:07:00.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:00.789
Jan 18 18:07:00.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 18:07:00.79
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:00.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:00.819
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/18/23 18:07:00.831
STEP: Patching the Job 01/18/23 18:07:00.839
STEP: Watching for Job to be patched 01/18/23 18:07:00.866
Jan 18 18:07:00.869: INFO: Event ADDED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 18:07:00.869: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 18 18:07:00.869: INFO: Event MODIFIED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/18/23 18:07:00.869
STEP: Watching for Job to be updated 01/18/23 18:07:00.889
Jan 18 18:07:00.892: INFO: Event MODIFIED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:00.892: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/18/23 18:07:00.892
Jan 18 18:07:00.898: INFO: Job: e2e-4r6xd as labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd]
STEP: Waiting for job to complete 01/18/23 18:07:00.898
STEP: Delete a job collection with a labelselector 01/18/23 18:07:08.907
STEP: Watching for Job to be deleted 01/18/23 18:07:08.927
Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 18 18:07:08.930: INFO: Event DELETED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/18/23 18:07:08.931
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 18:07:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8111" for this suite. 01/18/23 18:07:08.946
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":274,"skipped":5183,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.172 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:00.789
    Jan 18 18:07:00.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 18:07:00.79
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:00.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:00.819
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/18/23 18:07:00.831
    STEP: Patching the Job 01/18/23 18:07:00.839
    STEP: Watching for Job to be patched 01/18/23 18:07:00.866
    Jan 18 18:07:00.869: INFO: Event ADDED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 18:07:00.869: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 18 18:07:00.869: INFO: Event MODIFIED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/18/23 18:07:00.869
    STEP: Watching for Job to be updated 01/18/23 18:07:00.889
    Jan 18 18:07:00.892: INFO: Event MODIFIED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:00.892: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/18/23 18:07:00.892
    Jan 18 18:07:00.898: INFO: Job: e2e-4r6xd as labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd]
    STEP: Waiting for job to complete 01/18/23 18:07:00.898
    STEP: Delete a job collection with a labelselector 01/18/23 18:07:08.907
    STEP: Watching for Job to be deleted 01/18/23 18:07:08.927
    Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:08.930: INFO: Event MODIFIED observed for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 18 18:07:08.930: INFO: Event DELETED found for Job e2e-4r6xd in namespace job-8111 with labels: map[e2e-4r6xd:patched e2e-job-label:e2e-4r6xd] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/18/23 18:07:08.931
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 18:07:08.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8111" for this suite. 01/18/23 18:07:08.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:08.968
Jan 18 18:07:08.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 18:07:08.969
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:08.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:08.994
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 18 18:07:08.998: INFO: Creating deployment "test-recreate-deployment"
Jan 18 18:07:09.007: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 18 18:07:09.019: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jan 18 18:07:11.038: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 18 18:07:11.046: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 18 18:07:11.066: INFO: Updating deployment test-recreate-deployment
Jan 18 18:07:11.066: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 18:07:11.220: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4074  bbc607a4-3570-4bd8-a35d-815e76583017 2631948572 2 2023-01-18 18:07:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b93788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 18:07:11 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 18:07:11 +0000 UTC,LastTransitionTime:2023-01-18 18:07:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 18:07:11.261: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4074  ea13ca61-4744-4c8f-aab3-eb5f45e1d83e 2631948570 1 2023-01-18 18:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bbc607a4-3570-4bd8-a35d-815e76583017 0xc0052d8100 0xc0052d8101}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc607a4-3570-4bd8-a35d-815e76583017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052d8198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:07:11.261: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 18 18:07:11.261: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4074  0576f1a5-19f9-42c2-9130-53dcecd345eb 2631948560 2 2023-01-18 18:07:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bbc607a4-3570-4bd8-a35d-815e76583017 0xc002b93fb7 0xc002b93fb8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc607a4-3570-4bd8-a35d-815e76583017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052d8098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:07:11.270: INFO: Pod "test-recreate-deployment-9d58999df-t6djx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-t6djx test-recreate-deployment-9d58999df- deployment-4074  22654ebe-2518-460a-a633-5602229a1e85 2631948569 0 2023-01-18 18:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df ea13ca61-4744-4c8f-aab3-eb5f45e1d83e 0xc0052d8630 0xc0052d8631}] [] [{kube-controller-manager Update v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea13ca61-4744-4c8f-aab3-eb5f45e1d83e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm5fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm5fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 18:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 18:07:11.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4074" for this suite. 01/18/23 18:07:11.278
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":275,"skipped":5241,"failed":0}
------------------------------
â€¢ [2.322 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:08.968
    Jan 18 18:07:08.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 18:07:08.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:08.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:08.994
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 18 18:07:08.998: INFO: Creating deployment "test-recreate-deployment"
    Jan 18 18:07:09.007: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 18 18:07:09.019: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jan 18 18:07:11.038: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 18 18:07:11.046: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 18 18:07:11.066: INFO: Updating deployment test-recreate-deployment
    Jan 18 18:07:11.066: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 18:07:11.220: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4074  bbc607a4-3570-4bd8-a35d-815e76583017 2631948572 2 2023-01-18 18:07:09 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b93788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-18 18:07:11 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-18 18:07:11 +0000 UTC,LastTransitionTime:2023-01-18 18:07:09 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 18:07:11.261: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-4074  ea13ca61-4744-4c8f-aab3-eb5f45e1d83e 2631948570 1 2023-01-18 18:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bbc607a4-3570-4bd8-a35d-815e76583017 0xc0052d8100 0xc0052d8101}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc607a4-3570-4bd8-a35d-815e76583017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052d8198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:07:11.261: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 18 18:07:11.261: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-4074  0576f1a5-19f9-42c2-9130-53dcecd345eb 2631948560 2 2023-01-18 18:07:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bbc607a4-3570-4bd8-a35d-815e76583017 0xc002b93fb7 0xc002b93fb8}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bbc607a4-3570-4bd8-a35d-815e76583017\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052d8098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:07:11.270: INFO: Pod "test-recreate-deployment-9d58999df-t6djx" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-t6djx test-recreate-deployment-9d58999df- deployment-4074  22654ebe-2518-460a-a633-5602229a1e85 2631948569 0 2023-01-18 18:07:11 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df ea13ca61-4744-4c8f-aab3-eb5f45e1d83e 0xc0052d8630 0xc0052d8631}] [] [{kube-controller-manager Update v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea13ca61-4744-4c8f-aab3-eb5f45e1d83e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 18:07:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm5fr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm5fr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:,StartTime:2023-01-18 18:07:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 18:07:11.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4074" for this suite. 01/18/23 18:07:11.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:11.291
Jan 18 18:07:11.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 18:07:11.292
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:11.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:11.318
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 18:07:11.348
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:07:11.792
STEP: Deploying the webhook pod 01/18/23 18:07:11.814
STEP: Wait for the deployment to be ready 01/18/23 18:07:11.838
Jan 18 18:07:11.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 18:07:13.877
STEP: Verifying the service has paired with the endpoint 01/18/23 18:07:13.9
Jan 18 18:07:14.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/18/23 18:07:14.91
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 18:07:14.95
STEP: Creating a configMap that should not be mutated 01/18/23 18:07:14.962
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 18:07:14.982
STEP: Creating a configMap that should be mutated 01/18/23 18:07:14.996
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:07:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9284" for this suite. 01/18/23 18:07:15.06
STEP: Destroying namespace "webhook-9284-markers" for this suite. 01/18/23 18:07:15.073
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":276,"skipped":5261,"failed":0}
------------------------------
â€¢ [3.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:11.291
    Jan 18 18:07:11.291: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 18:07:11.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:11.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:11.318
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 18:07:11.348
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:07:11.792
    STEP: Deploying the webhook pod 01/18/23 18:07:11.814
    STEP: Wait for the deployment to be ready 01/18/23 18:07:11.838
    Jan 18 18:07:11.850: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 18:07:13.877
    STEP: Verifying the service has paired with the endpoint 01/18/23 18:07:13.9
    Jan 18 18:07:14.900: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/18/23 18:07:14.91
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/18/23 18:07:14.95
    STEP: Creating a configMap that should not be mutated 01/18/23 18:07:14.962
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/18/23 18:07:14.982
    STEP: Creating a configMap that should be mutated 01/18/23 18:07:14.996
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:07:15.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9284" for this suite. 01/18/23 18:07:15.06
    STEP: Destroying namespace "webhook-9284-markers" for this suite. 01/18/23 18:07:15.073
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:15.156
Jan 18 18:07:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:07:15.157
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:15.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:15.187
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 18 18:07:15.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 18:07:19.523
Jan 18 18:07:19.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 create -f -'
Jan 18 18:07:21.069: INFO: stderr: ""
Jan 18 18:07:21.069: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 18:07:21.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 delete e2e-test-crd-publish-openapi-4693-crds test-cr'
Jan 18 18:07:21.179: INFO: stderr: ""
Jan 18 18:07:21.179: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 18 18:07:21.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 apply -f -'
Jan 18 18:07:21.480: INFO: stderr: ""
Jan 18 18:07:21.480: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 18 18:07:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 delete e2e-test-crd-publish-openapi-4693-crds test-cr'
Jan 18 18:07:21.591: INFO: stderr: ""
Jan 18 18:07:21.591: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/18/23 18:07:21.591
Jan 18 18:07:21.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 explain e2e-test-crd-publish-openapi-4693-crds'
Jan 18 18:07:21.854: INFO: stderr: ""
Jan 18 18:07:21.854: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4693-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:07:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-802" for this suite. 01/18/23 18:07:24.161
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":277,"skipped":5264,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.019 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:15.156
    Jan 18 18:07:15.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:07:15.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:15.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:15.187
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 18 18:07:15.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/18/23 18:07:19.523
    Jan 18 18:07:19.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 create -f -'
    Jan 18 18:07:21.069: INFO: stderr: ""
    Jan 18 18:07:21.069: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 18:07:21.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 delete e2e-test-crd-publish-openapi-4693-crds test-cr'
    Jan 18 18:07:21.179: INFO: stderr: ""
    Jan 18 18:07:21.179: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 18 18:07:21.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 apply -f -'
    Jan 18 18:07:21.480: INFO: stderr: ""
    Jan 18 18:07:21.480: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 18 18:07:21.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 --namespace=crd-publish-openapi-802 delete e2e-test-crd-publish-openapi-4693-crds test-cr'
    Jan 18 18:07:21.591: INFO: stderr: ""
    Jan 18 18:07:21.591: INFO: stdout: "e2e-test-crd-publish-openapi-4693-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/18/23 18:07:21.591
    Jan 18 18:07:21.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-802 explain e2e-test-crd-publish-openapi-4693-crds'
    Jan 18 18:07:21.854: INFO: stderr: ""
    Jan 18 18:07:21.854: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4693-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:07:24.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-802" for this suite. 01/18/23 18:07:24.161
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:24.176
Jan 18 18:07:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:07:24.177
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:24.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:24.207
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 18:07:24.211
Jan 18 18:07:24.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
Jan 18 18:07:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:07:38.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4759" for this suite. 01/18/23 18:07:39.004
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":278,"skipped":5266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.842 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:24.176
    Jan 18 18:07:24.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:07:24.177
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:24.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:24.207
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/18/23 18:07:24.211
    Jan 18 18:07:24.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    Jan 18 18:07:27.283: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:07:38.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4759" for this suite. 01/18/23 18:07:39.004
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:39.019
Jan 18 18:07:39.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename controllerrevisions 01/18/23 18:07:39.02
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:39.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:39.045
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-x8x86-daemon-set" 01/18/23 18:07:39.074
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 18:07:39.084
Jan 18 18:07:39.098: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
Jan 18 18:07:39.098: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 18:07:40.115: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
Jan 18 18:07:40.115: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 18:07:41.115: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 2
Jan 18 18:07:41.116: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-x8x86-daemon-set
STEP: Confirm DaemonSet "e2e-x8x86-daemon-set" successfully created with "daemonset-name=e2e-x8x86-daemon-set" label 01/18/23 18:07:41.164
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-x8x86-daemon-set" 01/18/23 18:07:41.181
Jan 18 18:07:41.188: INFO: Located ControllerRevision: "e2e-x8x86-daemon-set-86559c95b9"
STEP: Patching ControllerRevision "e2e-x8x86-daemon-set-86559c95b9" 01/18/23 18:07:41.195
Jan 18 18:07:41.207: INFO: e2e-x8x86-daemon-set-86559c95b9 has been patched
STEP: Create a new ControllerRevision 01/18/23 18:07:41.207
Jan 18 18:07:41.216: INFO: Created ControllerRevision: e2e-x8x86-daemon-set-56c8d6fb97
STEP: Confirm that there are two ControllerRevisions 01/18/23 18:07:41.216
Jan 18 18:07:41.216: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 18:07:41.222: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-x8x86-daemon-set-86559c95b9" 01/18/23 18:07:41.222
STEP: Confirm that there is only one ControllerRevision 01/18/23 18:07:41.233
Jan 18 18:07:41.233: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 18:07:41.239: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-x8x86-daemon-set-56c8d6fb97" 01/18/23 18:07:41.245
Jan 18 18:07:41.261: INFO: e2e-x8x86-daemon-set-56c8d6fb97 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 18:07:41.261
W0118 18:07:41.272331      21 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/18/23 18:07:41.272
Jan 18 18:07:41.272: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 18:07:42.279: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 18:07:42.287: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-x8x86-daemon-set-56c8d6fb97=updated" 01/18/23 18:07:42.287
STEP: Confirm that there is only one ControllerRevision 01/18/23 18:07:42.305
Jan 18 18:07:42.305: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 18 18:07:42.313: INFO: Found 1 ControllerRevisions
Jan 18 18:07:42.318: INFO: ControllerRevision "e2e-x8x86-daemon-set-686454fcd4" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-x8x86-daemon-set" 01/18/23 18:07:42.324
STEP: deleting DaemonSet.extensions e2e-x8x86-daemon-set in namespace controllerrevisions-8261, will wait for the garbage collector to delete the pods 01/18/23 18:07:42.324
Jan 18 18:07:42.398: INFO: Deleting DaemonSet.extensions e2e-x8x86-daemon-set took: 13.626235ms
Jan 18 18:07:42.499: INFO: Terminating DaemonSet.extensions e2e-x8x86-daemon-set pods took: 101.056096ms
Jan 18 18:07:43.507: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
Jan 18 18:07:43.507: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-x8x86-daemon-set
Jan 18 18:07:43.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631949777"},"items":null}

Jan 18 18:07:43.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631949777"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:07:43.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-8261" for this suite. 01/18/23 18:07:43.552
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":279,"skipped":5268,"failed":0}
------------------------------
â€¢ [4.547 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:39.019
    Jan 18 18:07:39.019: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename controllerrevisions 01/18/23 18:07:39.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:39.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:39.045
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-x8x86-daemon-set" 01/18/23 18:07:39.074
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 18:07:39.084
    Jan 18 18:07:39.098: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
    Jan 18 18:07:39.098: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 18:07:40.115: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
    Jan 18 18:07:40.115: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 18:07:41.115: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 2
    Jan 18 18:07:41.116: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-x8x86-daemon-set
    STEP: Confirm DaemonSet "e2e-x8x86-daemon-set" successfully created with "daemonset-name=e2e-x8x86-daemon-set" label 01/18/23 18:07:41.164
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-x8x86-daemon-set" 01/18/23 18:07:41.181
    Jan 18 18:07:41.188: INFO: Located ControllerRevision: "e2e-x8x86-daemon-set-86559c95b9"
    STEP: Patching ControllerRevision "e2e-x8x86-daemon-set-86559c95b9" 01/18/23 18:07:41.195
    Jan 18 18:07:41.207: INFO: e2e-x8x86-daemon-set-86559c95b9 has been patched
    STEP: Create a new ControllerRevision 01/18/23 18:07:41.207
    Jan 18 18:07:41.216: INFO: Created ControllerRevision: e2e-x8x86-daemon-set-56c8d6fb97
    STEP: Confirm that there are two ControllerRevisions 01/18/23 18:07:41.216
    Jan 18 18:07:41.216: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 18:07:41.222: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-x8x86-daemon-set-86559c95b9" 01/18/23 18:07:41.222
    STEP: Confirm that there is only one ControllerRevision 01/18/23 18:07:41.233
    Jan 18 18:07:41.233: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 18:07:41.239: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-x8x86-daemon-set-56c8d6fb97" 01/18/23 18:07:41.245
    Jan 18 18:07:41.261: INFO: e2e-x8x86-daemon-set-56c8d6fb97 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/18/23 18:07:41.261
    W0118 18:07:41.272331      21 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/18/23 18:07:41.272
    Jan 18 18:07:41.272: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 18:07:42.279: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 18:07:42.287: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-x8x86-daemon-set-56c8d6fb97=updated" 01/18/23 18:07:42.287
    STEP: Confirm that there is only one ControllerRevision 01/18/23 18:07:42.305
    Jan 18 18:07:42.305: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 18 18:07:42.313: INFO: Found 1 ControllerRevisions
    Jan 18 18:07:42.318: INFO: ControllerRevision "e2e-x8x86-daemon-set-686454fcd4" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-x8x86-daemon-set" 01/18/23 18:07:42.324
    STEP: deleting DaemonSet.extensions e2e-x8x86-daemon-set in namespace controllerrevisions-8261, will wait for the garbage collector to delete the pods 01/18/23 18:07:42.324
    Jan 18 18:07:42.398: INFO: Deleting DaemonSet.extensions e2e-x8x86-daemon-set took: 13.626235ms
    Jan 18 18:07:42.499: INFO: Terminating DaemonSet.extensions e2e-x8x86-daemon-set pods took: 101.056096ms
    Jan 18 18:07:43.507: INFO: Number of nodes with available pods controlled by daemonset e2e-x8x86-daemon-set: 0
    Jan 18 18:07:43.507: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-x8x86-daemon-set
    Jan 18 18:07:43.516: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631949777"},"items":null}

    Jan 18 18:07:43.523: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631949777"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:07:43.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-8261" for this suite. 01/18/23 18:07:43.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:43.57
Jan 18 18:07:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 18:07:43.571
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:43.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:43.601
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan 18 18:07:43.621: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 18 18:07:48.630: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 18:07:48.63
Jan 18 18:07:48.630: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 18:07:48.649
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 18:07:48.672: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9297  4f0ad27a-857b-4823-a568-4684a5f6fbbc 2631949997 1 2023-01-18 18:07:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005229f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 18 18:07:48.680: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9297  ba00b77d-44be-4648-b899-3290ba519394 2631950000 1 2023-01-18 18:07:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 4f0ad27a-857b-4823-a568-4684a5f6fbbc 0xc00534c587 0xc00534c588}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f0ad27a-857b-4823-a568-4684a5f6fbbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00534c638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:07:48.680: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 18 18:07:48.680: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9297  b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e 2631949999 1 2023-01-18 18:07:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4f0ad27a-857b-4823-a568-4684a5f6fbbc 0xc00534c387 0xc00534c388}] [] [{e2e.test Update apps/v1 2023-01-18 18:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4f0ad27a-857b-4823-a568-4684a5f6fbbc\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00534c4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:07:48.689: INFO: Pod "test-cleanup-controller-v9lsp" is available:
&Pod{ObjectMeta:{test-cleanup-controller-v9lsp test-cleanup-controller- deployment-9297  4307e7be-e1aa-438d-affb-dd07e0c20d9f 2631949853 0 2023-01-18 18:07:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b9446ae3e887ecc07af6636175eac20670b823141885ef5a66eae30e61975398 cni.projectcalico.org/podIP:10.100.145.186/32 cni.projectcalico.org/podIPs:10.100.145.186/32] [{apps/v1 ReplicaSet test-cleanup-controller b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e 0xc0052c6bf7 0xc0052c6bf8}] [] [{kube-controller-manager Update v1 2023-01-18 18:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 18:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 18:07:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9k8pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9k8pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.186,StartTime:2023-01-18 18:07:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 18:07:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://747c5b764a0085db23c70bb5795d54ca3e33279edb5be8967966d000ff85c03a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 18:07:48.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9297" for this suite. 01/18/23 18:07:48.697
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":280,"skipped":5296,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.140 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:43.57
    Jan 18 18:07:43.570: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 18:07:43.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:43.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:43.601
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan 18 18:07:43.621: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 18 18:07:48.630: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 18:07:48.63
    Jan 18 18:07:48.630: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/18/23 18:07:48.649
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 18:07:48.672: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9297  4f0ad27a-857b-4823-a568-4684a5f6fbbc 2631949997 1 2023-01-18 18:07:48 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005229f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 18 18:07:48.680: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-9297  ba00b77d-44be-4648-b899-3290ba519394 2631950000 1 2023-01-18 18:07:48 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 4f0ad27a-857b-4823-a568-4684a5f6fbbc 0xc00534c587 0xc00534c588}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f0ad27a-857b-4823-a568-4684a5f6fbbc\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00534c638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:07:48.680: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 18 18:07:48.680: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9297  b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e 2631949999 1 2023-01-18 18:07:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 4f0ad27a-857b-4823-a568-4684a5f6fbbc 0xc00534c387 0xc00534c388}] [] [{e2e.test Update apps/v1 2023-01-18 18:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:07:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-18 18:07:48 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"4f0ad27a-857b-4823-a568-4684a5f6fbbc\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00534c4d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:07:48.689: INFO: Pod "test-cleanup-controller-v9lsp" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-v9lsp test-cleanup-controller- deployment-9297  4307e7be-e1aa-438d-affb-dd07e0c20d9f 2631949853 0 2023-01-18 18:07:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:b9446ae3e887ecc07af6636175eac20670b823141885ef5a66eae30e61975398 cni.projectcalico.org/podIP:10.100.145.186/32 cni.projectcalico.org/podIPs:10.100.145.186/32] [{apps/v1 ReplicaSet test-cleanup-controller b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e 0xc0052c6bf7 0xc0052c6bf8}] [] [{kube-controller-manager Update v1 2023-01-18 18:07:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b6db4e2a-9b78-4d11-b699-5b1e8ad6aa3e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-01-18 18:07:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-01-18 18:07:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.186\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9k8pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9k8pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:07:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.186,StartTime:2023-01-18 18:07:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 18:07:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://747c5b764a0085db23c70bb5795d54ca3e33279edb5be8967966d000ff85c03a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.186,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 18:07:48.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9297" for this suite. 01/18/23 18:07:48.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:48.714
Jan 18 18:07:48.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:07:48.715
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:48.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:48.784
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-5605 01/18/23 18:07:48.789
STEP: creating replication controller nodeport-test in namespace services-5605 01/18/23 18:07:48.818
I0118 18:07:48.829187      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5605, replica count: 2
I0118 18:07:51.879733      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 18:07:51.879: INFO: Creating new exec pod
Jan 18 18:07:51.889: INFO: Waiting up to 5m0s for pod "execpodss8p4" in namespace "services-5605" to be "running"
Jan 18 18:07:51.897: INFO: Pod "execpodss8p4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6574ms
Jan 18 18:07:53.905: INFO: Pod "execpodss8p4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015729476s
Jan 18 18:07:53.905: INFO: Pod "execpodss8p4" satisfied condition "running"
Jan 18 18:07:54.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 18:07:55.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 18:07:55.114: INFO: stdout: ""
Jan 18 18:07:56.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 18:07:56.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 18:07:56.311: INFO: stdout: ""
Jan 18 18:07:57.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 18 18:07:57.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 18 18:07:57.304: INFO: stdout: "nodeport-test-zf2z7"
Jan 18 18:07:57.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.236.203 80'
Jan 18 18:07:57.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.236.203 80\nConnection to 10.96.236.203 80 port [tcp/http] succeeded!\n"
Jan 18 18:07:57.496: INFO: stdout: ""
Jan 18 18:07:58.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.236.203 80'
Jan 18 18:07:58.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.236.203 80\nConnection to 10.96.236.203 80 port [tcp/http] succeeded!\n"
Jan 18 18:07:58.701: INFO: stdout: "nodeport-test-zf2z7"
Jan 18 18:07:58.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 32243'
Jan 18 18:07:58.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 32243\nConnection to 10.195.74.123 32243 port [tcp/*] succeeded!\n"
Jan 18 18:07:58.898: INFO: stdout: "nodeport-test-zf2z7"
Jan 18 18:07:58.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 32243'
Jan 18 18:07:59.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 32243\nConnection to 10.195.78.23 32243 port [tcp/*] succeeded!\n"
Jan 18 18:07:59.105: INFO: stdout: "nodeport-test-zf2z7"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:07:59.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5605" for this suite. 01/18/23 18:07:59.112
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":281,"skipped":5314,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.412 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:48.714
    Jan 18 18:07:48.714: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:07:48.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:48.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:48.784
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-5605 01/18/23 18:07:48.789
    STEP: creating replication controller nodeport-test in namespace services-5605 01/18/23 18:07:48.818
    I0118 18:07:48.829187      21 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-5605, replica count: 2
    I0118 18:07:51.879733      21 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 18:07:51.879: INFO: Creating new exec pod
    Jan 18 18:07:51.889: INFO: Waiting up to 5m0s for pod "execpodss8p4" in namespace "services-5605" to be "running"
    Jan 18 18:07:51.897: INFO: Pod "execpodss8p4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.6574ms
    Jan 18 18:07:53.905: INFO: Pod "execpodss8p4": Phase="Running", Reason="", readiness=true. Elapsed: 2.015729476s
    Jan 18 18:07:53.905: INFO: Pod "execpodss8p4" satisfied condition "running"
    Jan 18 18:07:54.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 18:07:55.114: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 18:07:55.114: INFO: stdout: ""
    Jan 18 18:07:56.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 18:07:56.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 18:07:56.311: INFO: stdout: ""
    Jan 18 18:07:57.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 18 18:07:57.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 18 18:07:57.304: INFO: stdout: "nodeport-test-zf2z7"
    Jan 18 18:07:57.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.236.203 80'
    Jan 18 18:07:57.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.236.203 80\nConnection to 10.96.236.203 80 port [tcp/http] succeeded!\n"
    Jan 18 18:07:57.496: INFO: stdout: ""
    Jan 18 18:07:58.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.236.203 80'
    Jan 18 18:07:58.701: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.236.203 80\nConnection to 10.96.236.203 80 port [tcp/http] succeeded!\n"
    Jan 18 18:07:58.701: INFO: stdout: "nodeport-test-zf2z7"
    Jan 18 18:07:58.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.74.123 32243'
    Jan 18 18:07:58.898: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.74.123 32243\nConnection to 10.195.74.123 32243 port [tcp/*] succeeded!\n"
    Jan 18 18:07:58.898: INFO: stdout: "nodeport-test-zf2z7"
    Jan 18 18:07:58.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-5605 exec execpodss8p4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.195.78.23 32243'
    Jan 18 18:07:59.105: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.195.78.23 32243\nConnection to 10.195.78.23 32243 port [tcp/*] succeeded!\n"
    Jan 18 18:07:59.105: INFO: stdout: "nodeport-test-zf2z7"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:07:59.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5605" for this suite. 01/18/23 18:07:59.112
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:07:59.128
Jan 18 18:07:59.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-runtime 01/18/23 18:07:59.129
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:59.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:59.158
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/18/23 18:07:59.162
STEP: wait for the container to reach Succeeded 01/18/23 18:07:59.174
STEP: get the container status 01/18/23 18:08:03.213
STEP: the container should be terminated 01/18/23 18:08:03.219
STEP: the termination message should be set 01/18/23 18:08:03.219
Jan 18 18:08:03.219: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/18/23 18:08:03.219
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 18:08:03.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6471" for this suite. 01/18/23 18:08:03.254
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":282,"skipped":5337,"failed":0}
------------------------------
â€¢ [4.138 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:07:59.128
    Jan 18 18:07:59.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 18:07:59.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:07:59.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:07:59.158
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/18/23 18:07:59.162
    STEP: wait for the container to reach Succeeded 01/18/23 18:07:59.174
    STEP: get the container status 01/18/23 18:08:03.213
    STEP: the container should be terminated 01/18/23 18:08:03.219
    STEP: the termination message should be set 01/18/23 18:08:03.219
    Jan 18 18:08:03.219: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/18/23 18:08:03.219
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 18:08:03.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6471" for this suite. 01/18/23 18:08:03.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:08:03.271
Jan 18 18:08:03.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:08:03.273
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:03.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:03.306
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-a9cc5ee8-e656-4882-a7a0-6091efdd7127 01/18/23 18:08:03.321
STEP: Creating the pod 01/18/23 18:08:03.329
Jan 18 18:08:03.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33" in namespace "configmap-320" to be "running"
Jan 18 18:08:03.352: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441033ms
Jan 18 18:08:05.362: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33": Phase="Running", Reason="", readiness=false. Elapsed: 2.015377285s
Jan 18 18:08:05.362: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33" satisfied condition "running"
STEP: Waiting for pod with text data 01/18/23 18:08:05.362
STEP: Waiting for pod with binary data 01/18/23 18:08:05.381
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:08:05.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-320" for this suite. 01/18/23 18:08:05.405
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":283,"skipped":5394,"failed":0}
------------------------------
â€¢ [2.145 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:08:03.271
    Jan 18 18:08:03.272: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:08:03.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:03.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:03.306
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-a9cc5ee8-e656-4882-a7a0-6091efdd7127 01/18/23 18:08:03.321
    STEP: Creating the pod 01/18/23 18:08:03.329
    Jan 18 18:08:03.346: INFO: Waiting up to 5m0s for pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33" in namespace "configmap-320" to be "running"
    Jan 18 18:08:03.352: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33": Phase="Pending", Reason="", readiness=false. Elapsed: 5.441033ms
    Jan 18 18:08:05.362: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33": Phase="Running", Reason="", readiness=false. Elapsed: 2.015377285s
    Jan 18 18:08:05.362: INFO: Pod "pod-configmaps-943cf87c-43cc-4d5c-b02e-05a61251af33" satisfied condition "running"
    STEP: Waiting for pod with text data 01/18/23 18:08:05.362
    STEP: Waiting for pod with binary data 01/18/23 18:08:05.381
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:08:05.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-320" for this suite. 01/18/23 18:08:05.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:08:05.418
Jan 18 18:08:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename proxy 01/18/23 18:08:05.419
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:05.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:05.449
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 18 18:08:05.458: INFO: Creating pod...
Jan 18 18:08:05.471: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7894" to be "running"
Jan 18 18:08:05.477: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.619953ms
Jan 18 18:08:07.484: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013349572s
Jan 18 18:08:07.484: INFO: Pod "agnhost" satisfied condition "running"
Jan 18 18:08:07.484: INFO: Creating service...
Jan 18 18:08:07.507: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/DELETE
Jan 18 18:08:07.522: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 18:08:07.522: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/GET
Jan 18 18:08:07.531: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 18:08:07.531: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/HEAD
Jan 18 18:08:07.540: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 18:08:07.540: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 18 18:08:07.549: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 18:08:07.549: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/PATCH
Jan 18 18:08:07.558: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 18:08:07.558: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/POST
Jan 18 18:08:07.565: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 18:08:07.565: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/PUT
Jan 18 18:08:07.574: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 18 18:08:07.574: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/DELETE
Jan 18 18:08:07.586: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 18 18:08:07.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/GET
Jan 18 18:08:07.600: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 18 18:08:07.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/HEAD
Jan 18 18:08:07.615: INFO: http.Client request:HEAD | StatusCode:200
Jan 18 18:08:07.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/OPTIONS
Jan 18 18:08:07.627: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 18 18:08:07.627: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/PATCH
Jan 18 18:08:07.640: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 18 18:08:07.640: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/POST
Jan 18 18:08:07.654: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 18 18:08:07.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/PUT
Jan 18 18:08:07.668: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 18 18:08:07.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7894" for this suite. 01/18/23 18:08:07.676
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":284,"skipped":5400,"failed":0}
------------------------------
â€¢ [2.269 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:08:05.418
    Jan 18 18:08:05.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename proxy 01/18/23 18:08:05.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:05.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:05.449
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 18 18:08:05.458: INFO: Creating pod...
    Jan 18 18:08:05.471: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-7894" to be "running"
    Jan 18 18:08:05.477: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.619953ms
    Jan 18 18:08:07.484: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013349572s
    Jan 18 18:08:07.484: INFO: Pod "agnhost" satisfied condition "running"
    Jan 18 18:08:07.484: INFO: Creating service...
    Jan 18 18:08:07.507: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/DELETE
    Jan 18 18:08:07.522: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 18:08:07.522: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/GET
    Jan 18 18:08:07.531: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 18:08:07.531: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/HEAD
    Jan 18 18:08:07.540: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 18:08:07.540: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 18 18:08:07.549: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 18:08:07.549: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/PATCH
    Jan 18 18:08:07.558: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 18:08:07.558: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/POST
    Jan 18 18:08:07.565: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 18:08:07.565: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/pods/agnhost/proxy/some/path/with/PUT
    Jan 18 18:08:07.574: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 18 18:08:07.574: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/DELETE
    Jan 18 18:08:07.586: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 18 18:08:07.586: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/GET
    Jan 18 18:08:07.600: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 18 18:08:07.600: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/HEAD
    Jan 18 18:08:07.615: INFO: http.Client request:HEAD | StatusCode:200
    Jan 18 18:08:07.615: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/OPTIONS
    Jan 18 18:08:07.627: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 18 18:08:07.627: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/PATCH
    Jan 18 18:08:07.640: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 18 18:08:07.640: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/POST
    Jan 18 18:08:07.654: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 18 18:08:07.654: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7894/services/test-service/proxy/some/path/with/PUT
    Jan 18 18:08:07.668: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 18 18:08:07.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-7894" for this suite. 01/18/23 18:08:07.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:08:07.688
Jan 18 18:08:07.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 18:08:07.689
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:07.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:07.719
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 18 18:08:07.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: creating the pod 01/18/23 18:08:07.724
STEP: submitting the pod to kubernetes 01/18/23 18:08:07.724
Jan 18 18:08:07.739: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2" in namespace "pods-7078" to be "running and ready"
Jan 18 18:08:07.747: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.635789ms
Jan 18 18:08:07.747: INFO: The phase of Pod pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:08:09.757: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017218574s
Jan 18 18:08:09.757: INFO: The phase of Pod pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2 is Running (Ready = true)
Jan 18 18:08:09.757: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 18:08:09.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7078" for this suite. 01/18/23 18:08:09.892
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":285,"skipped":5412,"failed":0}
------------------------------
â€¢ [2.221 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:08:07.688
    Jan 18 18:08:07.688: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 18:08:07.689
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:07.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:07.719
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 18 18:08:07.724: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: creating the pod 01/18/23 18:08:07.724
    STEP: submitting the pod to kubernetes 01/18/23 18:08:07.724
    Jan 18 18:08:07.739: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2" in namespace "pods-7078" to be "running and ready"
    Jan 18 18:08:07.747: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.635789ms
    Jan 18 18:08:07.747: INFO: The phase of Pod pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:08:09.757: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017218574s
    Jan 18 18:08:09.757: INFO: The phase of Pod pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2 is Running (Ready = true)
    Jan 18 18:08:09.757: INFO: Pod "pod-exec-websocket-73d60643-d2bd-446d-8dc5-f2b684c737a2" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 18:08:09.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7078" for this suite. 01/18/23 18:08:09.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:08:09.911
Jan 18 18:08:09.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:08:09.912
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:09.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:09.944
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/18/23 18:08:09.949
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:08:09.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5780" for this suite. 01/18/23 18:08:09.964
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":286,"skipped":5447,"failed":0}
------------------------------
â€¢ [0.068 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:08:09.911
    Jan 18 18:08:09.911: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:08:09.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:09.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:09.944
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/18/23 18:08:09.949
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:08:09.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5780" for this suite. 01/18/23 18:08:09.964
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:08:09.98
Jan 18 18:08:09.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename cronjob 01/18/23 18:08:09.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:10.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:10.01
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/18/23 18:08:10.015
STEP: Ensuring a job is scheduled 01/18/23 18:08:10.024
STEP: Ensuring exactly one is scheduled 01/18/23 18:09:02.034
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 18:09:02.043
STEP: Ensuring no more jobs are scheduled 01/18/23 18:09:02.052
STEP: Removing cronjob 01/18/23 18:14:02.067
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 18:14:02.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2682" for this suite. 01/18/23 18:14:02.089
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":287,"skipped":5463,"failed":0}
------------------------------
â€¢ [SLOW TEST] [352.121 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:08:09.98
    Jan 18 18:08:09.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename cronjob 01/18/23 18:08:09.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:08:10.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:08:10.01
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/18/23 18:08:10.015
    STEP: Ensuring a job is scheduled 01/18/23 18:08:10.024
    STEP: Ensuring exactly one is scheduled 01/18/23 18:09:02.034
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/18/23 18:09:02.043
    STEP: Ensuring no more jobs are scheduled 01/18/23 18:09:02.052
    STEP: Removing cronjob 01/18/23 18:14:02.067
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 18:14:02.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2682" for this suite. 01/18/23 18:14:02.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:14:02.104
Jan 18 18:14:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 18:14:02.105
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:02.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:02.13
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/18/23 18:14:02.135
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/18/23 18:14:02.159
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/18/23 18:14:02.16
STEP: creating a pod to probe DNS 01/18/23 18:14:02.16
STEP: submitting the pod to kubernetes 01/18/23 18:14:02.16
Jan 18 18:14:02.178: INFO: Waiting up to 15m0s for pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078" in namespace "dns-2807" to be "running"
Jan 18 18:14:02.185: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229312ms
Jan 18 18:14:04.193: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015295247s
Jan 18 18:14:06.194: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Running", Reason="", readiness=true. Elapsed: 4.01663945s
Jan 18 18:14:06.194: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078" satisfied condition "running"
STEP: retrieving the pod 01/18/23 18:14:06.195
STEP: looking for the results for each expected name from probers 01/18/23 18:14:06.202
Jan 18 18:14:06.246: INFO: DNS probes using dns-2807/dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078 succeeded

STEP: deleting the pod 01/18/23 18:14:06.246
STEP: deleting the test headless service 01/18/23 18:14:06.267
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 18:14:06.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2807" for this suite. 01/18/23 18:14:06.292
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":288,"skipped":5478,"failed":0}
------------------------------
â€¢ [4.199 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:14:02.104
    Jan 18 18:14:02.104: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 18:14:02.105
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:02.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:02.13
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/18/23 18:14:02.135
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/18/23 18:14:02.159
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2807.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/18/23 18:14:02.16
    STEP: creating a pod to probe DNS 01/18/23 18:14:02.16
    STEP: submitting the pod to kubernetes 01/18/23 18:14:02.16
    Jan 18 18:14:02.178: INFO: Waiting up to 15m0s for pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078" in namespace "dns-2807" to be "running"
    Jan 18 18:14:02.185: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229312ms
    Jan 18 18:14:04.193: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015295247s
    Jan 18 18:14:06.194: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078": Phase="Running", Reason="", readiness=true. Elapsed: 4.01663945s
    Jan 18 18:14:06.194: INFO: Pod "dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 18:14:06.195
    STEP: looking for the results for each expected name from probers 01/18/23 18:14:06.202
    Jan 18 18:14:06.246: INFO: DNS probes using dns-2807/dns-test-077a04f9-d68a-41a0-9146-9cc4d9615078 succeeded

    STEP: deleting the pod 01/18/23 18:14:06.246
    STEP: deleting the test headless service 01/18/23 18:14:06.267
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 18:14:06.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2807" for this suite. 01/18/23 18:14:06.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:14:06.303
Jan 18 18:14:06.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:14:06.304
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:06.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:06.333
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/18/23 18:14:06.338
Jan 18 18:14:06.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964" in namespace "downward-api-9882" to be "Succeeded or Failed"
Jan 18 18:14:06.368: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Pending", Reason="", readiness=false. Elapsed: 7.254112ms
Jan 18 18:14:08.376: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015869007s
Jan 18 18:14:10.378: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018040752s
STEP: Saw pod success 01/18/23 18:14:10.378
Jan 18 18:14:10.378: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964" satisfied condition "Succeeded or Failed"
Jan 18 18:14:10.385: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 container client-container: <nil>
STEP: delete the pod 01/18/23 18:14:10.418
Jan 18 18:14:10.439: INFO: Waiting for pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 to disappear
Jan 18 18:14:10.446: INFO: Pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 18:14:10.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9882" for this suite. 01/18/23 18:14:10.454
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":289,"skipped":5483,"failed":0}
------------------------------
â€¢ [4.164 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:14:06.303
    Jan 18 18:14:06.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:14:06.304
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:06.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:06.333
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/18/23 18:14:06.338
    Jan 18 18:14:06.360: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964" in namespace "downward-api-9882" to be "Succeeded or Failed"
    Jan 18 18:14:06.368: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Pending", Reason="", readiness=false. Elapsed: 7.254112ms
    Jan 18 18:14:08.376: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015869007s
    Jan 18 18:14:10.378: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018040752s
    STEP: Saw pod success 01/18/23 18:14:10.378
    Jan 18 18:14:10.378: INFO: Pod "downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964" satisfied condition "Succeeded or Failed"
    Jan 18 18:14:10.385: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 container client-container: <nil>
    STEP: delete the pod 01/18/23 18:14:10.418
    Jan 18 18:14:10.439: INFO: Waiting for pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 to disappear
    Jan 18 18:14:10.446: INFO: Pod downwardapi-volume-ca7d4f15-cea8-40f6-9df5-a4d3a9b3d964 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 18:14:10.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9882" for this suite. 01/18/23 18:14:10.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:14:10.469
Jan 18 18:14:10.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-pred 01/18/23 18:14:10.471
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:10.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:10.497
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 18:14:10.502: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 18:14:10.516: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 18:14:10.523: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
Jan 18 18:14:10.533: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 18:14:10.533: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 18:14:10.533: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 18:14:10.533: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 18:14:10.533: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 18:14:10.533: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 18:14:10.533: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
Jan 18 18:14:10.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:14:10.533: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 18:14:10.533: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
Jan 18 18:14:10.544: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 18:14:10.544: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 18:14:10.544: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container coredns ready: true, restart count 0
Jan 18 18:14:10.544: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 18:14:10.544: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 18:14:10.544: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 18:14:10.544: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 18:14:10.544: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 18:14:10.544: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 18:14:10.544: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 18:14:10.544: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container e2e ready: true, restart count 0
Jan 18 18:14:10.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:14:10.544: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 18:14:10.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:14:10.544: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 18:14:10.544
Jan 18 18:14:10.560: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5235" to be "running"
Jan 18 18:14:10.566: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.717402ms
Jan 18 18:14:12.574: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013904236s
Jan 18 18:14:12.574: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 18:14:12.581
STEP: Trying to apply a random label on the found node. 01/18/23 18:14:12.608
STEP: verifying the node has the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e 95 01/18/23 18:14:12.628
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 18:14:12.636
Jan 18 18:14:12.647: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5235" to be "not pending"
Jan 18 18:14:12.652: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357499ms
Jan 18 18:14:14.660: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012491738s
Jan 18 18:14:14.660: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.195.74.123 on the node which pod4 resides and expect not scheduled 01/18/23 18:14:14.66
Jan 18 18:14:14.675: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5235" to be "not pending"
Jan 18 18:14:14.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.696713ms
Jan 18 18:14:16.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01590711s
Jan 18 18:14:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016855821s
Jan 18 18:14:20.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015588061s
Jan 18 18:14:22.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01884331s
Jan 18 18:14:24.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018572924s
Jan 18 18:14:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017065741s
Jan 18 18:14:28.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01855795s
Jan 18 18:14:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.018214543s
Jan 18 18:14:32.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017420731s
Jan 18 18:14:34.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018213881s
Jan 18 18:14:36.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.017246208s
Jan 18 18:14:38.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018398s
Jan 18 18:14:40.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019372207s
Jan 18 18:14:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018399566s
Jan 18 18:14:44.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.018094515s
Jan 18 18:14:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018511659s
Jan 18 18:14:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016084158s
Jan 18 18:14:50.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015569779s
Jan 18 18:14:52.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016384655s
Jan 18 18:14:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018978155s
Jan 18 18:14:56.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01844523s
Jan 18 18:14:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016636341s
Jan 18 18:15:00.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018376343s
Jan 18 18:15:02.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017642314s
Jan 18 18:15:04.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019875606s
Jan 18 18:15:06.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019946148s
Jan 18 18:15:08.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023667255s
Jan 18 18:15:10.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018909867s
Jan 18 18:15:12.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.016128293s
Jan 18 18:15:14.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.019617108s
Jan 18 18:15:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.017879383s
Jan 18 18:15:18.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015279089s
Jan 18 18:15:20.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018119502s
Jan 18 18:15:22.703: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028221407s
Jan 18 18:15:24.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018461131s
Jan 18 18:15:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017584818s
Jan 18 18:15:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016731373s
Jan 18 18:15:30.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016064536s
Jan 18 18:15:32.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017802655s
Jan 18 18:15:34.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018011269s
Jan 18 18:15:36.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017381996s
Jan 18 18:15:38.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.015930043s
Jan 18 18:15:40.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015285415s
Jan 18 18:15:42.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017103251s
Jan 18 18:15:44.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017382322s
Jan 18 18:15:46.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019641857s
Jan 18 18:15:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016788442s
Jan 18 18:15:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017468441s
Jan 18 18:15:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018923747s
Jan 18 18:15:54.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.017828847s
Jan 18 18:15:56.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.017907773s
Jan 18 18:15:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016655545s
Jan 18 18:16:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015962031s
Jan 18 18:16:02.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018079073s
Jan 18 18:16:04.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017032759s
Jan 18 18:16:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018740205s
Jan 18 18:16:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015792705s
Jan 18 18:16:10.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019159267s
Jan 18 18:16:12.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017025203s
Jan 18 18:16:14.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020146157s
Jan 18 18:16:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.018526684s
Jan 18 18:16:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017564767s
Jan 18 18:16:20.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.017166484s
Jan 18 18:16:22.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017284837s
Jan 18 18:16:24.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017568278s
Jan 18 18:16:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.017063953s
Jan 18 18:16:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.01631869s
Jan 18 18:16:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017897904s
Jan 18 18:16:32.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018206597s
Jan 18 18:16:34.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.018850138s
Jan 18 18:16:36.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.018535991s
Jan 18 18:16:38.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01536429s
Jan 18 18:16:40.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017927927s
Jan 18 18:16:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.018103195s
Jan 18 18:16:44.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.023060437s
Jan 18 18:16:46.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.017502861s
Jan 18 18:16:48.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016877083s
Jan 18 18:16:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017560609s
Jan 18 18:16:52.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.017415375s
Jan 18 18:16:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.019291534s
Jan 18 18:16:56.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015821959s
Jan 18 18:16:58.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015699788s
Jan 18 18:17:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016335118s
Jan 18 18:17:02.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016852078s
Jan 18 18:17:04.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.018384278s
Jan 18 18:17:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018843035s
Jan 18 18:17:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.014875117s
Jan 18 18:17:10.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.017720067s
Jan 18 18:17:12.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01720354s
Jan 18 18:17:14.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.018815821s
Jan 18 18:17:16.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019792481s
Jan 18 18:17:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017711672s
Jan 18 18:17:20.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017237029s
Jan 18 18:17:22.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.017531051s
Jan 18 18:17:24.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.016797649s
Jan 18 18:17:26.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018110234s
Jan 18 18:17:28.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.020716872s
Jan 18 18:17:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018150905s
Jan 18 18:17:32.702: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.026868775s
Jan 18 18:17:34.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017683693s
Jan 18 18:17:36.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019124448s
Jan 18 18:17:38.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017229493s
Jan 18 18:17:40.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019414606s
Jan 18 18:17:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018513093s
Jan 18 18:17:44.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.017494578s
Jan 18 18:17:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018297773s
Jan 18 18:17:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016471249s
Jan 18 18:17:50.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.019067682s
Jan 18 18:17:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.018987146s
Jan 18 18:17:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019668446s
Jan 18 18:17:56.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.020622245s
Jan 18 18:17:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016664265s
Jan 18 18:18:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015847087s
Jan 18 18:18:02.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019613853s
Jan 18 18:18:04.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.022351386s
Jan 18 18:18:06.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020361804s
Jan 18 18:18:08.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016240924s
Jan 18 18:18:10.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.016791743s
Jan 18 18:18:12.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018677741s
Jan 18 18:18:14.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018641794s
Jan 18 18:18:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.018458383s
Jan 18 18:18:18.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.015695811s
Jan 18 18:18:20.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.01585919s
Jan 18 18:18:22.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.019424452s
Jan 18 18:18:24.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016838148s
Jan 18 18:18:26.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.018357622s
Jan 18 18:18:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015982955s
Jan 18 18:18:30.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016957049s
Jan 18 18:18:32.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.020173378s
Jan 18 18:18:34.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.017770564s
Jan 18 18:18:36.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.018805412s
Jan 18 18:18:38.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.019041852s
Jan 18 18:18:40.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017877672s
Jan 18 18:18:42.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.017438304s
Jan 18 18:18:44.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016145318s
Jan 18 18:18:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.018468159s
Jan 18 18:18:48.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.017014628s
Jan 18 18:18:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.017664692s
Jan 18 18:18:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.019106849s
Jan 18 18:18:54.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.019880179s
Jan 18 18:18:56.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.019996522s
Jan 18 18:18:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016520878s
Jan 18 18:19:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016739149s
Jan 18 18:19:02.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.02560527s
Jan 18 18:19:04.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.018059862s
Jan 18 18:19:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018586889s
Jan 18 18:19:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.015122676s
Jan 18 18:19:10.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017844106s
Jan 18 18:19:12.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017875375s
Jan 18 18:19:14.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017111666s
Jan 18 18:19:14.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025292584s
STEP: removing the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:19:14.7
STEP: verifying the node doesn't have the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e 01/18/23 18:19:14.728
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:19:14.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5235" for this suite. 01/18/23 18:19:14.746
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":290,"skipped":5493,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.294 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:14:10.469
    Jan 18 18:14:10.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 18:14:10.471
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:14:10.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:14:10.497
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 18:14:10.502: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 18:14:10.516: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 18:14:10.523: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
    Jan 18 18:14:10.533: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
    Jan 18 18:14:10.533: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 18:14:10.533: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
    Jan 18 18:14:10.544: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 18:14:10.544: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:14:10.544: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/18/23 18:14:10.544
    Jan 18 18:14:10.560: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5235" to be "running"
    Jan 18 18:14:10.566: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.717402ms
    Jan 18 18:14:12.574: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.013904236s
    Jan 18 18:14:12.574: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/18/23 18:14:12.581
    STEP: Trying to apply a random label on the found node. 01/18/23 18:14:12.608
    STEP: verifying the node has the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e 95 01/18/23 18:14:12.628
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/18/23 18:14:12.636
    Jan 18 18:14:12.647: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5235" to be "not pending"
    Jan 18 18:14:12.652: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.357499ms
    Jan 18 18:14:14.660: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.012491738s
    Jan 18 18:14:14.660: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.195.74.123 on the node which pod4 resides and expect not scheduled 01/18/23 18:14:14.66
    Jan 18 18:14:14.675: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5235" to be "not pending"
    Jan 18 18:14:14.683: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.696713ms
    Jan 18 18:14:16.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01590711s
    Jan 18 18:14:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016855821s
    Jan 18 18:14:20.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015588061s
    Jan 18 18:14:22.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01884331s
    Jan 18 18:14:24.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018572924s
    Jan 18 18:14:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.017065741s
    Jan 18 18:14:28.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.01855795s
    Jan 18 18:14:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.018214543s
    Jan 18 18:14:32.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.017420731s
    Jan 18 18:14:34.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.018213881s
    Jan 18 18:14:36.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.017246208s
    Jan 18 18:14:38.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.018398s
    Jan 18 18:14:40.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.019372207s
    Jan 18 18:14:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.018399566s
    Jan 18 18:14:44.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.018094515s
    Jan 18 18:14:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.018511659s
    Jan 18 18:14:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.016084158s
    Jan 18 18:14:50.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.015569779s
    Jan 18 18:14:52.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.016384655s
    Jan 18 18:14:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.018978155s
    Jan 18 18:14:56.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.01844523s
    Jan 18 18:14:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.016636341s
    Jan 18 18:15:00.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.018376343s
    Jan 18 18:15:02.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.017642314s
    Jan 18 18:15:04.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.019875606s
    Jan 18 18:15:06.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.019946148s
    Jan 18 18:15:08.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.023667255s
    Jan 18 18:15:10.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.018909867s
    Jan 18 18:15:12.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.016128293s
    Jan 18 18:15:14.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.019617108s
    Jan 18 18:15:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.017879383s
    Jan 18 18:15:18.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.015279089s
    Jan 18 18:15:20.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.018119502s
    Jan 18 18:15:22.703: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.028221407s
    Jan 18 18:15:24.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.018461131s
    Jan 18 18:15:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.017584818s
    Jan 18 18:15:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.016731373s
    Jan 18 18:15:30.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.016064536s
    Jan 18 18:15:32.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.017802655s
    Jan 18 18:15:34.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.018011269s
    Jan 18 18:15:36.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.017381996s
    Jan 18 18:15:38.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.015930043s
    Jan 18 18:15:40.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.015285415s
    Jan 18 18:15:42.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.017103251s
    Jan 18 18:15:44.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.017382322s
    Jan 18 18:15:46.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.019641857s
    Jan 18 18:15:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.016788442s
    Jan 18 18:15:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.017468441s
    Jan 18 18:15:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.018923747s
    Jan 18 18:15:54.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.017828847s
    Jan 18 18:15:56.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.017907773s
    Jan 18 18:15:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.016655545s
    Jan 18 18:16:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.015962031s
    Jan 18 18:16:02.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.018079073s
    Jan 18 18:16:04.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.017032759s
    Jan 18 18:16:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.018740205s
    Jan 18 18:16:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.015792705s
    Jan 18 18:16:10.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.019159267s
    Jan 18 18:16:12.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017025203s
    Jan 18 18:16:14.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.020146157s
    Jan 18 18:16:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.018526684s
    Jan 18 18:16:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.017564767s
    Jan 18 18:16:20.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.017166484s
    Jan 18 18:16:22.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.017284837s
    Jan 18 18:16:24.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.017568278s
    Jan 18 18:16:26.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.017063953s
    Jan 18 18:16:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.01631869s
    Jan 18 18:16:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.017897904s
    Jan 18 18:16:32.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.018206597s
    Jan 18 18:16:34.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.018850138s
    Jan 18 18:16:36.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.018535991s
    Jan 18 18:16:38.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01536429s
    Jan 18 18:16:40.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.017927927s
    Jan 18 18:16:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.018103195s
    Jan 18 18:16:44.698: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.023060437s
    Jan 18 18:16:46.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.017502861s
    Jan 18 18:16:48.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.016877083s
    Jan 18 18:16:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.017560609s
    Jan 18 18:16:52.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.017415375s
    Jan 18 18:16:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.019291534s
    Jan 18 18:16:56.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.015821959s
    Jan 18 18:16:58.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.015699788s
    Jan 18 18:17:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.016335118s
    Jan 18 18:17:02.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.016852078s
    Jan 18 18:17:04.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.018384278s
    Jan 18 18:17:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.018843035s
    Jan 18 18:17:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.014875117s
    Jan 18 18:17:10.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.017720067s
    Jan 18 18:17:12.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.01720354s
    Jan 18 18:17:14.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.018815821s
    Jan 18 18:17:16.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.019792481s
    Jan 18 18:17:18.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.017711672s
    Jan 18 18:17:20.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.017237029s
    Jan 18 18:17:22.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.017531051s
    Jan 18 18:17:24.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.016797649s
    Jan 18 18:17:26.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.018110234s
    Jan 18 18:17:28.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.020716872s
    Jan 18 18:17:30.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.018150905s
    Jan 18 18:17:32.702: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.026868775s
    Jan 18 18:17:34.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.017683693s
    Jan 18 18:17:36.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.019124448s
    Jan 18 18:17:38.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.017229493s
    Jan 18 18:17:40.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.019414606s
    Jan 18 18:17:42.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.018513093s
    Jan 18 18:17:44.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.017494578s
    Jan 18 18:17:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.018297773s
    Jan 18 18:17:48.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.016471249s
    Jan 18 18:17:50.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.019067682s
    Jan 18 18:17:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.018987146s
    Jan 18 18:17:54.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.019668446s
    Jan 18 18:17:56.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.020622245s
    Jan 18 18:17:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.016664265s
    Jan 18 18:18:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.015847087s
    Jan 18 18:18:02.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.019613853s
    Jan 18 18:18:04.697: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.022351386s
    Jan 18 18:18:06.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.020361804s
    Jan 18 18:18:08.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016240924s
    Jan 18 18:18:10.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.016791743s
    Jan 18 18:18:12.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.018677741s
    Jan 18 18:18:14.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.018641794s
    Jan 18 18:18:16.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.018458383s
    Jan 18 18:18:18.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.015695811s
    Jan 18 18:18:20.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.01585919s
    Jan 18 18:18:22.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.019424452s
    Jan 18 18:18:24.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.016838148s
    Jan 18 18:18:26.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.018357622s
    Jan 18 18:18:28.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.015982955s
    Jan 18 18:18:30.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.016957049s
    Jan 18 18:18:32.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.020173378s
    Jan 18 18:18:34.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.017770564s
    Jan 18 18:18:36.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.018805412s
    Jan 18 18:18:38.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.019041852s
    Jan 18 18:18:40.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.017877672s
    Jan 18 18:18:42.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.017438304s
    Jan 18 18:18:44.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.016145318s
    Jan 18 18:18:46.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.018468159s
    Jan 18 18:18:48.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.017014628s
    Jan 18 18:18:50.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.017664692s
    Jan 18 18:18:52.694: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.019106849s
    Jan 18 18:18:54.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.019880179s
    Jan 18 18:18:56.695: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.019996522s
    Jan 18 18:18:58.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.016520878s
    Jan 18 18:19:00.691: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.016739149s
    Jan 18 18:19:02.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.02560527s
    Jan 18 18:19:04.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.018059862s
    Jan 18 18:19:06.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.018586889s
    Jan 18 18:19:08.690: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.015122676s
    Jan 18 18:19:10.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.017844106s
    Jan 18 18:19:12.693: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.017875375s
    Jan 18 18:19:14.692: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017111666s
    Jan 18 18:19:14.700: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.025292584s
    STEP: removing the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:19:14.7
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-71c762f0-a576-4963-bfbf-dc0b8302055e 01/18/23 18:19:14.728
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:19:14.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5235" for this suite. 01/18/23 18:19:14.746
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:14.764
Jan 18 18:19:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename cronjob 01/18/23 18:19:14.765
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:14.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:14.8
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/18/23 18:19:14.805
STEP: creating 01/18/23 18:19:14.805
STEP: getting 01/18/23 18:19:14.815
STEP: listing 01/18/23 18:19:14.822
STEP: watching 01/18/23 18:19:14.83
Jan 18 18:19:14.830: INFO: starting watch
STEP: cluster-wide listing 01/18/23 18:19:14.832
STEP: cluster-wide watching 01/18/23 18:19:14.837
Jan 18 18:19:14.837: INFO: starting watch
STEP: patching 01/18/23 18:19:14.84
STEP: updating 01/18/23 18:19:14.855
Jan 18 18:19:14.872: INFO: waiting for watch events with expected annotations
Jan 18 18:19:14.872: INFO: saw patched and updated annotations
STEP: patching /status 01/18/23 18:19:14.872
STEP: updating /status 01/18/23 18:19:14.883
STEP: get /status 01/18/23 18:19:14.896
STEP: deleting 01/18/23 18:19:14.907
STEP: deleting a collection 01/18/23 18:19:14.937
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 18 18:19:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2700" for this suite. 01/18/23 18:19:14.97
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":291,"skipped":5503,"failed":0}
------------------------------
â€¢ [0.222 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:14.764
    Jan 18 18:19:14.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename cronjob 01/18/23 18:19:14.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:14.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:14.8
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/18/23 18:19:14.805
    STEP: creating 01/18/23 18:19:14.805
    STEP: getting 01/18/23 18:19:14.815
    STEP: listing 01/18/23 18:19:14.822
    STEP: watching 01/18/23 18:19:14.83
    Jan 18 18:19:14.830: INFO: starting watch
    STEP: cluster-wide listing 01/18/23 18:19:14.832
    STEP: cluster-wide watching 01/18/23 18:19:14.837
    Jan 18 18:19:14.837: INFO: starting watch
    STEP: patching 01/18/23 18:19:14.84
    STEP: updating 01/18/23 18:19:14.855
    Jan 18 18:19:14.872: INFO: waiting for watch events with expected annotations
    Jan 18 18:19:14.872: INFO: saw patched and updated annotations
    STEP: patching /status 01/18/23 18:19:14.872
    STEP: updating /status 01/18/23 18:19:14.883
    STEP: get /status 01/18/23 18:19:14.896
    STEP: deleting 01/18/23 18:19:14.907
    STEP: deleting a collection 01/18/23 18:19:14.937
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 18 18:19:14.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2700" for this suite. 01/18/23 18:19:14.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:14.987
Jan 18 18:19:14.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 18:19:14.989
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:15.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:15.025
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 18 18:19:15.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: creating the pod 01/18/23 18:19:15.032
STEP: submitting the pod to kubernetes 01/18/23 18:19:15.032
Jan 18 18:19:15.048: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737" in namespace "pods-9197" to be "running and ready"
Jan 18 18:19:15.054: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58733ms
Jan 18 18:19:15.054: INFO: The phase of Pod pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:19:17.063: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737": Phase="Running", Reason="", readiness=true. Elapsed: 2.015071672s
Jan 18 18:19:17.064: INFO: The phase of Pod pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737 is Running (Ready = true)
Jan 18 18:19:17.064: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 18 18:19:17.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9197" for this suite. 01/18/23 18:19:17.12
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":292,"skipped":5517,"failed":0}
------------------------------
â€¢ [2.146 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:14.987
    Jan 18 18:19:14.987: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 18:19:14.989
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:15.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:15.025
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 18 18:19:15.031: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: creating the pod 01/18/23 18:19:15.032
    STEP: submitting the pod to kubernetes 01/18/23 18:19:15.032
    Jan 18 18:19:15.048: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737" in namespace "pods-9197" to be "running and ready"
    Jan 18 18:19:15.054: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737": Phase="Pending", Reason="", readiness=false. Elapsed: 5.58733ms
    Jan 18 18:19:15.054: INFO: The phase of Pod pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:19:17.063: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737": Phase="Running", Reason="", readiness=true. Elapsed: 2.015071672s
    Jan 18 18:19:17.064: INFO: The phase of Pod pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737 is Running (Ready = true)
    Jan 18 18:19:17.064: INFO: Pod "pod-logs-websocket-dd31bee7-2cf7-499f-868e-a9183e8d8737" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 18 18:19:17.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9197" for this suite. 01/18/23 18:19:17.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:17.134
Jan 18 18:19:17.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename var-expansion 01/18/23 18:19:17.135
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:17.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:17.165
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jan 18 18:19:17.185: INFO: Waiting up to 2m0s for pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" in namespace "var-expansion-16" to be "container 0 failed with reason CreateContainerConfigError"
Jan 18 18:19:17.194: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712": Phase="Pending", Reason="", readiness=false. Elapsed: 8.902524ms
Jan 18 18:19:19.203: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017631448s
Jan 18 18:19:19.203: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 18 18:19:19.203: INFO: Deleting pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" in namespace "var-expansion-16"
Jan 18 18:19:19.217: INFO: Wait up to 5m0s for pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 18 18:19:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-16" for this suite. 01/18/23 18:19:23.242
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":293,"skipped":5526,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.120 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:17.134
    Jan 18 18:19:17.134: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename var-expansion 01/18/23 18:19:17.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:17.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:17.165
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jan 18 18:19:17.185: INFO: Waiting up to 2m0s for pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" in namespace "var-expansion-16" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 18 18:19:17.194: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712": Phase="Pending", Reason="", readiness=false. Elapsed: 8.902524ms
    Jan 18 18:19:19.203: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017631448s
    Jan 18 18:19:19.203: INFO: Pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 18 18:19:19.203: INFO: Deleting pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" in namespace "var-expansion-16"
    Jan 18 18:19:19.217: INFO: Wait up to 5m0s for pod "var-expansion-71b2a480-e1a3-477d-8107-a0391e3ca712" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 18 18:19:23.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-16" for this suite. 01/18/23 18:19:23.242
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:23.257
Jan 18 18:19:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:19:23.258
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:23.279
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:23.284
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-9f72e60b-709f-4c7e-9516-f636b2b7973e 01/18/23 18:19:23.288
STEP: Creating a pod to test consume secrets 01/18/23 18:19:23.298
Jan 18 18:19:23.313: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605" in namespace "projected-4735" to be "Succeeded or Failed"
Jan 18 18:19:23.320: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756799ms
Jan 18 18:19:25.327: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014512511s
Jan 18 18:19:27.330: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016986376s
STEP: Saw pod success 01/18/23 18:19:27.33
Jan 18 18:19:27.330: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605" satisfied condition "Succeeded or Failed"
Jan 18 18:19:27.339: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/18/23 18:19:27.355
Jan 18 18:19:27.380: INFO: Waiting for pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 to disappear
Jan 18 18:19:27.386: INFO: Pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 18:19:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4735" for this suite. 01/18/23 18:19:27.394
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":294,"skipped":5569,"failed":0}
------------------------------
â€¢ [4.151 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:23.257
    Jan 18 18:19:23.257: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:19:23.258
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:23.279
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:23.284
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-9f72e60b-709f-4c7e-9516-f636b2b7973e 01/18/23 18:19:23.288
    STEP: Creating a pod to test consume secrets 01/18/23 18:19:23.298
    Jan 18 18:19:23.313: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605" in namespace "projected-4735" to be "Succeeded or Failed"
    Jan 18 18:19:23.320: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756799ms
    Jan 18 18:19:25.327: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014512511s
    Jan 18 18:19:27.330: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016986376s
    STEP: Saw pod success 01/18/23 18:19:27.33
    Jan 18 18:19:27.330: INFO: Pod "pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605" satisfied condition "Succeeded or Failed"
    Jan 18 18:19:27.339: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 18:19:27.355
    Jan 18 18:19:27.380: INFO: Waiting for pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 to disappear
    Jan 18 18:19:27.386: INFO: Pod pod-projected-secrets-a8cc4f10-18a2-4bd5-9941-b0a593090605 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 18:19:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4735" for this suite. 01/18/23 18:19:27.394
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:27.409
Jan 18 18:19:27.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:19:27.41
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:27.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:27.439
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-719e7fc9-5af3-45d8-bbb3-5fd709d24110 01/18/23 18:19:27.444
STEP: Creating a pod to test consume configMaps 01/18/23 18:19:27.453
Jan 18 18:19:27.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8" in namespace "configmap-2804" to be "Succeeded or Failed"
Jan 18 18:19:27.478: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221643ms
Jan 18 18:19:29.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020399919s
Jan 18 18:19:31.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020619503s
STEP: Saw pod success 01/18/23 18:19:31.489
Jan 18 18:19:31.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8" satisfied condition "Succeeded or Failed"
Jan 18 18:19:31.497: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 container agnhost-container: <nil>
STEP: delete the pod 01/18/23 18:19:31.515
Jan 18 18:19:31.536: INFO: Waiting for pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 to disappear
Jan 18 18:19:31.543: INFO: Pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:19:31.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2804" for this suite. 01/18/23 18:19:31.552
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":295,"skipped":5587,"failed":0}
------------------------------
â€¢ [4.155 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:27.409
    Jan 18 18:19:27.409: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:19:27.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:27.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:27.439
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-719e7fc9-5af3-45d8-bbb3-5fd709d24110 01/18/23 18:19:27.444
    STEP: Creating a pod to test consume configMaps 01/18/23 18:19:27.453
    Jan 18 18:19:27.468: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8" in namespace "configmap-2804" to be "Succeeded or Failed"
    Jan 18 18:19:27.478: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.221643ms
    Jan 18 18:19:29.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020399919s
    Jan 18 18:19:31.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020619503s
    STEP: Saw pod success 01/18/23 18:19:31.489
    Jan 18 18:19:31.489: INFO: Pod "pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8" satisfied condition "Succeeded or Failed"
    Jan 18 18:19:31.497: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 18:19:31.515
    Jan 18 18:19:31.536: INFO: Waiting for pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 to disappear
    Jan 18 18:19:31.543: INFO: Pod pod-configmaps-5a93df13-da76-47b8-adf5-aa67a7bf74d8 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:19:31.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2804" for this suite. 01/18/23 18:19:31.552
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:31.565
Jan 18 18:19:31.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 18:19:31.566
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:31.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:31.596
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/18/23 18:19:31.602
Jan 18 18:19:31.602: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 18 18:19:31.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:32.917: INFO: stderr: ""
Jan 18 18:19:32.917: INFO: stdout: "service/agnhost-replica created\n"
Jan 18 18:19:32.917: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 18 18:19:32.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:33.140: INFO: stderr: ""
Jan 18 18:19:33.141: INFO: stdout: "service/agnhost-primary created\n"
Jan 18 18:19:33.141: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 18 18:19:33.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:33.444: INFO: stderr: ""
Jan 18 18:19:33.444: INFO: stdout: "service/frontend created\n"
Jan 18 18:19:33.444: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 18 18:19:33.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:33.740: INFO: stderr: ""
Jan 18 18:19:33.740: INFO: stdout: "deployment.apps/frontend created\n"
Jan 18 18:19:33.740: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 18:19:33.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:34.014: INFO: stderr: ""
Jan 18 18:19:34.014: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 18 18:19:34.015: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 18 18:19:34.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
Jan 18 18:19:34.274: INFO: stderr: ""
Jan 18 18:19:34.274: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/18/23 18:19:34.274
Jan 18 18:19:34.274: INFO: Waiting for all frontend pods to be Running.
Jan 18 18:19:39.327: INFO: Waiting for frontend to serve content.
Jan 18 18:19:39.350: INFO: Trying to add a new entry to the guestbook.
Jan 18 18:19:39.374: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/18/23 18:19:39.391
Jan 18 18:19:39.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:39.518: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:39.518: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 18:19:39.518
Jan 18 18:19:39.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:39.655: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:39.655: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 18:19:39.655
Jan 18 18:19:39.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:39.777: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:39.777: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 18:19:39.777
Jan 18 18:19:39.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:39.881: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:39.882: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 18:19:39.882
Jan 18 18:19:39.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:39.991: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:39.991: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/18/23 18:19:39.992
Jan 18 18:19:39.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jan 18 18:19:40.106: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:19:40.106: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 18:19:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2856" for this suite. 01/18/23 18:19:40.114
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":296,"skipped":5588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.562 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:31.565
    Jan 18 18:19:31.565: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 18:19:31.566
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:31.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:31.596
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/18/23 18:19:31.602
    Jan 18 18:19:31.602: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 18 18:19:31.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:32.917: INFO: stderr: ""
    Jan 18 18:19:32.917: INFO: stdout: "service/agnhost-replica created\n"
    Jan 18 18:19:32.917: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 18 18:19:32.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:33.140: INFO: stderr: ""
    Jan 18 18:19:33.141: INFO: stdout: "service/agnhost-primary created\n"
    Jan 18 18:19:33.141: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 18 18:19:33.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:33.444: INFO: stderr: ""
    Jan 18 18:19:33.444: INFO: stdout: "service/frontend created\n"
    Jan 18 18:19:33.444: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 18 18:19:33.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:33.740: INFO: stderr: ""
    Jan 18 18:19:33.740: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 18 18:19:33.740: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 18:19:33.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:34.014: INFO: stderr: ""
    Jan 18 18:19:34.014: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 18 18:19:34.015: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 18 18:19:34.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 create -f -'
    Jan 18 18:19:34.274: INFO: stderr: ""
    Jan 18 18:19:34.274: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/18/23 18:19:34.274
    Jan 18 18:19:34.274: INFO: Waiting for all frontend pods to be Running.
    Jan 18 18:19:39.327: INFO: Waiting for frontend to serve content.
    Jan 18 18:19:39.350: INFO: Trying to add a new entry to the guestbook.
    Jan 18 18:19:39.374: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/18/23 18:19:39.391
    Jan 18 18:19:39.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:39.518: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:39.518: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 18:19:39.518
    Jan 18 18:19:39.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:39.655: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:39.655: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 18:19:39.655
    Jan 18 18:19:39.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:39.777: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:39.777: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 18:19:39.777
    Jan 18 18:19:39.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:39.881: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:39.882: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 18:19:39.882
    Jan 18 18:19:39.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:39.991: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:39.991: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/18/23 18:19:39.992
    Jan 18 18:19:39.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jan 18 18:19:40.106: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:19:40.106: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 18:19:40.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2856" for this suite. 01/18/23 18:19:40.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:40.128
Jan 18 18:19:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:19:40.129
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:40.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:40.157
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/18/23 18:19:40.162
Jan 18 18:19:40.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a" in namespace "downward-api-7136" to be "Succeeded or Failed"
Jan 18 18:19:40.186: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175992ms
Jan 18 18:19:42.194: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015338432s
Jan 18 18:19:44.192: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013152349s
STEP: Saw pod success 01/18/23 18:19:44.192
Jan 18 18:19:44.192: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a" satisfied condition "Succeeded or Failed"
Jan 18 18:19:44.199: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a container client-container: <nil>
STEP: delete the pod 01/18/23 18:19:44.213
Jan 18 18:19:44.233: INFO: Waiting for pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a to disappear
Jan 18 18:19:44.238: INFO: Pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 18:19:44.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7136" for this suite. 01/18/23 18:19:44.249
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":297,"skipped":5640,"failed":0}
------------------------------
â€¢ [4.131 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:40.128
    Jan 18 18:19:40.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:19:40.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:40.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:40.157
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/18/23 18:19:40.162
    Jan 18 18:19:40.179: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a" in namespace "downward-api-7136" to be "Succeeded or Failed"
    Jan 18 18:19:40.186: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175992ms
    Jan 18 18:19:42.194: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015338432s
    Jan 18 18:19:44.192: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013152349s
    STEP: Saw pod success 01/18/23 18:19:44.192
    Jan 18 18:19:44.192: INFO: Pod "downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a" satisfied condition "Succeeded or Failed"
    Jan 18 18:19:44.199: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a container client-container: <nil>
    STEP: delete the pod 01/18/23 18:19:44.213
    Jan 18 18:19:44.233: INFO: Waiting for pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a to disappear
    Jan 18 18:19:44.238: INFO: Pod downwardapi-volume-c892240e-ecd7-4392-9e34-e04a6c6f9d5a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 18:19:44.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7136" for this suite. 01/18/23 18:19:44.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:19:44.261
Jan 18 18:19:44.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 18:19:44.263
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:44.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:44.288
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1585 01/18/23 18:19:44.292
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/18/23 18:19:44.302
Jan 18 18:19:44.318: INFO: Found 0 stateful pods, waiting for 3
Jan 18 18:19:54.327: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:19:54.327: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:19:54.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 18:19:54.352
Jan 18 18:19:54.380: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/18/23 18:19:54.38
STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 18:20:04.416
STEP: Performing a canary update 01/18/23 18:20:04.417
Jan 18 18:20:04.444: INFO: Updating stateful set ss2
Jan 18 18:20:04.455: INFO: Waiting for Pod statefulset-1585/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 18:20:14.472
Jan 18 18:20:14.520: INFO: Found 1 stateful pods, waiting for 3
Jan 18 18:20:24.531: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:20:24.531: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 18 18:20:24.531: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/18/23 18:20:24.545
Jan 18 18:20:24.576: INFO: Updating stateful set ss2
Jan 18 18:20:24.589: INFO: Waiting for Pod statefulset-1585/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 18 18:20:34.633: INFO: Updating stateful set ss2
Jan 18 18:20:34.651: INFO: Waiting for StatefulSet statefulset-1585/ss2 to complete update
Jan 18 18:20:34.652: INFO: Waiting for Pod statefulset-1585/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 18:20:44.666: INFO: Deleting all statefulset in ns statefulset-1585
Jan 18 18:20:44.673: INFO: Scaling statefulset ss2 to 0
Jan 18 18:20:54.710: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 18:20:54.717: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 18:20:54.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1585" for this suite. 01/18/23 18:20:54.753
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":298,"skipped":5647,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.506 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:19:44.261
    Jan 18 18:19:44.262: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 18:19:44.263
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:19:44.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:19:44.288
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1585 01/18/23 18:19:44.292
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/18/23 18:19:44.302
    Jan 18 18:19:44.318: INFO: Found 0 stateful pods, waiting for 3
    Jan 18 18:19:54.327: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:19:54.327: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:19:54.327: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/18/23 18:19:54.352
    Jan 18 18:19:54.380: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/18/23 18:19:54.38
    STEP: Not applying an update when the partition is greater than the number of replicas 01/18/23 18:20:04.416
    STEP: Performing a canary update 01/18/23 18:20:04.417
    Jan 18 18:20:04.444: INFO: Updating stateful set ss2
    Jan 18 18:20:04.455: INFO: Waiting for Pod statefulset-1585/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/18/23 18:20:14.472
    Jan 18 18:20:14.520: INFO: Found 1 stateful pods, waiting for 3
    Jan 18 18:20:24.531: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:20:24.531: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 18 18:20:24.531: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/18/23 18:20:24.545
    Jan 18 18:20:24.576: INFO: Updating stateful set ss2
    Jan 18 18:20:24.589: INFO: Waiting for Pod statefulset-1585/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 18 18:20:34.633: INFO: Updating stateful set ss2
    Jan 18 18:20:34.651: INFO: Waiting for StatefulSet statefulset-1585/ss2 to complete update
    Jan 18 18:20:34.652: INFO: Waiting for Pod statefulset-1585/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 18:20:44.666: INFO: Deleting all statefulset in ns statefulset-1585
    Jan 18 18:20:44.673: INFO: Scaling statefulset ss2 to 0
    Jan 18 18:20:54.710: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 18:20:54.717: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 18:20:54.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1585" for this suite. 01/18/23 18:20:54.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:20:54.769
Jan 18 18:20:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 18:20:54.77
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:20:54.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:20:54.802
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-80e11f6f-a179-4f77-85b2-89ca278de713 01/18/23 18:20:54.816
STEP: Creating secret with name s-test-opt-upd-b063e2ca-6c4c-4f74-a144-1b57e321407f 01/18/23 18:20:54.827
STEP: Creating the pod 01/18/23 18:20:54.837
Jan 18 18:20:54.853: INFO: Waiting up to 5m0s for pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207" in namespace "secrets-5936" to be "running and ready"
Jan 18 18:20:54.861: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207": Phase="Pending", Reason="", readiness=false. Elapsed: 7.305082ms
Jan 18 18:20:54.861: INFO: The phase of Pod pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:20:56.871: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207": Phase="Running", Reason="", readiness=true. Elapsed: 2.01743611s
Jan 18 18:20:56.871: INFO: The phase of Pod pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207 is Running (Ready = true)
Jan 18 18:20:56.871: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-80e11f6f-a179-4f77-85b2-89ca278de713 01/18/23 18:20:56.923
STEP: Updating secret s-test-opt-upd-b063e2ca-6c4c-4f74-a144-1b57e321407f 01/18/23 18:20:56.936
STEP: Creating secret with name s-test-opt-create-c8d3c2de-d6d3-41a5-aa00-facd7034c312 01/18/23 18:20:56.946
STEP: waiting to observe update in volume 01/18/23 18:20:56.954
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 18:20:59.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5936" for this suite. 01/18/23 18:20:59.024
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":299,"skipped":5683,"failed":0}
------------------------------
â€¢ [4.268 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:20:54.769
    Jan 18 18:20:54.769: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 18:20:54.77
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:20:54.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:20:54.802
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-80e11f6f-a179-4f77-85b2-89ca278de713 01/18/23 18:20:54.816
    STEP: Creating secret with name s-test-opt-upd-b063e2ca-6c4c-4f74-a144-1b57e321407f 01/18/23 18:20:54.827
    STEP: Creating the pod 01/18/23 18:20:54.837
    Jan 18 18:20:54.853: INFO: Waiting up to 5m0s for pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207" in namespace "secrets-5936" to be "running and ready"
    Jan 18 18:20:54.861: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207": Phase="Pending", Reason="", readiness=false. Elapsed: 7.305082ms
    Jan 18 18:20:54.861: INFO: The phase of Pod pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:20:56.871: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207": Phase="Running", Reason="", readiness=true. Elapsed: 2.01743611s
    Jan 18 18:20:56.871: INFO: The phase of Pod pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207 is Running (Ready = true)
    Jan 18 18:20:56.871: INFO: Pod "pod-secrets-21c033f1-d507-4a3f-aa08-04203ce52207" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-80e11f6f-a179-4f77-85b2-89ca278de713 01/18/23 18:20:56.923
    STEP: Updating secret s-test-opt-upd-b063e2ca-6c4c-4f74-a144-1b57e321407f 01/18/23 18:20:56.936
    STEP: Creating secret with name s-test-opt-create-c8d3c2de-d6d3-41a5-aa00-facd7034c312 01/18/23 18:20:56.946
    STEP: waiting to observe update in volume 01/18/23 18:20:56.954
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 18:20:59.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5936" for this suite. 01/18/23 18:20:59.024
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:20:59.038
Jan 18 18:20:59.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 18:20:59.039
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:20:59.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:20:59.068
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/18/23 18:20:59.073
STEP: Ensuring ResourceQuota status is calculated 01/18/23 18:20:59.084
STEP: Creating a ResourceQuota with not terminating scope 01/18/23 18:21:01.092
STEP: Ensuring ResourceQuota status is calculated 01/18/23 18:21:01.103
STEP: Creating a long running pod 01/18/23 18:21:03.111
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 18:21:03.138
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 18:21:05.166
STEP: Deleting the pod 01/18/23 18:21:07.174
STEP: Ensuring resource quota status released the pod usage 01/18/23 18:21:07.195
STEP: Creating a terminating pod 01/18/23 18:21:09.204
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 18:21:09.224
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 18:21:11.232
STEP: Deleting the pod 01/18/23 18:21:13.24
STEP: Ensuring resource quota status released the pod usage 01/18/23 18:21:13.26
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 18:21:15.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-497" for this suite. 01/18/23 18:21:15.278
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":300,"skipped":5686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.254 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:20:59.038
    Jan 18 18:20:59.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 18:20:59.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:20:59.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:20:59.068
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/18/23 18:20:59.073
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 18:20:59.084
    STEP: Creating a ResourceQuota with not terminating scope 01/18/23 18:21:01.092
    STEP: Ensuring ResourceQuota status is calculated 01/18/23 18:21:01.103
    STEP: Creating a long running pod 01/18/23 18:21:03.111
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/18/23 18:21:03.138
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/18/23 18:21:05.166
    STEP: Deleting the pod 01/18/23 18:21:07.174
    STEP: Ensuring resource quota status released the pod usage 01/18/23 18:21:07.195
    STEP: Creating a terminating pod 01/18/23 18:21:09.204
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/18/23 18:21:09.224
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/18/23 18:21:11.232
    STEP: Deleting the pod 01/18/23 18:21:13.24
    STEP: Ensuring resource quota status released the pod usage 01/18/23 18:21:13.26
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 18:21:15.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-497" for this suite. 01/18/23 18:21:15.278
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:15.293
Jan 18 18:21:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 18:21:15.294
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:15.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:15.323
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/18/23 18:21:15.327
Jan 18 18:21:15.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 create -f -'
Jan 18 18:21:15.605: INFO: stderr: ""
Jan 18 18:21:15.605: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:15.605
Jan 18 18:21:15.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 18:21:15.706: INFO: stderr: ""
Jan 18 18:21:15.706: INFO: stdout: "update-demo-nautilus-9l42m update-demo-nautilus-pp467 "
Jan 18 18:21:15.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:15.806: INFO: stderr: ""
Jan 18 18:21:15.806: INFO: stdout: ""
Jan 18 18:21:15.806: INFO: update-demo-nautilus-9l42m is created but not running
Jan 18 18:21:20.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 18:21:20.907: INFO: stderr: ""
Jan 18 18:21:20.907: INFO: stdout: "update-demo-nautilus-9l42m update-demo-nautilus-pp467 "
Jan 18 18:21:20.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:21.003: INFO: stderr: ""
Jan 18 18:21:21.003: INFO: stdout: "true"
Jan 18 18:21:21.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 18:21:21.102: INFO: stderr: ""
Jan 18 18:21:21.102: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 18:21:21.102: INFO: validating pod update-demo-nautilus-9l42m
Jan 18 18:21:21.118: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 18:21:21.118: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 18:21:21.118: INFO: update-demo-nautilus-9l42m is verified up and running
Jan 18 18:21:21.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:21.213: INFO: stderr: ""
Jan 18 18:21:21.213: INFO: stdout: "true"
Jan 18 18:21:21.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 18:21:21.306: INFO: stderr: ""
Jan 18 18:21:21.306: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 18:21:21.306: INFO: validating pod update-demo-nautilus-pp467
Jan 18 18:21:21.321: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 18:21:21.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 18:21:21.321: INFO: update-demo-nautilus-pp467 is verified up and running
STEP: scaling down the replication controller 01/18/23 18:21:21.321
Jan 18 18:21:21.323: INFO: scanned /root for discovery docs: <nil>
Jan 18 18:21:21.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 18 18:21:22.449: INFO: stderr: ""
Jan 18 18:21:22.449: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:22.449
Jan 18 18:21:22.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 18:21:22.546: INFO: stderr: ""
Jan 18 18:21:22.546: INFO: stdout: "update-demo-nautilus-pp467 "
Jan 18 18:21:22.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:22.642: INFO: stderr: ""
Jan 18 18:21:22.642: INFO: stdout: "true"
Jan 18 18:21:22.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 18:21:22.734: INFO: stderr: ""
Jan 18 18:21:22.734: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 18:21:22.734: INFO: validating pod update-demo-nautilus-pp467
Jan 18 18:21:22.744: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 18:21:22.744: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 18:21:22.744: INFO: update-demo-nautilus-pp467 is verified up and running
STEP: scaling up the replication controller 01/18/23 18:21:22.744
Jan 18 18:21:22.746: INFO: scanned /root for discovery docs: <nil>
Jan 18 18:21:22.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 18 18:21:23.873: INFO: stderr: ""
Jan 18 18:21:23.873: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:23.873
Jan 18 18:21:23.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 18:21:23.973: INFO: stderr: ""
Jan 18 18:21:23.973: INFO: stdout: "update-demo-nautilus-mjjkl update-demo-nautilus-pp467 "
Jan 18 18:21:23.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:24.074: INFO: stderr: ""
Jan 18 18:21:24.074: INFO: stdout: ""
Jan 18 18:21:24.074: INFO: update-demo-nautilus-mjjkl is created but not running
Jan 18 18:21:29.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 18 18:21:29.175: INFO: stderr: ""
Jan 18 18:21:29.175: INFO: stdout: "update-demo-nautilus-mjjkl update-demo-nautilus-pp467 "
Jan 18 18:21:29.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:29.271: INFO: stderr: ""
Jan 18 18:21:29.271: INFO: stdout: "true"
Jan 18 18:21:29.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 18:21:29.364: INFO: stderr: ""
Jan 18 18:21:29.364: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 18:21:29.364: INFO: validating pod update-demo-nautilus-mjjkl
Jan 18 18:21:29.380: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 18:21:29.380: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 18:21:29.380: INFO: update-demo-nautilus-mjjkl is verified up and running
Jan 18 18:21:29.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 18 18:21:29.474: INFO: stderr: ""
Jan 18 18:21:29.474: INFO: stdout: "true"
Jan 18 18:21:29.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 18 18:21:29.569: INFO: stderr: ""
Jan 18 18:21:29.569: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 18 18:21:29.569: INFO: validating pod update-demo-nautilus-pp467
Jan 18 18:21:29.581: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 18 18:21:29.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 18 18:21:29.581: INFO: update-demo-nautilus-pp467 is verified up and running
STEP: using delete to clean up resources 01/18/23 18:21:29.581
Jan 18 18:21:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 delete --grace-period=0 --force -f -'
Jan 18 18:21:29.674: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:21:29.674: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 18 18:21:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get rc,svc -l name=update-demo --no-headers'
Jan 18 18:21:29.789: INFO: stderr: "No resources found in kubectl-525 namespace.\n"
Jan 18 18:21:29.789: INFO: stdout: ""
Jan 18 18:21:29.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 18:21:29.895: INFO: stderr: ""
Jan 18 18:21:29.895: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 18:21:29.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-525" for this suite. 01/18/23 18:21:29.904
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":301,"skipped":5700,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.627 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:15.293
    Jan 18 18:21:15.293: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 18:21:15.294
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:15.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:15.323
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/18/23 18:21:15.327
    Jan 18 18:21:15.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 create -f -'
    Jan 18 18:21:15.605: INFO: stderr: ""
    Jan 18 18:21:15.605: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:15.605
    Jan 18 18:21:15.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 18:21:15.706: INFO: stderr: ""
    Jan 18 18:21:15.706: INFO: stdout: "update-demo-nautilus-9l42m update-demo-nautilus-pp467 "
    Jan 18 18:21:15.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:15.806: INFO: stderr: ""
    Jan 18 18:21:15.806: INFO: stdout: ""
    Jan 18 18:21:15.806: INFO: update-demo-nautilus-9l42m is created but not running
    Jan 18 18:21:20.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 18:21:20.907: INFO: stderr: ""
    Jan 18 18:21:20.907: INFO: stdout: "update-demo-nautilus-9l42m update-demo-nautilus-pp467 "
    Jan 18 18:21:20.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:21.003: INFO: stderr: ""
    Jan 18 18:21:21.003: INFO: stdout: "true"
    Jan 18 18:21:21.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-9l42m -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 18:21:21.102: INFO: stderr: ""
    Jan 18 18:21:21.102: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 18:21:21.102: INFO: validating pod update-demo-nautilus-9l42m
    Jan 18 18:21:21.118: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 18:21:21.118: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 18:21:21.118: INFO: update-demo-nautilus-9l42m is verified up and running
    Jan 18 18:21:21.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:21.213: INFO: stderr: ""
    Jan 18 18:21:21.213: INFO: stdout: "true"
    Jan 18 18:21:21.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 18:21:21.306: INFO: stderr: ""
    Jan 18 18:21:21.306: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 18:21:21.306: INFO: validating pod update-demo-nautilus-pp467
    Jan 18 18:21:21.321: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 18:21:21.321: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 18:21:21.321: INFO: update-demo-nautilus-pp467 is verified up and running
    STEP: scaling down the replication controller 01/18/23 18:21:21.321
    Jan 18 18:21:21.323: INFO: scanned /root for discovery docs: <nil>
    Jan 18 18:21:21.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 18 18:21:22.449: INFO: stderr: ""
    Jan 18 18:21:22.449: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:22.449
    Jan 18 18:21:22.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 18:21:22.546: INFO: stderr: ""
    Jan 18 18:21:22.546: INFO: stdout: "update-demo-nautilus-pp467 "
    Jan 18 18:21:22.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:22.642: INFO: stderr: ""
    Jan 18 18:21:22.642: INFO: stdout: "true"
    Jan 18 18:21:22.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 18:21:22.734: INFO: stderr: ""
    Jan 18 18:21:22.734: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 18:21:22.734: INFO: validating pod update-demo-nautilus-pp467
    Jan 18 18:21:22.744: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 18:21:22.744: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 18:21:22.744: INFO: update-demo-nautilus-pp467 is verified up and running
    STEP: scaling up the replication controller 01/18/23 18:21:22.744
    Jan 18 18:21:22.746: INFO: scanned /root for discovery docs: <nil>
    Jan 18 18:21:22.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 18 18:21:23.873: INFO: stderr: ""
    Jan 18 18:21:23.873: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/18/23 18:21:23.873
    Jan 18 18:21:23.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 18:21:23.973: INFO: stderr: ""
    Jan 18 18:21:23.973: INFO: stdout: "update-demo-nautilus-mjjkl update-demo-nautilus-pp467 "
    Jan 18 18:21:23.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:24.074: INFO: stderr: ""
    Jan 18 18:21:24.074: INFO: stdout: ""
    Jan 18 18:21:24.074: INFO: update-demo-nautilus-mjjkl is created but not running
    Jan 18 18:21:29.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 18 18:21:29.175: INFO: stderr: ""
    Jan 18 18:21:29.175: INFO: stdout: "update-demo-nautilus-mjjkl update-demo-nautilus-pp467 "
    Jan 18 18:21:29.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:29.271: INFO: stderr: ""
    Jan 18 18:21:29.271: INFO: stdout: "true"
    Jan 18 18:21:29.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-mjjkl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 18:21:29.364: INFO: stderr: ""
    Jan 18 18:21:29.364: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 18:21:29.364: INFO: validating pod update-demo-nautilus-mjjkl
    Jan 18 18:21:29.380: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 18:21:29.380: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 18:21:29.380: INFO: update-demo-nautilus-mjjkl is verified up and running
    Jan 18 18:21:29.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 18 18:21:29.474: INFO: stderr: ""
    Jan 18 18:21:29.474: INFO: stdout: "true"
    Jan 18 18:21:29.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods update-demo-nautilus-pp467 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 18 18:21:29.569: INFO: stderr: ""
    Jan 18 18:21:29.569: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 18 18:21:29.569: INFO: validating pod update-demo-nautilus-pp467
    Jan 18 18:21:29.581: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 18 18:21:29.581: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 18 18:21:29.581: INFO: update-demo-nautilus-pp467 is verified up and running
    STEP: using delete to clean up resources 01/18/23 18:21:29.581
    Jan 18 18:21:29.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 delete --grace-period=0 --force -f -'
    Jan 18 18:21:29.674: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:21:29.674: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 18 18:21:29.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get rc,svc -l name=update-demo --no-headers'
    Jan 18 18:21:29.789: INFO: stderr: "No resources found in kubectl-525 namespace.\n"
    Jan 18 18:21:29.789: INFO: stdout: ""
    Jan 18 18:21:29.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-525 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 18:21:29.895: INFO: stderr: ""
    Jan 18 18:21:29.895: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 18:21:29.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-525" for this suite. 01/18/23 18:21:29.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:29.92
Jan 18 18:21:29.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-runtime 01/18/23 18:21:29.921
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:29.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:29.953
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/18/23 18:21:29.957
STEP: wait for the container to reach Succeeded 01/18/23 18:21:29.975
STEP: get the container status 01/18/23 18:21:34.018
STEP: the container should be terminated 01/18/23 18:21:34.025
STEP: the termination message should be set 01/18/23 18:21:34.025
Jan 18 18:21:34.025: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/18/23 18:21:34.025
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 18 18:21:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5832" for this suite. 01/18/23 18:21:34.063
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":302,"skipped":5712,"failed":0}
------------------------------
â€¢ [4.156 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:29.92
    Jan 18 18:21:29.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-runtime 01/18/23 18:21:29.921
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:29.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:29.953
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/18/23 18:21:29.957
    STEP: wait for the container to reach Succeeded 01/18/23 18:21:29.975
    STEP: get the container status 01/18/23 18:21:34.018
    STEP: the container should be terminated 01/18/23 18:21:34.025
    STEP: the termination message should be set 01/18/23 18:21:34.025
    Jan 18 18:21:34.025: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/18/23 18:21:34.025
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 18 18:21:34.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5832" for this suite. 01/18/23 18:21:34.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:34.078
Jan 18 18:21:34.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 18:21:34.079
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:34.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:34.109
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/18/23 18:21:34.114
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 18:21:34.117
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 18:21:34.117
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 18:21:34.117
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 18:21:34.119
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 18:21:34.119
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 18:21:34.121
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:21:34.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1735" for this suite. 01/18/23 18:21:34.13
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":303,"skipped":5741,"failed":0}
------------------------------
â€¢ [0.063 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:34.078
    Jan 18 18:21:34.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 18:21:34.079
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:34.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:34.109
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/18/23 18:21:34.114
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/18/23 18:21:34.117
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/18/23 18:21:34.117
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/18/23 18:21:34.117
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/18/23 18:21:34.119
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 18:21:34.119
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/18/23 18:21:34.121
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:21:34.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1735" for this suite. 01/18/23 18:21:34.13
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:34.142
Jan 18 18:21:34.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 18:21:34.143
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:34.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:34.172
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/18/23 18:21:34.177
Jan 18 18:21:34.177: INFO: namespace kubectl-6267
Jan 18 18:21:34.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 create -f -'
Jan 18 18:21:34.430: INFO: stderr: ""
Jan 18 18:21:34.430: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/18/23 18:21:34.43
Jan 18 18:21:35.439: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 18:21:35.439: INFO: Found 1 / 1
Jan 18 18:21:35.439: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 18 18:21:35.446: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 18 18:21:35.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 18 18:21:35.446: INFO: wait on agnhost-primary startup in kubectl-6267 
Jan 18 18:21:35.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 logs agnhost-primary-nvz7q agnhost-primary'
Jan 18 18:21:35.557: INFO: stderr: ""
Jan 18 18:21:35.557: INFO: stdout: "Paused\n"
STEP: exposing RC 01/18/23 18:21:35.557
Jan 18 18:21:35.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 18 18:21:35.677: INFO: stderr: ""
Jan 18 18:21:35.677: INFO: stdout: "service/rm2 exposed\n"
Jan 18 18:21:35.684: INFO: Service rm2 in namespace kubectl-6267 found.
STEP: exposing service 01/18/23 18:21:37.7
Jan 18 18:21:37.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 18 18:21:37.818: INFO: stderr: ""
Jan 18 18:21:37.818: INFO: stdout: "service/rm3 exposed\n"
Jan 18 18:21:37.827: INFO: Service rm3 in namespace kubectl-6267 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 18:21:39.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6267" for this suite. 01/18/23 18:21:39.866
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":304,"skipped":5744,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.746 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:34.142
    Jan 18 18:21:34.142: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 18:21:34.143
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:34.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:34.172
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/18/23 18:21:34.177
    Jan 18 18:21:34.177: INFO: namespace kubectl-6267
    Jan 18 18:21:34.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 create -f -'
    Jan 18 18:21:34.430: INFO: stderr: ""
    Jan 18 18:21:34.430: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/18/23 18:21:34.43
    Jan 18 18:21:35.439: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 18:21:35.439: INFO: Found 1 / 1
    Jan 18 18:21:35.439: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 18 18:21:35.446: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 18 18:21:35.446: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 18 18:21:35.446: INFO: wait on agnhost-primary startup in kubectl-6267 
    Jan 18 18:21:35.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 logs agnhost-primary-nvz7q agnhost-primary'
    Jan 18 18:21:35.557: INFO: stderr: ""
    Jan 18 18:21:35.557: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/18/23 18:21:35.557
    Jan 18 18:21:35.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 18 18:21:35.677: INFO: stderr: ""
    Jan 18 18:21:35.677: INFO: stdout: "service/rm2 exposed\n"
    Jan 18 18:21:35.684: INFO: Service rm2 in namespace kubectl-6267 found.
    STEP: exposing service 01/18/23 18:21:37.7
    Jan 18 18:21:37.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-6267 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 18 18:21:37.818: INFO: stderr: ""
    Jan 18 18:21:37.818: INFO: stdout: "service/rm3 exposed\n"
    Jan 18 18:21:37.827: INFO: Service rm3 in namespace kubectl-6267 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 18:21:39.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6267" for this suite. 01/18/23 18:21:39.866
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:39.888
Jan 18 18:21:39.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:21:39.889
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:39.922
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:39.928
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/18/23 18:21:39.933
Jan 18 18:21:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: mark a version not serverd 01/18/23 18:21:48.812
STEP: check the unserved version gets removed 01/18/23 18:21:48.844
STEP: check the other version is not changed 01/18/23 18:21:52.756
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:21:58.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9060" for this suite. 01/18/23 18:21:58.253
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":305,"skipped":5748,"failed":0}
------------------------------
â€¢ [SLOW TEST] [18.379 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:39.888
    Jan 18 18:21:39.888: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:21:39.889
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:39.922
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:39.928
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/18/23 18:21:39.933
    Jan 18 18:21:39.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: mark a version not serverd 01/18/23 18:21:48.812
    STEP: check the unserved version gets removed 01/18/23 18:21:48.844
    STEP: check the other version is not changed 01/18/23 18:21:52.756
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:21:58.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9060" for this suite. 01/18/23 18:21:58.253
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:58.268
Jan 18 18:21:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:21:58.269
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:58.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:58.303
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:21:58.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8805" for this suite. 01/18/23 18:21:58.324
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":306,"skipped":5755,"failed":0}
------------------------------
â€¢ [0.068 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:58.268
    Jan 18 18:21:58.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:21:58.269
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:58.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:58.303
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:21:58.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8805" for this suite. 01/18/23 18:21:58.324
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:21:58.336
Jan 18 18:21:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 18:21:58.337
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:58.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:58.366
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/18/23 18:21:58.37
STEP: delete the rc 01/18/23 18:22:03.39
STEP: wait for all pods to be garbage collected 01/18/23 18:22:03.405
STEP: Gathering metrics 01/18/23 18:22:08.42
W0118 18:22:08.432057      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 18:22:08.432: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 18:22:08.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4888" for this suite. 01/18/23 18:22:08.441
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":307,"skipped":5756,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.118 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:21:58.336
    Jan 18 18:21:58.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 18:21:58.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:21:58.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:21:58.366
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/18/23 18:21:58.37
    STEP: delete the rc 01/18/23 18:22:03.39
    STEP: wait for all pods to be garbage collected 01/18/23 18:22:03.405
    STEP: Gathering metrics 01/18/23 18:22:08.42
    W0118 18:22:08.432057      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 18:22:08.432: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 18:22:08.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4888" for this suite. 01/18/23 18:22:08.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:08.455
Jan 18 18:22:08.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename watch 01/18/23 18:22:08.456
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:08.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:08.485
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/18/23 18:22:08.49
STEP: creating a new configmap 01/18/23 18:22:08.493
STEP: modifying the configmap once 01/18/23 18:22:08.5
STEP: closing the watch once it receives two notifications 01/18/23 18:22:08.516
Jan 18 18:22:08.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981607 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:08.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981608 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/18/23 18:22:08.517
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 18:22:08.532
STEP: deleting the configmap 01/18/23 18:22:08.535
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 18:22:08.547
Jan 18 18:22:08.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981609 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:08.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981610 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 18:22:08.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3179" for this suite. 01/18/23 18:22:08.558
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":308,"skipped":5765,"failed":0}
------------------------------
â€¢ [0.116 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:08.455
    Jan 18 18:22:08.455: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename watch 01/18/23 18:22:08.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:08.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:08.485
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/18/23 18:22:08.49
    STEP: creating a new configmap 01/18/23 18:22:08.493
    STEP: modifying the configmap once 01/18/23 18:22:08.5
    STEP: closing the watch once it receives two notifications 01/18/23 18:22:08.516
    Jan 18 18:22:08.516: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981607 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:08.516: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981608 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/18/23 18:22:08.517
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/18/23 18:22:08.532
    STEP: deleting the configmap 01/18/23 18:22:08.535
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/18/23 18:22:08.547
    Jan 18 18:22:08.547: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981609 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:08.548: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3179  94e537d4-d961-47b0-93b6-751d41d11c3a 2631981610 0 2023-01-18 18:22:08 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:08 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 18:22:08.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3179" for this suite. 01/18/23 18:22:08.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:08.572
Jan 18 18:22:08.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-pred 01/18/23 18:22:08.573
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:08.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:08.599
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 18 18:22:08.603: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 18 18:22:08.617: INFO: Waiting for terminating namespaces to be deleted...
Jan 18 18:22:08.624: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
Jan 18 18:22:08.635: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 18:22:08.635: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 18:22:08.635: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 18:22:08.635: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 18:22:08.635: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 18:22:08.635: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 18:22:08.635: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
Jan 18 18:22:08.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:22:08.635: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 18 18:22:08.635: INFO: 
Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
Jan 18 18:22:08.646: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.646: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jan 18 18:22:08.646: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container calico-node ready: true, restart count 0
Jan 18 18:22:08.647: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container coredns ready: true, restart count 0
Jan 18 18:22:08.647: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jan 18 18:22:08.647: INFO: 	Container csi-plugin ready: true, restart count 0
Jan 18 18:22:08.647: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan 18 18:22:08.647: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container kube-proxy ready: true, restart count 0
Jan 18 18:22:08.647: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.647: INFO: 	Container metrics-server ready: true, restart count 0
Jan 18 18:22:08.647: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.648: INFO: 	Container node-problem-detector ready: true, restart count 0
Jan 18 18:22:08.648: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
Jan 18 18:22:08.648: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 18 18:22:08.648: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 18:22:08.648: INFO: 	Container e2e ready: true, restart count 0
Jan 18 18:22:08.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:22:08.648: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
Jan 18 18:22:08.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 18 18:22:08.648: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:22:08.687
STEP: verifying the node has the label node scw-conformance125-default-788643c0f7cd4128a5c 01/18/23 18:22:08.71
Jan 18 18:22:08.730: INFO: Pod calico-kube-controllers-78c6654d69-vblh8 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod calico-node-t8dbv requesting resource cpu=250m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod calico-node-trfh9 requesting resource cpu=250m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.730: INFO: Pod coredns-789d8d4d47-5xlp2 requesting resource cpu=100m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod csi-node-bcj2l requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.730: INFO: Pod csi-node-vmgw4 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod konnectivity-agent-bpts2 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod konnectivity-agent-vglg7 requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.730: INFO: Pod kube-proxy-6s5vw requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.730: INFO: Pod kube-proxy-mjbct requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod metrics-server-bc6968547-lkzx7 requesting resource cpu=100m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.730: INFO: Pod node-problem-detector-pcj72 requesting resource cpu=10m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.730: INFO: Pod node-problem-detector-ztrlj requesting resource cpu=10m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.731: INFO: Pod sonobuoy requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.731: INFO: Pod sonobuoy-e2e-job-d2521d19dc7b4f2c requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.731: INFO: Pod sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.731: INFO: Pod sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 18:22:08.731
Jan 18 18:22:08.731: INFO: Creating a pod which consumes cpu=2478m on Node scw-conformance125-default-61c39bbf4d81476a8e3
Jan 18 18:22:08.746: INFO: Creating a pod which consumes cpu=2338m on Node scw-conformance125-default-788643c0f7cd4128a5c
Jan 18 18:22:08.758: INFO: Waiting up to 5m0s for pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2" in namespace "sched-pred-5747" to be "running"
Jan 18 18:22:08.767: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.238568ms
Jan 18 18:22:10.776: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017980528s
Jan 18 18:22:10.776: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2" satisfied condition "running"
Jan 18 18:22:10.776: INFO: Waiting up to 5m0s for pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb" in namespace "sched-pred-5747" to be "running"
Jan 18 18:22:10.785: INFO: Pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb": Phase="Running", Reason="", readiness=true. Elapsed: 8.52236ms
Jan 18 18:22:10.785: INFO: Pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 18:22:10.785
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a46471e6a4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5747/filler-pod-9635dc54-68e3-4701-87be-655713032ae2 to scw-conformance125-default-61c39bbf4d81476a8e3] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a4674bceadb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a46771a4601], Reason = [Created], Message = [Created container filler-pod-9635dc54-68e3-4701-87be-655713032ae2] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a467dacd40b], Reason = [Started], Message = [Started container filler-pod-9635dc54-68e3-4701-87be-655713032ae2] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4647c08013], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5747/filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb to scw-conformance125-default-788643c0f7cd4128a5c] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4674379506], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 18:22:10.792
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4676716405], Reason = [Created], Message = [Created container filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb] 01/18/23 18:22:10.793
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a467cc32901], Reason = [Started], Message = [Started container filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb] 01/18/23 18:22:10.793
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173b7a46c1425760], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 01/18/23 18:22:10.821
STEP: removing the label node off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:22:11.813
STEP: verifying the node doesn't have the label node 01/18/23 18:22:11.847
STEP: removing the label node off the node scw-conformance125-default-788643c0f7cd4128a5c 01/18/23 18:22:11.856
STEP: verifying the node doesn't have the label node 01/18/23 18:22:11.883
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:22:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5747" for this suite. 01/18/23 18:22:11.898
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":309,"skipped":5773,"failed":0}
------------------------------
â€¢ [3.341 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:08.572
    Jan 18 18:22:08.573: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-pred 01/18/23 18:22:08.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:08.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:08.599
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 18 18:22:08.603: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 18 18:22:08.617: INFO: Waiting for terminating namespaces to be deleted...
    Jan 18 18:22:08.624: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-61c39bbf4d81476a8e3 before test
    Jan 18 18:22:08.635: INFO: calico-node-trfh9 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: csi-node-bcj2l from kube-system started at 2023-01-18 16:14:59 +0000 UTC (2 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: konnectivity-agent-vglg7 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: kube-proxy-6s5vw from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: node-problem-detector-pcj72 from kube-system started at 2023-01-18 16:14:59 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m from sonobuoy started at 2023-01-18 16:58:10 +0000 UTC (2 container statuses recorded)
    Jan 18 18:22:08.635: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 18 18:22:08.635: INFO: 
    Logging pods the apiserver thinks is on node scw-conformance125-default-788643c0f7cd4128a5c before test
    Jan 18 18:22:08.646: INFO: calico-kube-controllers-78c6654d69-vblh8 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.646: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jan 18 18:22:08.646: INFO: calico-node-t8dbv from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container calico-node ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: coredns-789d8d4d47-5xlp2 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container coredns ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: csi-node-vmgw4 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (2 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: 	Container csi-plugin ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: konnectivity-agent-bpts2 from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: kube-proxy-mjbct from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: metrics-server-bc6968547-lkzx7 from kube-system started at 2023-01-18 17:03:07 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.647: INFO: 	Container metrics-server ready: true, restart count 0
    Jan 18 18:22:08.647: INFO: node-problem-detector-ztrlj from kube-system started at 2023-01-18 16:14:50 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.648: INFO: 	Container node-problem-detector ready: true, restart count 0
    Jan 18 18:22:08.648: INFO: sonobuoy from sonobuoy started at 2023-01-18 16:58:06 +0000 UTC (1 container statuses recorded)
    Jan 18 18:22:08.648: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 18 18:22:08.648: INFO: sonobuoy-e2e-job-d2521d19dc7b4f2c from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 18:22:08.648: INFO: 	Container e2e ready: true, restart count 0
    Jan 18 18:22:08.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:22:08.648: INFO: sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk from sonobuoy started at 2023-01-18 16:58:09 +0000 UTC (2 container statuses recorded)
    Jan 18 18:22:08.648: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 18 18:22:08.648: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:22:08.687
    STEP: verifying the node has the label node scw-conformance125-default-788643c0f7cd4128a5c 01/18/23 18:22:08.71
    Jan 18 18:22:08.730: INFO: Pod calico-kube-controllers-78c6654d69-vblh8 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod calico-node-t8dbv requesting resource cpu=250m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod calico-node-trfh9 requesting resource cpu=250m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.730: INFO: Pod coredns-789d8d4d47-5xlp2 requesting resource cpu=100m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod csi-node-bcj2l requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.730: INFO: Pod csi-node-vmgw4 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod konnectivity-agent-bpts2 requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod konnectivity-agent-vglg7 requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.730: INFO: Pod kube-proxy-6s5vw requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.730: INFO: Pod kube-proxy-mjbct requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod metrics-server-bc6968547-lkzx7 requesting resource cpu=100m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.730: INFO: Pod node-problem-detector-pcj72 requesting resource cpu=10m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.730: INFO: Pod node-problem-detector-ztrlj requesting resource cpu=10m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.731: INFO: Pod sonobuoy requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.731: INFO: Pod sonobuoy-e2e-job-d2521d19dc7b4f2c requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.731: INFO: Pod sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-95lvk requesting resource cpu=0m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.731: INFO: Pod sonobuoy-systemd-logs-daemon-set-0060235529ac48b6-txx2m requesting resource cpu=0m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    STEP: Starting Pods to consume most of the cluster CPU. 01/18/23 18:22:08.731
    Jan 18 18:22:08.731: INFO: Creating a pod which consumes cpu=2478m on Node scw-conformance125-default-61c39bbf4d81476a8e3
    Jan 18 18:22:08.746: INFO: Creating a pod which consumes cpu=2338m on Node scw-conformance125-default-788643c0f7cd4128a5c
    Jan 18 18:22:08.758: INFO: Waiting up to 5m0s for pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2" in namespace "sched-pred-5747" to be "running"
    Jan 18 18:22:08.767: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.238568ms
    Jan 18 18:22:10.776: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2": Phase="Running", Reason="", readiness=true. Elapsed: 2.017980528s
    Jan 18 18:22:10.776: INFO: Pod "filler-pod-9635dc54-68e3-4701-87be-655713032ae2" satisfied condition "running"
    Jan 18 18:22:10.776: INFO: Waiting up to 5m0s for pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb" in namespace "sched-pred-5747" to be "running"
    Jan 18 18:22:10.785: INFO: Pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb": Phase="Running", Reason="", readiness=true. Elapsed: 8.52236ms
    Jan 18 18:22:10.785: INFO: Pod "filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/18/23 18:22:10.785
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a46471e6a4a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5747/filler-pod-9635dc54-68e3-4701-87be-655713032ae2 to scw-conformance125-default-61c39bbf4d81476a8e3] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a4674bceadb], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a46771a4601], Reason = [Created], Message = [Created container filler-pod-9635dc54-68e3-4701-87be-655713032ae2] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-9635dc54-68e3-4701-87be-655713032ae2.173b7a467dacd40b], Reason = [Started], Message = [Started container filler-pod-9635dc54-68e3-4701-87be-655713032ae2] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4647c08013], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5747/filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb to scw-conformance125-default-788643c0f7cd4128a5c] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4674379506], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/18/23 18:22:10.792
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a4676716405], Reason = [Created], Message = [Created container filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb] 01/18/23 18:22:10.793
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb.173b7a467cc32901], Reason = [Started], Message = [Started container filler-pod-d621194f-e446-4463-8b9d-ec0ad9fa9abb] 01/18/23 18:22:10.793
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173b7a46c1425760], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.] 01/18/23 18:22:10.821
    STEP: removing the label node off the node scw-conformance125-default-61c39bbf4d81476a8e3 01/18/23 18:22:11.813
    STEP: verifying the node doesn't have the label node 01/18/23 18:22:11.847
    STEP: removing the label node off the node scw-conformance125-default-788643c0f7cd4128a5c 01/18/23 18:22:11.856
    STEP: verifying the node doesn't have the label node 01/18/23 18:22:11.883
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:22:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5747" for this suite. 01/18/23 18:22:11.898
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:11.915
Jan 18 18:22:11.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:22:11.917
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:11.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:11.949
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/18/23 18:22:11.953
Jan 18 18:22:11.970: INFO: Waiting up to 5m0s for pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb" in namespace "downward-api-8030" to be "running and ready"
Jan 18 18:22:11.977: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.980849ms
Jan 18 18:22:11.977: INFO: The phase of Pod annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:22:13.986: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016235193s
Jan 18 18:22:13.987: INFO: The phase of Pod annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb is Running (Ready = true)
Jan 18 18:22:13.987: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb" satisfied condition "running and ready"
Jan 18 18:22:14.530: INFO: Successfully updated pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 18 18:22:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8030" for this suite. 01/18/23 18:22:18.592
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":310,"skipped":5777,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.689 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:11.915
    Jan 18 18:22:11.916: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:22:11.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:11.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:11.949
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/18/23 18:22:11.953
    Jan 18 18:22:11.970: INFO: Waiting up to 5m0s for pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb" in namespace "downward-api-8030" to be "running and ready"
    Jan 18 18:22:11.977: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.980849ms
    Jan 18 18:22:11.977: INFO: The phase of Pod annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:22:13.986: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb": Phase="Running", Reason="", readiness=true. Elapsed: 2.016235193s
    Jan 18 18:22:13.987: INFO: The phase of Pod annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb is Running (Ready = true)
    Jan 18 18:22:13.987: INFO: Pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb" satisfied condition "running and ready"
    Jan 18 18:22:14.530: INFO: Successfully updated pod "annotationupdatea8038fb4-f971-4588-ae7e-69c34d08cbfb"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 18 18:22:18.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8030" for this suite. 01/18/23 18:22:18.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:18.605
Jan 18 18:22:18.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename watch 01/18/23 18:22:18.606
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:18.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:18.633
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/18/23 18:22:18.638
STEP: starting a background goroutine to produce watch events 01/18/23 18:22:18.642
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 18:22:18.643
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 18:22:21.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7103" for this suite. 01/18/23 18:22:21.463
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":311,"skipped":5791,"failed":0}
------------------------------
â€¢ [2.915 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:18.605
    Jan 18 18:22:18.605: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename watch 01/18/23 18:22:18.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:18.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:18.633
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/18/23 18:22:18.638
    STEP: starting a background goroutine to produce watch events 01/18/23 18:22:18.642
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/18/23 18:22:18.643
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 18:22:21.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7103" for this suite. 01/18/23 18:22:21.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:21.521
Jan 18 18:22:21.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 18:22:21.522
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:21.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:21.551
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1560 01/18/23 18:22:21.556
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-1560 01/18/23 18:22:21.574
Jan 18 18:22:21.590: INFO: Found 0 stateful pods, waiting for 1
Jan 18 18:22:31.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/18/23 18:22:31.614
STEP: Getting /status 01/18/23 18:22:31.631
Jan 18 18:22:31.639: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/18/23 18:22:31.639
Jan 18 18:22:31.657: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/18/23 18:22:31.657
Jan 18 18:22:31.660: INFO: Observed &StatefulSet event: ADDED
Jan 18 18:22:31.660: INFO: Found Statefulset ss in namespace statefulset-1560 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 18 18:22:31.660: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/18/23 18:22:31.66
Jan 18 18:22:31.660: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 18 18:22:31.673: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/18/23 18:22:31.673
Jan 18 18:22:31.676: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 18:22:31.676: INFO: Deleting all statefulset in ns statefulset-1560
Jan 18 18:22:31.682: INFO: Scaling statefulset ss to 0
Jan 18 18:22:41.716: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 18:22:41.726: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 18:22:41.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1560" for this suite. 01/18/23 18:22:41.761
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":312,"skipped":5807,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.250 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:21.521
    Jan 18 18:22:21.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 18:22:21.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:21.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:21.551
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1560 01/18/23 18:22:21.556
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-1560 01/18/23 18:22:21.574
    Jan 18 18:22:21.590: INFO: Found 0 stateful pods, waiting for 1
    Jan 18 18:22:31.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/18/23 18:22:31.614
    STEP: Getting /status 01/18/23 18:22:31.631
    Jan 18 18:22:31.639: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/18/23 18:22:31.639
    Jan 18 18:22:31.657: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/18/23 18:22:31.657
    Jan 18 18:22:31.660: INFO: Observed &StatefulSet event: ADDED
    Jan 18 18:22:31.660: INFO: Found Statefulset ss in namespace statefulset-1560 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 18 18:22:31.660: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/18/23 18:22:31.66
    Jan 18 18:22:31.660: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 18 18:22:31.673: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/18/23 18:22:31.673
    Jan 18 18:22:31.676: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 18:22:31.676: INFO: Deleting all statefulset in ns statefulset-1560
    Jan 18 18:22:31.682: INFO: Scaling statefulset ss to 0
    Jan 18 18:22:41.716: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 18:22:41.726: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 18:22:41.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1560" for this suite. 01/18/23 18:22:41.761
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:41.774
Jan 18 18:22:41.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 18:22:41.775
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:41.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:41.805
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-3ec25dbb-f7e1-42fa-a703-be3f381586bf 01/18/23 18:22:41.81
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 18 18:22:41.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1446" for this suite. 01/18/23 18:22:41.826
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":313,"skipped":5828,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:41.774
    Jan 18 18:22:41.774: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 18:22:41.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:41.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:41.805
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-3ec25dbb-f7e1-42fa-a703-be3f381586bf 01/18/23 18:22:41.81
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 18:22:41.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1446" for this suite. 01/18/23 18:22:41.826
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:41.838
Jan 18 18:22:41.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename daemonsets 01/18/23 18:22:41.839
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:41.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:41.868
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/18/23 18:22:41.903
STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 18:22:41.915
Jan 18 18:22:41.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 18:22:41.931: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 18:22:42.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 18:22:42.947: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
Jan 18 18:22:43.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 18:22:43.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 18:22:43.953
Jan 18 18:22:43.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 18:22:43.991: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 18:22:45.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 18:22:45.008: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 18:22:46.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 18:22:46.009: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 18:22:47.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 18 18:22:47.007: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
Jan 18 18:22:48.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 18 18:22:48.008: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/18/23 18:22:48.016
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-177, will wait for the garbage collector to delete the pods 01/18/23 18:22:48.016
Jan 18 18:22:48.089: INFO: Deleting DaemonSet.extensions daemon-set took: 13.399825ms
Jan 18 18:22:48.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.744632ms
Jan 18 18:22:50.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 18 18:22:50.797: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 18 18:22:50.803: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631983366"},"items":null}

Jan 18 18:22:50.809: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631983366"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:22:50.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-177" for this suite. 01/18/23 18:22:50.839
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":314,"skipped":5829,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.013 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:41.838
    Jan 18 18:22:41.838: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename daemonsets 01/18/23 18:22:41.839
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:41.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:41.868
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/18/23 18:22:41.903
    STEP: Check that daemon pods launch on every node of the cluster. 01/18/23 18:22:41.915
    Jan 18 18:22:41.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 18:22:41.931: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 18:22:42.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 18:22:42.947: INFO: Node scw-conformance125-default-61c39bbf4d81476a8e3 is running 0 daemon pod, expected 1
    Jan 18 18:22:43.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 18:22:43.948: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/18/23 18:22:43.953
    Jan 18 18:22:43.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 18:22:43.991: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 18:22:45.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 18:22:45.008: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 18:22:46.009: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 18:22:46.009: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 18:22:47.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 18 18:22:47.007: INFO: Node scw-conformance125-default-788643c0f7cd4128a5c is running 0 daemon pod, expected 1
    Jan 18 18:22:48.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 18 18:22:48.008: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/18/23 18:22:48.016
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-177, will wait for the garbage collector to delete the pods 01/18/23 18:22:48.016
    Jan 18 18:22:48.089: INFO: Deleting DaemonSet.extensions daemon-set took: 13.399825ms
    Jan 18 18:22:48.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.744632ms
    Jan 18 18:22:50.797: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 18 18:22:50.797: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 18 18:22:50.803: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"2631983366"},"items":null}

    Jan 18 18:22:50.809: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"2631983366"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:22:50.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-177" for this suite. 01/18/23 18:22:50.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:22:50.854
Jan 18 18:22:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename watch 01/18/23 18:22:50.855
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:50.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:50.886
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/18/23 18:22:50.891
STEP: creating a watch on configmaps with label B 01/18/23 18:22:50.893
STEP: creating a watch on configmaps with label A or B 01/18/23 18:22:50.896
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.898
Jan 18 18:22:50.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983373 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:50.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983373 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.906
Jan 18 18:22:50.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983374 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:50.921: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983374 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 18:22:50.921
Jan 18 18:22:50.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983375 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:50.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983375 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.938
Jan 18 18:22:50.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983377 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:50.952: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983377 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 18:22:50.952
Jan 18 18:22:50.961: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983378 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:22:50.961: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983378 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 18:23:00.962
Jan 18 18:23:00.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983755 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 18 18:23:00.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983755 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 18 18:23:10.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8338" for this suite. 01/18/23 18:23:10.987
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":315,"skipped":5869,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.147 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:22:50.854
    Jan 18 18:22:50.854: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename watch 01/18/23 18:22:50.855
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:22:50.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:22:50.886
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/18/23 18:22:50.891
    STEP: creating a watch on configmaps with label B 01/18/23 18:22:50.893
    STEP: creating a watch on configmaps with label A or B 01/18/23 18:22:50.896
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.898
    Jan 18 18:22:50.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983373 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:50.906: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983373 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.906
    Jan 18 18:22:50.920: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983374 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:50.921: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983374 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/18/23 18:22:50.921
    Jan 18 18:22:50.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983375 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:50.938: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983375 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/18/23 18:22:50.938
    Jan 18 18:22:50.951: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983377 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:50.952: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8338  ae47b45b-7b49-409e-9c61-9fe090e84f3d 2631983377 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/18/23 18:22:50.952
    Jan 18 18:22:50.961: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983378 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:22:50.961: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983378 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/18/23 18:23:00.962
    Jan 18 18:23:00.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983755 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 18 18:23:00.975: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8338  c479adea-509b-4bbc-b377-99cabc7e29c4 2631983755 0 2023-01-18 18:22:50 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-18 18:22:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 18 18:23:10.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-8338" for this suite. 01/18/23 18:23:10.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:11.003
Jan 18 18:23:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 18:23:11.004
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:11.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:11.032
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/18/23 18:23:11.037
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_tcp@PTR;sleep 1; done
 01/18/23 18:23:11.067
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_tcp@PTR;sleep 1; done
 01/18/23 18:23:11.067
STEP: creating a pod to probe DNS 01/18/23 18:23:11.067
STEP: submitting the pod to kubernetes 01/18/23 18:23:11.068
Jan 18 18:23:11.085: INFO: Waiting up to 15m0s for pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b" in namespace "dns-1632" to be "running"
Jan 18 18:23:11.093: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786535ms
Jan 18 18:23:13.103: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017589587s
Jan 18 18:23:13.103: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b" satisfied condition "running"
STEP: retrieving the pod 01/18/23 18:23:13.103
STEP: looking for the results for each expected name from probers 01/18/23 18:23:13.11
Jan 18 18:23:13.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.144: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.152: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.196: INFO: Unable to read jessie_udp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.214: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.223: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
Jan 18 18:23:13.261: INFO: Lookups using dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b failed for: [wheezy_udp@dns-test-service.dns-1632.svc.cluster.local wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local jessie_udp@dns-test-service.dns-1632.svc.cluster.local jessie_tcp@dns-test-service.dns-1632.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local]

Jan 18 18:23:18.428: INFO: DNS probes using dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b succeeded

STEP: deleting the pod 01/18/23 18:23:18.428
STEP: deleting the test service 01/18/23 18:23:18.456
STEP: deleting the test headless service 01/18/23 18:23:18.488
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 18:23:18.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1632" for this suite. 01/18/23 18:23:18.515
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":316,"skipped":5875,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.523 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:11.003
    Jan 18 18:23:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 18:23:11.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:11.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:11.032
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/18/23 18:23:11.037
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_tcp@PTR;sleep 1; done
     01/18/23 18:23:11.067
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1632.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1632.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1632.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_udp@PTR;check="$$(dig +tcp +noall +answer +search 213.45.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.45.213_tcp@PTR;sleep 1; done
     01/18/23 18:23:11.067
    STEP: creating a pod to probe DNS 01/18/23 18:23:11.067
    STEP: submitting the pod to kubernetes 01/18/23 18:23:11.068
    Jan 18 18:23:11.085: INFO: Waiting up to 15m0s for pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b" in namespace "dns-1632" to be "running"
    Jan 18 18:23:11.093: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786535ms
    Jan 18 18:23:13.103: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b": Phase="Running", Reason="", readiness=true. Elapsed: 2.017589587s
    Jan 18 18:23:13.103: INFO: Pod "dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 18:23:13.103
    STEP: looking for the results for each expected name from probers 01/18/23 18:23:13.11
    Jan 18 18:23:13.124: INFO: Unable to read wheezy_udp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.134: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.144: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.152: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.196: INFO: Unable to read jessie_udp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.214: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.223: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local from pod dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b: the server could not find the requested resource (get pods dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b)
    Jan 18 18:23:13.261: INFO: Lookups using dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b failed for: [wheezy_udp@dns-test-service.dns-1632.svc.cluster.local wheezy_tcp@dns-test-service.dns-1632.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local jessie_udp@dns-test-service.dns-1632.svc.cluster.local jessie_tcp@dns-test-service.dns-1632.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1632.svc.cluster.local]

    Jan 18 18:23:18.428: INFO: DNS probes using dns-1632/dns-test-6f02dc38-4a47-476a-acdc-00e8a785f65b succeeded

    STEP: deleting the pod 01/18/23 18:23:18.428
    STEP: deleting the test service 01/18/23 18:23:18.456
    STEP: deleting the test headless service 01/18/23 18:23:18.488
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 18:23:18.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1632" for this suite. 01/18/23 18:23:18.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:18.532
Jan 18 18:23:18.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename security-context-test 01/18/23 18:23:18.534
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:18.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:18.572
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 18 18:23:18.595: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca" in namespace "security-context-test-3350" to be "Succeeded or Failed"
Jan 18 18:23:18.602: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07356ms
Jan 18 18:23:20.610: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014716145s
Jan 18 18:23:22.612: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016452965s
Jan 18 18:23:24.611: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015438188s
Jan 18 18:23:24.611: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 18 18:23:24.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3350" for this suite. 01/18/23 18:23:24.638
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5895,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.118 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:18.532
    Jan 18 18:23:18.533: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename security-context-test 01/18/23 18:23:18.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:18.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:18.572
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 18 18:23:18.595: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca" in namespace "security-context-test-3350" to be "Succeeded or Failed"
    Jan 18 18:23:18.602: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07356ms
    Jan 18 18:23:20.610: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014716145s
    Jan 18 18:23:22.612: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016452965s
    Jan 18 18:23:24.611: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015438188s
    Jan 18 18:23:24.611: INFO: Pod "alpine-nnp-false-7058f908-3c65-45e0-85c8-1a3ed5f915ca" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 18 18:23:24.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3350" for this suite. 01/18/23 18:23:24.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:24.651
Jan 18 18:23:24.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:23:24.652
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:24.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:24.685
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/18/23 18:23:24.69
Jan 18 18:23:24.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4" in namespace "projected-1877" to be "Succeeded or Failed"
Jan 18 18:23:24.712: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442148ms
Jan 18 18:23:26.720: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013830184s
Jan 18 18:23:28.719: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01234456s
STEP: Saw pod success 01/18/23 18:23:28.719
Jan 18 18:23:28.719: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4" satisfied condition "Succeeded or Failed"
Jan 18 18:23:28.726: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 container client-container: <nil>
STEP: delete the pod 01/18/23 18:23:28.741
Jan 18 18:23:28.760: INFO: Waiting for pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 to disappear
Jan 18 18:23:28.765: INFO: Pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 18:23:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1877" for this suite. 01/18/23 18:23:28.773
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":5910,"failed":0}
------------------------------
â€¢ [4.134 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:24.651
    Jan 18 18:23:24.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:23:24.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:24.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:24.685
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/18/23 18:23:24.69
    Jan 18 18:23:24.707: INFO: Waiting up to 5m0s for pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4" in namespace "projected-1877" to be "Succeeded or Failed"
    Jan 18 18:23:24.712: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442148ms
    Jan 18 18:23:26.720: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013830184s
    Jan 18 18:23:28.719: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01234456s
    STEP: Saw pod success 01/18/23 18:23:28.719
    Jan 18 18:23:28.719: INFO: Pod "downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4" satisfied condition "Succeeded or Failed"
    Jan 18 18:23:28.726: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 container client-container: <nil>
    STEP: delete the pod 01/18/23 18:23:28.741
    Jan 18 18:23:28.760: INFO: Waiting for pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 to disappear
    Jan 18 18:23:28.765: INFO: Pod downwardapi-volume-324e7c9c-bb88-49ed-9e47-12a2f6cfada4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 18:23:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1877" for this suite. 01/18/23 18:23:28.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:28.788
Jan 18 18:23:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubectl 01/18/23 18:23:28.789
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:28.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:28.818
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/18/23 18:23:28.823
Jan 18 18:23:28.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 create -f -'
Jan 18 18:23:30.083: INFO: stderr: ""
Jan 18 18:23:30.083: INFO: stdout: "pod/pause created\n"
Jan 18 18:23:30.083: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 18 18:23:30.083: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3904" to be "running and ready"
Jan 18 18:23:30.089: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903662ms
Jan 18 18:23:30.089: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'scw-conformance125-default-61c39bbf4d81476a8e3' to be 'Running' but was 'Pending'
Jan 18 18:23:32.096: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012909286s
Jan 18 18:23:32.096: INFO: Pod "pause" satisfied condition "running and ready"
Jan 18 18:23:32.096: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 18:23:32.096
Jan 18 18:23:32.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 label pods pause testing-label=testing-label-value'
Jan 18 18:23:32.204: INFO: stderr: ""
Jan 18 18:23:32.204: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 18:23:32.204
Jan 18 18:23:32.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pod pause -L testing-label'
Jan 18 18:23:32.297: INFO: stderr: ""
Jan 18 18:23:32.297: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/18/23 18:23:32.297
Jan 18 18:23:32.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 label pods pause testing-label-'
Jan 18 18:23:32.405: INFO: stderr: ""
Jan 18 18:23:32.405: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/18/23 18:23:32.405
Jan 18 18:23:32.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pod pause -L testing-label'
Jan 18 18:23:32.503: INFO: stderr: ""
Jan 18 18:23:32.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/18/23 18:23:32.503
Jan 18 18:23:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 delete --grace-period=0 --force -f -'
Jan 18 18:23:32.617: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 18 18:23:32.617: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 18 18:23:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get rc,svc -l name=pause --no-headers'
Jan 18 18:23:32.718: INFO: stderr: "No resources found in kubectl-3904 namespace.\n"
Jan 18 18:23:32.718: INFO: stdout: ""
Jan 18 18:23:32.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 18 18:23:32.802: INFO: stderr: ""
Jan 18 18:23:32.802: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 18 18:23:32.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3904" for this suite. 01/18/23 18:23:32.81
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":319,"skipped":5942,"failed":0}
------------------------------
â€¢ [4.035 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:28.788
    Jan 18 18:23:28.789: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubectl 01/18/23 18:23:28.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:28.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:28.818
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/18/23 18:23:28.823
    Jan 18 18:23:28.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 create -f -'
    Jan 18 18:23:30.083: INFO: stderr: ""
    Jan 18 18:23:30.083: INFO: stdout: "pod/pause created\n"
    Jan 18 18:23:30.083: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 18 18:23:30.083: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3904" to be "running and ready"
    Jan 18 18:23:30.089: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.903662ms
    Jan 18 18:23:30.089: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'scw-conformance125-default-61c39bbf4d81476a8e3' to be 'Running' but was 'Pending'
    Jan 18 18:23:32.096: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.012909286s
    Jan 18 18:23:32.096: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 18 18:23:32.096: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/18/23 18:23:32.096
    Jan 18 18:23:32.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 label pods pause testing-label=testing-label-value'
    Jan 18 18:23:32.204: INFO: stderr: ""
    Jan 18 18:23:32.204: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/18/23 18:23:32.204
    Jan 18 18:23:32.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pod pause -L testing-label'
    Jan 18 18:23:32.297: INFO: stderr: ""
    Jan 18 18:23:32.297: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/18/23 18:23:32.297
    Jan 18 18:23:32.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 label pods pause testing-label-'
    Jan 18 18:23:32.405: INFO: stderr: ""
    Jan 18 18:23:32.405: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/18/23 18:23:32.405
    Jan 18 18:23:32.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pod pause -L testing-label'
    Jan 18 18:23:32.503: INFO: stderr: ""
    Jan 18 18:23:32.503: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/18/23 18:23:32.503
    Jan 18 18:23:32.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 delete --grace-period=0 --force -f -'
    Jan 18 18:23:32.617: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 18 18:23:32.617: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 18 18:23:32.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get rc,svc -l name=pause --no-headers'
    Jan 18 18:23:32.718: INFO: stderr: "No resources found in kubectl-3904 namespace.\n"
    Jan 18 18:23:32.718: INFO: stdout: ""
    Jan 18 18:23:32.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=kubectl-3904 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 18 18:23:32.802: INFO: stderr: ""
    Jan 18 18:23:32.802: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 18 18:23:32.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3904" for this suite. 01/18/23 18:23:32.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:32.825
Jan 18 18:23:32.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 18:23:32.826
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:32.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:32.857
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/18/23 18:23:32.861
STEP: Ensuring job reaches completions 01/18/23 18:23:32.872
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 18:23:44.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4971" for this suite. 01/18/23 18:23:44.892
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":320,"skipped":5973,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.080 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:32.825
    Jan 18 18:23:32.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 18:23:32.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:32.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:32.857
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/18/23 18:23:32.861
    STEP: Ensuring job reaches completions 01/18/23 18:23:32.872
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 18:23:44.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4971" for this suite. 01/18/23 18:23:44.892
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:44.906
Jan 18 18:23:44.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename emptydir 01/18/23 18:23:44.907
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:44.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:44.938
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 18:23:44.943
Jan 18 18:23:44.959: INFO: Waiting up to 5m0s for pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8" in namespace "emptydir-7709" to be "Succeeded or Failed"
Jan 18 18:23:44.969: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.665439ms
Jan 18 18:23:46.977: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017886022s
Jan 18 18:23:48.978: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01865253s
STEP: Saw pod success 01/18/23 18:23:48.978
Jan 18 18:23:48.978: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8" satisfied condition "Succeeded or Failed"
Jan 18 18:23:48.984: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 container test-container: <nil>
STEP: delete the pod 01/18/23 18:23:49.001
Jan 18 18:23:49.022: INFO: Waiting for pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 to disappear
Jan 18 18:23:49.033: INFO: Pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 18 18:23:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7709" for this suite. 01/18/23 18:23:49.041
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":321,"skipped":5974,"failed":0}
------------------------------
â€¢ [4.149 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:44.906
    Jan 18 18:23:44.906: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename emptydir 01/18/23 18:23:44.907
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:44.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:44.938
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/18/23 18:23:44.943
    Jan 18 18:23:44.959: INFO: Waiting up to 5m0s for pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8" in namespace "emptydir-7709" to be "Succeeded or Failed"
    Jan 18 18:23:44.969: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.665439ms
    Jan 18 18:23:46.977: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017886022s
    Jan 18 18:23:48.978: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01865253s
    STEP: Saw pod success 01/18/23 18:23:48.978
    Jan 18 18:23:48.978: INFO: Pod "pod-136f4de4-070c-449a-851b-f32fefbb8dd8" satisfied condition "Succeeded or Failed"
    Jan 18 18:23:48.984: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 container test-container: <nil>
    STEP: delete the pod 01/18/23 18:23:49.001
    Jan 18 18:23:49.022: INFO: Waiting for pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 to disappear
    Jan 18 18:23:49.033: INFO: Pod pod-136f4de4-070c-449a-851b-f32fefbb8dd8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 18 18:23:49.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7709" for this suite. 01/18/23 18:23:49.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:49.056
Jan 18 18:23:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename services 01/18/23 18:23:49.057
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:49.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:49.084
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-8885 01/18/23 18:23:49.089
STEP: creating service affinity-clusterip-transition in namespace services-8885 01/18/23 18:23:49.089
STEP: creating replication controller affinity-clusterip-transition in namespace services-8885 01/18/23 18:23:49.112
I0118 18:23:49.124169      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8885, replica count: 3
I0118 18:23:52.175418      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 18 18:23:52.191: INFO: Creating new exec pod
Jan 18 18:23:52.202: INFO: Waiting up to 5m0s for pod "execpod-affinityfswqm" in namespace "services-8885" to be "running"
Jan 18 18:23:52.209: INFO: Pod "execpod-affinityfswqm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.907029ms
Jan 18 18:23:54.217: INFO: Pod "execpod-affinityfswqm": Phase="Running", Reason="", readiness=true. Elapsed: 2.015541963s
Jan 18 18:23:54.217: INFO: Pod "execpod-affinityfswqm" satisfied condition "running"
Jan 18 18:23:55.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 18 18:23:55.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 18 18:23:55.421: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 18:23:55.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.64.55 80'
Jan 18 18:23:55.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.64.55 80\nConnection to 10.96.64.55 80 port [tcp/http] succeeded!\n"
Jan 18 18:23:55.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 18 18:23:55.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.64.55:80/ ; done'
Jan 18 18:23:55.925: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n"
Jan 18 18:23:55.925: INFO: stdout: "\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw"
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:55.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.64.55:80/ ; done'
Jan 18 18:23:56.190: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n"
Jan 18 18:23:56.190: INFO: stdout: "\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw"
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
Jan 18 18:23:56.190: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8885, will wait for the garbage collector to delete the pods 01/18/23 18:23:56.217
Jan 18 18:23:56.289: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.828855ms
Jan 18 18:23:56.389: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.348717ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 18 18:23:58.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8885" for this suite. 01/18/23 18:23:58.93
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":322,"skipped":5983,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.886 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:49.056
    Jan 18 18:23:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename services 01/18/23 18:23:49.057
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:49.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:49.084
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-8885 01/18/23 18:23:49.089
    STEP: creating service affinity-clusterip-transition in namespace services-8885 01/18/23 18:23:49.089
    STEP: creating replication controller affinity-clusterip-transition in namespace services-8885 01/18/23 18:23:49.112
    I0118 18:23:49.124169      21 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-8885, replica count: 3
    I0118 18:23:52.175418      21 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 18 18:23:52.191: INFO: Creating new exec pod
    Jan 18 18:23:52.202: INFO: Waiting up to 5m0s for pod "execpod-affinityfswqm" in namespace "services-8885" to be "running"
    Jan 18 18:23:52.209: INFO: Pod "execpod-affinityfswqm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.907029ms
    Jan 18 18:23:54.217: INFO: Pod "execpod-affinityfswqm": Phase="Running", Reason="", readiness=true. Elapsed: 2.015541963s
    Jan 18 18:23:54.217: INFO: Pod "execpod-affinityfswqm" satisfied condition "running"
    Jan 18 18:23:55.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 18 18:23:55.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 18 18:23:55.421: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 18:23:55.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.64.55 80'
    Jan 18 18:23:55.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.64.55 80\nConnection to 10.96.64.55 80 port [tcp/http] succeeded!\n"
    Jan 18 18:23:55.612: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 18 18:23:55.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.64.55:80/ ; done'
    Jan 18 18:23:55.925: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n"
    Jan 18 18:23:55.925: INFO: stdout: "\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-798z7\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-hvmct\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw"
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-798z7
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-hvmct
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.925: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:55.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=services-8885 exec execpod-affinityfswqm -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.96.64.55:80/ ; done'
    Jan 18 18:23:56.190: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.96.64.55:80/\n"
    Jan 18 18:23:56.190: INFO: stdout: "\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw\naffinity-clusterip-transition-gkthw"
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Received response from host: affinity-clusterip-transition-gkthw
    Jan 18 18:23:56.190: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-8885, will wait for the garbage collector to delete the pods 01/18/23 18:23:56.217
    Jan 18 18:23:56.289: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.828855ms
    Jan 18 18:23:56.389: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.348717ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 18 18:23:58.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8885" for this suite. 01/18/23 18:23:58.93
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:58.945
Jan 18 18:23:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:23:58.946
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:58.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:58.977
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/18/23 18:23:58.982
STEP: fetching the ConfigMap 01/18/23 18:23:58.991
STEP: patching the ConfigMap 01/18/23 18:23:58.997
STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 18:23:59.009
STEP: deleting the ConfigMap by collection with a label selector 01/18/23 18:23:59.016
STEP: listing all ConfigMaps in test namespace 01/18/23 18:23:59.033
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:23:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-789" for this suite. 01/18/23 18:23:59.046
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":323,"skipped":6026,"failed":0}
------------------------------
â€¢ [0.113 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:58.945
    Jan 18 18:23:58.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:23:58.946
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:58.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:58.977
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/18/23 18:23:58.982
    STEP: fetching the ConfigMap 01/18/23 18:23:58.991
    STEP: patching the ConfigMap 01/18/23 18:23:58.997
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/18/23 18:23:59.009
    STEP: deleting the ConfigMap by collection with a label selector 01/18/23 18:23:59.016
    STEP: listing all ConfigMaps in test namespace 01/18/23 18:23:59.033
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:23:59.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-789" for this suite. 01/18/23 18:23:59.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:23:59.064
Jan 18 18:23:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename downward-api 01/18/23 18:23:59.065
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:59.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:59.093
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/18/23 18:23:59.097
Jan 18 18:23:59.114: INFO: Waiting up to 5m0s for pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c" in namespace "downward-api-3004" to be "Succeeded or Failed"
Jan 18 18:23:59.121: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.361725ms
Jan 18 18:24:01.129: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014817973s
Jan 18 18:24:03.129: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015172023s
STEP: Saw pod success 01/18/23 18:24:03.129
Jan 18 18:24:03.130: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c" satisfied condition "Succeeded or Failed"
Jan 18 18:24:03.136: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c container dapi-container: <nil>
STEP: delete the pod 01/18/23 18:24:03.153
Jan 18 18:24:03.177: INFO: Waiting for pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c to disappear
Jan 18 18:24:03.183: INFO: Pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 18 18:24:03.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3004" for this suite. 01/18/23 18:24:03.192
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":324,"skipped":6050,"failed":0}
------------------------------
â€¢ [4.139 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:23:59.064
    Jan 18 18:23:59.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename downward-api 01/18/23 18:23:59.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:23:59.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:23:59.093
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/18/23 18:23:59.097
    Jan 18 18:23:59.114: INFO: Waiting up to 5m0s for pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c" in namespace "downward-api-3004" to be "Succeeded or Failed"
    Jan 18 18:23:59.121: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.361725ms
    Jan 18 18:24:01.129: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014817973s
    Jan 18 18:24:03.129: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015172023s
    STEP: Saw pod success 01/18/23 18:24:03.129
    Jan 18 18:24:03.130: INFO: Pod "downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c" satisfied condition "Succeeded or Failed"
    Jan 18 18:24:03.136: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c container dapi-container: <nil>
    STEP: delete the pod 01/18/23 18:24:03.153
    Jan 18 18:24:03.177: INFO: Waiting for pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c to disappear
    Jan 18 18:24:03.183: INFO: Pod downward-api-db0426b9-0076-438d-97b4-9cbcb9000b5c no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 18 18:24:03.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3004" for this suite. 01/18/23 18:24:03.192
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:03.204
Jan 18 18:24:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 18:24:03.205
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:03.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:03.233
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 18 18:24:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:24:04.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-277" for this suite. 01/18/23 18:24:04.284
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":325,"skipped":6054,"failed":0}
------------------------------
â€¢ [1.090 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:03.204
    Jan 18 18:24:03.204: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename custom-resource-definition 01/18/23 18:24:03.205
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:03.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:03.233
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 18 18:24:03.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:24:04.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-277" for this suite. 01/18/23 18:24:04.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:04.295
Jan 18 18:24:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 18:24:04.295
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:04.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:04.319
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/18/23 18:24:04.324
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-714;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-714;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +notcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_tcp@PTR;sleep 1; done
 01/18/23 18:24:04.371
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-714;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-714;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +notcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_tcp@PTR;sleep 1; done
 01/18/23 18:24:04.371
STEP: creating a pod to probe DNS 01/18/23 18:24:04.371
STEP: submitting the pod to kubernetes 01/18/23 18:24:04.372
Jan 18 18:24:04.387: INFO: Waiting up to 15m0s for pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a" in namespace "dns-714" to be "running"
Jan 18 18:24:04.392: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.954485ms
Jan 18 18:24:06.401: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014549969s
Jan 18 18:24:06.402: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a" satisfied condition "running"
STEP: retrieving the pod 01/18/23 18:24:06.402
STEP: looking for the results for each expected name from probers 01/18/23 18:24:06.41
Jan 18 18:24:06.424: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.433: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.469: INFO: Unable to read wheezy_tcp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.477: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.563: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.571: INFO: Unable to read jessie_udp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.588: INFO: Unable to read jessie_udp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.596: INFO: Unable to read jessie_tcp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.602: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
Jan 18 18:24:06.641: INFO: Lookups using dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-714 wheezy_tcp@dns-test-service.dns-714 wheezy_udp@dns-test-service.dns-714.svc wheezy_tcp@dns-test-service.dns-714.svc wheezy_udp@_http._tcp.dns-test-service.dns-714.svc wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-714 jessie_tcp@dns-test-service.dns-714 jessie_udp@dns-test-service.dns-714.svc jessie_tcp@dns-test-service.dns-714.svc jessie_udp@_http._tcp.dns-test-service.dns-714.svc jessie_tcp@_http._tcp.dns-test-service.dns-714.svc]

Jan 18 18:24:11.855: INFO: DNS probes using dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a succeeded

STEP: deleting the pod 01/18/23 18:24:11.855
STEP: deleting the test service 01/18/23 18:24:11.877
STEP: deleting the test headless service 01/18/23 18:24:11.912
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 18:24:11.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-714" for this suite. 01/18/23 18:24:11.98
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":326,"skipped":6060,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.698 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:04.295
    Jan 18 18:24:04.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 18:24:04.295
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:04.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:04.319
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/18/23 18:24:04.324
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-714;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-714;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +notcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_tcp@PTR;sleep 1; done
     01/18/23 18:24:04.371
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-714;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-714;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-714.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-714.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-714.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-714.svc;check="$$(dig +notcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_udp@PTR;check="$$(dig +tcp +noall +answer +search 232.121.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.121.232_tcp@PTR;sleep 1; done
     01/18/23 18:24:04.371
    STEP: creating a pod to probe DNS 01/18/23 18:24:04.371
    STEP: submitting the pod to kubernetes 01/18/23 18:24:04.372
    Jan 18 18:24:04.387: INFO: Waiting up to 15m0s for pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a" in namespace "dns-714" to be "running"
    Jan 18 18:24:04.392: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.954485ms
    Jan 18 18:24:06.401: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a": Phase="Running", Reason="", readiness=true. Elapsed: 2.014549969s
    Jan 18 18:24:06.402: INFO: Pod "dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 18:24:06.402
    STEP: looking for the results for each expected name from probers 01/18/23 18:24:06.41
    Jan 18 18:24:06.424: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.433: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.442: INFO: Unable to read wheezy_udp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.450: INFO: Unable to read wheezy_tcp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.461: INFO: Unable to read wheezy_udp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.469: INFO: Unable to read wheezy_tcp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.477: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.486: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.563: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.571: INFO: Unable to read jessie_udp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-714 from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.588: INFO: Unable to read jessie_udp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.596: INFO: Unable to read jessie_tcp@dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.602: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.611: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-714.svc from pod dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a: the server could not find the requested resource (get pods dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a)
    Jan 18 18:24:06.641: INFO: Lookups using dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-714 wheezy_tcp@dns-test-service.dns-714 wheezy_udp@dns-test-service.dns-714.svc wheezy_tcp@dns-test-service.dns-714.svc wheezy_udp@_http._tcp.dns-test-service.dns-714.svc wheezy_tcp@_http._tcp.dns-test-service.dns-714.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-714 jessie_tcp@dns-test-service.dns-714 jessie_udp@dns-test-service.dns-714.svc jessie_tcp@dns-test-service.dns-714.svc jessie_udp@_http._tcp.dns-test-service.dns-714.svc jessie_tcp@_http._tcp.dns-test-service.dns-714.svc]

    Jan 18 18:24:11.855: INFO: DNS probes using dns-714/dns-test-88a7f48a-7aaa-4846-b61f-3939d46c108a succeeded

    STEP: deleting the pod 01/18/23 18:24:11.855
    STEP: deleting the test service 01/18/23 18:24:11.877
    STEP: deleting the test headless service 01/18/23 18:24:11.912
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 18:24:11.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-714" for this suite. 01/18/23 18:24:11.98
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:11.993
Jan 18 18:24:11.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:24:11.994
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:12.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:12.021
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2437/configmap-test-ff88fde1-6cb5-4d78-9faf-e8c62d7229fd 01/18/23 18:24:12.025
STEP: Creating a pod to test consume configMaps 01/18/23 18:24:12.033
Jan 18 18:24:12.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722" in namespace "configmap-2437" to be "Succeeded or Failed"
Jan 18 18:24:12.057: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760081ms
Jan 18 18:24:14.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017821738s
Jan 18 18:24:16.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017604688s
STEP: Saw pod success 01/18/23 18:24:16.066
Jan 18 18:24:16.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722" satisfied condition "Succeeded or Failed"
Jan 18 18:24:16.073: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 container env-test: <nil>
STEP: delete the pod 01/18/23 18:24:16.089
Jan 18 18:24:16.111: INFO: Waiting for pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 to disappear
Jan 18 18:24:16.119: INFO: Pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:24:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2437" for this suite. 01/18/23 18:24:16.127
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":327,"skipped":6064,"failed":0}
------------------------------
â€¢ [4.147 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:11.993
    Jan 18 18:24:11.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:24:11.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:12.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:12.021
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2437/configmap-test-ff88fde1-6cb5-4d78-9faf-e8c62d7229fd 01/18/23 18:24:12.025
    STEP: Creating a pod to test consume configMaps 01/18/23 18:24:12.033
    Jan 18 18:24:12.048: INFO: Waiting up to 5m0s for pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722" in namespace "configmap-2437" to be "Succeeded or Failed"
    Jan 18 18:24:12.057: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Pending", Reason="", readiness=false. Elapsed: 8.760081ms
    Jan 18 18:24:14.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017821738s
    Jan 18 18:24:16.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017604688s
    STEP: Saw pod success 01/18/23 18:24:16.066
    Jan 18 18:24:16.066: INFO: Pod "pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722" satisfied condition "Succeeded or Failed"
    Jan 18 18:24:16.073: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 container env-test: <nil>
    STEP: delete the pod 01/18/23 18:24:16.089
    Jan 18 18:24:16.111: INFO: Waiting for pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 to disappear
    Jan 18 18:24:16.119: INFO: Pod pod-configmaps-d0d2ad0e-7546-4424-bcc8-9d06ea0d2722 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:24:16.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2437" for this suite. 01/18/23 18:24:16.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:16.141
Jan 18 18:24:16.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 18:24:16.142
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:16.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:16.171
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-52ce8adb-d6df-4d74-b9f8-839d6198a3cc 01/18/23 18:24:16.176
STEP: Creating a pod to test consume secrets 01/18/23 18:24:16.184
Jan 18 18:24:16.197: INFO: Waiting up to 5m0s for pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e" in namespace "secrets-7361" to be "Succeeded or Failed"
Jan 18 18:24:16.204: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.888108ms
Jan 18 18:24:18.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014888836s
Jan 18 18:24:20.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015181667s
STEP: Saw pod success 01/18/23 18:24:20.212
Jan 18 18:24:20.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e" satisfied condition "Succeeded or Failed"
Jan 18 18:24:20.219: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e container secret-volume-test: <nil>
STEP: delete the pod 01/18/23 18:24:20.238
Jan 18 18:24:20.257: INFO: Waiting for pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e to disappear
Jan 18 18:24:20.263: INFO: Pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 18:24:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7361" for this suite. 01/18/23 18:24:20.271
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":328,"skipped":6071,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:16.141
    Jan 18 18:24:16.141: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 18:24:16.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:16.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:16.171
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-52ce8adb-d6df-4d74-b9f8-839d6198a3cc 01/18/23 18:24:16.176
    STEP: Creating a pod to test consume secrets 01/18/23 18:24:16.184
    Jan 18 18:24:16.197: INFO: Waiting up to 5m0s for pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e" in namespace "secrets-7361" to be "Succeeded or Failed"
    Jan 18 18:24:16.204: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.888108ms
    Jan 18 18:24:18.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014888836s
    Jan 18 18:24:20.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015181667s
    STEP: Saw pod success 01/18/23 18:24:20.212
    Jan 18 18:24:20.212: INFO: Pod "pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e" satisfied condition "Succeeded or Failed"
    Jan 18 18:24:20.219: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e container secret-volume-test: <nil>
    STEP: delete the pod 01/18/23 18:24:20.238
    Jan 18 18:24:20.257: INFO: Waiting for pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e to disappear
    Jan 18 18:24:20.263: INFO: Pod pod-secrets-1e94a0cb-5f53-4210-9a54-da0a36fdfc8e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 18:24:20.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7361" for this suite. 01/18/23 18:24:20.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:20.285
Jan 18 18:24:20.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename podtemplate 01/18/23 18:24:20.286
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:20.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:20.316
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 18 18:24:20.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8365" for this suite. 01/18/23 18:24:20.387
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":329,"skipped":6078,"failed":0}
------------------------------
â€¢ [0.115 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:20.285
    Jan 18 18:24:20.285: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename podtemplate 01/18/23 18:24:20.286
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:20.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:20.316
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 18 18:24:20.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8365" for this suite. 01/18/23 18:24:20.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:20.403
Jan 18 18:24:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:24:20.404
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:20.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:20.432
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 18 18:24:20.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 18:24:24.207
Jan 18 18:24:24.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
Jan 18 18:24:24.887: INFO: stderr: ""
Jan 18 18:24:24.887: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 18:24:24.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-8087-crds test-foo'
Jan 18 18:24:24.996: INFO: stderr: ""
Jan 18 18:24:24.996: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 18 18:24:24.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
Jan 18 18:24:25.259: INFO: stderr: ""
Jan 18 18:24:25.259: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 18 18:24:25.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-8087-crds test-foo'
Jan 18 18:24:25.357: INFO: stderr: ""
Jan 18 18:24:25.357: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 18:24:25.357
Jan 18 18:24:25.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
Jan 18 18:24:25.667: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 18:24:25.667
Jan 18 18:24:25.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
Jan 18 18:24:26.064: INFO: rc: 1
Jan 18 18:24:26.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
Jan 18 18:24:26.355: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 18:24:26.355
Jan 18 18:24:26.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
Jan 18 18:24:26.612: INFO: rc: 1
Jan 18 18:24:26.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
Jan 18 18:24:26.869: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/18/23 18:24:26.869
Jan 18 18:24:26.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds'
Jan 18 18:24:27.116: INFO: stderr: ""
Jan 18 18:24:27.116: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/18/23 18:24:27.116
Jan 18 18:24:27.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.metadata'
Jan 18 18:24:27.351: INFO: stderr: ""
Jan 18 18:24:27.351: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 18 18:24:27.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec'
Jan 18 18:24:27.546: INFO: stderr: ""
Jan 18 18:24:27.547: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 18 18:24:27.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec.bars'
Jan 18 18:24:27.780: INFO: stderr: ""
Jan 18 18:24:27.780: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 18:24:27.78
Jan 18 18:24:27.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec.bars2'
Jan 18 18:24:28.004: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:24:30.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-375" for this suite. 01/18/23 18:24:30.666
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":330,"skipped":6113,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.279 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:20.403
    Jan 18 18:24:20.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename crd-publish-openapi 01/18/23 18:24:20.404
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:20.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:20.432
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 18 18:24:20.437: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/18/23 18:24:24.207
    Jan 18 18:24:24.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
    Jan 18 18:24:24.887: INFO: stderr: ""
    Jan 18 18:24:24.887: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 18:24:24.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-8087-crds test-foo'
    Jan 18 18:24:24.996: INFO: stderr: ""
    Jan 18 18:24:24.996: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 18 18:24:24.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
    Jan 18 18:24:25.259: INFO: stderr: ""
    Jan 18 18:24:25.259: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 18 18:24:25.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 delete e2e-test-crd-publish-openapi-8087-crds test-foo'
    Jan 18 18:24:25.357: INFO: stderr: ""
    Jan 18 18:24:25.357: INFO: stdout: "e2e-test-crd-publish-openapi-8087-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/18/23 18:24:25.357
    Jan 18 18:24:25.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
    Jan 18 18:24:25.667: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/18/23 18:24:25.667
    Jan 18 18:24:25.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
    Jan 18 18:24:26.064: INFO: rc: 1
    Jan 18 18:24:26.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
    Jan 18 18:24:26.355: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/18/23 18:24:26.355
    Jan 18 18:24:26.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 create -f -'
    Jan 18 18:24:26.612: INFO: rc: 1
    Jan 18 18:24:26.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 --namespace=crd-publish-openapi-375 apply -f -'
    Jan 18 18:24:26.869: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/18/23 18:24:26.869
    Jan 18 18:24:26.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds'
    Jan 18 18:24:27.116: INFO: stderr: ""
    Jan 18 18:24:27.116: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/18/23 18:24:27.116
    Jan 18 18:24:27.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.metadata'
    Jan 18 18:24:27.351: INFO: stderr: ""
    Jan 18 18:24:27.351: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 18 18:24:27.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec'
    Jan 18 18:24:27.546: INFO: stderr: ""
    Jan 18 18:24:27.547: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 18 18:24:27.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec.bars'
    Jan 18 18:24:27.780: INFO: stderr: ""
    Jan 18 18:24:27.780: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8087-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/18/23 18:24:27.78
    Jan 18 18:24:27.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1714167000 --namespace=crd-publish-openapi-375 explain e2e-test-crd-publish-openapi-8087-crds.spec.bars2'
    Jan 18 18:24:28.004: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:24:30.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-375" for this suite. 01/18/23 18:24:30.666
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:30.682
Jan 18 18:24:30.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename kubelet-test 01/18/23 18:24:30.684
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:30.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:30.71
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 18 18:24:34.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7108" for this suite. 01/18/23 18:24:34.757
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":331,"skipped":6117,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:30.682
    Jan 18 18:24:30.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename kubelet-test 01/18/23 18:24:30.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:30.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:30.71
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 18 18:24:34.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7108" for this suite. 01/18/23 18:24:34.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:24:34.776
Jan 18 18:24:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-probe 01/18/23 18:24:34.777
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:34.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:34.808
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 18 18:25:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4230" for this suite. 01/18/23 18:25:34.845
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":332,"skipped":6150,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.082 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:24:34.776
    Jan 18 18:24:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-probe 01/18/23 18:24:34.777
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:24:34.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:24:34.808
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 18 18:25:34.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4230" for this suite. 01/18/23 18:25:34.845
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:34.858
Jan 18 18:25:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename job 01/18/23 18:25:34.86
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:34.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:34.894
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/18/23 18:25:34.899
STEP: Ensuring active pods == parallelism 01/18/23 18:25:34.91
STEP: Orphaning one of the Job's Pods 01/18/23 18:25:36.921
Jan 18 18:25:37.453: INFO: Successfully updated pod "adopt-release-642nf"
STEP: Checking that the Job readopts the Pod 01/18/23 18:25:37.453
Jan 18 18:25:37.454: INFO: Waiting up to 15m0s for pod "adopt-release-642nf" in namespace "job-8644" to be "adopted"
Jan 18 18:25:37.460: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 6.356775ms
Jan 18 18:25:39.468: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014640537s
Jan 18 18:25:39.468: INFO: Pod "adopt-release-642nf" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/18/23 18:25:39.468
Jan 18 18:25:39.989: INFO: Successfully updated pod "adopt-release-642nf"
STEP: Checking that the Job releases the Pod 01/18/23 18:25:39.989
Jan 18 18:25:39.989: INFO: Waiting up to 15m0s for pod "adopt-release-642nf" in namespace "job-8644" to be "released"
Jan 18 18:25:39.995: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 5.142836ms
Jan 18 18:25:42.003: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013464246s
Jan 18 18:25:42.003: INFO: Pod "adopt-release-642nf" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 18 18:25:42.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8644" for this suite. 01/18/23 18:25:42.013
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":333,"skipped":6152,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.169 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:34.858
    Jan 18 18:25:34.859: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename job 01/18/23 18:25:34.86
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:34.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:34.894
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/18/23 18:25:34.899
    STEP: Ensuring active pods == parallelism 01/18/23 18:25:34.91
    STEP: Orphaning one of the Job's Pods 01/18/23 18:25:36.921
    Jan 18 18:25:37.453: INFO: Successfully updated pod "adopt-release-642nf"
    STEP: Checking that the Job readopts the Pod 01/18/23 18:25:37.453
    Jan 18 18:25:37.454: INFO: Waiting up to 15m0s for pod "adopt-release-642nf" in namespace "job-8644" to be "adopted"
    Jan 18 18:25:37.460: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 6.356775ms
    Jan 18 18:25:39.468: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.014640537s
    Jan 18 18:25:39.468: INFO: Pod "adopt-release-642nf" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/18/23 18:25:39.468
    Jan 18 18:25:39.989: INFO: Successfully updated pod "adopt-release-642nf"
    STEP: Checking that the Job releases the Pod 01/18/23 18:25:39.989
    Jan 18 18:25:39.989: INFO: Waiting up to 15m0s for pod "adopt-release-642nf" in namespace "job-8644" to be "released"
    Jan 18 18:25:39.995: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 5.142836ms
    Jan 18 18:25:42.003: INFO: Pod "adopt-release-642nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013464246s
    Jan 18 18:25:42.003: INFO: Pod "adopt-release-642nf" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 18 18:25:42.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8644" for this suite. 01/18/23 18:25:42.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:42.029
Jan 18 18:25:42.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename runtimeclass 01/18/23 18:25:42.03
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:42.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:42.057
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 18 18:25:42.084: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7437 to be scheduled
Jan 18 18:25:42.090: INFO: 1 pods are not scheduled: [runtimeclass-7437/test-runtimeclass-runtimeclass-7437-preconfigured-handler-j7vd2(2e84300f-c00f-4e24-a773-db7d785c01ca)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 18 18:25:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7437" for this suite. 01/18/23 18:25:44.124
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":334,"skipped":6167,"failed":0}
------------------------------
â€¢ [2.106 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:42.029
    Jan 18 18:25:42.029: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename runtimeclass 01/18/23 18:25:42.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:42.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:42.057
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 18 18:25:42.084: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7437 to be scheduled
    Jan 18 18:25:42.090: INFO: 1 pods are not scheduled: [runtimeclass-7437/test-runtimeclass-runtimeclass-7437-preconfigured-handler-j7vd2(2e84300f-c00f-4e24-a773-db7d785c01ca)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 18 18:25:44.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7437" for this suite. 01/18/23 18:25:44.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:44.137
Jan 18 18:25:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 18:25:44.138
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:44.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:44.167
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 18 18:25:44.193: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 18 18:25:49.201: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 18:25:49.201
STEP: Scaling up "test-rs" replicaset  01/18/23 18:25:49.202
Jan 18 18:25:49.219: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/18/23 18:25:49.219
W0118 18:25:49.231636      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 18 18:25:49.234: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 18:25:49.259: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 18:25:49.292: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 18:25:49.304: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
Jan 18 18:25:50.212: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 2, AvailableReplicas 2
Jan 18 18:25:51.190: INFO: observed Replicaset test-rs in namespace replicaset-8729 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 18:25:51.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8729" for this suite. 01/18/23 18:25:51.199
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":335,"skipped":6204,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:44.137
    Jan 18 18:25:44.137: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 18:25:44.138
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:44.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:44.167
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 18 18:25:44.193: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 18 18:25:49.201: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 18:25:49.201
    STEP: Scaling up "test-rs" replicaset  01/18/23 18:25:49.202
    Jan 18 18:25:49.219: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/18/23 18:25:49.219
    W0118 18:25:49.231636      21 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 18 18:25:49.234: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 18:25:49.259: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 18:25:49.292: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 18:25:49.304: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 1, AvailableReplicas 1
    Jan 18 18:25:50.212: INFO: observed ReplicaSet test-rs in namespace replicaset-8729 with ReadyReplicas 2, AvailableReplicas 2
    Jan 18 18:25:51.190: INFO: observed Replicaset test-rs in namespace replicaset-8729 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 18:25:51.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8729" for this suite. 01/18/23 18:25:51.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:51.212
Jan 18 18:25:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:25:51.213
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:51.239
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:51.245
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-26130321-8cf6-4f04-8aad-6e33b23e814c 01/18/23 18:25:51.256
STEP: Creating configMap with name cm-test-opt-upd-8838cc86-082b-483f-b5e0-a36a1f47ee3f 01/18/23 18:25:51.264
STEP: Creating the pod 01/18/23 18:25:51.275
Jan 18 18:25:51.293: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463" in namespace "projected-9895" to be "running and ready"
Jan 18 18:25:51.299: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545587ms
Jan 18 18:25:51.299: INFO: The phase of Pod pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463 is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:25:53.307: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463": Phase="Running", Reason="", readiness=true. Elapsed: 2.014610073s
Jan 18 18:25:53.307: INFO: The phase of Pod pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463 is Running (Ready = true)
Jan 18 18:25:53.307: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-26130321-8cf6-4f04-8aad-6e33b23e814c 01/18/23 18:25:53.373
STEP: Updating configmap cm-test-opt-upd-8838cc86-082b-483f-b5e0-a36a1f47ee3f 01/18/23 18:25:53.386
STEP: Creating configMap with name cm-test-opt-create-34a57330-9495-4b16-806b-d800edc0f5f6 01/18/23 18:25:53.394
STEP: waiting to observe update in volume 01/18/23 18:25:53.405
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 18:25:57.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9895" for this suite. 01/18/23 18:25:57.495
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":336,"skipped":6209,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.295 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:51.212
    Jan 18 18:25:51.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:25:51.213
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:51.239
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:51.245
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-26130321-8cf6-4f04-8aad-6e33b23e814c 01/18/23 18:25:51.256
    STEP: Creating configMap with name cm-test-opt-upd-8838cc86-082b-483f-b5e0-a36a1f47ee3f 01/18/23 18:25:51.264
    STEP: Creating the pod 01/18/23 18:25:51.275
    Jan 18 18:25:51.293: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463" in namespace "projected-9895" to be "running and ready"
    Jan 18 18:25:51.299: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545587ms
    Jan 18 18:25:51.299: INFO: The phase of Pod pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463 is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:25:53.307: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463": Phase="Running", Reason="", readiness=true. Elapsed: 2.014610073s
    Jan 18 18:25:53.307: INFO: The phase of Pod pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463 is Running (Ready = true)
    Jan 18 18:25:53.307: INFO: Pod "pod-projected-configmaps-b783e0c7-7154-4486-82e2-242299d80463" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-26130321-8cf6-4f04-8aad-6e33b23e814c 01/18/23 18:25:53.373
    STEP: Updating configmap cm-test-opt-upd-8838cc86-082b-483f-b5e0-a36a1f47ee3f 01/18/23 18:25:53.386
    STEP: Creating configMap with name cm-test-opt-create-34a57330-9495-4b16-806b-d800edc0f5f6 01/18/23 18:25:53.394
    STEP: waiting to observe update in volume 01/18/23 18:25:53.405
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 18:25:57.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9895" for this suite. 01/18/23 18:25:57.495
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:57.511
Jan 18 18:25:57.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename lease-test 01/18/23 18:25:57.513
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:57.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:57.538
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 18 18:25:57.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4945" for this suite. 01/18/23 18:25:57.659
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":337,"skipped":6240,"failed":0}
------------------------------
â€¢ [0.160 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:57.511
    Jan 18 18:25:57.512: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename lease-test 01/18/23 18:25:57.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:57.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:57.538
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 18 18:25:57.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-4945" for this suite. 01/18/23 18:25:57.659
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:25:57.672
Jan 18 18:25:57.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:25:57.673
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:57.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:57.703
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-bb58479d-91b1-4cc3-b902-4c73aaf40f46 01/18/23 18:25:57.715
STEP: Creating secret with name s-test-opt-upd-82379697-56d7-4606-a6a4-84a2c8e08ae6 01/18/23 18:25:57.724
STEP: Creating the pod 01/18/23 18:25:57.733
Jan 18 18:25:57.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad" in namespace "projected-2345" to be "running and ready"
Jan 18 18:25:57.754: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062195ms
Jan 18 18:25:57.754: INFO: The phase of Pod pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:25:59.762: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.012801125s
Jan 18 18:25:59.762: INFO: The phase of Pod pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad is Running (Ready = true)
Jan 18 18:25:59.762: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-bb58479d-91b1-4cc3-b902-4c73aaf40f46 01/18/23 18:25:59.838
STEP: Updating secret s-test-opt-upd-82379697-56d7-4606-a6a4-84a2c8e08ae6 01/18/23 18:25:59.851
STEP: Creating secret with name s-test-opt-create-71fd7012-b518-4cd6-8e5f-6a6c4a7a6eb2 01/18/23 18:25:59.862
STEP: waiting to observe update in volume 01/18/23 18:25:59.872
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 18 18:26:01.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2345" for this suite. 01/18/23 18:26:01.94
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":338,"skipped":6256,"failed":0}
------------------------------
â€¢ [4.283 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:25:57.672
    Jan 18 18:25:57.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:25:57.673
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:25:57.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:25:57.703
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-bb58479d-91b1-4cc3-b902-4c73aaf40f46 01/18/23 18:25:57.715
    STEP: Creating secret with name s-test-opt-upd-82379697-56d7-4606-a6a4-84a2c8e08ae6 01/18/23 18:25:57.724
    STEP: Creating the pod 01/18/23 18:25:57.733
    Jan 18 18:25:57.749: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad" in namespace "projected-2345" to be "running and ready"
    Jan 18 18:25:57.754: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062195ms
    Jan 18 18:25:57.754: INFO: The phase of Pod pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:25:59.762: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.012801125s
    Jan 18 18:25:59.762: INFO: The phase of Pod pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad is Running (Ready = true)
    Jan 18 18:25:59.762: INFO: Pod "pod-projected-secrets-8511796b-3488-4f7f-b7ce-e0fda81ba4ad" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-bb58479d-91b1-4cc3-b902-4c73aaf40f46 01/18/23 18:25:59.838
    STEP: Updating secret s-test-opt-upd-82379697-56d7-4606-a6a4-84a2c8e08ae6 01/18/23 18:25:59.851
    STEP: Creating secret with name s-test-opt-create-71fd7012-b518-4cd6-8e5f-6a6c4a7a6eb2 01/18/23 18:25:59.862
    STEP: waiting to observe update in volume 01/18/23 18:25:59.872
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 18 18:26:01.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2345" for this suite. 01/18/23 18:26:01.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:01.956
Jan 18 18:26:01.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 18:26:01.958
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:01.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:01.988
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 18:26:02.017
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:26:02.706
STEP: Deploying the webhook pod 01/18/23 18:26:02.72
STEP: Wait for the deployment to be ready 01/18/23 18:26:02.737
Jan 18 18:26:02.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 18 18:26:04.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/18/23 18:26:06.781
STEP: Verifying the service has paired with the endpoint 01/18/23 18:26:06.8
Jan 18 18:26:07.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 18:26:07.808
STEP: create a pod that should be updated by the webhook 01/18/23 18:26:07.836
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:26:07.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8086" for this suite. 01/18/23 18:26:07.886
STEP: Destroying namespace "webhook-8086-markers" for this suite. 01/18/23 18:26:07.896
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":339,"skipped":6266,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.021 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:01.956
    Jan 18 18:26:01.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 18:26:01.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:01.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:01.988
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 18:26:02.017
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:26:02.706
    STEP: Deploying the webhook pod 01/18/23 18:26:02.72
    STEP: Wait for the deployment to be ready 01/18/23 18:26:02.737
    Jan 18 18:26:02.754: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 18 18:26:04.773: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 26, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/18/23 18:26:06.781
    STEP: Verifying the service has paired with the endpoint 01/18/23 18:26:06.8
    Jan 18 18:26:07.801: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/18/23 18:26:07.808
    STEP: create a pod that should be updated by the webhook 01/18/23 18:26:07.836
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:26:07.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8086" for this suite. 01/18/23 18:26:07.886
    STEP: Destroying namespace "webhook-8086-markers" for this suite. 01/18/23 18:26:07.896
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:07.979
Jan 18 18:26:07.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replicaset 01/18/23 18:26:07.981
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:08.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:08.008
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/18/23 18:26:08.012
STEP: Verify that the required pods have come up 01/18/23 18:26:08.021
Jan 18 18:26:08.028: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 18 18:26:13.036: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/18/23 18:26:13.036
Jan 18 18:26:13.041: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/18/23 18:26:13.041
STEP: DeleteCollection of the ReplicaSets 01/18/23 18:26:13.047
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 18:26:13.063
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 18 18:26:13.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4994" for this suite. 01/18/23 18:26:13.079
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":340,"skipped":6289,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.115 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:07.979
    Jan 18 18:26:07.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replicaset 01/18/23 18:26:07.981
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:08.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:08.008
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/18/23 18:26:08.012
    STEP: Verify that the required pods have come up 01/18/23 18:26:08.021
    Jan 18 18:26:08.028: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 18 18:26:13.036: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/18/23 18:26:13.036
    Jan 18 18:26:13.041: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/18/23 18:26:13.041
    STEP: DeleteCollection of the ReplicaSets 01/18/23 18:26:13.047
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/18/23 18:26:13.063
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 18 18:26:13.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4994" for this suite. 01/18/23 18:26:13.079
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:13.107
Jan 18 18:26:13.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:26:13.108
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:13.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:13.185
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/18/23 18:26:13.189
Jan 18 18:26:13.203: INFO: Waiting up to 5m0s for pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b" in namespace "projected-1613" to be "running and ready"
Jan 18 18:26:13.277: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 73.158454ms
Jan 18 18:26:13.277: INFO: The phase of Pod annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:26:15.284: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.080488564s
Jan 18 18:26:15.284: INFO: The phase of Pod annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b is Running (Ready = true)
Jan 18 18:26:15.284: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b" satisfied condition "running and ready"
Jan 18 18:26:15.829: INFO: Successfully updated pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 18 18:26:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1613" for this suite. 01/18/23 18:26:19.888
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":341,"skipped":6292,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.796 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:13.107
    Jan 18 18:26:13.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:26:13.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:13.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:13.185
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/18/23 18:26:13.189
    Jan 18 18:26:13.203: INFO: Waiting up to 5m0s for pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b" in namespace "projected-1613" to be "running and ready"
    Jan 18 18:26:13.277: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 73.158454ms
    Jan 18 18:26:13.277: INFO: The phase of Pod annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:26:15.284: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b": Phase="Running", Reason="", readiness=true. Elapsed: 2.080488564s
    Jan 18 18:26:15.284: INFO: The phase of Pod annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b is Running (Ready = true)
    Jan 18 18:26:15.284: INFO: Pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b" satisfied condition "running and ready"
    Jan 18 18:26:15.829: INFO: Successfully updated pod "annotationupdatea0358858-b7b3-44da-87bd-3e4f360c2e4b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 18 18:26:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1613" for this suite. 01/18/23 18:26:19.888
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:19.905
Jan 18 18:26:19.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename gc 01/18/23 18:26:19.906
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:19.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:19.941
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/18/23 18:26:19.954
STEP: create the rc2 01/18/23 18:26:19.963
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 18:26:24.98
STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 18:26:27.388
STEP: wait for the rc to be deleted 01/18/23 18:26:27.459
Jan 18 18:26:32.487: INFO: 73 pods remaining
Jan 18 18:26:32.487: INFO: 73 pods has nil DeletionTimestamp
Jan 18 18:26:32.487: INFO: 
STEP: Gathering metrics 01/18/23 18:26:37.485
W0118 18:26:37.498190      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 18 18:26:37.498: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 18 18:26:37.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-24wx5" in namespace "gc-3363"
Jan 18 18:26:37.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-25v4w" in namespace "gc-3363"
Jan 18 18:26:37.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-28b9z" in namespace "gc-3363"
Jan 18 18:26:37.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hxjf" in namespace "gc-3363"
Jan 18 18:26:37.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g8s7" in namespace "gc-3363"
Jan 18 18:26:37.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mkb6" in namespace "gc-3363"
Jan 18 18:26:37.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-56kp6" in namespace "gc-3363"
Jan 18 18:26:37.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fs5s" in namespace "gc-3363"
Jan 18 18:26:37.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-5krql" in namespace "gc-3363"
Jan 18 18:26:37.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n8mj" in namespace "gc-3363"
Jan 18 18:26:37.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p49r" in namespace "gc-3363"
Jan 18 18:26:37.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pzkp" in namespace "gc-3363"
Jan 18 18:26:37.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-5shxh" in namespace "gc-3363"
Jan 18 18:26:37.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mdcd" in namespace "gc-3363"
Jan 18 18:26:38.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mdqt" in namespace "gc-3363"
Jan 18 18:26:38.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wjjs" in namespace "gc-3363"
Jan 18 18:26:38.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-787sm" in namespace "gc-3363"
Jan 18 18:26:38.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vbv6" in namespace "gc-3363"
Jan 18 18:26:38.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-86b4g" in namespace "gc-3363"
Jan 18 18:26:38.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wn2r" in namespace "gc-3363"
Jan 18 18:26:38.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zhrd" in namespace "gc-3363"
Jan 18 18:26:38.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-98b5f" in namespace "gc-3363"
Jan 18 18:26:38.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lchd" in namespace "gc-3363"
Jan 18 18:26:38.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rwwp" in namespace "gc-3363"
Jan 18 18:26:38.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7dft" in namespace "gc-3363"
Jan 18 18:26:38.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9cgs" in namespace "gc-3363"
Jan 18 18:26:38.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfp7v" in namespace "gc-3363"
Jan 18 18:26:38.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpsss" in namespace "gc-3363"
Jan 18 18:26:38.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-brkf4" in namespace "gc-3363"
Jan 18 18:26:38.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx5gc" in namespace "gc-3363"
Jan 18 18:26:38.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-c229d" in namespace "gc-3363"
Jan 18 18:26:38.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckph2" in namespace "gc-3363"
Jan 18 18:26:38.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxxg4" in namespace "gc-3363"
Jan 18 18:26:38.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhfnz" in namespace "gc-3363"
Jan 18 18:26:38.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhw7p" in namespace "gc-3363"
Jan 18 18:26:39.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-djbkt" in namespace "gc-3363"
Jan 18 18:26:39.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlzwf" in namespace "gc-3363"
Jan 18 18:26:39.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5rrn" in namespace "gc-3363"
Jan 18 18:26:39.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdt7b" in namespace "gc-3363"
Jan 18 18:26:39.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhvl9" in namespace "gc-3363"
Jan 18 18:26:39.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs7sj" in namespace "gc-3363"
Jan 18 18:26:39.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft2nj" in namespace "gc-3363"
Jan 18 18:26:39.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-grb9w" in namespace "gc-3363"
Jan 18 18:26:39.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc2g7" in namespace "gc-3363"
Jan 18 18:26:39.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmq9h" in namespace "gc-3363"
Jan 18 18:26:40.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp22s" in namespace "gc-3363"
Jan 18 18:26:40.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-j54gh" in namespace "gc-3363"
Jan 18 18:26:40.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9jhz" in namespace "gc-3363"
Jan 18 18:26:40.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdxn6" in namespace "gc-3363"
Jan 18 18:26:40.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbf7k" in namespace "gc-3363"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 18 18:26:40.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3363" for this suite. 01/18/23 18:26:40.383
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":342,"skipped":6293,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.491 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:19.905
    Jan 18 18:26:19.905: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename gc 01/18/23 18:26:19.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:19.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:19.941
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/18/23 18:26:19.954
    STEP: create the rc2 01/18/23 18:26:19.963
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/18/23 18:26:24.98
    STEP: delete the rc simpletest-rc-to-be-deleted 01/18/23 18:26:27.388
    STEP: wait for the rc to be deleted 01/18/23 18:26:27.459
    Jan 18 18:26:32.487: INFO: 73 pods remaining
    Jan 18 18:26:32.487: INFO: 73 pods has nil DeletionTimestamp
    Jan 18 18:26:32.487: INFO: 
    STEP: Gathering metrics 01/18/23 18:26:37.485
    W0118 18:26:37.498190      21 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 18 18:26:37.498: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 18 18:26:37.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-24wx5" in namespace "gc-3363"
    Jan 18 18:26:37.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-25v4w" in namespace "gc-3363"
    Jan 18 18:26:37.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-28b9z" in namespace "gc-3363"
    Jan 18 18:26:37.595: INFO: Deleting pod "simpletest-rc-to-be-deleted-2hxjf" in namespace "gc-3363"
    Jan 18 18:26:37.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-4g8s7" in namespace "gc-3363"
    Jan 18 18:26:37.682: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mkb6" in namespace "gc-3363"
    Jan 18 18:26:37.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-56kp6" in namespace "gc-3363"
    Jan 18 18:26:37.759: INFO: Deleting pod "simpletest-rc-to-be-deleted-5fs5s" in namespace "gc-3363"
    Jan 18 18:26:37.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-5krql" in namespace "gc-3363"
    Jan 18 18:26:37.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n8mj" in namespace "gc-3363"
    Jan 18 18:26:37.882: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p49r" in namespace "gc-3363"
    Jan 18 18:26:37.899: INFO: Deleting pod "simpletest-rc-to-be-deleted-5pzkp" in namespace "gc-3363"
    Jan 18 18:26:37.970: INFO: Deleting pod "simpletest-rc-to-be-deleted-5shxh" in namespace "gc-3363"
    Jan 18 18:26:37.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mdcd" in namespace "gc-3363"
    Jan 18 18:26:38.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mdqt" in namespace "gc-3363"
    Jan 18 18:26:38.083: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wjjs" in namespace "gc-3363"
    Jan 18 18:26:38.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-787sm" in namespace "gc-3363"
    Jan 18 18:26:38.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-7vbv6" in namespace "gc-3363"
    Jan 18 18:26:38.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-86b4g" in namespace "gc-3363"
    Jan 18 18:26:38.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wn2r" in namespace "gc-3363"
    Jan 18 18:26:38.377: INFO: Deleting pod "simpletest-rc-to-be-deleted-8zhrd" in namespace "gc-3363"
    Jan 18 18:26:38.396: INFO: Deleting pod "simpletest-rc-to-be-deleted-98b5f" in namespace "gc-3363"
    Jan 18 18:26:38.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lchd" in namespace "gc-3363"
    Jan 18 18:26:38.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rwwp" in namespace "gc-3363"
    Jan 18 18:26:38.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7dft" in namespace "gc-3363"
    Jan 18 18:26:38.577: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9cgs" in namespace "gc-3363"
    Jan 18 18:26:38.596: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfp7v" in namespace "gc-3363"
    Jan 18 18:26:38.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-bpsss" in namespace "gc-3363"
    Jan 18 18:26:38.680: INFO: Deleting pod "simpletest-rc-to-be-deleted-brkf4" in namespace "gc-3363"
    Jan 18 18:26:38.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-bx5gc" in namespace "gc-3363"
    Jan 18 18:26:38.764: INFO: Deleting pod "simpletest-rc-to-be-deleted-c229d" in namespace "gc-3363"
    Jan 18 18:26:38.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckph2" in namespace "gc-3363"
    Jan 18 18:26:38.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxxg4" in namespace "gc-3363"
    Jan 18 18:26:38.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhfnz" in namespace "gc-3363"
    Jan 18 18:26:38.977: INFO: Deleting pod "simpletest-rc-to-be-deleted-dhw7p" in namespace "gc-3363"
    Jan 18 18:26:39.039: INFO: Deleting pod "simpletest-rc-to-be-deleted-djbkt" in namespace "gc-3363"
    Jan 18 18:26:39.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlzwf" in namespace "gc-3363"
    Jan 18 18:26:39.101: INFO: Deleting pod "simpletest-rc-to-be-deleted-f5rrn" in namespace "gc-3363"
    Jan 18 18:26:39.180: INFO: Deleting pod "simpletest-rc-to-be-deleted-fdt7b" in namespace "gc-3363"
    Jan 18 18:26:39.200: INFO: Deleting pod "simpletest-rc-to-be-deleted-fhvl9" in namespace "gc-3363"
    Jan 18 18:26:39.368: INFO: Deleting pod "simpletest-rc-to-be-deleted-fs7sj" in namespace "gc-3363"
    Jan 18 18:26:39.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft2nj" in namespace "gc-3363"
    Jan 18 18:26:39.666: INFO: Deleting pod "simpletest-rc-to-be-deleted-grb9w" in namespace "gc-3363"
    Jan 18 18:26:39.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-hc2g7" in namespace "gc-3363"
    Jan 18 18:26:39.969: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmq9h" in namespace "gc-3363"
    Jan 18 18:26:40.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp22s" in namespace "gc-3363"
    Jan 18 18:26:40.184: INFO: Deleting pod "simpletest-rc-to-be-deleted-j54gh" in namespace "gc-3363"
    Jan 18 18:26:40.263: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9jhz" in namespace "gc-3363"
    Jan 18 18:26:40.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdxn6" in namespace "gc-3363"
    Jan 18 18:26:40.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-kbf7k" in namespace "gc-3363"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 18 18:26:40.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-3363" for this suite. 01/18/23 18:26:40.383
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:40.396
Jan 18 18:26:40.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename limitrange 01/18/23 18:26:40.396
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:40.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:40.471
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/18/23 18:26:40.475
STEP: Setting up watch 01/18/23 18:26:40.475
STEP: Submitting a LimitRange 01/18/23 18:26:40.584
STEP: Verifying LimitRange creation was observed 01/18/23 18:26:40.593
STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 18:26:40.593
Jan 18 18:26:40.601: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 18:26:40.601: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/18/23 18:26:40.601
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 18:26:40.61
Jan 18 18:26:40.659: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 18 18:26:40.659: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/18/23 18:26:40.659
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 18:26:40.669
Jan 18 18:26:40.684: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 18 18:26:40.684: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/18/23 18:26:40.684
STEP: Failing to create a Pod with more than max resources 01/18/23 18:26:40.688
STEP: Updating a LimitRange 01/18/23 18:26:40.691
STEP: Verifying LimitRange updating is effective 01/18/23 18:26:40.699
STEP: Creating a Pod with less than former min resources 01/18/23 18:26:42.707
STEP: Failing to create a Pod with more than max resources 01/18/23 18:26:42.715
STEP: Deleting a LimitRange 01/18/23 18:26:42.719
STEP: Verifying the LimitRange was deleted 01/18/23 18:26:42.767
Jan 18 18:26:47.774: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/18/23 18:26:47.774
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 18 18:26:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8368" for this suite. 01/18/23 18:26:47.8
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":343,"skipped":6297,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.416 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:40.396
    Jan 18 18:26:40.396: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename limitrange 01/18/23 18:26:40.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:40.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:40.471
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/18/23 18:26:40.475
    STEP: Setting up watch 01/18/23 18:26:40.475
    STEP: Submitting a LimitRange 01/18/23 18:26:40.584
    STEP: Verifying LimitRange creation was observed 01/18/23 18:26:40.593
    STEP: Fetching the LimitRange to ensure it has proper values 01/18/23 18:26:40.593
    Jan 18 18:26:40.601: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 18:26:40.601: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/18/23 18:26:40.601
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/18/23 18:26:40.61
    Jan 18 18:26:40.659: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 18 18:26:40.659: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/18/23 18:26:40.659
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/18/23 18:26:40.669
    Jan 18 18:26:40.684: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 18 18:26:40.684: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/18/23 18:26:40.684
    STEP: Failing to create a Pod with more than max resources 01/18/23 18:26:40.688
    STEP: Updating a LimitRange 01/18/23 18:26:40.691
    STEP: Verifying LimitRange updating is effective 01/18/23 18:26:40.699
    STEP: Creating a Pod with less than former min resources 01/18/23 18:26:42.707
    STEP: Failing to create a Pod with more than max resources 01/18/23 18:26:42.715
    STEP: Deleting a LimitRange 01/18/23 18:26:42.719
    STEP: Verifying the LimitRange was deleted 01/18/23 18:26:42.767
    Jan 18 18:26:47.774: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/18/23 18:26:47.774
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 18 18:26:47.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-8368" for this suite. 01/18/23 18:26:47.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:26:47.813
Jan 18 18:26:47.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename statefulset 01/18/23 18:26:47.815
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:47.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:47.844
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2642 01/18/23 18:26:47.848
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/18/23 18:26:47.857
STEP: Creating pod with conflicting port in namespace statefulset-2642 01/18/23 18:26:47.869
STEP: Waiting until pod test-pod will start running in namespace statefulset-2642 01/18/23 18:26:47.887
Jan 18 18:26:47.887: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2642" to be "running"
Jan 18 18:26:47.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266557ms
Jan 18 18:26:49.900: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013264131s
Jan 18 18:26:49.900: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2642 01/18/23 18:26:49.901
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2642 01/18/23 18:26:49.912
Jan 18 18:26:49.934: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Pending. Waiting for statefulset controller to delete.
Jan 18 18:26:49.951: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 18:26:49.966: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Failed. Waiting for statefulset controller to delete.
Jan 18 18:26:49.971: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2642
STEP: Removing pod with conflicting port in namespace statefulset-2642 01/18/23 18:26:49.971
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2642 and will be in running state 01/18/23 18:26:49.99
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 18 18:26:52.007: INFO: Deleting all statefulset in ns statefulset-2642
Jan 18 18:26:52.015: INFO: Scaling statefulset ss to 0
Jan 18 18:27:02.049: INFO: Waiting for statefulset status.replicas updated to 0
Jan 18 18:27:02.056: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 18 18:27:02.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2642" for this suite. 01/18/23 18:27:02.096
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":344,"skipped":6302,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.293 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:26:47.813
    Jan 18 18:26:47.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename statefulset 01/18/23 18:26:47.815
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:26:47.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:26:47.844
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2642 01/18/23 18:26:47.848
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/18/23 18:26:47.857
    STEP: Creating pod with conflicting port in namespace statefulset-2642 01/18/23 18:26:47.869
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2642 01/18/23 18:26:47.887
    Jan 18 18:26:47.887: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2642" to be "running"
    Jan 18 18:26:47.892: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266557ms
    Jan 18 18:26:49.900: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013264131s
    Jan 18 18:26:49.900: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2642 01/18/23 18:26:49.901
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2642 01/18/23 18:26:49.912
    Jan 18 18:26:49.934: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 18 18:26:49.951: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 18:26:49.966: INFO: Observed stateful pod in namespace: statefulset-2642, name: ss-0, uid: 1b50823e-c3d0-4bb9-8edd-7f015b181ed9, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 18 18:26:49.971: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2642
    STEP: Removing pod with conflicting port in namespace statefulset-2642 01/18/23 18:26:49.971
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2642 and will be in running state 01/18/23 18:26:49.99
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 18 18:26:52.007: INFO: Deleting all statefulset in ns statefulset-2642
    Jan 18 18:26:52.015: INFO: Scaling statefulset ss to 0
    Jan 18 18:27:02.049: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 18 18:27:02.056: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 18 18:27:02.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2642" for this suite. 01/18/23 18:27:02.096
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:27:02.107
Jan 18 18:27:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename secrets 01/18/23 18:27:02.108
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:02.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:02.136
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 18 18:27:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9351" for this suite. 01/18/23 18:27:02.225
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":345,"skipped":6302,"failed":0}
------------------------------
â€¢ [0.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:27:02.107
    Jan 18 18:27:02.107: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename secrets 01/18/23 18:27:02.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:02.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:02.136
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 18 18:27:02.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9351" for this suite. 01/18/23 18:27:02.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:27:02.237
Jan 18 18:27:02.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 18:27:02.238
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:02.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:02.268
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/18/23 18:27:02.279
Jan 18 18:27:02.292: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-327" to be "running and ready"
Jan 18 18:27:02.298: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038562ms
Jan 18 18:27:02.298: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:27:04.305: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013131363s
Jan 18 18:27:04.305: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 18 18:27:04.305: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/18/23 18:27:04.312
Jan 18 18:27:04.322: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-327" to be "running and ready"
Jan 18 18:27:04.327: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332948ms
Jan 18 18:27:04.327: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 18 18:27:06.336: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013929919s
Jan 18 18:27:06.336: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 18 18:27:06.336: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/18/23 18:27:06.342
STEP: delete the pod with lifecycle hook 01/18/23 18:27:06.357
Jan 18 18:27:06.373: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 18:27:06.380: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 18:27:08.381: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 18:27:08.387: INFO: Pod pod-with-poststart-http-hook still exists
Jan 18 18:27:10.381: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 18 18:27:10.388: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 18 18:27:10.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-327" for this suite. 01/18/23 18:27:10.397
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":346,"skipped":6310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.171 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:27:02.237
    Jan 18 18:27:02.237: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/18/23 18:27:02.238
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:02.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:02.268
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/18/23 18:27:02.279
    Jan 18 18:27:02.292: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-327" to be "running and ready"
    Jan 18 18:27:02.298: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038562ms
    Jan 18 18:27:02.298: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:27:04.305: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013131363s
    Jan 18 18:27:04.305: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 18 18:27:04.305: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/18/23 18:27:04.312
    Jan 18 18:27:04.322: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-327" to be "running and ready"
    Jan 18 18:27:04.327: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.332948ms
    Jan 18 18:27:04.327: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 18 18:27:06.336: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.013929919s
    Jan 18 18:27:06.336: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 18 18:27:06.336: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/18/23 18:27:06.342
    STEP: delete the pod with lifecycle hook 01/18/23 18:27:06.357
    Jan 18 18:27:06.373: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 18:27:06.380: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 18:27:08.381: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 18:27:08.387: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 18 18:27:10.381: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 18 18:27:10.388: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 18 18:27:10.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-327" for this suite. 01/18/23 18:27:10.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:27:10.411
Jan 18 18:27:10.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:27:10.412
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:10.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:10.441
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:27:10.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2397" for this suite. 01/18/23 18:27:10.518
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":347,"skipped":6346,"failed":0}
------------------------------
â€¢ [0.119 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:27:10.411
    Jan 18 18:27:10.411: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:27:10.412
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:10.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:10.441
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:27:10.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2397" for this suite. 01/18/23 18:27:10.518
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:27:10.531
Jan 18 18:27:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename svcaccounts 01/18/23 18:27:10.532
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:10.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:10.56
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 18 18:27:10.589: INFO: created pod
Jan 18 18:27:10.589: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6629" to be "Succeeded or Failed"
Jan 18 18:27:10.598: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.656828ms
Jan 18 18:27:12.607: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018682769s
Jan 18 18:27:14.605: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016483333s
STEP: Saw pod success 01/18/23 18:27:14.605
Jan 18 18:27:14.605: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 18 18:27:44.606: INFO: polling logs
Jan 18 18:27:44.621: INFO: Pod logs: 
I0118 18:27:11.551853       1 log.go:195] OK: Got token
I0118 18:27:11.551911       1 log.go:195] validating with in-cluster discovery
I0118 18:27:11.552316       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0118 18:27:11.552363       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674067030, NotBefore:1674066430, IssuedAt:1674066430, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c13d9977-2884-4ee6-81e5-8c5815f97dfb"}}}
I0118 18:27:11.566760       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0118 18:27:11.575398       1 log.go:195] OK: Validated signature on JWT
I0118 18:27:11.575470       1 log.go:195] OK: Got valid claims from token!
I0118 18:27:11.575488       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674067030, NotBefore:1674066430, IssuedAt:1674066430, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c13d9977-2884-4ee6-81e5-8c5815f97dfb"}}}

Jan 18 18:27:44.621: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 18 18:27:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6629" for this suite. 01/18/23 18:27:44.639
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":348,"skipped":6348,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.128 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:27:10.531
    Jan 18 18:27:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename svcaccounts 01/18/23 18:27:10.532
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:10.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:10.56
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 18 18:27:10.589: INFO: created pod
    Jan 18 18:27:10.589: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-6629" to be "Succeeded or Failed"
    Jan 18 18:27:10.598: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.656828ms
    Jan 18 18:27:12.607: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018682769s
    Jan 18 18:27:14.605: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016483333s
    STEP: Saw pod success 01/18/23 18:27:14.605
    Jan 18 18:27:14.605: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 18 18:27:44.606: INFO: polling logs
    Jan 18 18:27:44.621: INFO: Pod logs: 
    I0118 18:27:11.551853       1 log.go:195] OK: Got token
    I0118 18:27:11.551911       1 log.go:195] validating with in-cluster discovery
    I0118 18:27:11.552316       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0118 18:27:11.552363       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674067030, NotBefore:1674066430, IssuedAt:1674066430, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c13d9977-2884-4ee6-81e5-8c5815f97dfb"}}}
    I0118 18:27:11.566760       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0118 18:27:11.575398       1 log.go:195] OK: Validated signature on JWT
    I0118 18:27:11.575470       1 log.go:195] OK: Got valid claims from token!
    I0118 18:27:11.575488       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-6629:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674067030, NotBefore:1674066430, IssuedAt:1674066430, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-6629", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c13d9977-2884-4ee6-81e5-8c5815f97dfb"}}}

    Jan 18 18:27:44.621: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 18 18:27:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6629" for this suite. 01/18/23 18:27:44.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:27:44.659
Jan 18 18:27:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 18:27:44.661
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:44.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:44.688
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 18:27:44.741: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 18:28:44.780: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/18/23 18:28:44.789
Jan 18 18:28:44.830: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 18:28:44.839: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 18:28:44.869: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 18:28:44.879: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 18:28:44.879
Jan 18 18:28:44.879: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8163" to be "running"
Jan 18 18:28:44.884: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704203ms
Jan 18 18:28:46.891: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011961218s
Jan 18 18:28:48.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014121324s
Jan 18 18:28:50.895: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015743165s
Jan 18 18:28:52.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01351754s
Jan 18 18:28:54.895: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.015071866s
Jan 18 18:28:54.895: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 18:28:54.895: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
Jan 18 18:28:54.901: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.458226ms
Jan 18 18:28:54.901: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 18:28:54.901: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
Jan 18 18:28:54.909: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.851815ms
Jan 18 18:28:54.909: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 18:28:54.909: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
Jan 18 18:28:54.918: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.927152ms
Jan 18 18:28:54.918: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 18:28:54.918
Jan 18 18:28:54.929: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8163" to be "running"
Jan 18 18:28:54.937: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.959045ms
Jan 18 18:28:56.943: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014607338s
Jan 18 18:28:58.951: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022131667s
Jan 18 18:28:58.951: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:28:58.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-8163" for this suite. 01/18/23 18:28:58.992
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":349,"skipped":6357,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.414 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:27:44.659
    Jan 18 18:27:44.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 18:27:44.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:27:44.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:27:44.688
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 18:27:44.741: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 18:28:44.780: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/18/23 18:28:44.789
    Jan 18 18:28:44.830: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 18:28:44.839: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 18:28:44.869: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 18:28:44.879: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 18:28:44.879
    Jan 18 18:28:44.879: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-8163" to be "running"
    Jan 18 18:28:44.884: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.704203ms
    Jan 18 18:28:46.891: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011961218s
    Jan 18 18:28:48.894: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014121324s
    Jan 18 18:28:50.895: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015743165s
    Jan 18 18:28:52.893: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01351754s
    Jan 18 18:28:54.895: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.015071866s
    Jan 18 18:28:54.895: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 18:28:54.895: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
    Jan 18 18:28:54.901: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.458226ms
    Jan 18 18:28:54.901: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 18:28:54.901: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
    Jan 18 18:28:54.909: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.851815ms
    Jan 18 18:28:54.909: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 18:28:54.909: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-8163" to be "running"
    Jan 18 18:28:54.918: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.927152ms
    Jan 18 18:28:54.918: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/18/23 18:28:54.918
    Jan 18 18:28:54.929: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-8163" to be "running"
    Jan 18 18:28:54.937: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.959045ms
    Jan 18 18:28:56.943: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014607338s
    Jan 18 18:28:58.951: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.022131667s
    Jan 18 18:28:58.951: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:28:58.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-8163" for this suite. 01/18/23 18:28:58.992
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:28:59.075
Jan 18 18:28:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename init-container 01/18/23 18:28:59.076
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:28:59.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:28:59.105
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/18/23 18:28:59.11
Jan 18 18:28:59.110: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 18:29:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2697" for this suite. 01/18/23 18:29:05.16
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":350,"skipped":6388,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.102 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:28:59.075
    Jan 18 18:28:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename init-container 01/18/23 18:28:59.076
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:28:59.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:28:59.105
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/18/23 18:28:59.11
    Jan 18 18:28:59.110: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 18:29:05.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2697" for this suite. 01/18/23 18:29:05.16
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:05.18
Jan 18 18:29:05.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename configmap 01/18/23 18:29:05.181
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:05.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:05.263
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-d4d2a83f-5ada-40d5-a89c-357b5f407207 01/18/23 18:29:05.268
STEP: Creating a pod to test consume configMaps 01/18/23 18:29:05.275
Jan 18 18:29:05.293: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6" in namespace "configmap-7477" to be "Succeeded or Failed"
Jan 18 18:29:05.300: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.683024ms
Jan 18 18:29:07.312: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019824534s
Jan 18 18:29:09.309: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016934368s
STEP: Saw pod success 01/18/23 18:29:09.31
Jan 18 18:29:09.310: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6" satisfied condition "Succeeded or Failed"
Jan 18 18:29:09.317: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 container configmap-volume-test: <nil>
STEP: delete the pod 01/18/23 18:29:09.333
Jan 18 18:29:09.350: INFO: Waiting for pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 to disappear
Jan 18 18:29:09.357: INFO: Pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 18 18:29:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7477" for this suite. 01/18/23 18:29:09.366
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":351,"skipped":6435,"failed":0}
------------------------------
â€¢ [4.199 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:05.18
    Jan 18 18:29:05.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename configmap 01/18/23 18:29:05.181
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:05.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:05.263
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-d4d2a83f-5ada-40d5-a89c-357b5f407207 01/18/23 18:29:05.268
    STEP: Creating a pod to test consume configMaps 01/18/23 18:29:05.275
    Jan 18 18:29:05.293: INFO: Waiting up to 5m0s for pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6" in namespace "configmap-7477" to be "Succeeded or Failed"
    Jan 18 18:29:05.300: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.683024ms
    Jan 18 18:29:07.312: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019824534s
    Jan 18 18:29:09.309: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016934368s
    STEP: Saw pod success 01/18/23 18:29:09.31
    Jan 18 18:29:09.310: INFO: Pod "pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6" satisfied condition "Succeeded or Failed"
    Jan 18 18:29:09.317: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 container configmap-volume-test: <nil>
    STEP: delete the pod 01/18/23 18:29:09.333
    Jan 18 18:29:09.350: INFO: Waiting for pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 to disappear
    Jan 18 18:29:09.357: INFO: Pod pod-configmaps-0ac6865a-8b63-4115-a645-70e3d95dcbb6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 18 18:29:09.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7477" for this suite. 01/18/23 18:29:09.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:09.382
Jan 18 18:29:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename subpath 01/18/23 18:29:09.383
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:09.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:09.413
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/18/23 18:29:09.417
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-94ld 01/18/23 18:29:09.437
STEP: Creating a pod to test atomic-volume-subpath 01/18/23 18:29:09.437
Jan 18 18:29:09.471: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-94ld" in namespace "subpath-9790" to be "Succeeded or Failed"
Jan 18 18:29:09.476: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230146ms
Jan 18 18:29:11.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 2.015008126s
Jan 18 18:29:13.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 4.01455451s
Jan 18 18:29:15.483: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 6.012273055s
Jan 18 18:29:17.488: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 8.017217291s
Jan 18 18:29:19.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 10.014710382s
Jan 18 18:29:21.487: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 12.015650128s
Jan 18 18:29:23.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 14.013044908s
Jan 18 18:29:25.488: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 16.016674463s
Jan 18 18:29:27.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 18.012817078s
Jan 18 18:29:29.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 20.014786052s
Jan 18 18:29:31.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=false. Elapsed: 22.012587732s
Jan 18 18:29:33.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013333453s
STEP: Saw pod success 01/18/23 18:29:33.484
Jan 18 18:29:33.485: INFO: Pod "pod-subpath-test-configmap-94ld" satisfied condition "Succeeded or Failed"
Jan 18 18:29:33.492: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-configmap-94ld container test-container-subpath-configmap-94ld: <nil>
STEP: delete the pod 01/18/23 18:29:33.511
Jan 18 18:29:33.531: INFO: Waiting for pod pod-subpath-test-configmap-94ld to disappear
Jan 18 18:29:33.538: INFO: Pod pod-subpath-test-configmap-94ld no longer exists
STEP: Deleting pod pod-subpath-test-configmap-94ld 01/18/23 18:29:33.538
Jan 18 18:29:33.538: INFO: Deleting pod "pod-subpath-test-configmap-94ld" in namespace "subpath-9790"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 18 18:29:33.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9790" for this suite. 01/18/23 18:29:33.551
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":352,"skipped":6483,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.179 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:09.382
    Jan 18 18:29:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename subpath 01/18/23 18:29:09.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:09.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:09.413
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/18/23 18:29:09.417
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-94ld 01/18/23 18:29:09.437
    STEP: Creating a pod to test atomic-volume-subpath 01/18/23 18:29:09.437
    Jan 18 18:29:09.471: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-94ld" in namespace "subpath-9790" to be "Succeeded or Failed"
    Jan 18 18:29:09.476: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Pending", Reason="", readiness=false. Elapsed: 5.230146ms
    Jan 18 18:29:11.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 2.015008126s
    Jan 18 18:29:13.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 4.01455451s
    Jan 18 18:29:15.483: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 6.012273055s
    Jan 18 18:29:17.488: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 8.017217291s
    Jan 18 18:29:19.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 10.014710382s
    Jan 18 18:29:21.487: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 12.015650128s
    Jan 18 18:29:23.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 14.013044908s
    Jan 18 18:29:25.488: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 16.016674463s
    Jan 18 18:29:27.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 18.012817078s
    Jan 18 18:29:29.486: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=true. Elapsed: 20.014786052s
    Jan 18 18:29:31.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Running", Reason="", readiness=false. Elapsed: 22.012587732s
    Jan 18 18:29:33.484: INFO: Pod "pod-subpath-test-configmap-94ld": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.013333453s
    STEP: Saw pod success 01/18/23 18:29:33.484
    Jan 18 18:29:33.485: INFO: Pod "pod-subpath-test-configmap-94ld" satisfied condition "Succeeded or Failed"
    Jan 18 18:29:33.492: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-subpath-test-configmap-94ld container test-container-subpath-configmap-94ld: <nil>
    STEP: delete the pod 01/18/23 18:29:33.511
    Jan 18 18:29:33.531: INFO: Waiting for pod pod-subpath-test-configmap-94ld to disappear
    Jan 18 18:29:33.538: INFO: Pod pod-subpath-test-configmap-94ld no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-94ld 01/18/23 18:29:33.538
    Jan 18 18:29:33.538: INFO: Deleting pod "pod-subpath-test-configmap-94ld" in namespace "subpath-9790"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 18 18:29:33.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9790" for this suite. 01/18/23 18:29:33.551
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:33.564
Jan 18 18:29:33.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sysctl 01/18/23 18:29:33.565
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:33.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:33.594
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 18:29:33.599
STEP: Watching for error events or started pod 01/18/23 18:29:33.614
STEP: Waiting for pod completion 01/18/23 18:29:35.623
Jan 18 18:29:35.623: INFO: Waiting up to 3m0s for pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf" in namespace "sysctl-2868" to be "completed"
Jan 18 18:29:35.630: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.14847ms
Jan 18 18:29:37.638: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015204713s
Jan 18 18:29:37.638: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/18/23 18:29:37.645
STEP: Getting logs from the pod 01/18/23 18:29:37.645
STEP: Checking that the sysctl is actually updated 01/18/23 18:29:37.66
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 18:29:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2868" for this suite. 01/18/23 18:29:37.668
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":353,"skipped":6514,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:33.564
    Jan 18 18:29:33.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sysctl 01/18/23 18:29:33.565
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:33.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:33.594
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/18/23 18:29:33.599
    STEP: Watching for error events or started pod 01/18/23 18:29:33.614
    STEP: Waiting for pod completion 01/18/23 18:29:35.623
    Jan 18 18:29:35.623: INFO: Waiting up to 3m0s for pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf" in namespace "sysctl-2868" to be "completed"
    Jan 18 18:29:35.630: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.14847ms
    Jan 18 18:29:37.638: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015204713s
    Jan 18 18:29:37.638: INFO: Pod "sysctl-69e1f7f2-1ae6-4add-8ec5-e281d75eb5bf" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/18/23 18:29:37.645
    STEP: Getting logs from the pod 01/18/23 18:29:37.645
    STEP: Checking that the sysctl is actually updated 01/18/23 18:29:37.66
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 18:29:37.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2868" for this suite. 01/18/23 18:29:37.668
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:37.682
Jan 18 18:29:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename resourcequota 01/18/23 18:29:37.684
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:37.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:37.711
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/18/23 18:29:37.716
STEP: Creating a ResourceQuota 01/18/23 18:29:42.723
STEP: Ensuring resource quota status is calculated 01/18/23 18:29:42.731
STEP: Creating a ReplicationController 01/18/23 18:29:44.74
STEP: Ensuring resource quota status captures replication controller creation 01/18/23 18:29:44.759
STEP: Deleting a ReplicationController 01/18/23 18:29:46.771
STEP: Ensuring resource quota status released usage 01/18/23 18:29:46.785
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 18 18:29:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7209" for this suite. 01/18/23 18:29:48.803
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":354,"skipped":6519,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.133 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:37.682
    Jan 18 18:29:37.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename resourcequota 01/18/23 18:29:37.684
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:37.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:37.711
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/18/23 18:29:37.716
    STEP: Creating a ResourceQuota 01/18/23 18:29:42.723
    STEP: Ensuring resource quota status is calculated 01/18/23 18:29:42.731
    STEP: Creating a ReplicationController 01/18/23 18:29:44.74
    STEP: Ensuring resource quota status captures replication controller creation 01/18/23 18:29:44.759
    STEP: Deleting a ReplicationController 01/18/23 18:29:46.771
    STEP: Ensuring resource quota status released usage 01/18/23 18:29:46.785
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 18 18:29:48.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7209" for this suite. 01/18/23 18:29:48.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:48.818
Jan 18 18:29:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename projected 01/18/23 18:29:48.819
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:48.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:48.85
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-fd0d3dc7-0367-4df5-868b-9a74bb7776c0 01/18/23 18:29:48.855
STEP: Creating a pod to test consume configMaps 01/18/23 18:29:48.865
Jan 18 18:29:48.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb" in namespace "projected-9419" to be "Succeeded or Failed"
Jan 18 18:29:48.893: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.913255ms
Jan 18 18:29:50.903: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018284388s
Jan 18 18:29:52.900: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014646903s
STEP: Saw pod success 01/18/23 18:29:52.9
Jan 18 18:29:52.900: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb" satisfied condition "Succeeded or Failed"
Jan 18 18:29:52.909: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb container agnhost-container: <nil>
STEP: delete the pod 01/18/23 18:29:52.932
Jan 18 18:29:52.950: INFO: Waiting for pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb to disappear
Jan 18 18:29:52.956: INFO: Pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 18 18:29:52.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9419" for this suite. 01/18/23 18:29:52.963
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":355,"skipped":6565,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:48.818
    Jan 18 18:29:48.818: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename projected 01/18/23 18:29:48.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:48.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:48.85
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-fd0d3dc7-0367-4df5-868b-9a74bb7776c0 01/18/23 18:29:48.855
    STEP: Creating a pod to test consume configMaps 01/18/23 18:29:48.865
    Jan 18 18:29:48.885: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb" in namespace "projected-9419" to be "Succeeded or Failed"
    Jan 18 18:29:48.893: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.913255ms
    Jan 18 18:29:50.903: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018284388s
    Jan 18 18:29:52.900: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014646903s
    STEP: Saw pod success 01/18/23 18:29:52.9
    Jan 18 18:29:52.900: INFO: Pod "pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb" satisfied condition "Succeeded or Failed"
    Jan 18 18:29:52.909: INFO: Trying to get logs from node scw-conformance125-default-61c39bbf4d81476a8e3 pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb container agnhost-container: <nil>
    STEP: delete the pod 01/18/23 18:29:52.932
    Jan 18 18:29:52.950: INFO: Waiting for pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb to disappear
    Jan 18 18:29:52.956: INFO: Pod pod-projected-configmaps-b7319403-a4c8-4ab1-8c59-849f8b3ecfcb no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 18 18:29:52.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9419" for this suite. 01/18/23 18:29:52.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:52.978
Jan 18 18:29:52.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename pods 01/18/23 18:29:52.979
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:53.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:53.005
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/18/23 18:29:53.01
STEP: submitting the pod to kubernetes 01/18/23 18:29:53.011
STEP: verifying QOS class is set on the pod 01/18/23 18:29:53.028
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 18 18:29:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1405" for this suite. 01/18/23 18:29:53.047
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":356,"skipped":6570,"failed":0}
------------------------------
â€¢ [0.083 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:52.978
    Jan 18 18:29:52.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename pods 01/18/23 18:29:52.979
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:53.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:53.005
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/18/23 18:29:53.01
    STEP: submitting the pod to kubernetes 01/18/23 18:29:53.011
    STEP: verifying QOS class is set on the pod 01/18/23 18:29:53.028
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 18 18:29:53.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1405" for this suite. 01/18/23 18:29:53.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:29:53.063
Jan 18 18:29:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename deployment 01/18/23 18:29:53.065
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:53.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:53.091
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan 18 18:29:53.111: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 18 18:29:58.118: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/18/23 18:29:58.118
Jan 18 18:29:58.118: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 18 18:30:00.127: INFO: Creating deployment "test-rollover-deployment"
Jan 18 18:30:00.143: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 18 18:30:02.156: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 18 18:30:02.170: INFO: Ensure that both replica sets have 1 created replica
Jan 18 18:30:02.182: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 18 18:30:02.201: INFO: Updating deployment test-rollover-deployment
Jan 18 18:30:02.201: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 18 18:30:04.216: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 18 18:30:04.235: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 18 18:30:04.248: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 18:30:04.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 18:30:06.262: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 18:30:06.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 18:30:08.264: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 18:30:08.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 18:30:10.265: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 18:30:10.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 18:30:12.266: INFO: all replica sets need to contain the pod-template-hash label
Jan 18 18:30:12.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 18 18:30:14.266: INFO: 
Jan 18 18:30:14.266: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 18 18:30:14.288: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-6881  58e414c6-c45d-4acf-ac98-0e5c9b6d501a 2632002090 2 2023-01-18 18:30:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d5c9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 18:30:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 18:30:13 +0000 UTC,LastTransitionTime:2023-01-18 18:30:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 18 18:30:14.294: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6881  6631e76c-0b11-462c-b855-4c91e57c82df 2632002083 2 2023-01-18 18:30:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc00152e217 0xc00152e218}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00152e2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:30:14.294: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 18 18:30:14.294: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6881  f0ff7cc0-b096-4e63-ba25-c533aadf1f5d 2632002089 2 2023-01-18 18:29:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc002c01e07 0xc002c01e08}] [] [{e2e.test Update apps/v1 2023-01-18 18:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c01ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:30:14.294: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6881  236e1ded-6f66-4943-a031-f9fed89452c6 2632001735 2 2023-01-18 18:30:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc002c01f47 0xc002c01f48}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00152e008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 18 18:30:14.301: INFO: Pod "test-rollover-deployment-6d45fd857b-rhnwq" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-rhnwq test-rollover-deployment-6d45fd857b- deployment-6881  da18db09-ac7f-4707-a3c0-2068cfe3130a 2632001762 0 2023-01-18 18:30:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:cb7964a9217745200943285dfeceec3c0b9847ca8b69b24e2fd74e4966cb7ec1 cni.projectcalico.org/podIP:10.100.145.145/32 cni.projectcalico.org/podIPs:10.100.145.145/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 6631e76c-0b11-462c-b855-4c91e57c82df 0xc00152e857 0xc00152e858}] [] [{calico Update v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6631e76c-0b11-462c-b855-4c91e57c82df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 18:30:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrkt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrkt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.145,StartTime:2023-01-18 18:30:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 18:30:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cc594fc35e20731fd00a5f82280c904b0a562a7a1f63c1af13da6050e6ea617b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 18 18:30:14.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6881" for this suite. 01/18/23 18:30:14.31
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":357,"skipped":6590,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.260 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:29:53.063
    Jan 18 18:29:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename deployment 01/18/23 18:29:53.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:29:53.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:29:53.091
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan 18 18:29:53.111: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 18 18:29:58.118: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/18/23 18:29:58.118
    Jan 18 18:29:58.118: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 18 18:30:00.127: INFO: Creating deployment "test-rollover-deployment"
    Jan 18 18:30:00.143: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 18 18:30:02.156: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 18 18:30:02.170: INFO: Ensure that both replica sets have 1 created replica
    Jan 18 18:30:02.182: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 18 18:30:02.201: INFO: Updating deployment test-rollover-deployment
    Jan 18 18:30:02.201: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 18 18:30:04.216: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 18 18:30:04.235: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 18 18:30:04.248: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 18:30:04.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 18:30:06.262: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 18:30:06.263: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 18:30:08.264: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 18:30:08.265: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 18:30:10.265: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 18:30:10.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 18:30:12.266: INFO: all replica sets need to contain the pod-template-hash label
    Jan 18 18:30:12.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 18, 18, 30, 3, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 18, 18, 30, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 18 18:30:14.266: INFO: 
    Jan 18 18:30:14.266: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 18 18:30:14.288: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6881  58e414c6-c45d-4acf-ac98-0e5c9b6d501a 2632002090 2 2023-01-18 18:30:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d5c9c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-18 18:30:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-18 18:30:13 +0000 UTC,LastTransitionTime:2023-01-18 18:30:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 18 18:30:14.294: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-6881  6631e76c-0b11-462c-b855-4c91e57c82df 2632002083 2 2023-01-18 18:30:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc00152e217 0xc00152e218}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00152e2d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:30:14.294: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 18 18:30:14.294: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6881  f0ff7cc0-b096-4e63-ba25-c533aadf1f5d 2632002089 2 2023-01-18 18:29:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc002c01e07 0xc002c01e08}] [] [{e2e.test Update apps/v1 2023-01-18 18:29:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002c01ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:30:14.294: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-6881  236e1ded-6f66-4943-a031-f9fed89452c6 2632001735 2 2023-01-18 18:30:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 58e414c6-c45d-4acf-ac98-0e5c9b6d501a 0xc002c01f47 0xc002c01f48}] [] [{kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58e414c6-c45d-4acf-ac98-0e5c9b6d501a\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00152e008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 18 18:30:14.301: INFO: Pod "test-rollover-deployment-6d45fd857b-rhnwq" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-rhnwq test-rollover-deployment-6d45fd857b- deployment-6881  da18db09-ac7f-4707-a3c0-2068cfe3130a 2632001762 0 2023-01-18 18:30:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:cb7964a9217745200943285dfeceec3c0b9847ca8b69b24e2fd74e4966cb7ec1 cni.projectcalico.org/podIP:10.100.145.145/32 cni.projectcalico.org/podIPs:10.100.145.145/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 6631e76c-0b11-462c-b855-4c91e57c82df 0xc00152e857 0xc00152e858}] [] [{calico Update v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-01-18 18:30:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6631e76c-0b11-462c-b855-4c91e57c82df\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-18 18:30:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.100.145.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rrkt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rrkt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:scw-conformance125-default-61c39bbf4d81476a8e3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-18 18:30:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.195.74.123,PodIP:10.100.145.145,StartTime:2023-01-18 18:30:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-18 18:30:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://cc594fc35e20731fd00a5f82280c904b0a562a7a1f63c1af13da6050e6ea617b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.100.145.145,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 18 18:30:14.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6881" for this suite. 01/18/23 18:30:14.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:30:14.324
Jan 18 18:30:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename webhook 01/18/23 18:30:14.325
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:30:14.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:30:14.353
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/18/23 18:30:14.38
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:30:14.662
STEP: Deploying the webhook pod 01/18/23 18:30:14.68
STEP: Wait for the deployment to be ready 01/18/23 18:30:14.698
Jan 18 18:30:14.711: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/18/23 18:30:16.738
STEP: Verifying the service has paired with the endpoint 01/18/23 18:30:16.758
Jan 18 18:30:17.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/18/23 18:30:17.882
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 18:30:18.168
STEP: Deleting the collection of validation webhooks 01/18/23 18:30:18.394
STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 18:30:18.514
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 18 18:30:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5923" for this suite. 01/18/23 18:30:18.552
STEP: Destroying namespace "webhook-5923-markers" for this suite. 01/18/23 18:30:18.564
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":358,"skipped":6595,"failed":0}
------------------------------
â€¢ [4.324 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:30:14.324
    Jan 18 18:30:14.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename webhook 01/18/23 18:30:14.325
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:30:14.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:30:14.353
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/18/23 18:30:14.38
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/18/23 18:30:14.662
    STEP: Deploying the webhook pod 01/18/23 18:30:14.68
    STEP: Wait for the deployment to be ready 01/18/23 18:30:14.698
    Jan 18 18:30:14.711: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/18/23 18:30:16.738
    STEP: Verifying the service has paired with the endpoint 01/18/23 18:30:16.758
    Jan 18 18:30:17.760: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/18/23 18:30:17.882
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 18:30:18.168
    STEP: Deleting the collection of validation webhooks 01/18/23 18:30:18.394
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/18/23 18:30:18.514
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 18 18:30:18.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5923" for this suite. 01/18/23 18:30:18.552
    STEP: Destroying namespace "webhook-5923-markers" for this suite. 01/18/23 18:30:18.564
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:30:18.651
Jan 18 18:30:18.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sched-preemption 01/18/23 18:30:18.652
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:30:18.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:30:18.685
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 18 18:30:18.711: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 18 18:31:18.745: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/18/23 18:31:18.752
Jan 18 18:31:18.796: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 18 18:31:18.810: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 18 18:31:18.859: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 18 18:31:18.871: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/18/23 18:31:18.871
Jan 18 18:31:18.872: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3619" to be "running"
Jan 18 18:31:18.877: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66419ms
Jan 18 18:31:20.887: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015747997s
Jan 18 18:31:20.887: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 18 18:31:20.887: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
Jan 18 18:31:20.897: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.314973ms
Jan 18 18:31:20.897: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 18:31:20.897: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
Jan 18 18:31:20.904: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.206706ms
Jan 18 18:31:20.904: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 18 18:31:20.904: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
Jan 18 18:31:20.911: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.629732ms
Jan 18 18:31:20.911: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 18:31:20.911
Jan 18 18:31:20.931: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 18 18:31:20.937: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138588ms
Jan 18 18:31:22.946: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014959574s
Jan 18 18:31:24.946: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015545967s
Jan 18 18:31:24.947: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 18 18:31:25.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-3619" for this suite. 01/18/23 18:31:25.022
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":359,"skipped":6628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [66.474 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:30:18.651
    Jan 18 18:30:18.651: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sched-preemption 01/18/23 18:30:18.652
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:30:18.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:30:18.685
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 18 18:30:18.711: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 18 18:31:18.745: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/18/23 18:31:18.752
    Jan 18 18:31:18.796: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 18 18:31:18.810: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 18 18:31:18.859: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 18 18:31:18.871: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/18/23 18:31:18.871
    Jan 18 18:31:18.872: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3619" to be "running"
    Jan 18 18:31:18.877: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.66419ms
    Jan 18 18:31:20.887: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.015747997s
    Jan 18 18:31:20.887: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 18 18:31:20.887: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
    Jan 18 18:31:20.897: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.314973ms
    Jan 18 18:31:20.897: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 18:31:20.897: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
    Jan 18 18:31:20.904: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.206706ms
    Jan 18 18:31:20.904: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 18 18:31:20.904: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3619" to be "running"
    Jan 18 18:31:20.911: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.629732ms
    Jan 18 18:31:20.911: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/18/23 18:31:20.911
    Jan 18 18:31:20.931: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 18 18:31:20.937: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.138588ms
    Jan 18 18:31:22.946: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014959574s
    Jan 18 18:31:24.946: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.015545967s
    Jan 18 18:31:24.947: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 18 18:31:25.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-3619" for this suite. 01/18/23 18:31:25.022
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:31:25.128
Jan 18 18:31:25.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename replication-controller 01/18/23 18:31:25.129
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:25.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:25.157
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/18/23 18:31:25.161
STEP: When the matched label of one of its pods change 01/18/23 18:31:25.17
Jan 18 18:31:25.176: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 18 18:31:30.186: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/18/23 18:31:30.204
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 18 18:31:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1892" for this suite. 01/18/23 18:31:31.227
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":360,"skipped":6649,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.136 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:31:25.128
    Jan 18 18:31:25.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename replication-controller 01/18/23 18:31:25.129
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:25.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:25.157
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/18/23 18:31:25.161
    STEP: When the matched label of one of its pods change 01/18/23 18:31:25.17
    Jan 18 18:31:25.176: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 18 18:31:30.186: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/18/23 18:31:30.204
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 18 18:31:31.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1892" for this suite. 01/18/23 18:31:31.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:31:31.265
Jan 18 18:31:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename sysctl 01/18/23 18:31:31.266
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:31.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:31.317
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 18:31:31.322
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 18 18:31:31.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-317" for this suite. 01/18/23 18:31:31.338
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":361,"skipped":6665,"failed":0}
------------------------------
â€¢ [0.105 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:31:31.265
    Jan 18 18:31:31.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename sysctl 01/18/23 18:31:31.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:31.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:31.317
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/18/23 18:31:31.322
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 18 18:31:31.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-317" for this suite. 01/18/23 18:31:31.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/18/23 18:31:31.373
Jan 18 18:31:31.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
STEP: Building a namespace api object, basename dns 01/18/23 18:31:31.374
STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:31.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:31.397
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8348.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8348.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/18/23 18:31:31.401
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8348.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8348.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/18/23 18:31:31.401
STEP: creating a pod to probe /etc/hosts 01/18/23 18:31:31.401
STEP: submitting the pod to kubernetes 01/18/23 18:31:31.401
Jan 18 18:31:31.469: INFO: Waiting up to 15m0s for pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e" in namespace "dns-8348" to be "running"
Jan 18 18:31:31.475: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.625053ms
Jan 18 18:31:33.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01643682s
Jan 18 18:31:35.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015910089s
Jan 18 18:31:35.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e" satisfied condition "running"
STEP: retrieving the pod 01/18/23 18:31:35.485
STEP: looking for the results for each expected name from probers 01/18/23 18:31:35.518
Jan 18 18:31:35.557: INFO: DNS probes using dns-8348/dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e succeeded

STEP: deleting the pod 01/18/23 18:31:35.557
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 18 18:31:35.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8348" for this suite. 01/18/23 18:31:35.585
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":362,"skipped":6696,"failed":0}
------------------------------
â€¢ [4.226 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/18/23 18:31:31.373
    Jan 18 18:31:31.373: INFO: >>> kubeConfig: /tmp/kubeconfig-1714167000
    STEP: Building a namespace api object, basename dns 01/18/23 18:31:31.374
    STEP: Waiting for a default service account to be provisioned in namespace 01/18/23 18:31:31.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/18/23 18:31:31.397
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8348.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8348.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/18/23 18:31:31.401
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8348.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8348.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/18/23 18:31:31.401
    STEP: creating a pod to probe /etc/hosts 01/18/23 18:31:31.401
    STEP: submitting the pod to kubernetes 01/18/23 18:31:31.401
    Jan 18 18:31:31.469: INFO: Waiting up to 15m0s for pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e" in namespace "dns-8348" to be "running"
    Jan 18 18:31:31.475: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.625053ms
    Jan 18 18:31:33.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01643682s
    Jan 18 18:31:35.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e": Phase="Running", Reason="", readiness=true. Elapsed: 4.015910089s
    Jan 18 18:31:35.485: INFO: Pod "dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e" satisfied condition "running"
    STEP: retrieving the pod 01/18/23 18:31:35.485
    STEP: looking for the results for each expected name from probers 01/18/23 18:31:35.518
    Jan 18 18:31:35.557: INFO: DNS probes using dns-8348/dns-test-039cade8-13e6-47bb-ba64-523ae1c4334e succeeded

    STEP: deleting the pod 01/18/23 18:31:35.557
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 18 18:31:35.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8348" for this suite. 01/18/23 18:31:35.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6704,"failed":0}
Jan 18 18:31:35.600: INFO: Running AfterSuite actions on all nodes
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 18 18:31:35.600: INFO: Running AfterSuite actions on node 1
Jan 18 18:31:35.600: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 18:31:35.600: INFO: Running AfterSuite actions on all nodes
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 18 18:31:35.600: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 18 18:31:35.600: INFO: Running AfterSuite actions on node 1
    Jan 18 18:31:35.600: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.085 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7066 Specs in 5592.701 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6704 Skipped
PASS

Ginkgo ran 1 suite in 1h33m13.018706424s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

